{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M odern D eep N etwork Toolkits for pyTor c h (MDNC) \u00b6 This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.2 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document. Overview \u00b6 The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools. Current progress \u00b6 Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model . summary () in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0% Compatibility test \u00b6 Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Overview"},{"location":"#modern-deep-network-toolkits-for-pytorch-mdnc","text":"This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.2 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document.","title":"Modern Deep Network Toolkits for pyTorch (MDNC)"},{"location":"#overview","text":"The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools.","title":"Overview"},{"location":"#current-progress","text":"Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model . summary () in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0%","title":"Current progress"},{"location":"#compatibility-test","text":"Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Compatibility test"},{"location":"installation/","text":"Installation \u00b6 Use the nightly version on Github \u00b6 Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by from mdnc import mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive Install the package \u00b6 Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#use-the-nightly-version-on-github","text":"Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by from mdnc import mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive","title":"Use the nightly version on Github"},{"location":"installation/#install-the-package","text":"Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Install the package"},{"location":"licenses/","text":"Licenses \u00b6 License of MDNC \u00b6 MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of pytorch-summary \u00b6 The third-party module, torchsummary used in MDNC ( sksq96/pytorch-summary ), grants: MIT License https://github.com/sksq96/pytorch-summary/blob/master/LICENSE License of MkDocs-Material Theme \u00b6 The theme of this website ( squidfunk/mkdocs-material ) grants: MIT License https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE License of VSCode-Codeicons \u00b6 The Codeicons pack ( microsoft/vscode-codicons ) used in this website grants: MIT License for all source codes https://github.com/microsoft/vscode-codicons/blob/main/LICENSE-CODE CC BY 4.0 License for all materials https://github.com/microsoft/vscode-codicons/blob/main/LICENSE","title":"Licenses"},{"location":"licenses/#licenses","text":"","title":"Licenses"},{"location":"licenses/#license-of-mdnc","text":"MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License of MDNC"},{"location":"licenses/#license-of-pytorch-summary","text":"The third-party module, torchsummary used in MDNC ( sksq96/pytorch-summary ), grants: MIT License https://github.com/sksq96/pytorch-summary/blob/master/LICENSE","title":"License of pytorch-summary"},{"location":"licenses/#license-of-mkdocs-material-theme","text":"The theme of this website ( squidfunk/mkdocs-material ) grants: MIT License https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE","title":"License of MkDocs-Material Theme"},{"location":"licenses/#license-of-vscode-codeicons","text":"The Codeicons pack ( microsoft/vscode-codicons ) used in this website grants: MIT License for all source codes https://github.com/microsoft/vscode-codicons/blob/main/LICENSE-CODE CC BY 4.0 License for all materials https://github.com/microsoft/vscode-codicons/blob/main/LICENSE","title":"License of VSCode-Codeicons"},{"location":"apis/overview/","text":"Overview \u00b6 The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup sequence(sequence) subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B; List of packages \u00b6 optimizers \u00b6 To be built ... modules \u00b6 To be documented ... models \u00b6 To be built ... data \u00b6 sequence : The infrastructures of CPU-based parallel I/O and processing. This module is used by all data loaders. h5py : Wrapped HDF5 datasets savers, data converters and data loaders. preprocs : Useful pre- and post- processing tools for all data loaders in this package. webtools : Web tools for downloading tarball-packed datasets from Github. funcs \u00b6 To be built ... utils \u00b6 To be documented ... contribs \u00b6 torchsummary : The revised sksq96/pytorch-summary . This is a Keras style model . summary () in pyTorch, with some bugs gotten fixed. To view my modified version, see sksq96/pytorch-summary!165 .","title":"Overview"},{"location":"apis/overview/#overview","text":"The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup sequence(sequence) subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B;","title":"Overview"},{"location":"apis/overview/#list-of-packages","text":"","title":"List of packages"},{"location":"apis/overview/#optimizers","text":"To be built ...","title":" optimizers"},{"location":"apis/overview/#modules","text":"To be documented ...","title":" modules"},{"location":"apis/overview/#models","text":"To be built ...","title":" models"},{"location":"apis/overview/#data","text":"sequence : The infrastructures of CPU-based parallel I/O and processing. This module is used by all data loaders. h5py : Wrapped HDF5 datasets savers, data converters and data loaders. preprocs : Useful pre- and post- processing tools for all data loaders in this package. webtools : Web tools for downloading tarball-packed datasets from Github.","title":" data"},{"location":"apis/overview/#funcs","text":"To be built ...","title":" funcs"},{"location":"apis/overview/#utils","text":"To be documented ...","title":" utils"},{"location":"apis/overview/#contribs","text":"torchsummary : The revised sksq96/pytorch-summary . This is a Keras style model . summary () in pyTorch, with some bugs gotten fixed. To view my modified version, see sksq96/pytorch-summary!165 .","title":" contribs"},{"location":"apis/contribs/torchsummary/summary/","text":"contribs.torchsummary.summary \u00b6 Function \u00b7 Source params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"<span class='magic-codeicon-function'>summary</span>"},{"location":"apis/contribs/torchsummary/summary/#contribstorchsummarysummary","text":"Function \u00b7 Source params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/contribs/torchsummary/summary_string/","text":"contribs.torchsummary.summary \u00b6 Function \u00b7 Source summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str The summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Examples \u00b6 See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"<span class='magic-codeicon-function'>summary_string</span>"},{"location":"apis/contribs/torchsummary/summary_string/#contribstorchsummarysummary","text":"Function \u00b7 Source summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary_string/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str The summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary_string/#examples","text":"See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"Examples"},{"location":"apis/data/h5py/H5CParser/","text":"data.h5py.H5CParser \u00b6 Class \u00b7 Source dset = mdnc . data . h5py . H5CParser ( file_name , keywords_sequence , keywords_single , batch_size = 32 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and parse it by mdnc.data.sequence.MPSequence . The realization could be described as: This parser is the upgraded version of mdnc.data.h5py.H5GParser , it is specially designed for parsing data to LSTM/ConvLSTM. A sequence dimension would be inserted between batches and channels . In each batch, the sequence is continuously extracted in the order of the batches. During each epoch, a sliding window would iterate the first axis (samples). The number of batches would be the same as using mdnc.data.h5py.H5GParser . For each variable specified by keywords_sequence , each sample in the mini-batch is a sequence. This parser could also read the dataset converted by mdnc.data.h5py.H5SeqConverter . The workflow is shown in the following figure: Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords_sequence ( str , ) The keyword of sequence data. The keywords in this list would be parsed as (B, S, C1, C2, ...) , where B and S are the sample number and sequence length (given by the argument sequence_size ) respectively. It should be a list of keywords (or a single keyword). keyword_single ( str , ) The keyword of single values. The keywords in this list would be parsed as (B, C1, C2, ...) , where B is the sample number. It should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. sequence_size int The size of each sequence. It represents S of (B, S, C1, C2, ...) . sequence_position int The aligned position between the single values and the sequence values. It should be in the range of >= 0 and < sequence_size . sequence_padding int The padding method for each epoch, it will influence the first or the final samples in the dataset. Could be 'same' , 'zero' or 'none' . If set None , the number of batches of each epoch would be a little bit smaller than the actual number. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip At least one keyword requires to be given in keywords_sequence or keyword_single . In some cases, we need to use both kinds of keywords. For example, the input could be a sequence, and the label may be a scalar. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5CParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. sequence_size \u00b6 dset . sequence_size The length of each sequence. This value is given by the argument sequence_size during the initialization. sequence_position \u00b6 dset . sequence_position The alignment between keywords_sequence and keyword_single . This value is given by the argument sequence_position during the initialization. sequence_padding \u00b6 dset . sequence_position The padding method of each sequence. This value is given by the argument sequence_padding during the initialization. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position = 0 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 1 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Example 2 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser_seq' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 10 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:,:], d2 . shape , d3 )","title":"<span class='magic-codeicon-class'>H5CParser</span>"},{"location":"apis/data/h5py/H5CParser/#datah5pyh5cparser","text":"Class \u00b7 Source dset = mdnc . data . h5py . H5CParser ( file_name , keywords_sequence , keywords_single , batch_size = 32 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and parse it by mdnc.data.sequence.MPSequence . The realization could be described as: This parser is the upgraded version of mdnc.data.h5py.H5GParser , it is specially designed for parsing data to LSTM/ConvLSTM. A sequence dimension would be inserted between batches and channels . In each batch, the sequence is continuously extracted in the order of the batches. During each epoch, a sliding window would iterate the first axis (samples). The number of batches would be the same as using mdnc.data.h5py.H5GParser . For each variable specified by keywords_sequence , each sample in the mini-batch is a sequence. This parser could also read the dataset converted by mdnc.data.h5py.H5SeqConverter . The workflow is shown in the following figure:","title":"data.h5py.H5CParser"},{"location":"apis/data/h5py/H5CParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords_sequence ( str , ) The keyword of sequence data. The keywords in this list would be parsed as (B, S, C1, C2, ...) , where B and S are the sample number and sequence length (given by the argument sequence_size ) respectively. It should be a list of keywords (or a single keyword). keyword_single ( str , ) The keyword of single values. The keywords in this list would be parsed as (B, C1, C2, ...) , where B is the sample number. It should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. sequence_size int The size of each sequence. It represents S of (B, S, C1, C2, ...) . sequence_position int The aligned position between the single values and the sequence values. It should be in the range of >= 0 and < sequence_size . sequence_padding int The padding method for each epoch, it will influence the first or the final samples in the dataset. Could be 'same' , 'zero' or 'none' . If set None , the number of batches of each epoch would be a little bit smaller than the actual number. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip At least one keyword requires to be given in keywords_sequence or keyword_single . In some cases, we need to use both kinds of keywords. For example, the input could be a sequence, and the label may be a scalar. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5CParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5CParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5CParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5CParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5CParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5CParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5CParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5CParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5CParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5CParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5CParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5CParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5CParser/#batch_size","text":"dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/h5py/H5CParser/#sequence_size","text":"dset . sequence_size The length of each sequence. This value is given by the argument sequence_size during the initialization.","title":" sequence_size"},{"location":"apis/data/h5py/H5CParser/#sequence_position","text":"dset . sequence_position The alignment between keywords_sequence and keyword_single . This value is given by the argument sequence_position during the initialization.","title":" sequence_position"},{"location":"apis/data/h5py/H5CParser/#sequence_padding","text":"dset . sequence_position The padding method of each sequence. This value is given by the argument sequence_padding during the initialization.","title":" sequence_padding"},{"location":"apis/data/h5py/H5CParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5CParser/#examples","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position = 0 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 1 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Example 2 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser_seq' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 10 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:,:], d2 . shape , d3 )","title":"Examples"},{"location":"apis/data/h5py/H5Converter/","text":"data.h5py.H5Converter \u00b6 Class \u00b7 Source converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Arguments \u00b6 Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () ) Methods \u00b6 convert \u00b6 converter . convert () Perform the data conversion. Examples \u00b6 Example Codes 1 2 3 4 5 6 import mdnc cvt_o = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. cvt_i = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset.","title":"<span class='magic-codeicon-class'>H5Converter</span>"},{"location":"apis/data/h5py/H5Converter/#datah5pyh5converter","text":"Class \u00b7 Source converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets .","title":"data.h5py.H5Converter"},{"location":"apis/data/h5py/H5Converter/#arguments","text":"Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () )","title":"Arguments"},{"location":"apis/data/h5py/H5Converter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5Converter/#convert","text":"converter . convert () Perform the data conversion.","title":" convert"},{"location":"apis/data/h5py/H5Converter/#examples","text":"Example Codes 1 2 3 4 5 6 import mdnc cvt_o = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. cvt_i = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset.","title":"Examples"},{"location":"apis/data/h5py/H5GParser/","text":"data.h5py.H5GParser \u00b6 Class \u00b7 Source dset = mdnc . data . h5py . H5GParser ( file_name , keywords , batch_size = 32 , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) Grouply parsing dataset. This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file indexer, this indexer would be initialized by sequence.MPSequence . It would use the user defined keywords to get a group of h5py . Dataset s. Estimate the h5py . Dataset sizes, each dataset should share the same size (but could have different shapes). Use the dataset size to create a sequence.MPSequence , and allows it to randomly shuffle the indices in each epoch. Invoke the sequence.MPSequence APIs to serve the parallel dataset parsing. Certainly, you could use this parser to load a single h5py . Dataset . To find details about the parallel parsing workflow, please check mdnc.data.sequence.MPSequence . Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Exampless \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = None ) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 2, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = mdnc . preprocs . ProcScaler ()) with dset . start () as p : for i , ( d_one , d_two ) in enumerate ( p ): d_one , d_two = d_one . cpu () . numpy (), d_two . cpu () . numpy () std_one , std_two = np . std ( d_one ), np . std ( d_two ) d_one , d_two = p . preproc . postprocess ( d_one , d_two ) std_one_ , std_two_ = np . std ( d_one ), np . std ( d_two ) print ( 'Before: {0} , {1} ; After: {0} , {1} .' . format ( std_one , std_two , std_one_ , std_two_ ))","title":"<span class='magic-codeicon-class'>H5GParser</span>"},{"location":"apis/data/h5py/H5GParser/#datah5pyh5gparser","text":"Class \u00b7 Source dset = mdnc . data . h5py . H5GParser ( file_name , keywords , batch_size = 32 , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) Grouply parsing dataset. This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file indexer, this indexer would be initialized by sequence.MPSequence . It would use the user defined keywords to get a group of h5py . Dataset s. Estimate the h5py . Dataset sizes, each dataset should share the same size (but could have different shapes). Use the dataset size to create a sequence.MPSequence , and allows it to randomly shuffle the indices in each epoch. Invoke the sequence.MPSequence APIs to serve the parallel dataset parsing. Certainly, you could use this parser to load a single h5py . Dataset . To find details about the parallel parsing workflow, please check mdnc.data.sequence.MPSequence .","title":"data.h5py.H5GParser"},{"location":"apis/data/h5py/H5GParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5GParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5GParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5GParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5GParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5GParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5GParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5GParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5GParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5GParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5GParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5GParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5GParser/#batch_size","text":"dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/h5py/H5GParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5GParser/#exampless","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = None ) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 2, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = mdnc . preprocs . ProcScaler ()) with dset . start () as p : for i , ( d_one , d_two ) in enumerate ( p ): d_one , d_two = d_one . cpu () . numpy (), d_two . cpu () . numpy () std_one , std_two = np . std ( d_one ), np . std ( d_two ) d_one , d_two = p . preproc . postprocess ( d_one , d_two ) std_one_ , std_two_ = np . std ( d_one ), np . std ( d_two ) print ( 'Before: {0} , {1} ; After: {0} , {1} .' . format ( std_one , std_two , std_one_ , std_two_ ))","title":"Exampless"},{"location":"apis/data/h5py/H5RParser/","text":"data.h5py.H5RParser \u00b6 Class \u00b7 Source dset = mdnc . data . h5py . H5RParser ( file_name , keywords , preprocfunc , batch_num = 100 , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file handle. Using the user defined keywords to get a group of datasets. Check the dataset size, and register the dataset list. For each batch, the data is randomly picked from the whole set. The h5py.Dataset variable would be transparent in the preprocfunc , i.e. the method how to pick up the random samples need to be implemented by users. Certainly, you could use this parser to load a single dataset. Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. This function is required because the random sampling needs to be implemented here. batch_num int Number of mini-batches in each epoch. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The preprocfunc is required in this case. The provided pre-processors in data.preprocs should not be used directly, because users need to implment their own random sampling pre-processor first. For example, Example Without data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] mdnc . data . h5py . H5RParser ( ... , preprocfunc = ProcCustom (), keywords = [ 'x_1' , 'x_2' ]) Use data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x mdnc . data . h5py . H5RParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape )","title":"<span class='magic-codeicon-class'>H5RParser</span>"},{"location":"apis/data/h5py/H5RParser/#datah5pyh5rparser","text":"Class \u00b7 Source dset = mdnc . data . h5py . H5RParser ( file_name , keywords , preprocfunc , batch_num = 100 , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file handle. Using the user defined keywords to get a group of datasets. Check the dataset size, and register the dataset list. For each batch, the data is randomly picked from the whole set. The h5py.Dataset variable would be transparent in the preprocfunc , i.e. the method how to pick up the random samples need to be implemented by users. Certainly, you could use this parser to load a single dataset.","title":"data.h5py.H5RParser"},{"location":"apis/data/h5py/H5RParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. This function is required because the random sampling needs to be implemented here. batch_num int Number of mini-batches in each epoch. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The preprocfunc is required in this case. The provided pre-processors in data.preprocs should not be used directly, because users need to implment their own random sampling pre-processor first. For example, Example Without data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] mdnc . data . h5py . H5RParser ( ... , preprocfunc = ProcCustom (), keywords = [ 'x_1' , 'x_2' ]) Use data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x mdnc . data . h5py . H5RParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5RParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5RParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5RParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5RParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5RParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5RParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5RParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5RParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5RParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5RParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5RParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5RParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5RParser/#examples","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape )","title":"Examples"},{"location":"apis/data/h5py/H5SeqConverter/","text":"data.h5py.H5SeqConverter \u00b6 Class \u00b7 Source converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group . Arguments \u00b6 Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . Methods \u00b6 config \u00b6 converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. convert \u00b6 converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. copy \u00b6 converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. open \u00b6 converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . close \u00b6 converter . close () Close the converter. Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc with mdnc . data . h5py . H5SeqConverter ( 'test_seqconverter.h5' ) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( 'test_seqconverter.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( 'test_seqconverter2.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' )","title":"<span class='magic-codeicon-class'>H5SeqConverter</span>"},{"location":"apis/data/h5py/H5SeqConverter/#datah5pyh5seqconverter","text":"Class \u00b7 Source converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group .","title":"data.h5py.H5SeqConverter"},{"location":"apis/data/h5py/H5SeqConverter/#arguments","text":"Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":"Arguments"},{"location":"apis/data/h5py/H5SeqConverter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SeqConverter/#config","text":"converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":" config"},{"location":"apis/data/h5py/H5SeqConverter/#convert","text":"converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" convert"},{"location":"apis/data/h5py/H5SeqConverter/#copy","text":"converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" copy"},{"location":"apis/data/h5py/H5SeqConverter/#open","text":"converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":" open"},{"location":"apis/data/h5py/H5SeqConverter/#close","text":"converter . close () Close the converter.","title":" close"},{"location":"apis/data/h5py/H5SeqConverter/#examples","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc with mdnc . data . h5py . H5SeqConverter ( 'test_seqconverter.h5' ) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( 'test_seqconverter.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( 'test_seqconverter2.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' )","title":"Examples"},{"location":"apis/data/h5py/H5SupSaver/","text":"data.h5py.H5SupSaver \u00b6 Class \u00b7 Source saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of h5py . Group and h5py . Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py . Softlink , h5py . Attributes and h5py . VirtualDataSet . Add context nesting supports for h5py . Group . This would makes the codes more elegant. Arguments \u00b6 Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. Methods \u00b6 config \u00b6 saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. get_config \u00b6 cfg = saver . get_config ( name = None ) Get the current configuration value by the given name . Requries Argument Type Description name str The name of the required config value. Returns Argument Description cfg The required config value. open \u00b6 saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value. close \u00b6 saver . close () Close the saver. dump \u00b6 saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. set_link \u00b6 saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset . set_attrs \u00b6 saver . set_attrs ( keyword , attrs = None , ** kwargs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () . set_virtual_set \u00b6 saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset. Properties \u00b6 attrs \u00b6 attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 7 import numpy as np import mdnc with mdnc . data . h5py . H5SupSaver ( 'test_h5supsaver.h5' , enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( 'test_h5supsaver.h5' ) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open ))","title":"<span class='magic-codeicon-class'>H5SupSaver</span>"},{"location":"apis/data/h5py/H5SupSaver/#datah5pyh5supsaver","text":"Class \u00b7 Source saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of h5py . Group and h5py . Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py . Softlink , h5py . Attributes and h5py . VirtualDataSet . Add context nesting supports for h5py . Group . This would makes the codes more elegant.","title":"data.h5py.H5SupSaver"},{"location":"apis/data/h5py/H5SupSaver/#arguments","text":"Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file.","title":"Arguments"},{"location":"apis/data/h5py/H5SupSaver/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SupSaver/#config","text":"saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":" config"},{"location":"apis/data/h5py/H5SupSaver/#get_config","text":"cfg = saver . get_config ( name = None ) Get the current configuration value by the given name . Requries Argument Type Description name str The name of the required config value. Returns Argument Description cfg The required config value.","title":" get_config"},{"location":"apis/data/h5py/H5SupSaver/#open","text":"saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value.","title":" open"},{"location":"apis/data/h5py/H5SupSaver/#close","text":"saver . close () Close the saver.","title":" close"},{"location":"apis/data/h5py/H5SupSaver/#dump","text":"saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" dump"},{"location":"apis/data/h5py/H5SupSaver/#set_link","text":"saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset .","title":" set_link"},{"location":"apis/data/h5py/H5SupSaver/#set_attrs","text":"saver . set_attrs ( keyword , attrs = None , ** kwargs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () .","title":" set_attrs"},{"location":"apis/data/h5py/H5SupSaver/#set_virtual_set","text":"saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset.","title":" set_virtual_set"},{"location":"apis/data/h5py/H5SupSaver/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5SupSaver/#attrs","text":"attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager .","title":" attrs"},{"location":"apis/data/h5py/H5SupSaver/#examples","text":"Example 1 Codes 1 2 3 4 5 6 7 import numpy as np import mdnc with mdnc . data . h5py . H5SupSaver ( 'test_h5supsaver.h5' , enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( 'test_h5supsaver.h5' ) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open ))","title":"Examples"},{"location":"apis/data/preprocs/ProcAbstract/","text":"data.preprocs.ProcAbstract \u00b6 Abstract Class \u00b7 Source proc = mdnc . data . preprocs . ProcAbstract ( inds = None , parent = None , _disable_inds = False ) The basic processor class supporting cascading and variable-level broadcasting: Cascading: It means the derived class of this abstract class will support using such a method ProcDerived ( parent = ProcDerived ( ... )) to create composition of processors. Variable level broadcasting: It means when _disable_inds = False the user-implemented methods, for example, def preprocess ( x ) , would be broadcasted to arbitrary number of input variables, like proc.preprocess(x1, x2, ...) . Info This is an abstract class, which means you could not create an instance of this class by codes like this proc = ProcAbstract ( ... ) The correct way to use this class it to implement a derived class from this class. The intertage has 2 requirements: The __init__ method of this class need to be called inside the __init__ method of the derived class. The preprocess() and postprocess() methods need to be implemented. We recommend to expose the argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation. Arguments \u00b6 Requries Argument Type Description inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . _disable_inds bool A flag used by developers. If set True , the broadcasting would not be used. It means that the user-implemented arguments would be exactly the arguments to be used. Warning The argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation. Abstract Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1: with inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . a = a def preprocess ( self , x ): # The input is an np.ndarray return self . a * x def postprocess ( self , x ): # The inverse operator return x / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) proc2 = ProcDerived ( a = 2.0 , inds = [ 1 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Example 2: without inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * x , self . a * y , self . a * z def postprocess ( self , x , y , z ): # The inverse operator return x / self . a , y / self . a , z / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) In the above two examples, the processor would multiply the inputs by 2.0 . The first implementation allows users to use the argument inds to determine which variables require to be processed. The user-implemented methods in the second example would fully control the input and output arguments. Actually, the second implementation allows user to change the number of output arguments, for example: Example 3: out args changed Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * np . mean (( x , y , z ), axis = 0 ) def postprocess ( self , x_m ): # The inverse operator x = x_m / self . a return x_m , x_m , x_m proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 5 , 2 ]), np . zeros ([ 5 , 2 ]) xm = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( xm ) print ( 'Processed shape:' , xm . shape ) print ( 'Processed error:' , np . amax ( np . abs ( xm - 2 * np . mean ([ x , y , z ], axis = 0 ))) print ( 'Inverse error:' , np . amax ( np . abs ( xm - xr )), np . amax ( np . abs ( xm - yr )), np . amax ( np . abs ( xm - zr ))) This operation is not invertible. We could find that the inverse error would be greater than 0 . All derived classes of this class could be cascaded with each other. See the tutorial for checking more examples.","title":"<span class='magic-codeicon-class'>ProcAbstract</span>"},{"location":"apis/data/preprocs/ProcAbstract/#datapreprocsprocabstract","text":"Abstract Class \u00b7 Source proc = mdnc . data . preprocs . ProcAbstract ( inds = None , parent = None , _disable_inds = False ) The basic processor class supporting cascading and variable-level broadcasting: Cascading: It means the derived class of this abstract class will support using such a method ProcDerived ( parent = ProcDerived ( ... )) to create composition of processors. Variable level broadcasting: It means when _disable_inds = False the user-implemented methods, for example, def preprocess ( x ) , would be broadcasted to arbitrary number of input variables, like proc.preprocess(x1, x2, ...) . Info This is an abstract class, which means you could not create an instance of this class by codes like this proc = ProcAbstract ( ... ) The correct way to use this class it to implement a derived class from this class. The intertage has 2 requirements: The __init__ method of this class need to be called inside the __init__ method of the derived class. The preprocess() and postprocess() methods need to be implemented. We recommend to expose the argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation.","title":"data.preprocs.ProcAbstract"},{"location":"apis/data/preprocs/ProcAbstract/#arguments","text":"Requries Argument Type Description inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . _disable_inds bool A flag used by developers. If set True , the broadcasting would not be used. It means that the user-implemented arguments would be exactly the arguments to be used. Warning The argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation.","title":"Arguments"},{"location":"apis/data/preprocs/ProcAbstract/#abstract-methods","text":"","title":"Abstract Methods"},{"location":"apis/data/preprocs/ProcAbstract/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcAbstract/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcAbstract/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcAbstract/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcAbstract/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcAbstract/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1: with inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . a = a def preprocess ( self , x ): # The input is an np.ndarray return self . a * x def postprocess ( self , x ): # The inverse operator return x / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) proc2 = ProcDerived ( a = 2.0 , inds = [ 1 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Example 2: without inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * x , self . a * y , self . a * z def postprocess ( self , x , y , z ): # The inverse operator return x / self . a , y / self . a , z / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) In the above two examples, the processor would multiply the inputs by 2.0 . The first implementation allows users to use the argument inds to determine which variables require to be processed. The user-implemented methods in the second example would fully control the input and output arguments. Actually, the second implementation allows user to change the number of output arguments, for example: Example 3: out args changed Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * np . mean (( x , y , z ), axis = 0 ) def postprocess ( self , x_m ): # The inverse operator x = x_m / self . a return x_m , x_m , x_m proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 5 , 2 ]), np . zeros ([ 5 , 2 ]) xm = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( xm ) print ( 'Processed shape:' , xm . shape ) print ( 'Processed error:' , np . amax ( np . abs ( xm - 2 * np . mean ([ x , y , z ], axis = 0 ))) print ( 'Inverse error:' , np . amax ( np . abs ( xm - xr )), np . amax ( np . abs ( xm - yr )), np . amax ( np . abs ( xm - zr ))) This operation is not invertible. We could find that the inverse error would be greater than 0 . All derived classes of this class could be cascaded with each other. See the tutorial for checking more examples.","title":"Examples"},{"location":"apis/data/preprocs/ProcFilter1d/","text":"data.preprocs.ProcFilter1d \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D IIR band-pass filters (also supports low-pass or high-pass filter). The IIR filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcFilter1d ( axis = 1 , parent = ProcFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing. Arguments \u00b6 Requries Argument Type Description axis int The axis where we apply the 1D filter. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40 Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , filter_type = 'butter' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"<span class='magic-codeicon-class'>ProcFilter1d</span>"},{"location":"apis/data/preprocs/ProcFilter1d/#datapreprocsprocfilter1d","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D IIR band-pass filters (also supports low-pass or high-pass filter). The IIR filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcFilter1d ( axis = 1 , parent = ProcFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing.","title":"data.preprocs.ProcFilter1d"},{"location":"apis/data/preprocs/ProcFilter1d/#arguments","text":"Requries Argument Type Description axis int The axis where we apply the 1D filter. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40","title":"Arguments"},{"location":"apis/data/preprocs/ProcFilter1d/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcFilter1d/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcFilter1d/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcFilter1d/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcFilter1d/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcFilter1d/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcFilter1d/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , filter_type = 'butter' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"Examples"},{"location":"apis/data/preprocs/ProcLifter/","text":"data.preprocs.ProcLifter \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcLifter ( a , inds = None , parent = None ) This is a homogeneous processor. It use the parameter a to perform such an invertible transform: \\mathbf{y}_n = \\mathrm{sign} (\\mathbf{x}_n) * \\log (1 + a * |\\mathbf{x}_n|) This transform could strengthen the low-amplitude parts of the signal, because the data is transformed into the log domain. Arguments \u00b6 Requries Argument Type Description a float The parameter used for log-lifting the data. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the log-lifting. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the lifting. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 a \u00b6 proc . a The lifting parameter \\(a\\) . parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( - 2 * np . pi , 2 * np . pi , 200 ) proc = mdnc . data . preprocs . ProcLifter ( a = 10.0 ) x = np . cos ( np . pi * t ) + 0.5 * np . cos ( 1.5 * np . pi * t + 0.1 ) + 0.2 * np . cos ( 2.5 * np . pi * t + 0.3 ) + 0.1 * np . cos ( 3.5 * np . pi * t + 0.7 ) x_ = proc . preprocess ( x ) xr = proc . postprocess ( x_ ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t , x_ ) axs [ 1 ] . plot ( t , xr ) axs [ 2 ] . plot ( t , x ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"<span class='magic-codeicon-class'>ProcLifter</span>"},{"location":"apis/data/preprocs/ProcLifter/#datapreprocsproclifter","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcLifter ( a , inds = None , parent = None ) This is a homogeneous processor. It use the parameter a to perform such an invertible transform: \\mathbf{y}_n = \\mathrm{sign} (\\mathbf{x}_n) * \\log (1 + a * |\\mathbf{x}_n|) This transform could strengthen the low-amplitude parts of the signal, because the data is transformed into the log domain.","title":"data.preprocs.ProcLifter"},{"location":"apis/data/preprocs/ProcLifter/#arguments","text":"Requries Argument Type Description a float The parameter used for log-lifting the data. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcLifter/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcLifter/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the log-lifting. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcLifter/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the lifting. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcLifter/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcLifter/#a","text":"proc . a The lifting parameter \\(a\\) .","title":" a"},{"location":"apis/data/preprocs/ProcLifter/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcLifter/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcLifter/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( - 2 * np . pi , 2 * np . pi , 200 ) proc = mdnc . data . preprocs . ProcLifter ( a = 10.0 ) x = np . cos ( np . pi * t ) + 0.5 * np . cos ( 1.5 * np . pi * t + 0.1 ) + 0.2 * np . cos ( 2.5 * np . pi * t + 0.3 ) + 0.1 * np . cos ( 3.5 * np . pi * t + 0.7 ) x_ = proc . preprocess ( x ) xr = proc . postprocess ( x_ ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t , x_ ) axs [ 1 ] . plot ( t , xr ) axs [ 2 ] . plot ( t , x ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"Examples"},{"location":"apis/data/preprocs/ProcMerge/","text":"data.preprocs.ProcMerge \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcMerge ( procs = None , num_procs = None , parent = None ) Merge manager. This processor is inhomogeneous, and designed for merging different processors by a more efficient way. For example, p = ProcMerge ([ Proc1 ( ... ), Proc2 ( ... )]) Would apply Proc1 to the first argument, and Proc2 to the second argument. It is equivalent to p = Proc1 ( ... , inds = 0 , parent = Proc2 ( ... , inds = 1 )) This class should not be used if any sub-processor does not return the results with the same number of the input variables (out-arg changed). One exception is, the parent of this class could be an out-arg changed processor. This API is more intuitive for users to concatenate serveral processors together. It will make your codes more readable and reduce the stack level of the processors. Arguments \u00b6 Requries Argument Type Description procs ( ProcAbstract , ) A sequence of processors. Each processor is derived from mdnc.data.preprocs.ProcAbstract . Could be used for initializing this merge processor. num_procs object The number of input arguments of this processor. If not set, would infer the number from the length of the argument procs . At least one of procs or num_procs needs to be specified. The two arguments could be specified together. parent ProcAbstract An instance derived from mdnc.data.preprocs.ProcAbstract . This instance would be used as the parent of the current instance. Warning The argument num_procs should be greater than procs , if both num_procs and procs are specified. Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Operators \u00b6 __getitem__ () \u00b6 proc_i = proc [ idx ] Get the i th sub-processor. Warning If one sub-processor is managing multiple indicies, the returned sub-processor would always be same for those indicies. For example, Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) proc_1 = proc [ 1 ] proc_2 = proc [ 2 ] print ( proc_m is proc_1 , proc_m is proc_2 ) This behavior is important if your proc_m is an inhomogeneous processor. It means although you get proc_2 by proc [ 2 ] , you still need to place your argument as the 2 nd input when using proc_2 . Requries Argument Type Description idx int The index of the sub-processor. Returns Argument Description proc_i An instance derived from ProcAbstract , the i th sub-processor. __setitem__ () \u00b6 proc [ idx ] = proc_i Info This method supports multiple assignment, for example: Codes proc = ProcMerge ( num_procs = 3 ) proc [:] = Proc1 ( ... ) proc [ 1 : 2 ] = Proc2 ( ... ) This would be equivalent to Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) Requries Argument Type Description idx int or slice or tuple The indicies that would be overwritten by the argument proc_i . proc_i ProcAbstract An instance derived from ProcAbstract , this sub-processor would be used for overriding one or more indicies. Properties \u00b6 num_procs \u00b6 proc . num_procs The number of sub-processors for this class. If one sub-processor is used for managing multiple indicies, it will be count for mutiple times. parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured. In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Certianly, it will always be proc . has_ind = True for this class. Examples \u00b6 There are many kinds of method for using this class. For example, Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcMerge ([ mdnc . data . preprocs . ProcScaler (), mdnc . data . preprocs . ProcNSTScaler ()]) random_rng = np . random . default_rng x , y = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import numpy as np import mdnc proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( inds = 0 , parent = mdnc . data . preprocs . ProcScaler ( inds = 1 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) This class could be also used for merge customized processor. But the customized processor should ensure the input and output numbers are the same, for example, Example 3 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y ): return self . a * x , ( 2 * self . a ) * y def postprocess ( self , x , y ): return x / self . a , y / ( 2 * self . a ) proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( parent = mdnc . data . preprocs . ProcDerived ( a = 2.0 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr )))","title":"<span class='magic-codeicon-class'>ProcMerge</span>"},{"location":"apis/data/preprocs/ProcMerge/#datapreprocsprocmerge","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcMerge ( procs = None , num_procs = None , parent = None ) Merge manager. This processor is inhomogeneous, and designed for merging different processors by a more efficient way. For example, p = ProcMerge ([ Proc1 ( ... ), Proc2 ( ... )]) Would apply Proc1 to the first argument, and Proc2 to the second argument. It is equivalent to p = Proc1 ( ... , inds = 0 , parent = Proc2 ( ... , inds = 1 )) This class should not be used if any sub-processor does not return the results with the same number of the input variables (out-arg changed). One exception is, the parent of this class could be an out-arg changed processor. This API is more intuitive for users to concatenate serveral processors together. It will make your codes more readable and reduce the stack level of the processors.","title":"data.preprocs.ProcMerge"},{"location":"apis/data/preprocs/ProcMerge/#arguments","text":"Requries Argument Type Description procs ( ProcAbstract , ) A sequence of processors. Each processor is derived from mdnc.data.preprocs.ProcAbstract . Could be used for initializing this merge processor. num_procs object The number of input arguments of this processor. If not set, would infer the number from the length of the argument procs . At least one of procs or num_procs needs to be specified. The two arguments could be specified together. parent ProcAbstract An instance derived from mdnc.data.preprocs.ProcAbstract . This instance would be used as the parent of the current instance. Warning The argument num_procs should be greater than procs , if both num_procs and procs are specified.","title":"Arguments"},{"location":"apis/data/preprocs/ProcMerge/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcMerge/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcMerge/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcMerge/#operators","text":"","title":"Operators"},{"location":"apis/data/preprocs/ProcMerge/#__getitem__","text":"proc_i = proc [ idx ] Get the i th sub-processor. Warning If one sub-processor is managing multiple indicies, the returned sub-processor would always be same for those indicies. For example, Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) proc_1 = proc [ 1 ] proc_2 = proc [ 2 ] print ( proc_m is proc_1 , proc_m is proc_2 ) This behavior is important if your proc_m is an inhomogeneous processor. It means although you get proc_2 by proc [ 2 ] , you still need to place your argument as the 2 nd input when using proc_2 . Requries Argument Type Description idx int The index of the sub-processor. Returns Argument Description proc_i An instance derived from ProcAbstract , the i th sub-processor.","title":" __getitem__()"},{"location":"apis/data/preprocs/ProcMerge/#__setitem__","text":"proc [ idx ] = proc_i Info This method supports multiple assignment, for example: Codes proc = ProcMerge ( num_procs = 3 ) proc [:] = Proc1 ( ... ) proc [ 1 : 2 ] = Proc2 ( ... ) This would be equivalent to Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) Requries Argument Type Description idx int or slice or tuple The indicies that would be overwritten by the argument proc_i . proc_i ProcAbstract An instance derived from ProcAbstract , this sub-processor would be used for overriding one or more indicies.","title":" __setitem__()"},{"location":"apis/data/preprocs/ProcMerge/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcMerge/#num_procs","text":"proc . num_procs The number of sub-processors for this class. If one sub-processor is used for managing multiple indicies, it will be count for mutiple times.","title":" num_procs"},{"location":"apis/data/preprocs/ProcMerge/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcMerge/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured. In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Certianly, it will always be proc . has_ind = True for this class.","title":" has_ind"},{"location":"apis/data/preprocs/ProcMerge/#examples","text":"There are many kinds of method for using this class. For example, Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcMerge ([ mdnc . data . preprocs . ProcScaler (), mdnc . data . preprocs . ProcNSTScaler ()]) random_rng = np . random . default_rng x , y = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import numpy as np import mdnc proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( inds = 0 , parent = mdnc . data . preprocs . ProcScaler ( inds = 1 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) This class could be also used for merge customized processor. But the customized processor should ensure the input and output numbers are the same, for example, Example 3 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y ): return self . a * x , ( 2 * self . a ) * y def postprocess ( self , x , y ): return x / self . a , y / ( 2 * self . a ) proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( parent = mdnc . data . preprocs . ProcDerived ( a = 2.0 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr )))","title":"Examples"},{"location":"apis/data/preprocs/ProcNSTFilter1d/","text":"data.preprocs.ProcNSTFilter1d \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , patch_length = 128 , patch_step = 64 , kaiser_coef = 1.0 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D non-stationary IIR (or FIR) band-pass filters (also supports low-pass or high-pass filter). The non-stationary filter is implemented by a sliding window. The overlapped windows would slide along the chosen axis, and the filter would be performed inside each window. After all windows filtered, they are stitched together. The filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcNSTFilter1d ( axis = 1 , parent = ProcNSTFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing. Arguments \u00b6 Requries Argument Type Description axis int The axis where we apply the 1D filter. length int The length of the to be processed data (along the chosen axis). patch_length int The length of the sliding windows. patch_step int The step between two sliding windows. This values should be smaller than patch_length , to make the windows overlapped. kaiser_coef float The coefficent of the Kaiser window taping function for each window. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description fft Use fft and sharp cut-off frequencies to perform the filter. This is an FIR filter. The filter_args would not be applied with this filter type. butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40 Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , filter_type = 'fft' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"<span class='magic-codeicon-class'>ProcNSTFilter1d</span>"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#datapreprocsprocnstfilter1d","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , patch_length = 128 , patch_step = 64 , kaiser_coef = 1.0 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D non-stationary IIR (or FIR) band-pass filters (also supports low-pass or high-pass filter). The non-stationary filter is implemented by a sliding window. The overlapped windows would slide along the chosen axis, and the filter would be performed inside each window. After all windows filtered, they are stitched together. The filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcNSTFilter1d ( axis = 1 , parent = ProcNSTFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing.","title":"data.preprocs.ProcNSTFilter1d"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#arguments","text":"Requries Argument Type Description axis int The axis where we apply the 1D filter. length int The length of the to be processed data (along the chosen axis). patch_length int The length of the sliding windows. patch_step int The step between two sliding windows. This values should be smaller than patch_length , to make the windows overlapped. kaiser_coef float The coefficent of the Kaiser window taping function for each window. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description fft Use fft and sharp cut-off frequencies to perform the filter. This is an FIR filter. The filter_args would not be applied with this filter type. butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40","title":"Arguments"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , filter_type = 'fft' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"Examples"},{"location":"apis/data/preprocs/ProcNSTScaler/","text":"data.preprocs.ProcNSTScaler \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTScaler ( dim = 2 , kernel_length = 9 , epsilon = 1e-6 , inds = None , parent = None ) This is a homogeneous processor. It would remove the lower-frequency part of the data (smoothed data), and use this part for normalization. The normalizer could be formulated as: \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{y}_n &= \\frac{\\mathbf{x}_n - \\boldsymbol{\\mu}_n}{\\boldsymbol{\\min(\\sigma}_n, \\varepsilon)}, \\\\ \\boldsymbol{\\mu}_n &= \\mathrm{Avg. pool}(\\mathbf{x}_n, L), \\\\ \\boldsymbol{\\sigma}_n &= \\mathrm{Max. pool}(\\mathbf{x}_n - \\boldsymbol{\\mu}_n, L). \\end{aligned} \\right. \\end{equation} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. The value \\(L\\) is the smoothing window length, i.e. kernel_length . The value \\(\\varepsilon\\) determines the lower bound of the divisor during the scaling. It is recommended to make kernel_length large enough especially when the data is very noisy. Arguments \u00b6 Requries Argument Type Description dim int The dimension of the input data, this value would also determine the dimension of the sliding window. kernel_length int or ( int , ) The length of the sliding window. Could provide a window shape by using a sequence. epsilon The lower bound of the divisor used for scaling. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the non-stationary re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the non-stationary scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 dim \u00b6 proc . dim The dimension of the input data. kernel_length \u00b6 proc . kernel_length The length of the sliding window when calculating the low-frequnecy shifting value and scaling value. epsilon \u00b6 proc . epsilon A value used as the lower bound of the divisor. This value is set small enough in most cases. parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , kernel_length = 9 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 100 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 200 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed max:' , np . amax ( x_ ), np . amax ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )))","title":"<span class='magic-codeicon-class'>ProcNSTScaler</span>"},{"location":"apis/data/preprocs/ProcNSTScaler/#datapreprocsprocnstscaler","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTScaler ( dim = 2 , kernel_length = 9 , epsilon = 1e-6 , inds = None , parent = None ) This is a homogeneous processor. It would remove the lower-frequency part of the data (smoothed data), and use this part for normalization. The normalizer could be formulated as: \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{y}_n &= \\frac{\\mathbf{x}_n - \\boldsymbol{\\mu}_n}{\\boldsymbol{\\min(\\sigma}_n, \\varepsilon)}, \\\\ \\boldsymbol{\\mu}_n &= \\mathrm{Avg. pool}(\\mathbf{x}_n, L), \\\\ \\boldsymbol{\\sigma}_n &= \\mathrm{Max. pool}(\\mathbf{x}_n - \\boldsymbol{\\mu}_n, L). \\end{aligned} \\right. \\end{equation} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. The value \\(L\\) is the smoothing window length, i.e. kernel_length . The value \\(\\varepsilon\\) determines the lower bound of the divisor during the scaling. It is recommended to make kernel_length large enough especially when the data is very noisy.","title":"data.preprocs.ProcNSTScaler"},{"location":"apis/data/preprocs/ProcNSTScaler/#arguments","text":"Requries Argument Type Description dim int The dimension of the input data, this value would also determine the dimension of the sliding window. kernel_length int or ( int , ) The length of the sliding window. Could provide a window shape by using a sequence. epsilon The lower bound of the divisor used for scaling. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcNSTScaler/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcNSTScaler/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the non-stationary re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcNSTScaler/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the non-stationary scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcNSTScaler/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcNSTScaler/#dim","text":"proc . dim The dimension of the input data.","title":" dim"},{"location":"apis/data/preprocs/ProcNSTScaler/#kernel_length","text":"proc . kernel_length The length of the sliding window when calculating the low-frequnecy shifting value and scaling value.","title":" kernel_length"},{"location":"apis/data/preprocs/ProcNSTScaler/#epsilon","text":"proc . epsilon A value used as the lower bound of the divisor. This value is set small enough in most cases.","title":" epsilon"},{"location":"apis/data/preprocs/ProcNSTScaler/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcNSTScaler/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcNSTScaler/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , kernel_length = 9 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 100 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 200 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed max:' , np . amax ( x_ ), np . amax ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )))","title":"Examples"},{"location":"apis/data/preprocs/ProcPad/","text":"data.preprocs.ProcPad \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcPad ( pad_width , inds = None , parent = None , ** kwargs ) This is a homogeneous processor. Use np.pad to pad the data. This processor supports all np.pad options. Besides, this processor also support cropping. If any element in the argument pad_width is negative, would perform cropping on that axis. For example: p = ProcPad ( pad_width = (( 5 , - 5 ),)) y = p ( x ) # x.shape=(20,), y.shape=(20,) In this case, the data is padded by 5 samples at the beginning, but cropped 5 samples at the end. This operator is not invertible when cropping is applied. The postprocess would try to revert the padding / cropping configurations to match the input data. Warning If cropping is used, this processor would be not invertible (unless we have the argument mode = 'wrap' ). The postprocess() method would try to pad the cropped part with the processed data. Arguments \u00b6 Requries Argument Type Description pad_width int or ( int , int ) (( int , int ), ... ) The padding_width argument of the np.pad function. If any element is negative, it means this elment is a cropping size. This argument only supports 3 cases: width : use the same padding / cropping width along all axes. ( begin , end ) : use the same padding / cropping length for both edges along all axes. (( begin , end ), ... ) : use different padding / cropping lengths for both edges along each axis. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the padding / cropping. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator is not invertible if cropping is used in preprocess() . In this case, the cropped part would be padded by processed data (y, ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 pad_width \u00b6 proc . pad_width The padding width of the processor. parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 )), mode = 'wrap' ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show () Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 ), ( - 10 , 10 )), mode = 'constant' , constant_values = 0.0 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 1 , ncols = 3 , sharex = True , sharey = True , figsize = ( 12 , 4 )) im1 = axs [ 0 ] . imshow ( t [ 2 ]) im2 = axs [ 1 ] . imshow ( t_b [ 0 ]) im3 = axs [ 2 ] . imshow ( data [ 0 ]) fig . colorbar ( im1 , ax = axs [ 0 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im2 , ax = axs [ 1 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im3 , ax = axs [ 2 ], pad = 0.1 , orientation = 'horizontal' ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"<span class='magic-codeicon-class'>ProcPad</span>"},{"location":"apis/data/preprocs/ProcPad/#datapreprocsprocpad","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcPad ( pad_width , inds = None , parent = None , ** kwargs ) This is a homogeneous processor. Use np.pad to pad the data. This processor supports all np.pad options. Besides, this processor also support cropping. If any element in the argument pad_width is negative, would perform cropping on that axis. For example: p = ProcPad ( pad_width = (( 5 , - 5 ),)) y = p ( x ) # x.shape=(20,), y.shape=(20,) In this case, the data is padded by 5 samples at the beginning, but cropped 5 samples at the end. This operator is not invertible when cropping is applied. The postprocess would try to revert the padding / cropping configurations to match the input data. Warning If cropping is used, this processor would be not invertible (unless we have the argument mode = 'wrap' ). The postprocess() method would try to pad the cropped part with the processed data.","title":"data.preprocs.ProcPad"},{"location":"apis/data/preprocs/ProcPad/#arguments","text":"Requries Argument Type Description pad_width int or ( int , int ) (( int , int ), ... ) The padding_width argument of the np.pad function. If any element is negative, it means this elment is a cropping size. This argument only supports 3 cases: width : use the same padding / cropping width along all axes. ( begin , end ) : use the same padding / cropping length for both edges along all axes. (( begin , end ), ... ) : use different padding / cropping lengths for both edges along each axis. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcPad/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcPad/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the padding / cropping. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcPad/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator is not invertible if cropping is used in preprocess() . In this case, the cropped part would be padded by processed data (y, ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcPad/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcPad/#pad_width","text":"proc . pad_width The padding width of the processor.","title":" pad_width"},{"location":"apis/data/preprocs/ProcPad/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcPad/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcPad/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 )), mode = 'wrap' ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show () Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 ), ( - 10 , 10 )), mode = 'constant' , constant_values = 0.0 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) fig , axs = plt . subplots ( nrows = 1 , ncols = 3 , sharex = True , sharey = True , figsize = ( 12 , 4 )) im1 = axs [ 0 ] . imshow ( t [ 2 ]) im2 = axs [ 1 ] . imshow ( t_b [ 0 ]) im3 = axs [ 2 ] . imshow ( data [ 0 ]) fig . colorbar ( im1 , ax = axs [ 0 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im2 , ax = axs [ 1 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im3 , ax = axs [ 2 ], pad = 0.1 , orientation = 'horizontal' ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show ()","title":"Examples"},{"location":"apis/data/preprocs/ProcScaler/","text":"data.preprocs.ProcScaler \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcScaler ( shift = None , scale = None , axis =- 1 , inds = None , parent = None ) This is a homogeneous processor. It accepts two variables shift ( \\(\\mu\\) ), scale ( \\(\\sigma\\) ) to perform the following normalization: \\begin{align} \\mathbf{y}_n = \\frac{1}{\\sigma} ( \\mathbf{x}_n - \\mu ), \\end{align} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. If not setting \\(\\mu\\) , would use the mean value of the input mini-batch to shift the argument, i.e. \\(\\mu_n = \\bar{\\mathbf{x}_n}\\) ; If not setting \\(\\sigma\\) , would use the max-abs value of the input mini-batch to scale the argument, i.e. \\(\\sigma_n = \\max |\\mathbf{x}_n - \\mu_n|\\) . The above two caulation is estimated on mini-batches. This configuration may cause unstable issues when the input mini-batches are not i.i.d.. Therefore, we recommend users to always set shift and scale manually. Arguments \u00b6 Requries Argument Type Description shift int or np . ndarray The \\(\\mu\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would shift the mean value of each mini-batch to 0. scale int or np . ndarray The \\(\\sigma\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would scale the max-abs value of each mini-batch to 1. axis int or ( int , ) The axis used for calculating the normalization parameters. If given a sequence, would calculate the paramters among higher-dimensional data. Only used when shift or scale is not None . inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 shift \u00b6 proc . shift The shifting value \\(\\mu\\) . scale \u00b6 proc . scale The scaling value \\(\\sigma\\) . parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcScaler ( shift = 1.0 , scale = 3.0 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 4 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 5 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed std:' , np . std ( x_ ), np . std ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from sklearn import preprocessing import mdnc random_rng = np . random . default_rng () x = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 100 , 5 ]) rsc = preprocessing . RobustScaler () rsc . fit ( x ) proc = mdnc . data . preprocs . ProcScaler ( shift = np . expand_dims ( rsc . center_ , axis = 0 ), scale = np . expand_dims ( rsc . scale_ , axis = 0 )) x_ = proc . preprocess ( x ) x_sl = rsc . transform ( x ) x_r = proc . postprocess ( x_ ) x_r_sl = rsc . inverse_transform ( x_sl ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x_sl ))) print ( 'Inverse error:' , np . amax ( np . abs ( x_r - x_r_sl )))","title":"<span class='magic-codeicon-class'>ProcScaler</span>"},{"location":"apis/data/preprocs/ProcScaler/#datapreprocsprocscaler","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcScaler ( shift = None , scale = None , axis =- 1 , inds = None , parent = None ) This is a homogeneous processor. It accepts two variables shift ( \\(\\mu\\) ), scale ( \\(\\sigma\\) ) to perform the following normalization: \\begin{align} \\mathbf{y}_n = \\frac{1}{\\sigma} ( \\mathbf{x}_n - \\mu ), \\end{align} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. If not setting \\(\\mu\\) , would use the mean value of the input mini-batch to shift the argument, i.e. \\(\\mu_n = \\bar{\\mathbf{x}_n}\\) ; If not setting \\(\\sigma\\) , would use the max-abs value of the input mini-batch to scale the argument, i.e. \\(\\sigma_n = \\max |\\mathbf{x}_n - \\mu_n|\\) . The above two caulation is estimated on mini-batches. This configuration may cause unstable issues when the input mini-batches are not i.i.d.. Therefore, we recommend users to always set shift and scale manually.","title":"data.preprocs.ProcScaler"},{"location":"apis/data/preprocs/ProcScaler/#arguments","text":"Requries Argument Type Description shift int or np . ndarray The \\(\\mu\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would shift the mean value of each mini-batch to 0. scale int or np . ndarray The \\(\\sigma\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would scale the max-abs value of each mini-batch to 1. axis int or ( int , ) The axis used for calculating the normalization parameters. If given a sequence, would calculate the paramters among higher-dimensional data. Only used when shift or scale is not None . inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcScaler/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcScaler/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcScaler/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcScaler/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcScaler/#shift","text":"proc . shift The shifting value \\(\\mu\\) .","title":" shift"},{"location":"apis/data/preprocs/ProcScaler/#scale","text":"proc . scale The scaling value \\(\\sigma\\) .","title":" scale"},{"location":"apis/data/preprocs/ProcScaler/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcScaler/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcScaler/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcScaler ( shift = 1.0 , scale = 3.0 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 4 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 5 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed std:' , np . std ( x_ ), np . std ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import numpy as np from sklearn import preprocessing import mdnc random_rng = np . random . default_rng () x = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 100 , 5 ]) rsc = preprocessing . RobustScaler () rsc . fit ( x ) proc = mdnc . data . preprocs . ProcScaler ( shift = np . expand_dims ( rsc . center_ , axis = 0 ), scale = np . expand_dims ( rsc . scale_ , axis = 0 )) x_ = proc . preprocess ( x ) x_sl = rsc . transform ( x ) x_r = proc . postprocess ( x_ ) x_r_sl = rsc . inverse_transform ( x_sl ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x_sl ))) print ( 'Inverse error:' , np . amax ( np . abs ( x_r - x_r_sl )))","title":"Examples"},{"location":"apis/data/sequence/MPSequence/","text":"data.sequence.MPSequence \u00b6 Class \u00b7 Source manager = mdnc . data . sequence . MPSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: multiprocessing module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MPSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MPSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MPSequence would store the indexer during the initialization. When the start() method is invoked, two process pools would be created. The first pool maintains several processes, each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MPSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes or threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Examples \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"<span class='magic-codeicon-class'>MPSequence</span>"},{"location":"apis/data/sequence/MPSequence/#datasequencempsequence","text":"Class \u00b7 Source manager = mdnc . data . sequence . MPSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: multiprocessing module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MPSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MPSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MPSequence would store the indexer during the initialization. When the start() method is invoked, two process pools would be created. The first pool maintains several processes, each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MPSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed).","title":"data.sequence.MPSequence"},{"location":"apis/data/sequence/MPSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes or threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used.","title":"Arguments"},{"location":"apis/data/sequence/MPSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MPSequence/#start","text":"manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MPSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MPSequence/#finish","text":"manager . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MPSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MPSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MPSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MPSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MPSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MPSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MPSequence/#examples","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"Examples"},{"location":"apis/data/sequence/MSequence/","text":"data.sequence.MSequence \u00b6 Class \u00b7 Source manager = mdnc . data . sequence . MSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , thread_type = 'proc' , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading or multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading and multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: threading and :fontawesome-solid-external-link-alt: multiprocessing modules respectively. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MSequence would store the indexer during the initialization. When the start() method is invoked, two process (or threading) pools would be created. The first pool maintains several processes (or threads), each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Warning We do not recommend to use mdnc.data.sequence.MSequence , because it is a base class. Instead, please use mdnc.data.sequence.MTSequence or mdnc.data.sequence.MPSequence according to your preference. The only case where you use this class is, you want to make the multi-threading or multi-processing options exposed to users. Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes or threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. thread_type str The backend of the MSequence , could be 'proc' or 'thread' . out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process (or threading) pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the process (or threading) pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Examples \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"<span class='magic-codeicon-class'>MSequence</span>"},{"location":"apis/data/sequence/MSequence/#datasequencemsequence","text":"Class \u00b7 Source manager = mdnc . data . sequence . MSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , thread_type = 'proc' , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading or multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading and multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: threading and :fontawesome-solid-external-link-alt: multiprocessing modules respectively. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MSequence would store the indexer during the initialization. When the start() method is invoked, two process (or threading) pools would be created. The first pool maintains several processes (or threads), each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Warning We do not recommend to use mdnc.data.sequence.MSequence , because it is a base class. Instead, please use mdnc.data.sequence.MTSequence or mdnc.data.sequence.MPSequence according to your preference. The only case where you use this class is, you want to make the multi-threading or multi-processing options exposed to users.","title":"data.sequence.MSequence"},{"location":"apis/data/sequence/MSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes or threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. thread_type str The backend of the MSequence , could be 'proc' or 'thread' . out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used.","title":"Arguments"},{"location":"apis/data/sequence/MSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MSequence/#start","text":"manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process (or threading) pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MSequence/#finish","text":"manager . finish () Finish the process (or threading) pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MSequence/#examples","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"Examples"},{"location":"apis/data/sequence/MTSequence/","text":"data.sequence.MTSequence \u00b6 Class \u00b7 Source manager = mdnc . data . sequence . MTSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading codes are built on top of the :fontawesome-solid-external-link-alt: threading module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MTSequence] subgraph procs [Threading Pool] proc1[[Thread 1]] proc2[[Thread 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Threading Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MTSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MTSequence would store the indexer during the initialization. When the start() method is invoked, two threading pools would be created. The first pool maintains several threads, each thread would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MTSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel threads (in pool 1) by the input queue . Each thread would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the thread would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Info The argument worker does not require to be picklable in this case, because all threads are mainted in the same process. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the threading pool. When this method is invoked, the thereading pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Only reserved for compatibility for switching from MPSequence to MTSequence . This flag would not influence anything. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the threading pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the threading pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Examples \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"<span class='magic-codeicon-class'>MTSequence</span>"},{"location":"apis/data/sequence/MTSequence/#datasequencemtsequence","text":"Class \u00b7 Source manager = mdnc . data . sequence . MTSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading codes are built on top of the :fontawesome-solid-external-link-alt: threading module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MTSequence] subgraph procs [Threading Pool] proc1[[Thread 1]] proc2[[Thread 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Threading Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MTSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MTSequence would store the indexer during the initialization. When the start() method is invoked, two threading pools would be created. The first pool maintains several threads, each thread would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MTSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel threads (in pool 1) by the input queue . Each thread would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the thread would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed).","title":"data.sequence.MTSequence"},{"location":"apis/data/sequence/MTSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Info The argument worker does not require to be picklable in this case, because all threads are mainted in the same process.","title":"Arguments"},{"location":"apis/data/sequence/MTSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MTSequence/#start","text":"manager . start ( compat = None ) Start the threading pool. When this method is invoked, the thereading pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Only reserved for compatibility for switching from MPSequence to MTSequence . This flag would not influence anything. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MTSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the threading pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MTSequence/#finish","text":"manager . finish () Finish the threading pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MTSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MTSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MTSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MTSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MTSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MTSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MTSequence/#examples","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"Examples"},{"location":"apis/data/webtools/DataChecker/","text":"data.webtools.Datachecker \u00b6 Class \u00b7 Source dchecker = mdnc . data . webtools . Datachecker ( root = './datasets' , set_list_file = 'web-data' , token = '' , verbose = False ) This data checker could check the local dataset folder, find the not existing datasets and fetch those required datasets from online repositories or links. A private repository requires a token. In this case, the argument token need to be not blank. Arguments \u00b6 Requries Argument Type Description root str The root path of all maintained local datasets. set_list_file str A json file recording the online repository paths (the file name extension could be absent) of the required datasets. token int or ( int , ) The default Github OAuth token for downloading files from private repositories. If not set, the downloading from public repositories would not be influenced. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request. Methods \u00b6 init_set_list \u00b6 dchecker . init_set_list ( file_name = 'web-data' ) This method should get used by users manually. It is used for creating an initialized .json config file for the DataChecker . Requries Argument Type Description file_name str The name of the to-be-created dataset config file. clear \u00b6 dchecker . clear () Clear the query list. The query list is a list to required dataset names. This function is not necessary to be used frequently, because DataChecker may only need to be invoked for one time. add_query_file \u00b6 dchecker . add_query_file ( file_names ) Add one or more file names in the query list. Add file names into the required dataset name list. For each different application, the required datasets could be different. The query file list should be a sub-set of the whole list given by set_list_file . Requries Argument Type Description file_name str ( str , ) The could be one or a list of file name strs, including all requried dataset names for the current program. This argument could also be one or a list of file name strs, including all requried dataset names for the current program. query \u00b6 dchecker . query () Search the files in the query list, and download the datasets. Properties \u00b6 token \u00b6 dchecker . token Check or set the Github OAuth token. Examples \u00b6 Here we show an example of creating and using the config file. Example Codes 1 2 3 4 5 6 7 8 9 import os import numpy as np import mdnc set_list_file = os . path . join ( './datasets' , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = './datasets' , set_list_file = set_list_file , token = '' , verbose = True ) dc . add_query_file ( 'dataset_file_name_01.txt' ) dc . query () Output data.webtools: There are required dataset missing. Start downloading from the online repository... Get test-datasets-1.tar.xz: 216B [00:00, 108kB/s] data.webtools: Successfully download all required datasets. The config file should be formatted like the following json examples: Example of meta-data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"set_list\" : [ { \"tag\" : \"test\" , \"asset\" : \"test-datasets-1.tar.xz\" , \"items\" : [ \"dataset_file_name_01.txt\" , \"dataset_file_name_02.txt\" ] } ], \"user\" : \"cainmagi\" , \"repo\" : \"MDNC\" } where the set_list contains a list of dictionaries. Each dictionary represents an xz file. The keyword items represnets the file name inside the xz file.","title":"<span class='magic-codeicon-class'>DataChecker</span>"},{"location":"apis/data/webtools/DataChecker/#datawebtoolsdatachecker","text":"Class \u00b7 Source dchecker = mdnc . data . webtools . Datachecker ( root = './datasets' , set_list_file = 'web-data' , token = '' , verbose = False ) This data checker could check the local dataset folder, find the not existing datasets and fetch those required datasets from online repositories or links. A private repository requires a token. In this case, the argument token need to be not blank.","title":"data.webtools.Datachecker"},{"location":"apis/data/webtools/DataChecker/#arguments","text":"Requries Argument Type Description root str The root path of all maintained local datasets. set_list_file str A json file recording the online repository paths (the file name extension could be absent) of the required datasets. token int or ( int , ) The default Github OAuth token for downloading files from private repositories. If not set, the downloading from public repositories would not be influenced. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/DataChecker/#methods","text":"","title":"Methods"},{"location":"apis/data/webtools/DataChecker/#init_set_list","text":"dchecker . init_set_list ( file_name = 'web-data' ) This method should get used by users manually. It is used for creating an initialized .json config file for the DataChecker . Requries Argument Type Description file_name str The name of the to-be-created dataset config file.","title":" init_set_list"},{"location":"apis/data/webtools/DataChecker/#clear","text":"dchecker . clear () Clear the query list. The query list is a list to required dataset names. This function is not necessary to be used frequently, because DataChecker may only need to be invoked for one time.","title":" clear"},{"location":"apis/data/webtools/DataChecker/#add_query_file","text":"dchecker . add_query_file ( file_names ) Add one or more file names in the query list. Add file names into the required dataset name list. For each different application, the required datasets could be different. The query file list should be a sub-set of the whole list given by set_list_file . Requries Argument Type Description file_name str ( str , ) The could be one or a list of file name strs, including all requried dataset names for the current program. This argument could also be one or a list of file name strs, including all requried dataset names for the current program.","title":" add_query_file"},{"location":"apis/data/webtools/DataChecker/#query","text":"dchecker . query () Search the files in the query list, and download the datasets.","title":" query"},{"location":"apis/data/webtools/DataChecker/#properties","text":"","title":"Properties"},{"location":"apis/data/webtools/DataChecker/#token","text":"dchecker . token Check or set the Github OAuth token.","title":" token"},{"location":"apis/data/webtools/DataChecker/#examples","text":"Here we show an example of creating and using the config file. Example Codes 1 2 3 4 5 6 7 8 9 import os import numpy as np import mdnc set_list_file = os . path . join ( './datasets' , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = './datasets' , set_list_file = set_list_file , token = '' , verbose = True ) dc . add_query_file ( 'dataset_file_name_01.txt' ) dc . query () Output data.webtools: There are required dataset missing. Start downloading from the online repository... Get test-datasets-1.tar.xz: 216B [00:00, 108kB/s] data.webtools: Successfully download all required datasets. The config file should be formatted like the following json examples: Example of meta-data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"set_list\" : [ { \"tag\" : \"test\" , \"asset\" : \"test-datasets-1.tar.xz\" , \"items\" : [ \"dataset_file_name_01.txt\" , \"dataset_file_name_02.txt\" ] } ], \"user\" : \"cainmagi\" , \"repo\" : \"MDNC\" } where the set_list contains a list of dictionaries. Each dictionary represents an xz file. The keyword items represnets the file name inside the xz file.","title":"Examples"},{"location":"apis/data/webtools/download_tarball/","text":"data.webtools.download_tarball \u00b6 Function \u00b7 Source download_tarball ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically. This tool is used for downloading the assets from github repositories. It would: Try to detect the data info in public mode; If fails (the Github repository could not be accessed), switch to private downloading mode. The private mode requires a Github OAuth token for getting access to the file. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details. Tip The mechanics of this function is a little bit complicated. It is mainly inspired by the following codes: https://gist.github.com/devhero/8ae2229d9ea1a59003ced4587c9cb236 https://gist.github.com/maxim/6e15aa45ba010ab030c4 Arguments \u00b6 Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example 1 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s] Example 2 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"<span class='magic-codeicon-function'>download_tarball</span>"},{"location":"apis/data/webtools/download_tarball/#datawebtoolsdownload_tarball","text":"Function \u00b7 Source download_tarball ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically. This tool is used for downloading the assets from github repositories. It would: Try to detect the data info in public mode; If fails (the Github repository could not be accessed), switch to private downloading mode. The private mode requires a Github OAuth token for getting access to the file. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details. Tip The mechanics of this function is a little bit complicated. It is mainly inspired by the following codes: https://gist.github.com/devhero/8ae2229d9ea1a59003ced4587c9cb236 https://gist.github.com/maxim/6e15aa45ba010ab030c4","title":"data.webtools.download_tarball"},{"location":"apis/data/webtools/download_tarball/#arguments","text":"Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball/#examples","text":"Example 1 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s] Example 2 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"Examples"},{"location":"apis/data/webtools/download_tarball_link/","text":"data.webtools.download_tarball_link \u00b6 Function \u00b7 Source download_tarball_link ( link , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a web link, and extract it automatically. This function is equivalent to using wget . For example, downloading a xz file: wget -O- <link>.tar.xz | tar xJ -C <path>/ || fail The tarball is directed by the link. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details. Arguments \u00b6 Requries Argument Type Description link str The whole web link, pointting to or redicted to the data file. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_link ( 'https://github.com/cainmagi/Dockerfiles/releases/download/xubuntu-v1.5-u20.04/share-pixmaps.tar.xz' , path = './downloads' , verbose = True ) Output Get share-pixmaps.tar.xz: 134kB [00:00, 1.65MB/s]","title":"<span class='magic-codeicon-function'>download_tarball_link</span>"},{"location":"apis/data/webtools/download_tarball_link/#datawebtoolsdownload_tarball_link","text":"Function \u00b7 Source download_tarball_link ( link , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a web link, and extract it automatically. This function is equivalent to using wget . For example, downloading a xz file: wget -O- <link>.tar.xz | tar xJ -C <path>/ || fail The tarball is directed by the link. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details.","title":"data.webtools.download_tarball_link"},{"location":"apis/data/webtools/download_tarball_link/#arguments","text":"Requries Argument Type Description link str The whole web link, pointting to or redicted to the data file. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball_link/#examples","text":"Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_link ( 'https://github.com/cainmagi/Dockerfiles/releases/download/xubuntu-v1.5-u20.04/share-pixmaps.tar.xz' , path = './downloads' , verbose = True ) Output Get share-pixmaps.tar.xz: 134kB [00:00, 1.65MB/s]","title":"Examples"},{"location":"apis/data/webtools/download_tarball_private/","text":"data.webtools.download_tarball_private \u00b6 Function \u00b7 Source download_tarball_private ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (private). This tool should only be used for downloading assets from private repositories. Although it could be also used for public repositories, we do not recommend to use it in those cases, because it would still require a token even the repository is public. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning Using the general interface mdnc.data.webtools.download_tarball is more recommended. Arguments \u00b6 Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_private ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"<span class='magic-codeicon-function'>download_tarball_private</span>"},{"location":"apis/data/webtools/download_tarball_private/#datawebtoolsdownload_tarball_private","text":"Function \u00b7 Source download_tarball_private ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (private). This tool should only be used for downloading assets from private repositories. Although it could be also used for public repositories, we do not recommend to use it in those cases, because it would still require a token even the repository is public. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning Using the general interface mdnc.data.webtools.download_tarball is more recommended.","title":"data.webtools.download_tarball_private"},{"location":"apis/data/webtools/download_tarball_private/#arguments","text":"Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball_private/#examples","text":"Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_private ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"Examples"},{"location":"apis/data/webtools/download_tarball_public/","text":"data.webtools.download_tarball_public \u00b6 Function \u00b7 Source download_tarball_public ( user , repo , tag , asset , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (public). This tool only supports public github repositories. This method could be replaced by mdnc.data.webtools.download_tarball_link , but we do not recommend to do that. Compared to that method, this function is more robust, because it fetches the meta-data before downloading the dataset. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning This function is only designed for downloading public data. If your repository is private, please use mdnc.data.webtools.download_tarball_private for instead. Certainly, using the general interface mdnc.data.webtools.download_tarball is more recommended. Arguments \u00b6 Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_public ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s]","title":"<span class='magic-codeicon-function'>download_tarball_public</span>"},{"location":"apis/data/webtools/download_tarball_public/#datawebtoolsdownload_tarball_public","text":"Function \u00b7 Source download_tarball_public ( user , repo , tag , asset , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (public). This tool only supports public github repositories. This method could be replaced by mdnc.data.webtools.download_tarball_link , but we do not recommend to do that. Compared to that method, this function is more robust, because it fetches the meta-data before downloading the dataset. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning This function is only designed for downloading public data. If your repository is private, please use mdnc.data.webtools.download_tarball_private for instead. Certainly, using the general interface mdnc.data.webtools.download_tarball is more recommended.","title":"data.webtools.download_tarball_public"},{"location":"apis/data/webtools/download_tarball_public/#arguments","text":"Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball_public/#examples","text":"Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_public ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s]","title":"Examples"},{"location":"apis/data/webtools/get_token/","text":"data.webtools.get_token \u00b6 Function \u00b7 Source token = get_token ( token = '' , silent = True ) Automatically get the Github OAuth token, if the argument token is missing. This function would try to get the token in the following orders: Try to find the value of the environmental variable GITTOKEN . If not found, try to find the value of the environmental variable GITHUB_API_TOKEN . If not found, and silent is False , would ask users to input the token. When silent is True , would return '' . Tip How to get the token? Please read this page: Git automation with OAuth tokens Tip The token could be formatted like the following two forms: With user name GITTOKEN = myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Without name GITTOKEN = b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Another tip is that, you could skip entering the user name and password if you clone a private repository like this: git clone https://myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc@github.com/myusername/myreponame.git myrepo A repository cloned by this way does not require the user name and password for pull and push . Arguments \u00b6 Requries Argument Type Description token str The given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. silent bool A flag. If set True and the token could not be found anywhere, this tool would not ask for a token, but just return '' . Returns Argument Description token A str . This is the detected OAuth token. Examples \u00b6 Example Codes Run bash , 1 $GITTOKEN = xxxxxxxxxxxxxx Then, run python , 1 2 3 4 import mdnc token = mdnc . data . webtools . get_token ( token = '' ) print ( token ) Output xxxxxxxxxxxxxx","title":"<span class='magic-codeicon-function'>get_token</span>"},{"location":"apis/data/webtools/get_token/#datawebtoolsget_token","text":"Function \u00b7 Source token = get_token ( token = '' , silent = True ) Automatically get the Github OAuth token, if the argument token is missing. This function would try to get the token in the following orders: Try to find the value of the environmental variable GITTOKEN . If not found, try to find the value of the environmental variable GITHUB_API_TOKEN . If not found, and silent is False , would ask users to input the token. When silent is True , would return '' . Tip How to get the token? Please read this page: Git automation with OAuth tokens Tip The token could be formatted like the following two forms: With user name GITTOKEN = myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Without name GITTOKEN = b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Another tip is that, you could skip entering the user name and password if you clone a private repository like this: git clone https://myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc@github.com/myusername/myreponame.git myrepo A repository cloned by this way does not require the user name and password for pull and push .","title":"data.webtools.get_token"},{"location":"apis/data/webtools/get_token/#arguments","text":"Requries Argument Type Description token str The given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. silent bool A flag. If set True and the token could not be found anywhere, this tool would not ask for a token, but just return '' . Returns Argument Description token A str . This is the detected OAuth token.","title":"Arguments"},{"location":"apis/data/webtools/get_token/#examples","text":"Example Codes Run bash , 1 $GITTOKEN = xxxxxxxxxxxxxx Then, run python , 1 2 3 4 import mdnc token = mdnc . data . webtools . get_token ( token = '' ) print ( token ) Output xxxxxxxxxxxxxx","title":"Examples"}]}