{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M odern D eep N etwork Toolkits for pyTor c h (MDNC) \u00b6 This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.5 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document. Overview \u00b6 The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools. Current progress \u00b6 Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model . summary () in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0% Compatibility test \u00b6 Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Overview"},{"location":"#modern-deep-network-toolkits-for-pytorch-mdnc","text":"This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.5 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document.","title":"Modern Deep Network Toolkits for pyTorch (MDNC)"},{"location":"#overview","text":"The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools.","title":"Overview"},{"location":"#current-progress","text":"Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model . summary () in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0%","title":"Current progress"},{"location":"#compatibility-test","text":"Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Compatibility test"},{"location":"installation/","text":"Installation \u00b6 Use the nightly version on Github \u00b6 Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by from mdnc import mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive Install the package \u00b6 Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#use-the-nightly-version-on-github","text":"Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by from mdnc import mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive","title":"Use the nightly version on Github"},{"location":"installation/#install-the-package","text":"Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Install the package"},{"location":"licenses/","text":"Licenses \u00b6 License of MDNC \u00b6 MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of pytorch-summary \u00b6 The third-party module, torchsummary used in MDNC ( sksq96/pytorch-summary ), grants: MIT License https://github.com/sksq96/pytorch-summary/blob/master/LICENSE License of MkDocs-Material Theme \u00b6 The theme of this website ( squidfunk/mkdocs-material ) grants: MIT License https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE License of VSCode-Codeicons \u00b6 The Codeicons pack ( microsoft/vscode-codicons ) used in this website grants: MIT License for all source codes https://github.com/microsoft/vscode-codicons/blob/main/LICENSE-CODE CC BY 4.0 License for all materials https://github.com/microsoft/vscode-codicons/blob/main/LICENSE","title":"Licenses"},{"location":"licenses/#licenses","text":"","title":"Licenses"},{"location":"licenses/#license-of-mdnc","text":"MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License of MDNC"},{"location":"licenses/#license-of-pytorch-summary","text":"The third-party module, torchsummary used in MDNC ( sksq96/pytorch-summary ), grants: MIT License https://github.com/sksq96/pytorch-summary/blob/master/LICENSE","title":"License of pytorch-summary"},{"location":"licenses/#license-of-mkdocs-material-theme","text":"The theme of this website ( squidfunk/mkdocs-material ) grants: MIT License https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE","title":"License of MkDocs-Material Theme"},{"location":"licenses/#license-of-vscode-codeicons","text":"The Codeicons pack ( microsoft/vscode-codicons ) used in this website grants: MIT License for all source codes https://github.com/microsoft/vscode-codicons/blob/main/LICENSE-CODE CC BY 4.0 License for all materials https://github.com/microsoft/vscode-codicons/blob/main/LICENSE","title":"License of VSCode-Codeicons"},{"location":"apis/overview/","text":"Overview \u00b6 The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup sequence(sequence) subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B; List of packages \u00b6 optimizers \u00b6 To be built ... modules \u00b6 conv : The implementation of the modern convolutional layer and convolutional networks. The networks include U-Net, auto-encoder, encoder and decoder (generator). The codes are inspried by: resnet : The implementation of the reisudal blocks and residual networks. The networks include U-Net, auto-encoder, encoder and decoder (generator). The codes are inspired by the following nice works: pytorch/vision/torchvision/models/resnet.py milesial/Pytorch-UNet nikhilroxtomar/Deep-Residual-Unet I would like to show my appreciation to them! models \u00b6 To be built ... data \u00b6 sequence : The infrastructures of CPU-based parallel I/O and processing. This module is used by all data loaders. h5py : Wrapped HDF5 datasets savers, data converters and data loaders. preprocs : Useful pre- and post- processing tools for all data loaders in this package. webtools : Web tools for downloading tarball-packed datasets from Github. funcs \u00b6 To be built ... utils \u00b6 tools : Light-weighted recording parsing tools used during training or testing. draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. This module also provide some specialized formatters. contribs \u00b6 torchsummary : The revised sksq96/pytorch-summary . This is a Keras style model . summary () in pyTorch, with some bugs gotten fixed. To view my modified version, see sksq96/pytorch-summary!165 .","title":"Overview"},{"location":"apis/overview/#overview","text":"The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup sequence(sequence) subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B;","title":"Overview"},{"location":"apis/overview/#list-of-packages","text":"","title":"List of packages"},{"location":"apis/overview/#optimizers","text":"To be built ...","title":" optimizers"},{"location":"apis/overview/#modules","text":"conv : The implementation of the modern convolutional layer and convolutional networks. The networks include U-Net, auto-encoder, encoder and decoder (generator). The codes are inspried by: resnet : The implementation of the reisudal blocks and residual networks. The networks include U-Net, auto-encoder, encoder and decoder (generator). The codes are inspired by the following nice works: pytorch/vision/torchvision/models/resnet.py milesial/Pytorch-UNet nikhilroxtomar/Deep-Residual-Unet I would like to show my appreciation to them!","title":" modules"},{"location":"apis/overview/#models","text":"To be built ...","title":" models"},{"location":"apis/overview/#data","text":"sequence : The infrastructures of CPU-based parallel I/O and processing. This module is used by all data loaders. h5py : Wrapped HDF5 datasets savers, data converters and data loaders. preprocs : Useful pre- and post- processing tools for all data loaders in this package. webtools : Web tools for downloading tarball-packed datasets from Github.","title":" data"},{"location":"apis/overview/#funcs","text":"To be built ...","title":" funcs"},{"location":"apis/overview/#utils","text":"tools : Light-weighted recording parsing tools used during training or testing. draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. This module also provide some specialized formatters.","title":" utils"},{"location":"apis/overview/#contribs","text":"torchsummary : The revised sksq96/pytorch-summary . This is a Keras style model . summary () in pyTorch, with some bugs gotten fixed. To view my modified version, see sksq96/pytorch-summary!165 .","title":" contribs"},{"location":"apis/contribs/torchsummary/summary/","text":"contribs.torchsummary.summary \u00b6 Function \u00b7 Source params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"<span class='magic-codeicon-function'>summary</span>"},{"location":"apis/contribs/torchsummary/summary/#contribstorchsummarysummary","text":"Function \u00b7 Source params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/contribs/torchsummary/summary_string/","text":"contribs.torchsummary.summary \u00b6 Function \u00b7 Source summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str The summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Examples \u00b6 See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"<span class='magic-codeicon-function'>summary_string</span>"},{"location":"apis/contribs/torchsummary/summary_string/#contribstorchsummarysummary","text":"Function \u00b7 Source summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary_string/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str The summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary_string/#examples","text":"See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"Examples"},{"location":"apis/data/h5py/H5CParser/","text":"data.h5py.H5CParser \u00b6 Class \u00b7 Context \u00b7 Source dset = mdnc . data . h5py . H5CParser ( file_name , keywords_sequence , keywords_single , batch_size = 32 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and parse it by mdnc.data.sequence.MPSequence . The realization could be described as: This parser is the upgraded version of mdnc.data.h5py.H5GParser , it is specially designed for parsing data to LSTM/ConvLSTM. A sequence dimension would be inserted between batches and channels . In each batch, the sequence is continuously extracted in the order of the batches. During each epoch, a sliding window would iterate the first axis (samples). The number of batches would be the same as using mdnc.data.h5py.H5GParser . For each variable specified by keywords_sequence , each sample in the mini-batch is a sequence. This parser could also read the dataset converted by mdnc.data.h5py.H5SeqConverter . The workflow is shown in the following figure: Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords_sequence ( str , ) The keyword of sequence data. The keywords in this list would be parsed as (B, S, C1, C2, ...) , where B and S are the sample number and sequence length (given by the argument sequence_size ) respectively. It should be a list of keywords (or a single keyword). keyword_single ( str , ) The keyword of single values. The keywords in this list would be parsed as (B, C1, C2, ...) , where B is the sample number. It should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. sequence_size int The size of each sequence. It represents S of (B, S, C1, C2, ...) . sequence_position int The aligned position between the single values and the sequence values. It should be in the range of >= 0 and < sequence_size . sequence_padding int The padding method for each epoch, it will influence the first or the final samples in the dataset. Could be 'same' , 'zero' or 'none' . If set None , the number of batches of each epoch would be a little bit smaller than the actual number. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip At least one keyword requires to be given in keywords_sequence or keyword_single . In some cases, we need to use both kinds of keywords. For example, the input could be a sequence, and the label may be a scalar. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5CParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. sequence_size \u00b6 dset . sequence_size The length of each sequence. This value is given by the argument sequence_size during the initialization. sequence_position \u00b6 dset . sequence_position The alignment between keywords_sequence and keyword_single . This value is given by the argument sequence_position during the initialization. sequence_padding \u00b6 dset . sequence_position The padding method of each sequence. This value is given by the argument sequence_padding during the initialization. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5cparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5CParser ( os . path . join ( root_folder , 'test_data_h5cparser' ), keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position = 0 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 1 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Output data.webtools: All required datasets are available. data.h5py: 0 tensor([[0., 1., 2., 3., 4.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1.], device='cuda:0') data.h5py: 1 tensor([[1., 2., 3., 4., 5.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([2.], device='cuda:0') data.h5py: 2 tensor([[2., 3., 4., 5., 6.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([3.], device='cuda:0') data.h5py: 3 tensor([[3., 4., 5., 6., 7.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([4.], device='cuda:0') data.h5py: 4 tensor([[4., 5., 6., 7., 8.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([5.], device='cuda:0') data.h5py: 5 tensor([[5., 6., 7., 8., 9.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([6.], device='cuda:0') data.h5py: 6 tensor([[ 6., 7., 8., 9., 10.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([7.], device='cuda:0') data.h5py: 7 tensor([[ 7., 8., 9., 10., 11.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([8.], device='cuda:0') data.h5py: 8 tensor([[ 8., 9., 10., 11., 12.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([9.], device='cuda:0') data.h5py: 9 tensor([[ 9., 10., 11., 12., 13.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([10.], device='cuda:0') data.h5py: 10 tensor([[10., 11., 12., 13., 14.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([11.], device='cuda:0') data.h5py: 11 tensor([[11., 12., 13., 14., 15.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([12.], device='cuda:0') data.h5py: 12 tensor([[12., 13., 14., 15., 16.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([13.], device='cuda:0') data.h5py: 13 tensor([[13., 14., 15., 16., 17.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([14.], device='cuda:0') data.h5py: 14 tensor([[14., 15., 16., 17., 18.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([15.], device='cuda:0') data.h5py: 15 tensor([[15., 16., 17., 18., 19.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([16.], device='cuda:0') data.h5py: 16 tensor([[16., 17., 18., 19., 20.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([17.], device='cuda:0') data.h5py: 17 tensor([[17., 18., 19., 20., 21.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([18.], device='cuda:0') data.h5py: 18 tensor([[18., 19., 20., 21., 22.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([19.], device='cuda:0') data.h5py: 19 tensor([[19., 20., 21., 22., 23.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([20.], device='cuda:0') data.h5py: 20 tensor([[20., 21., 22., 23., 24.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([21.], device='cuda:0') data.h5py: 21 tensor([[21., 22., 23., 24., 25.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([22.], device='cuda:0') data.h5py: 22 tensor([[22., 23., 24., 25., 26.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([23.], device='cuda:0') data.h5py: 23 tensor([[23., 24., 25., 26., 27.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([24.], device='cuda:0') data.h5py: 24 tensor([[24., 25., 26., 27., 28.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([25.], device='cuda:0') data.h5py: 25 tensor([[25., 26., 27., 28., 29.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([26.], device='cuda:0') data.h5py: 26 tensor([[26., 27., 28., 29., 30.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([27.], device='cuda:0') data.h5py: 27 tensor([[27., 28., 29., 30., 31.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([28.], device='cuda:0') data.h5py: 28 tensor([[28., 29., 30., 31., 32.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([29.], device='cuda:0') data.h5py: 29 tensor([[29., 30., 31., 32., 33.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([30.], device='cuda:0') data.h5py: 30 tensor([[30., 31., 32., 33., 34.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([31.], device='cuda:0') data.h5py: 31 tensor([[31., 32., 33., 34., 35.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([32.], device='cuda:0') data.h5py: 32 tensor([[32., 33., 34., 35., 36.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([33.], device='cuda:0') data.h5py: 33 tensor([[33., 34., 35., 36., 37.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([34.], device='cuda:0') data.h5py: 34 tensor([[34., 35., 36., 37., 38.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([35.], device='cuda:0') data.h5py: 35 tensor([[35., 36., 37., 38., 39.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([36.], device='cuda:0') data.h5py: 36 tensor([[36., 37., 38., 39., 40.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([37.], device='cuda:0') data.h5py: 37 tensor([[37., 38., 39., 40., 41.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([38.], device='cuda:0') data.h5py: 38 tensor([[38., 39., 40., 41., 42.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([39.], device='cuda:0') data.h5py: 39 tensor([[39., 40., 41., 42., 43.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([40.], device='cuda:0') data.h5py: 40 tensor([[40., 41., 42., 43., 44.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([41.], device='cuda:0') data.h5py: 41 tensor([[41., 42., 43., 44., 45.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([42.], device='cuda:0') data.h5py: 42 tensor([[42., 43., 44., 45., 46.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([43.], device='cuda:0') data.h5py: 43 tensor([[43., 44., 45., 46., 47.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([44.], device='cuda:0') data.h5py: 44 tensor([[44., 45., 46., 47., 48.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([45.], device='cuda:0') data.h5py: 45 tensor([[45., 46., 47., 48., 49.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([46.], device='cuda:0') data.h5py: 46 tensor([[46., 47., 48., 49., 50.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([47.], device='cuda:0') data.h5py: 47 tensor([[47., 48., 49., 50., 51.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([48.], device='cuda:0') data.h5py: 48 tensor([[48., 49., 50., 51., 52.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([49.], device='cuda:0') data.h5py: 49 tensor([[49., 50., 51., 52., 53.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([50.], device='cuda:0') data.h5py: 50 tensor([[50., 51., 52., 53., 54.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([51.], device='cuda:0') data.h5py: 51 tensor([[51., 52., 53., 54., 55.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([52.], device='cuda:0') data.h5py: 52 tensor([[52., 53., 54., 55., 56.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([53.], device='cuda:0') data.h5py: 53 tensor([[53., 54., 55., 56., 57.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([54.], device='cuda:0') data.h5py: 54 tensor([[54., 55., 56., 57., 58.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([55.], device='cuda:0') data.h5py: 55 tensor([[55., 56., 57., 58., 59.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([56.], device='cuda:0') data.h5py: 56 tensor([[56., 57., 58., 59., 60.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([57.], device='cuda:0') data.h5py: 57 tensor([[57., 58., 59., 60., 61.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([58.], device='cuda:0') data.h5py: 58 tensor([[58., 59., 60., 61., 62.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([59.], device='cuda:0') data.h5py: 59 tensor([[59., 60., 61., 62., 63.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([60.], device='cuda:0') data.h5py: 60 tensor([[60., 61., 62., 63., 64.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([61.], device='cuda:0') data.h5py: 61 tensor([[61., 62., 63., 64., 65.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([62.], device='cuda:0') data.h5py: 62 tensor([[62., 63., 64., 65., 66.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([63.], device='cuda:0') data.h5py: 63 tensor([[63., 64., 65., 66., 67.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([64.], device='cuda:0') data.h5py: 64 tensor([[64., 65., 66., 67., 68.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([65.], device='cuda:0') data.h5py: 65 tensor([[65., 66., 67., 68., 69.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([66.], device='cuda:0') data.h5py: 66 tensor([[66., 67., 68., 69., 70.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([67.], device='cuda:0') data.h5py: 67 tensor([[67., 68., 69., 70., 71.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([68.], device='cuda:0') data.h5py: 68 tensor([[68., 69., 70., 71., 72.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([69.], device='cuda:0') data.h5py: 69 tensor([[69., 70., 71., 72., 73.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([70.], device='cuda:0') data.h5py: 70 tensor([[70., 71., 72., 73., 74.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([71.], device='cuda:0') data.h5py: 71 tensor([[71., 72., 73., 74., 75.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([72.], device='cuda:0') data.h5py: 72 tensor([[72., 73., 74., 75., 76.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([73.], device='cuda:0') data.h5py: 73 tensor([[73., 74., 75., 76., 77.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([74.], device='cuda:0') data.h5py: 74 tensor([[74., 75., 76., 77., 78.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([75.], device='cuda:0') data.h5py: 75 tensor([[75., 76., 77., 78., 79.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([76.], device='cuda:0') data.h5py: 76 tensor([[76., 77., 78., 79., 80.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([77.], device='cuda:0') data.h5py: 77 tensor([[77., 78., 79., 80., 81.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([78.], device='cuda:0') data.h5py: 78 tensor([[78., 79., 80., 81., 82.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([79.], device='cuda:0') data.h5py: 79 tensor([[79., 80., 81., 82., 83.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([80.], device='cuda:0') data.h5py: 80 tensor([[80., 81., 82., 83., 84.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([81.], device='cuda:0') data.h5py: 81 tensor([[81., 82., 83., 84., 85.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([82.], device='cuda:0') data.h5py: 82 tensor([[82., 83., 84., 85., 86.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([83.], device='cuda:0') data.h5py: 83 tensor([[83., 84., 85., 86., 87.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([84.], device='cuda:0') data.h5py: 84 tensor([[84., 85., 86., 87., 88.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([85.], device='cuda:0') data.h5py: 85 tensor([[85., 86., 87., 88., 89.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([86.], device='cuda:0') data.h5py: 86 tensor([[86., 87., 88., 89., 90.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([87.], device='cuda:0') data.h5py: 87 tensor([[87., 88., 89., 90., 91.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([88.], device='cuda:0') data.h5py: 88 tensor([[88., 89., 90., 91., 92.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([89.], device='cuda:0') data.h5py: 89 tensor([[89., 90., 91., 92., 93.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([90.], device='cuda:0') data.h5py: 90 tensor([[90., 91., 92., 93., 94.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([91.], device='cuda:0') data.h5py: 91 tensor([[91., 92., 93., 94., 95.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([92.], device='cuda:0') data.h5py: 92 tensor([[92., 93., 94., 95., 96.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([93.], device='cuda:0') data.h5py: 93 tensor([[93., 94., 95., 96., 97.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([94.], device='cuda:0') data.h5py: 94 tensor([[94., 95., 96., 97., 98.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([95.], device='cuda:0') data.h5py: 95 tensor([[95., 96., 97., 98., 99.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([96.], device='cuda:0') data.h5py: 96 tensor([[ 96., 97., 98., 99., 100.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([97.], device='cuda:0') data.h5py: 97 tensor([[ 97., 98., 99., 100., 101.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([98.], device='cuda:0') data.h5py: 98 tensor([[ 98., 99., 100., 101., 102.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([99.], device='cuda:0') data.h5py: 99 tensor([[ 99., 100., 101., 102., 103.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([100.], device='cuda:0') data.h5py: 100 tensor([[100., 101., 102., 103., 104.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([101.], device='cuda:0') data.h5py: 101 tensor([[101., 102., 103., 104., 105.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([102.], device='cuda:0') data.h5py: 102 tensor([[102., 103., 104., 105., 106.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([103.], device='cuda:0') data.h5py: 103 tensor([[103., 104., 105., 106., 107.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([104.], device='cuda:0') data.h5py: 104 tensor([[104., 105., 106., 107., 108.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([105.], device='cuda:0') data.h5py: 105 tensor([[105., 106., 107., 108., 109.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([106.], device='cuda:0') data.h5py: 106 tensor([[106., 107., 108., 109., 110.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([107.], device='cuda:0') data.h5py: 107 tensor([[107., 108., 109., 110., 111.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([108.], device='cuda:0') data.h5py: 108 tensor([[108., 109., 110., 111., 112.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([109.], device='cuda:0') data.h5py: 109 tensor([[109., 110., 111., 112., 113.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([110.], device='cuda:0') data.h5py: 110 tensor([[110., 111., 112., 113., 114.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([111.], device='cuda:0') data.h5py: 111 tensor([[111., 112., 113., 114., 115.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([112.], device='cuda:0') data.h5py: 112 tensor([[112., 113., 114., 115., 116.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([113.], device='cuda:0') data.h5py: 113 tensor([[113., 114., 115., 116., 117.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([114.], device='cuda:0') data.h5py: 114 tensor([[114., 115., 116., 117., 118.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([115.], device='cuda:0') data.h5py: 115 tensor([[115., 116., 117., 118., 119.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([116.], device='cuda:0') data.h5py: 116 tensor([[116., 117., 118., 119., 120.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([117.], device='cuda:0') data.h5py: 117 tensor([[117., 118., 119., 120., 121.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([118.], device='cuda:0') data.h5py: 118 tensor([[118., 119., 120., 121., 122.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([119.], device='cuda:0') data.h5py: 119 tensor([[119., 120., 121., 122., 123.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([120.], device='cuda:0') data.h5py: 120 tensor([[120., 121., 122., 123., 124.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([121.], device='cuda:0') data.h5py: 121 tensor([[121., 122., 123., 124., 125.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([122.], device='cuda:0') data.h5py: 122 tensor([[122., 123., 124., 125., 126.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([123.], device='cuda:0') data.h5py: 123 tensor([[123., 124., 125., 126., 127.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([124.], device='cuda:0') data.h5py: 124 tensor([[124., 125., 126., 127., 128.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([125.], device='cuda:0') data.h5py: 125 tensor([[125., 126., 127., 128., 129.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([126.], device='cuda:0') data.h5py: 126 tensor([[126., 127., 128., 129., 130.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([127.], device='cuda:0') data.h5py: 127 tensor([[127., 128., 129., 130., 131.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([128.], device='cuda:0') data.h5py: 128 tensor([[128., 129., 130., 131., 132.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([129.], device='cuda:0') data.h5py: 129 tensor([[129., 130., 131., 132., 133.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([130.], device='cuda:0') data.h5py: 130 tensor([[130., 131., 132., 133., 134.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([131.], device='cuda:0') data.h5py: 131 tensor([[131., 132., 133., 134., 135.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([132.], device='cuda:0') data.h5py: 132 tensor([[132., 133., 134., 135., 136.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([133.], device='cuda:0') data.h5py: 133 tensor([[133., 134., 135., 136., 137.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([134.], device='cuda:0') data.h5py: 134 tensor([[134., 135., 136., 137., 138.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([135.], device='cuda:0') data.h5py: 135 tensor([[135., 136., 137., 138., 139.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([136.], device='cuda:0') data.h5py: 136 tensor([[136., 137., 138., 139., 140.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([137.], device='cuda:0') data.h5py: 137 tensor([[137., 138., 139., 140., 141.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([138.], device='cuda:0') data.h5py: 138 tensor([[138., 139., 140., 141., 142.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([139.], device='cuda:0') data.h5py: 139 tensor([[139., 140., 141., 142., 143.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([140.], device='cuda:0') data.h5py: 140 tensor([[140., 141., 142., 143., 144.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([141.], device='cuda:0') data.h5py: 141 tensor([[141., 142., 143., 144., 145.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([142.], device='cuda:0') data.h5py: 142 tensor([[142., 143., 144., 145., 146.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([143.], device='cuda:0') data.h5py: 143 tensor([[143., 144., 145., 146., 147.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([144.], device='cuda:0') data.h5py: 144 tensor([[144., 145., 146., 147., 148.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([145.], device='cuda:0') data.h5py: 145 tensor([[145., 146., 147., 148., 149.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([146.], device='cuda:0') data.h5py: 146 tensor([[146., 147., 148., 149., 150.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([147.], device='cuda:0') data.h5py: 147 tensor([[147., 148., 149., 150., 151.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([148.], device='cuda:0') data.h5py: 148 tensor([[148., 149., 150., 151., 152.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([149.], device='cuda:0') data.h5py: 149 tensor([[149., 150., 151., 152., 153.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([150.], device='cuda:0') data.h5py: 150 tensor([[150., 151., 152., 153., 154.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([151.], device='cuda:0') data.h5py: 151 tensor([[151., 152., 153., 154., 155.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([152.], device='cuda:0') data.h5py: 152 tensor([[152., 153., 154., 155., 156.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([153.], device='cuda:0') data.h5py: 153 tensor([[153., 154., 155., 156., 157.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([154.], device='cuda:0') data.h5py: 154 tensor([[154., 155., 156., 157., 158.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([155.], device='cuda:0') data.h5py: 155 tensor([[155., 156., 157., 158., 159.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([156.], device='cuda:0') data.h5py: 156 tensor([[156., 157., 158., 159., 160.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([157.], device='cuda:0') data.h5py: 157 tensor([[157., 158., 159., 160., 161.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([158.], device='cuda:0') data.h5py: 158 tensor([[158., 159., 160., 161., 162.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([159.], device='cuda:0') data.h5py: 159 tensor([[159., 160., 161., 162., 163.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([160.], device='cuda:0') data.h5py: 160 tensor([[160., 161., 162., 163., 164.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([161.], device='cuda:0') data.h5py: 161 tensor([[161., 162., 163., 164., 165.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([162.], device='cuda:0') data.h5py: 162 tensor([[162., 163., 164., 165., 166.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([163.], device='cuda:0') data.h5py: 163 tensor([[163., 164., 165., 166., 167.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([164.], device='cuda:0') data.h5py: 164 tensor([[164., 165., 166., 167., 168.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([165.], device='cuda:0') data.h5py: 165 tensor([[165., 166., 167., 168., 169.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([166.], device='cuda:0') data.h5py: 166 tensor([[166., 167., 168., 169., 170.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([167.], device='cuda:0') data.h5py: 167 tensor([[167., 168., 169., 170., 171.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([168.], device='cuda:0') data.h5py: 168 tensor([[168., 169., 170., 171., 172.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([169.], device='cuda:0') data.h5py: 169 tensor([[169., 170., 171., 172., 173.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([170.], device='cuda:0') data.h5py: 170 tensor([[170., 171., 172., 173., 174.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([171.], device='cuda:0') data.h5py: 171 tensor([[171., 172., 173., 174., 175.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([172.], device='cuda:0') data.h5py: 172 tensor([[172., 173., 174., 175., 176.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([173.], device='cuda:0') data.h5py: 173 tensor([[173., 174., 175., 176., 177.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([174.], device='cuda:0') data.h5py: 174 tensor([[174., 175., 176., 177., 178.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([175.], device='cuda:0') data.h5py: 175 tensor([[175., 176., 177., 178., 179.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([176.], device='cuda:0') data.h5py: 176 tensor([[176., 177., 178., 179., 180.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([177.], device='cuda:0') data.h5py: 177 tensor([[177., 178., 179., 180., 181.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([178.], device='cuda:0') data.h5py: 178 tensor([[178., 179., 180., 181., 182.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([179.], device='cuda:0') data.h5py: 179 tensor([[179., 180., 181., 182., 183.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([180.], device='cuda:0') data.h5py: 180 tensor([[180., 181., 182., 183., 184.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([181.], device='cuda:0') data.h5py: 181 tensor([[181., 182., 183., 184., 185.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([182.], device='cuda:0') data.h5py: 182 tensor([[182., 183., 184., 185., 186.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([183.], device='cuda:0') data.h5py: 183 tensor([[183., 184., 185., 186., 187.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([184.], device='cuda:0') data.h5py: 184 tensor([[184., 185., 186., 187., 188.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([185.], device='cuda:0') data.h5py: 185 tensor([[185., 186., 187., 188., 189.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([186.], device='cuda:0') data.h5py: 186 tensor([[186., 187., 188., 189., 190.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([187.], device='cuda:0') data.h5py: 187 tensor([[187., 188., 189., 190., 191.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([188.], device='cuda:0') data.h5py: 188 tensor([[188., 189., 190., 191., 192.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([189.], device='cuda:0') data.h5py: 189 tensor([[189., 190., 191., 192., 193.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([190.], device='cuda:0') data.h5py: 190 tensor([[190., 191., 192., 193., 194.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([191.], device='cuda:0') data.h5py: 191 tensor([[191., 192., 193., 194., 195.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([192.], device='cuda:0') data.h5py: 192 tensor([[192., 193., 194., 195., 196.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([193.], device='cuda:0') data.h5py: 193 tensor([[193., 194., 195., 196., 197.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([194.], device='cuda:0') data.h5py: 194 tensor([[194., 195., 196., 197., 198.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([195.], device='cuda:0') data.h5py: 195 tensor([[195., 196., 197., 198., 199.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([196.], device='cuda:0') data.h5py: 196 tensor([[196., 197., 198., 199., 200.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([197.], device='cuda:0') data.h5py: 197 tensor([[197., 198., 199., 200., 201.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([198.], device='cuda:0') data.h5py: 198 tensor([[198., 199., 200., 201., 202.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([199.], device='cuda:0') data.h5py: 199 tensor([[199., 200., 201., 202., 203.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([200.], device='cuda:0') data.h5py: 200 tensor([[200., 201., 202., 203., 204.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([201.], device='cuda:0') data.h5py: 201 tensor([[201., 202., 203., 204., 205.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([202.], device='cuda:0') data.h5py: 202 tensor([[202., 203., 204., 205., 206.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([203.], device='cuda:0') data.h5py: 203 tensor([[203., 204., 205., 206., 207.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([204.], device='cuda:0') data.h5py: 204 tensor([[204., 205., 206., 207., 208.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([205.], device='cuda:0') data.h5py: 205 tensor([[205., 206., 207., 208., 209.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([206.], device='cuda:0') data.h5py: 206 tensor([[206., 207., 208., 209., 210.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([207.], device='cuda:0') data.h5py: 207 tensor([[207., 208., 209., 210., 211.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([208.], device='cuda:0') data.h5py: 208 tensor([[208., 209., 210., 211., 212.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([209.], device='cuda:0') data.h5py: 209 tensor([[209., 210., 211., 212., 213.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([210.], device='cuda:0') data.h5py: 210 tensor([[210., 211., 212., 213., 214.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([211.], device='cuda:0') data.h5py: 211 tensor([[211., 212., 213., 214., 215.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([212.], device='cuda:0') data.h5py: 212 tensor([[212., 213., 214., 215., 216.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([213.], device='cuda:0') data.h5py: 213 tensor([[213., 214., 215., 216., 217.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([214.], device='cuda:0') data.h5py: 214 tensor([[214., 215., 216., 217., 218.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([215.], device='cuda:0') data.h5py: 215 tensor([[215., 216., 217., 218., 219.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([216.], device='cuda:0') data.h5py: 216 tensor([[216., 217., 218., 219., 220.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([217.], device='cuda:0') data.h5py: 217 tensor([[217., 218., 219., 220., 221.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([218.], device='cuda:0') data.h5py: 218 tensor([[218., 219., 220., 221., 222.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([219.], device='cuda:0') data.h5py: 219 tensor([[219., 220., 221., 222., 223.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([220.], device='cuda:0') data.h5py: 220 tensor([[220., 221., 222., 223., 224.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([221.], device='cuda:0') data.h5py: 221 tensor([[221., 222., 223., 224., 225.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([222.], device='cuda:0') data.h5py: 222 tensor([[222., 223., 224., 225., 226.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([223.], device='cuda:0') data.h5py: 223 tensor([[223., 224., 225., 226., 227.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([224.], device='cuda:0') data.h5py: 224 tensor([[224., 225., 226., 227., 228.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([225.], device='cuda:0') data.h5py: 225 tensor([[225., 226., 227., 228., 229.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([226.], device='cuda:0') data.h5py: 226 tensor([[226., 227., 228., 229., 230.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([227.], device='cuda:0') data.h5py: 227 tensor([[227., 228., 229., 230., 231.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([228.], device='cuda:0') data.h5py: 228 tensor([[228., 229., 230., 231., 232.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([229.], device='cuda:0') data.h5py: 229 tensor([[229., 230., 231., 232., 233.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([230.], device='cuda:0') data.h5py: 230 tensor([[230., 231., 232., 233., 234.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([231.], device='cuda:0') data.h5py: 231 tensor([[231., 232., 233., 234., 235.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([232.], device='cuda:0') data.h5py: 232 tensor([[232., 233., 234., 235., 236.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([233.], device='cuda:0') data.h5py: 233 tensor([[233., 234., 235., 236., 237.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([234.], device='cuda:0') data.h5py: 234 tensor([[234., 235., 236., 237., 238.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([235.], device='cuda:0') data.h5py: 235 tensor([[235., 236., 237., 238., 239.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([236.], device='cuda:0') data.h5py: 236 tensor([[236., 237., 238., 239., 240.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([237.], device='cuda:0') data.h5py: 237 tensor([[237., 238., 239., 240., 241.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([238.], device='cuda:0') data.h5py: 238 tensor([[238., 239., 240., 241., 242.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([239.], device='cuda:0') data.h5py: 239 tensor([[239., 240., 241., 242., 243.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([240.], device='cuda:0') data.h5py: 240 tensor([[240., 241., 242., 243., 244.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([241.], device='cuda:0') data.h5py: 241 tensor([[241., 242., 243., 244., 245.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([242.], device='cuda:0') data.h5py: 242 tensor([[242., 243., 244., 245., 246.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([243.], device='cuda:0') data.h5py: 243 tensor([[243., 244., 245., 246., 247.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([244.], device='cuda:0') data.h5py: 244 tensor([[244., 245., 246., 247., 248.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([245.], device='cuda:0') data.h5py: 245 tensor([[245., 246., 247., 248., 249.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([246.], device='cuda:0') data.h5py: 246 tensor([[246., 247., 248., 249., 250.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([247.], device='cuda:0') data.h5py: 247 tensor([[247., 248., 249., 250., 251.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([248.], device='cuda:0') data.h5py: 248 tensor([[248., 249., 250., 251., 252.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([249.], device='cuda:0') data.h5py: 249 tensor([[249., 250., 251., 252., 253.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([250.], device='cuda:0') data.h5py: 250 tensor([[250., 251., 252., 253., 254.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([251.], device='cuda:0') data.h5py: 251 tensor([[251., 252., 253., 254., 255.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([252.], device='cuda:0') data.h5py: 252 tensor([[252., 253., 254., 255., 256.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([253.], device='cuda:0') data.h5py: 253 tensor([[253., 254., 255., 256., 257.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([254.], device='cuda:0') data.h5py: 254 tensor([[254., 255., 256., 257., 258.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([255.], device='cuda:0') data.h5py: 255 tensor([[255., 256., 257., 258., 259.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([256.], device='cuda:0') data.h5py: 256 tensor([[256., 257., 258., 259., 260.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([257.], device='cuda:0') data.h5py: 257 tensor([[257., 258., 259., 260., 261.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([258.], device='cuda:0') data.h5py: 258 tensor([[258., 259., 260., 261., 262.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([259.], device='cuda:0') data.h5py: 259 tensor([[259., 260., 261., 262., 263.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([260.], device='cuda:0') data.h5py: 260 tensor([[260., 261., 262., 263., 264.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([261.], device='cuda:0') data.h5py: 261 tensor([[261., 262., 263., 264., 265.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([262.], device='cuda:0') data.h5py: 262 tensor([[262., 263., 264., 265., 266.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([263.], device='cuda:0') data.h5py: 263 tensor([[263., 264., 265., 266., 267.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([264.], device='cuda:0') data.h5py: 264 tensor([[264., 265., 266., 267., 268.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([265.], device='cuda:0') data.h5py: 265 tensor([[265., 266., 267., 268., 269.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([266.], device='cuda:0') data.h5py: 266 tensor([[266., 267., 268., 269., 270.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([267.], device='cuda:0') data.h5py: 267 tensor([[267., 268., 269., 270., 271.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([268.], device='cuda:0') data.h5py: 268 tensor([[268., 269., 270., 271., 272.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([269.], device='cuda:0') data.h5py: 269 tensor([[269., 270., 271., 272., 273.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([270.], device='cuda:0') data.h5py: 270 tensor([[270., 271., 272., 273., 274.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([271.], device='cuda:0') data.h5py: 271 tensor([[271., 272., 273., 274., 275.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([272.], device='cuda:0') data.h5py: 272 tensor([[272., 273., 274., 275., 276.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([273.], device='cuda:0') data.h5py: 273 tensor([[273., 274., 275., 276., 277.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([274.], device='cuda:0') data.h5py: 274 tensor([[274., 275., 276., 277., 278.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([275.], device='cuda:0') data.h5py: 275 tensor([[275., 276., 277., 278., 279.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([276.], device='cuda:0') data.h5py: 276 tensor([[276., 277., 278., 279., 280.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([277.], device='cuda:0') data.h5py: 277 tensor([[277., 278., 279., 280., 281.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([278.], device='cuda:0') data.h5py: 278 tensor([[278., 279., 280., 281., 282.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([279.], device='cuda:0') data.h5py: 279 tensor([[279., 280., 281., 282., 283.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([280.], device='cuda:0') data.h5py: 280 tensor([[280., 281., 282., 283., 284.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([281.], device='cuda:0') data.h5py: 281 tensor([[281., 282., 283., 284., 285.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([282.], device='cuda:0') data.h5py: 282 tensor([[282., 283., 284., 285., 286.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([283.], device='cuda:0') data.h5py: 283 tensor([[283., 284., 285., 286., 287.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([284.], device='cuda:0') data.h5py: 284 tensor([[284., 285., 286., 287., 288.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([285.], device='cuda:0') data.h5py: 285 tensor([[285., 286., 287., 288., 289.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([286.], device='cuda:0') data.h5py: 286 tensor([[286., 287., 288., 289., 290.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([287.], device='cuda:0') data.h5py: 287 tensor([[287., 288., 289., 290., 291.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([288.], device='cuda:0') data.h5py: 288 tensor([[288., 289., 290., 291., 292.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([289.], device='cuda:0') data.h5py: 289 tensor([[289., 290., 291., 292., 293.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([290.], device='cuda:0') data.h5py: 290 tensor([[290., 291., 292., 293., 294.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([291.], device='cuda:0') data.h5py: 291 tensor([[291., 292., 293., 294., 295.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([292.], device='cuda:0') data.h5py: 292 tensor([[292., 293., 294., 295., 296.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([293.], device='cuda:0') data.h5py: 293 tensor([[293., 294., 295., 296., 297.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([294.], device='cuda:0') data.h5py: 294 tensor([[294., 295., 296., 297., 298.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([295.], device='cuda:0') data.h5py: 295 tensor([[295., 296., 297., 298., 299.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([296.], device='cuda:0') data.h5py: 296 tensor([[296., 297., 298., 299., 300.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([297.], device='cuda:0') data.h5py: 297 tensor([[297., 298., 299., 300., 301.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([298.], device='cuda:0') data.h5py: 298 tensor([[298., 299., 300., 301., 302.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([299.], device='cuda:0') data.h5py: 299 tensor([[299., 300., 301., 302., 303.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([300.], device='cuda:0') data.h5py: 300 tensor([[300., 301., 302., 303., 304.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([301.], device='cuda:0') data.h5py: 301 tensor([[301., 302., 303., 304., 305.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([302.], device='cuda:0') data.h5py: 302 tensor([[302., 303., 304., 305., 306.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([303.], device='cuda:0') data.h5py: 303 tensor([[303., 304., 305., 306., 307.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([304.], device='cuda:0') data.h5py: 304 tensor([[304., 305., 306., 307., 308.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([305.], device='cuda:0') data.h5py: 305 tensor([[305., 306., 307., 308., 309.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([306.], device='cuda:0') data.h5py: 306 tensor([[306., 307., 308., 309., 310.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([307.], device='cuda:0') data.h5py: 307 tensor([[307., 308., 309., 310., 311.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([308.], device='cuda:0') data.h5py: 308 tensor([[308., 309., 310., 311., 312.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([309.], device='cuda:0') data.h5py: 309 tensor([[309., 310., 311., 312., 313.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([310.], device='cuda:0') data.h5py: 310 tensor([[310., 311., 312., 313., 314.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([311.], device='cuda:0') data.h5py: 311 tensor([[311., 312., 313., 314., 315.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([312.], device='cuda:0') data.h5py: 312 tensor([[312., 313., 314., 315., 316.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([313.], device='cuda:0') data.h5py: 313 tensor([[313., 314., 315., 316., 317.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([314.], device='cuda:0') data.h5py: 314 tensor([[314., 315., 316., 317., 318.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([315.], device='cuda:0') data.h5py: 315 tensor([[315., 316., 317., 318., 319.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([316.], device='cuda:0') data.h5py: 316 tensor([[316., 317., 318., 319., 320.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([317.], device='cuda:0') data.h5py: 317 tensor([[317., 318., 319., 320., 321.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([318.], device='cuda:0') data.h5py: 318 tensor([[318., 319., 320., 321., 322.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([319.], device='cuda:0') data.h5py: 319 tensor([[319., 320., 321., 322., 323.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([320.], device='cuda:0') data.h5py: 320 tensor([[320., 321., 322., 323., 324.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([321.], device='cuda:0') data.h5py: 321 tensor([[321., 322., 323., 324., 325.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([322.], device='cuda:0') data.h5py: 322 tensor([[322., 323., 324., 325., 326.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([323.], device='cuda:0') data.h5py: 323 tensor([[323., 324., 325., 326., 327.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([324.], device='cuda:0') data.h5py: 324 tensor([[324., 325., 326., 327., 328.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([325.], device='cuda:0') data.h5py: 325 tensor([[325., 326., 327., 328., 329.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([326.], device='cuda:0') data.h5py: 326 tensor([[326., 327., 328., 329., 330.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([327.], device='cuda:0') data.h5py: 327 tensor([[327., 328., 329., 330., 331.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([328.], device='cuda:0') data.h5py: 328 tensor([[328., 329., 330., 331., 332.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([329.], device='cuda:0') data.h5py: 329 tensor([[329., 330., 331., 332., 333.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([330.], device='cuda:0') data.h5py: 330 tensor([[330., 331., 332., 333., 334.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([331.], device='cuda:0') data.h5py: 331 tensor([[331., 332., 333., 334., 335.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([332.], device='cuda:0') data.h5py: 332 tensor([[332., 333., 334., 335., 336.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([333.], device='cuda:0') data.h5py: 333 tensor([[333., 334., 335., 336., 337.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([334.], device='cuda:0') data.h5py: 334 tensor([[334., 335., 336., 337., 338.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([335.], device='cuda:0') data.h5py: 335 tensor([[335., 336., 337., 338., 339.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([336.], device='cuda:0') data.h5py: 336 tensor([[336., 337., 338., 339., 340.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([337.], device='cuda:0') data.h5py: 337 tensor([[337., 338., 339., 340., 341.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([338.], device='cuda:0') data.h5py: 338 tensor([[338., 339., 340., 341., 342.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([339.], device='cuda:0') data.h5py: 339 tensor([[339., 340., 341., 342., 343.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([340.], device='cuda:0') data.h5py: 340 tensor([[340., 341., 342., 343., 344.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([341.], device='cuda:0') data.h5py: 341 tensor([[341., 342., 343., 344., 345.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([342.], device='cuda:0') data.h5py: 342 tensor([[342., 343., 344., 345., 346.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([343.], device='cuda:0') data.h5py: 343 tensor([[343., 344., 345., 346., 347.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([344.], device='cuda:0') data.h5py: 344 tensor([[344., 345., 346., 347., 348.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([345.], device='cuda:0') data.h5py: 345 tensor([[345., 346., 347., 348., 349.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([346.], device='cuda:0') data.h5py: 346 tensor([[346., 347., 348., 349., 350.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([347.], device='cuda:0') data.h5py: 347 tensor([[347., 348., 349., 350., 351.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([348.], device='cuda:0') data.h5py: 348 tensor([[348., 349., 350., 351., 352.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([349.], device='cuda:0') data.h5py: 349 tensor([[349., 350., 351., 352., 353.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([350.], device='cuda:0') data.h5py: 350 tensor([[350., 351., 352., 353., 354.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([351.], device='cuda:0') data.h5py: 351 tensor([[351., 352., 353., 354., 355.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([352.], device='cuda:0') data.h5py: 352 tensor([[352., 353., 354., 355., 356.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([353.], device='cuda:0') data.h5py: 353 tensor([[353., 354., 355., 356., 357.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([354.], device='cuda:0') data.h5py: 354 tensor([[354., 355., 356., 357., 358.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([355.], device='cuda:0') data.h5py: 355 tensor([[355., 356., 357., 358., 359.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([356.], device='cuda:0') data.h5py: 356 tensor([[356., 357., 358., 359., 360.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([357.], device='cuda:0') data.h5py: 357 tensor([[357., 358., 359., 360., 361.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([358.], device='cuda:0') data.h5py: 358 tensor([[358., 359., 360., 361., 362.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([359.], device='cuda:0') data.h5py: 359 tensor([[359., 360., 361., 362., 363.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([360.], device='cuda:0') data.h5py: 360 tensor([[360., 361., 362., 363., 364.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([361.], device='cuda:0') data.h5py: 361 tensor([[361., 362., 363., 364., 365.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([362.], device='cuda:0') data.h5py: 362 tensor([[362., 363., 364., 365., 366.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([363.], device='cuda:0') data.h5py: 363 tensor([[363., 364., 365., 366., 367.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([364.], device='cuda:0') data.h5py: 364 tensor([[364., 365., 366., 367., 368.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([365.], device='cuda:0') data.h5py: 365 tensor([[365., 366., 367., 368., 369.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([366.], device='cuda:0') data.h5py: 366 tensor([[366., 367., 368., 369., 370.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([367.], device='cuda:0') data.h5py: 367 tensor([[367., 368., 369., 370., 371.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([368.], device='cuda:0') data.h5py: 368 tensor([[368., 369., 370., 371., 372.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([369.], device='cuda:0') data.h5py: 369 tensor([[369., 370., 371., 372., 373.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([370.], device='cuda:0') data.h5py: 370 tensor([[370., 371., 372., 373., 374.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([371.], device='cuda:0') data.h5py: 371 tensor([[371., 372., 373., 374., 375.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([372.], device='cuda:0') data.h5py: 372 tensor([[372., 373., 374., 375., 376.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([373.], device='cuda:0') data.h5py: 373 tensor([[373., 374., 375., 376., 377.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([374.], device='cuda:0') data.h5py: 374 tensor([[374., 375., 376., 377., 378.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([375.], device='cuda:0') data.h5py: 375 tensor([[375., 376., 377., 378., 379.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([376.], device='cuda:0') data.h5py: 376 tensor([[376., 377., 378., 379., 380.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([377.], device='cuda:0') data.h5py: 377 tensor([[377., 378., 379., 380., 381.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([378.], device='cuda:0') data.h5py: 378 tensor([[378., 379., 380., 381., 382.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([379.], device='cuda:0') data.h5py: 379 tensor([[379., 380., 381., 382., 383.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([380.], device='cuda:0') data.h5py: 380 tensor([[380., 381., 382., 383., 384.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([381.], device='cuda:0') data.h5py: 381 tensor([[381., 382., 383., 384., 385.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([382.], device='cuda:0') data.h5py: 382 tensor([[382., 383., 384., 385., 386.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([383.], device='cuda:0') data.h5py: 383 tensor([[383., 384., 385., 386., 387.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([384.], device='cuda:0') data.h5py: 384 tensor([[384., 385., 386., 387., 388.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([385.], device='cuda:0') data.h5py: 385 tensor([[385., 386., 387., 388., 389.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([386.], device='cuda:0') data.h5py: 386 tensor([[386., 387., 388., 389., 390.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([387.], device='cuda:0') data.h5py: 387 tensor([[387., 388., 389., 390., 391.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([388.], device='cuda:0') data.h5py: 388 tensor([[388., 389., 390., 391., 392.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([389.], device='cuda:0') data.h5py: 389 tensor([[389., 390., 391., 392., 393.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([390.], device='cuda:0') data.h5py: 390 tensor([[390., 391., 392., 393., 394.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([391.], device='cuda:0') data.h5py: 391 tensor([[391., 392., 393., 394., 395.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([392.], device='cuda:0') data.h5py: 392 tensor([[392., 393., 394., 395., 396.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([393.], device='cuda:0') data.h5py: 393 tensor([[393., 394., 395., 396., 397.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([394.], device='cuda:0') data.h5py: 394 tensor([[394., 395., 396., 397., 398.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([395.], device='cuda:0') data.h5py: 395 tensor([[395., 396., 397., 398., 399.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([396.], device='cuda:0') data.h5py: 396 tensor([[396., 397., 398., 399., 400.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([397.], device='cuda:0') data.h5py: 397 tensor([[397., 398., 399., 400., 401.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([398.], device='cuda:0') data.h5py: 398 tensor([[398., 399., 400., 401., 402.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([399.], device='cuda:0') data.h5py: 399 tensor([[399., 400., 401., 402., 403.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([400.], device='cuda:0') data.h5py: 400 tensor([[400., 401., 402., 403., 404.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([401.], device='cuda:0') data.h5py: 401 tensor([[401., 402., 403., 404., 405.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([402.], device='cuda:0') data.h5py: 402 tensor([[402., 403., 404., 405., 406.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([403.], device='cuda:0') data.h5py: 403 tensor([[403., 404., 405., 406., 407.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([404.], device='cuda:0') data.h5py: 404 tensor([[404., 405., 406., 407., 408.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([405.], device='cuda:0') data.h5py: 405 tensor([[405., 406., 407., 408., 409.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([406.], device='cuda:0') data.h5py: 406 tensor([[406., 407., 408., 409., 410.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([407.], device='cuda:0') data.h5py: 407 tensor([[407., 408., 409., 410., 411.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([408.], device='cuda:0') data.h5py: 408 tensor([[408., 409., 410., 411., 412.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([409.], device='cuda:0') data.h5py: 409 tensor([[409., 410., 411., 412., 413.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([410.], device='cuda:0') data.h5py: 410 tensor([[410., 411., 412., 413., 414.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([411.], device='cuda:0') data.h5py: 411 tensor([[411., 412., 413., 414., 415.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([412.], device='cuda:0') data.h5py: 412 tensor([[412., 413., 414., 415., 416.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([413.], device='cuda:0') data.h5py: 413 tensor([[413., 414., 415., 416., 417.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([414.], device='cuda:0') data.h5py: 414 tensor([[414., 415., 416., 417., 418.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([415.], device='cuda:0') data.h5py: 415 tensor([[415., 416., 417., 418., 419.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([416.], device='cuda:0') data.h5py: 416 tensor([[416., 417., 418., 419., 420.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([417.], device='cuda:0') data.h5py: 417 tensor([[417., 418., 419., 420., 421.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([418.], device='cuda:0') data.h5py: 418 tensor([[418., 419., 420., 421., 422.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([419.], device='cuda:0') data.h5py: 419 tensor([[419., 420., 421., 422., 423.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([420.], device='cuda:0') data.h5py: 420 tensor([[420., 421., 422., 423., 424.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([421.], device='cuda:0') data.h5py: 421 tensor([[421., 422., 423., 424., 425.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([422.], device='cuda:0') data.h5py: 422 tensor([[422., 423., 424., 425., 426.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([423.], device='cuda:0') data.h5py: 423 tensor([[423., 424., 425., 426., 427.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([424.], device='cuda:0') data.h5py: 424 tensor([[424., 425., 426., 427., 428.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([425.], device='cuda:0') data.h5py: 425 tensor([[425., 426., 427., 428., 429.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([426.], device='cuda:0') data.h5py: 426 tensor([[426., 427., 428., 429., 430.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([427.], device='cuda:0') data.h5py: 427 tensor([[427., 428., 429., 430., 431.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([428.], device='cuda:0') data.h5py: 428 tensor([[428., 429., 430., 431., 432.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([429.], device='cuda:0') data.h5py: 429 tensor([[429., 430., 431., 432., 433.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([430.], device='cuda:0') data.h5py: 430 tensor([[430., 431., 432., 433., 434.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([431.], device='cuda:0') data.h5py: 431 tensor([[431., 432., 433., 434., 435.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([432.], device='cuda:0') data.h5py: 432 tensor([[432., 433., 434., 435., 436.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([433.], device='cuda:0') data.h5py: 433 tensor([[433., 434., 435., 436., 437.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([434.], device='cuda:0') data.h5py: 434 tensor([[434., 435., 436., 437., 438.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([435.], device='cuda:0') data.h5py: 435 tensor([[435., 436., 437., 438., 439.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([436.], device='cuda:0') data.h5py: 436 tensor([[436., 437., 438., 439., 440.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([437.], device='cuda:0') data.h5py: 437 tensor([[437., 438., 439., 440., 441.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([438.], device='cuda:0') data.h5py: 438 tensor([[438., 439., 440., 441., 442.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([439.], device='cuda:0') data.h5py: 439 tensor([[439., 440., 441., 442., 443.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([440.], device='cuda:0') data.h5py: 440 tensor([[440., 441., 442., 443., 444.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([441.], device='cuda:0') data.h5py: 441 tensor([[441., 442., 443., 444., 445.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([442.], device='cuda:0') data.h5py: 442 tensor([[442., 443., 444., 445., 446.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([443.], device='cuda:0') data.h5py: 443 tensor([[443., 444., 445., 446., 447.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([444.], device='cuda:0') data.h5py: 444 tensor([[444., 445., 446., 447., 448.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([445.], device='cuda:0') data.h5py: 445 tensor([[445., 446., 447., 448., 449.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([446.], device='cuda:0') data.h5py: 446 tensor([[446., 447., 448., 449., 450.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([447.], device='cuda:0') data.h5py: 447 tensor([[447., 448., 449., 450., 451.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([448.], device='cuda:0') data.h5py: 448 tensor([[448., 449., 450., 451., 452.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([449.], device='cuda:0') data.h5py: 449 tensor([[449., 450., 451., 452., 453.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([450.], device='cuda:0') data.h5py: 450 tensor([[450., 451., 452., 453., 454.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([451.], device='cuda:0') data.h5py: 451 tensor([[451., 452., 453., 454., 455.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([452.], device='cuda:0') data.h5py: 452 tensor([[452., 453., 454., 455., 456.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([453.], device='cuda:0') data.h5py: 453 tensor([[453., 454., 455., 456., 457.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([454.], device='cuda:0') data.h5py: 454 tensor([[454., 455., 456., 457., 458.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([455.], device='cuda:0') data.h5py: 455 tensor([[455., 456., 457., 458., 459.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([456.], device='cuda:0') data.h5py: 456 tensor([[456., 457., 458., 459., 460.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([457.], device='cuda:0') data.h5py: 457 tensor([[457., 458., 459., 460., 461.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([458.], device='cuda:0') data.h5py: 458 tensor([[458., 459., 460., 461., 462.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([459.], device='cuda:0') data.h5py: 459 tensor([[459., 460., 461., 462., 463.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([460.], device='cuda:0') data.h5py: 460 tensor([[460., 461., 462., 463., 464.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([461.], device='cuda:0') data.h5py: 461 tensor([[461., 462., 463., 464., 465.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([462.], device='cuda:0') data.h5py: 462 tensor([[462., 463., 464., 465., 466.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([463.], device='cuda:0') data.h5py: 463 tensor([[463., 464., 465., 466., 467.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([464.], device='cuda:0') data.h5py: 464 tensor([[464., 465., 466., 467., 468.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([465.], device='cuda:0') data.h5py: 465 tensor([[465., 466., 467., 468., 469.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([466.], device='cuda:0') data.h5py: 466 tensor([[466., 467., 468., 469., 470.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([467.], device='cuda:0') data.h5py: 467 tensor([[467., 468., 469., 470., 471.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([468.], device='cuda:0') data.h5py: 468 tensor([[468., 469., 470., 471., 472.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([469.], device='cuda:0') data.h5py: 469 tensor([[469., 470., 471., 472., 473.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([470.], device='cuda:0') data.h5py: 470 tensor([[470., 471., 472., 473., 474.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([471.], device='cuda:0') data.h5py: 471 tensor([[471., 472., 473., 474., 475.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([472.], device='cuda:0') data.h5py: 472 tensor([[472., 473., 474., 475., 476.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([473.], device='cuda:0') data.h5py: 473 tensor([[473., 474., 475., 476., 477.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([474.], device='cuda:0') data.h5py: 474 tensor([[474., 475., 476., 477., 478.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([475.], device='cuda:0') data.h5py: 475 tensor([[475., 476., 477., 478., 479.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([476.], device='cuda:0') data.h5py: 476 tensor([[476., 477., 478., 479., 480.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([477.], device='cuda:0') data.h5py: 477 tensor([[477., 478., 479., 480., 481.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([478.], device='cuda:0') data.h5py: 478 tensor([[478., 479., 480., 481., 482.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([479.], device='cuda:0') data.h5py: 479 tensor([[479., 480., 481., 482., 483.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([480.], device='cuda:0') data.h5py: 480 tensor([[480., 481., 482., 483., 484.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([481.], device='cuda:0') data.h5py: 481 tensor([[481., 482., 483., 484., 485.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([482.], device='cuda:0') data.h5py: 482 tensor([[482., 483., 484., 485., 486.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([483.], device='cuda:0') data.h5py: 483 tensor([[483., 484., 485., 486., 487.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([484.], device='cuda:0') data.h5py: 484 tensor([[484., 485., 486., 487., 488.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([485.], device='cuda:0') data.h5py: 485 tensor([[485., 486., 487., 488., 489.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([486.], device='cuda:0') data.h5py: 486 tensor([[486., 487., 488., 489., 490.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([487.], device='cuda:0') data.h5py: 487 tensor([[487., 488., 489., 490., 491.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([488.], device='cuda:0') data.h5py: 488 tensor([[488., 489., 490., 491., 492.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([489.], device='cuda:0') data.h5py: 489 tensor([[489., 490., 491., 492., 493.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([490.], device='cuda:0') data.h5py: 490 tensor([[490., 491., 492., 493., 494.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([491.], device='cuda:0') data.h5py: 491 tensor([[491., 492., 493., 494., 495.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([492.], device='cuda:0') data.h5py: 492 tensor([[492., 493., 494., 495., 496.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([493.], device='cuda:0') data.h5py: 493 tensor([[493., 494., 495., 496., 497.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([494.], device='cuda:0') data.h5py: 494 tensor([[494., 495., 496., 497., 498.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([495.], device='cuda:0') data.h5py: 495 tensor([[495., 496., 497., 498., 499.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([496.], device='cuda:0') data.h5py: 496 tensor([[496., 497., 498., 499., 500.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([497.], device='cuda:0') data.h5py: 497 tensor([[497., 498., 499., 500., 501.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([498.], device='cuda:0') data.h5py: 498 tensor([[498., 499., 500., 501., 502.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([499.], device='cuda:0') data.h5py: 499 tensor([[499., 500., 501., 502., 503.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([500.], device='cuda:0') data.h5py: 500 tensor([[500., 501., 502., 503., 504.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([501.], device='cuda:0') data.h5py: 501 tensor([[501., 502., 503., 504., 505.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([502.], device='cuda:0') data.h5py: 502 tensor([[502., 503., 504., 505., 506.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([503.], device='cuda:0') data.h5py: 503 tensor([[503., 504., 505., 506., 507.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([504.], device='cuda:0') data.h5py: 504 tensor([[504., 505., 506., 507., 508.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([505.], device='cuda:0') data.h5py: 505 tensor([[505., 506., 507., 508., 509.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([506.], device='cuda:0') data.h5py: 506 tensor([[506., 507., 508., 509., 510.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([507.], device='cuda:0') data.h5py: 507 tensor([[507., 508., 509., 510., 511.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([508.], device='cuda:0') data.h5py: 508 tensor([[508., 509., 510., 511., 512.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([509.], device='cuda:0') data.h5py: 509 tensor([[509., 510., 511., 512., 513.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([510.], device='cuda:0') data.h5py: 510 tensor([[510., 511., 512., 513., 514.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([511.], device='cuda:0') data.h5py: 511 tensor([[511., 512., 513., 514., 515.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([512.], device='cuda:0') data.h5py: 512 tensor([[512., 513., 514., 515., 516.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([513.], device='cuda:0') data.h5py: 513 tensor([[513., 514., 515., 516., 517.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([514.], device='cuda:0') data.h5py: 514 tensor([[514., 515., 516., 517., 518.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([515.], device='cuda:0') data.h5py: 515 tensor([[515., 516., 517., 518., 519.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([516.], device='cuda:0') data.h5py: 516 tensor([[516., 517., 518., 519., 520.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([517.], device='cuda:0') data.h5py: 517 tensor([[517., 518., 519., 520., 521.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([518.], device='cuda:0') data.h5py: 518 tensor([[518., 519., 520., 521., 522.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([519.], device='cuda:0') data.h5py: 519 tensor([[519., 520., 521., 522., 523.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([520.], device='cuda:0') data.h5py: 520 tensor([[520., 521., 522., 523., 524.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([521.], device='cuda:0') data.h5py: 521 tensor([[521., 522., 523., 524., 525.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([522.], device='cuda:0') data.h5py: 522 tensor([[522., 523., 524., 525., 526.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([523.], device='cuda:0') data.h5py: 523 tensor([[523., 524., 525., 526., 527.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([524.], device='cuda:0') data.h5py: 524 tensor([[524., 525., 526., 527., 528.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([525.], device='cuda:0') data.h5py: 525 tensor([[525., 526., 527., 528., 529.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([526.], device='cuda:0') data.h5py: 526 tensor([[526., 527., 528., 529., 530.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([527.], device='cuda:0') data.h5py: 527 tensor([[527., 528., 529., 530., 531.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([528.], device='cuda:0') data.h5py: 528 tensor([[528., 529., 530., 531., 532.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([529.], device='cuda:0') data.h5py: 529 tensor([[529., 530., 531., 532., 533.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([530.], device='cuda:0') data.h5py: 530 tensor([[530., 531., 532., 533., 534.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([531.], device='cuda:0') data.h5py: 531 tensor([[531., 532., 533., 534., 535.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([532.], device='cuda:0') data.h5py: 532 tensor([[532., 533., 534., 535., 536.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([533.], device='cuda:0') data.h5py: 533 tensor([[533., 534., 535., 536., 537.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([534.], device='cuda:0') data.h5py: 534 tensor([[534., 535., 536., 537., 538.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([535.], device='cuda:0') data.h5py: 535 tensor([[535., 536., 537., 538., 539.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([536.], device='cuda:0') data.h5py: 536 tensor([[536., 537., 538., 539., 540.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([537.], device='cuda:0') data.h5py: 537 tensor([[537., 538., 539., 540., 541.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([538.], device='cuda:0') data.h5py: 538 tensor([[538., 539., 540., 541., 542.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([539.], device='cuda:0') data.h5py: 539 tensor([[539., 540., 541., 542., 543.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([540.], device='cuda:0') data.h5py: 540 tensor([[540., 541., 542., 543., 544.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([541.], device='cuda:0') data.h5py: 541 tensor([[541., 542., 543., 544., 545.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([542.], device='cuda:0') data.h5py: 542 tensor([[542., 543., 544., 545., 546.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([543.], device='cuda:0') data.h5py: 543 tensor([[543., 544., 545., 546., 547.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([544.], device='cuda:0') data.h5py: 544 tensor([[544., 545., 546., 547., 548.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([545.], device='cuda:0') data.h5py: 545 tensor([[545., 546., 547., 548., 549.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([546.], device='cuda:0') data.h5py: 546 tensor([[546., 547., 548., 549., 550.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([547.], device='cuda:0') data.h5py: 547 tensor([[547., 548., 549., 550., 551.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([548.], device='cuda:0') data.h5py: 548 tensor([[548., 549., 550., 551., 552.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([549.], device='cuda:0') data.h5py: 549 tensor([[549., 550., 551., 552., 553.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([550.], device='cuda:0') data.h5py: 550 tensor([[550., 551., 552., 553., 554.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([551.], device='cuda:0') data.h5py: 551 tensor([[551., 552., 553., 554., 555.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([552.], device='cuda:0') data.h5py: 552 tensor([[552., 553., 554., 555., 556.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([553.], device='cuda:0') data.h5py: 553 tensor([[553., 554., 555., 556., 557.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([554.], device='cuda:0') data.h5py: 554 tensor([[554., 555., 556., 557., 558.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([555.], device='cuda:0') data.h5py: 555 tensor([[555., 556., 557., 558., 559.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([556.], device='cuda:0') data.h5py: 556 tensor([[556., 557., 558., 559., 560.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([557.], device='cuda:0') data.h5py: 557 tensor([[557., 558., 559., 560., 561.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([558.], device='cuda:0') data.h5py: 558 tensor([[558., 559., 560., 561., 562.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([559.], device='cuda:0') data.h5py: 559 tensor([[559., 560., 561., 562., 563.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([560.], device='cuda:0') data.h5py: 560 tensor([[560., 561., 562., 563., 564.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([561.], device='cuda:0') data.h5py: 561 tensor([[561., 562., 563., 564., 565.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([562.], device='cuda:0') data.h5py: 562 tensor([[562., 563., 564., 565., 566.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([563.], device='cuda:0') data.h5py: 563 tensor([[563., 564., 565., 566., 567.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([564.], device='cuda:0') data.h5py: 564 tensor([[564., 565., 566., 567., 568.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([565.], device='cuda:0') data.h5py: 565 tensor([[565., 566., 567., 568., 569.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([566.], device='cuda:0') data.h5py: 566 tensor([[566., 567., 568., 569., 570.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([567.], device='cuda:0') data.h5py: 567 tensor([[567., 568., 569., 570., 571.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([568.], device='cuda:0') data.h5py: 568 tensor([[568., 569., 570., 571., 572.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([569.], device='cuda:0') data.h5py: 569 tensor([[569., 570., 571., 572., 573.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([570.], device='cuda:0') data.h5py: 570 tensor([[570., 571., 572., 573., 574.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([571.], device='cuda:0') data.h5py: 571 tensor([[571., 572., 573., 574., 575.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([572.], device='cuda:0') data.h5py: 572 tensor([[572., 573., 574., 575., 576.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([573.], device='cuda:0') data.h5py: 573 tensor([[573., 574., 575., 576., 577.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([574.], device='cuda:0') data.h5py: 574 tensor([[574., 575., 576., 577., 578.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([575.], device='cuda:0') data.h5py: 575 tensor([[575., 576., 577., 578., 579.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([576.], device='cuda:0') data.h5py: 576 tensor([[576., 577., 578., 579., 580.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([577.], device='cuda:0') data.h5py: 577 tensor([[577., 578., 579., 580., 581.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([578.], device='cuda:0') data.h5py: 578 tensor([[578., 579., 580., 581., 582.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([579.], device='cuda:0') data.h5py: 579 tensor([[579., 580., 581., 582., 583.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([580.], device='cuda:0') data.h5py: 580 tensor([[580., 581., 582., 583., 584.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([581.], device='cuda:0') data.h5py: 581 tensor([[581., 582., 583., 584., 585.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([582.], device='cuda:0') data.h5py: 582 tensor([[582., 583., 584., 585., 586.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([583.], device='cuda:0') data.h5py: 583 tensor([[583., 584., 585., 586., 587.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([584.], device='cuda:0') data.h5py: 584 tensor([[584., 585., 586., 587., 588.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([585.], device='cuda:0') data.h5py: 585 tensor([[585., 586., 587., 588., 589.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([586.], device='cuda:0') data.h5py: 586 tensor([[586., 587., 588., 589., 590.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([587.], device='cuda:0') data.h5py: 587 tensor([[587., 588., 589., 590., 591.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([588.], device='cuda:0') data.h5py: 588 tensor([[588., 589., 590., 591., 592.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([589.], device='cuda:0') data.h5py: 589 tensor([[589., 590., 591., 592., 593.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([590.], device='cuda:0') data.h5py: 590 tensor([[590., 591., 592., 593., 594.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([591.], device='cuda:0') data.h5py: 591 tensor([[591., 592., 593., 594., 595.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([592.], device='cuda:0') data.h5py: 592 tensor([[592., 593., 594., 595., 596.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([593.], device='cuda:0') data.h5py: 593 tensor([[593., 594., 595., 596., 597.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([594.], device='cuda:0') data.h5py: 594 tensor([[594., 595., 596., 597., 598.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([595.], device='cuda:0') data.h5py: 595 tensor([[595., 596., 597., 598., 599.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([596.], device='cuda:0') data.h5py: 596 tensor([[596., 597., 598., 599., 600.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([597.], device='cuda:0') data.h5py: 597 tensor([[597., 598., 599., 600., 601.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([598.], device='cuda:0') data.h5py: 598 tensor([[598., 599., 600., 601., 602.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([599.], device='cuda:0') data.h5py: 599 tensor([[599., 600., 601., 602., 603.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([600.], device='cuda:0') data.h5py: 600 tensor([[600., 601., 602., 603., 604.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([601.], device='cuda:0') data.h5py: 601 tensor([[601., 602., 603., 604., 605.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([602.], device='cuda:0') data.h5py: 602 tensor([[602., 603., 604., 605., 606.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([603.], device='cuda:0') data.h5py: 603 tensor([[603., 604., 605., 606., 607.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([604.], device='cuda:0') data.h5py: 604 tensor([[604., 605., 606., 607., 608.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([605.], device='cuda:0') data.h5py: 605 tensor([[605., 606., 607., 608., 609.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([606.], device='cuda:0') data.h5py: 606 tensor([[606., 607., 608., 609., 610.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([607.], device='cuda:0') data.h5py: 607 tensor([[607., 608., 609., 610., 611.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([608.], device='cuda:0') data.h5py: 608 tensor([[608., 609., 610., 611., 612.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([609.], device='cuda:0') data.h5py: 609 tensor([[609., 610., 611., 612., 613.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([610.], device='cuda:0') data.h5py: 610 tensor([[610., 611., 612., 613., 614.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([611.], device='cuda:0') data.h5py: 611 tensor([[611., 612., 613., 614., 615.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([612.], device='cuda:0') data.h5py: 612 tensor([[612., 613., 614., 615., 616.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([613.], device='cuda:0') data.h5py: 613 tensor([[613., 614., 615., 616., 617.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([614.], device='cuda:0') data.h5py: 614 tensor([[614., 615., 616., 617., 618.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([615.], device='cuda:0') data.h5py: 615 tensor([[615., 616., 617., 618., 619.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([616.], device='cuda:0') data.h5py: 616 tensor([[616., 617., 618., 619., 620.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([617.], device='cuda:0') data.h5py: 617 tensor([[617., 618., 619., 620., 621.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([618.], device='cuda:0') data.h5py: 618 tensor([[618., 619., 620., 621., 622.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([619.], device='cuda:0') data.h5py: 619 tensor([[619., 620., 621., 622., 623.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([620.], device='cuda:0') data.h5py: 620 tensor([[620., 621., 622., 623., 624.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([621.], device='cuda:0') data.h5py: 621 tensor([[621., 622., 623., 624., 625.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([622.], device='cuda:0') data.h5py: 622 tensor([[622., 623., 624., 625., 626.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([623.], device='cuda:0') data.h5py: 623 tensor([[623., 624., 625., 626., 627.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([624.], device='cuda:0') data.h5py: 624 tensor([[624., 625., 626., 627., 628.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([625.], device='cuda:0') data.h5py: 625 tensor([[625., 626., 627., 628., 629.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([626.], device='cuda:0') data.h5py: 626 tensor([[626., 627., 628., 629., 630.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([627.], device='cuda:0') data.h5py: 627 tensor([[627., 628., 629., 630., 631.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([628.], device='cuda:0') data.h5py: 628 tensor([[628., 629., 630., 631., 632.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([629.], device='cuda:0') data.h5py: 629 tensor([[629., 630., 631., 632., 633.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([630.], device='cuda:0') data.h5py: 630 tensor([[630., 631., 632., 633., 634.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([631.], device='cuda:0') data.h5py: 631 tensor([[631., 632., 633., 634., 635.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([632.], device='cuda:0') data.h5py: 632 tensor([[632., 633., 634., 635., 636.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([633.], device='cuda:0') data.h5py: 633 tensor([[633., 634., 635., 636., 637.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([634.], device='cuda:0') data.h5py: 634 tensor([[634., 635., 636., 637., 638.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([635.], device='cuda:0') data.h5py: 635 tensor([[635., 636., 637., 638., 639.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([636.], device='cuda:0') data.h5py: 636 tensor([[636., 637., 638., 639., 640.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([637.], device='cuda:0') data.h5py: 637 tensor([[637., 638., 639., 640., 641.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([638.], device='cuda:0') data.h5py: 638 tensor([[638., 639., 640., 641., 642.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([639.], device='cuda:0') data.h5py: 639 tensor([[639., 640., 641., 642., 643.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([640.], device='cuda:0') data.h5py: 640 tensor([[640., 641., 642., 643., 644.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([641.], device='cuda:0') data.h5py: 641 tensor([[641., 642., 643., 644., 645.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([642.], device='cuda:0') data.h5py: 642 tensor([[642., 643., 644., 645., 646.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([643.], device='cuda:0') data.h5py: 643 tensor([[643., 644., 645., 646., 647.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([644.], device='cuda:0') data.h5py: 644 tensor([[644., 645., 646., 647., 648.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([645.], device='cuda:0') data.h5py: 645 tensor([[645., 646., 647., 648., 649.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([646.], device='cuda:0') data.h5py: 646 tensor([[646., 647., 648., 649., 650.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([647.], device='cuda:0') data.h5py: 647 tensor([[647., 648., 649., 650., 651.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([648.], device='cuda:0') data.h5py: 648 tensor([[648., 649., 650., 651., 652.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([649.], device='cuda:0') data.h5py: 649 tensor([[649., 650., 651., 652., 653.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([650.], device='cuda:0') data.h5py: 650 tensor([[650., 651., 652., 653., 654.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([651.], device='cuda:0') data.h5py: 651 tensor([[651., 652., 653., 654., 655.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([652.], device='cuda:0') data.h5py: 652 tensor([[652., 653., 654., 655., 656.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([653.], device='cuda:0') data.h5py: 653 tensor([[653., 654., 655., 656., 657.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([654.], device='cuda:0') data.h5py: 654 tensor([[654., 655., 656., 657., 658.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([655.], device='cuda:0') data.h5py: 655 tensor([[655., 656., 657., 658., 659.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([656.], device='cuda:0') data.h5py: 656 tensor([[656., 657., 658., 659., 660.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([657.], device='cuda:0') data.h5py: 657 tensor([[657., 658., 659., 660., 661.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([658.], device='cuda:0') data.h5py: 658 tensor([[658., 659., 660., 661., 662.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([659.], device='cuda:0') data.h5py: 659 tensor([[659., 660., 661., 662., 663.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([660.], device='cuda:0') data.h5py: 660 tensor([[660., 661., 662., 663., 664.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([661.], device='cuda:0') data.h5py: 661 tensor([[661., 662., 663., 664., 665.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([662.], device='cuda:0') data.h5py: 662 tensor([[662., 663., 664., 665., 666.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([663.], device='cuda:0') data.h5py: 663 tensor([[663., 664., 665., 666., 667.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([664.], device='cuda:0') data.h5py: 664 tensor([[664., 665., 666., 667., 668.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([665.], device='cuda:0') data.h5py: 665 tensor([[665., 666., 667., 668., 669.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([666.], device='cuda:0') data.h5py: 666 tensor([[666., 667., 668., 669., 670.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([667.], device='cuda:0') data.h5py: 667 tensor([[667., 668., 669., 670., 671.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([668.], device='cuda:0') data.h5py: 668 tensor([[668., 669., 670., 671., 672.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([669.], device='cuda:0') data.h5py: 669 tensor([[669., 670., 671., 672., 673.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([670.], device='cuda:0') data.h5py: 670 tensor([[670., 671., 672., 673., 674.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([671.], device='cuda:0') data.h5py: 671 tensor([[671., 672., 673., 674., 675.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([672.], device='cuda:0') data.h5py: 672 tensor([[672., 673., 674., 675., 676.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([673.], device='cuda:0') data.h5py: 673 tensor([[673., 674., 675., 676., 677.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([674.], device='cuda:0') data.h5py: 674 tensor([[674., 675., 676., 677., 678.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([675.], device='cuda:0') data.h5py: 675 tensor([[675., 676., 677., 678., 679.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([676.], device='cuda:0') data.h5py: 676 tensor([[676., 677., 678., 679., 680.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([677.], device='cuda:0') data.h5py: 677 tensor([[677., 678., 679., 680., 681.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([678.], device='cuda:0') data.h5py: 678 tensor([[678., 679., 680., 681., 682.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([679.], device='cuda:0') data.h5py: 679 tensor([[679., 680., 681., 682., 683.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([680.], device='cuda:0') data.h5py: 680 tensor([[680., 681., 682., 683., 684.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([681.], device='cuda:0') data.h5py: 681 tensor([[681., 682., 683., 684., 685.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([682.], device='cuda:0') data.h5py: 682 tensor([[682., 683., 684., 685., 686.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([683.], device='cuda:0') data.h5py: 683 tensor([[683., 684., 685., 686., 687.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([684.], device='cuda:0') data.h5py: 684 tensor([[684., 685., 686., 687., 688.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([685.], device='cuda:0') data.h5py: 685 tensor([[685., 686., 687., 688., 689.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([686.], device='cuda:0') data.h5py: 686 tensor([[686., 687., 688., 689., 690.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([687.], device='cuda:0') data.h5py: 687 tensor([[687., 688., 689., 690., 691.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([688.], device='cuda:0') data.h5py: 688 tensor([[688., 689., 690., 691., 692.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([689.], device='cuda:0') data.h5py: 689 tensor([[689., 690., 691., 692., 693.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([690.], device='cuda:0') data.h5py: 690 tensor([[690., 691., 692., 693., 694.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([691.], device='cuda:0') data.h5py: 691 tensor([[691., 692., 693., 694., 695.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([692.], device='cuda:0') data.h5py: 692 tensor([[692., 693., 694., 695., 696.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([693.], device='cuda:0') data.h5py: 693 tensor([[693., 694., 695., 696., 697.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([694.], device='cuda:0') data.h5py: 694 tensor([[694., 695., 696., 697., 698.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([695.], device='cuda:0') data.h5py: 695 tensor([[695., 696., 697., 698., 699.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([696.], device='cuda:0') data.h5py: 696 tensor([[696., 697., 698., 699., 700.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([697.], device='cuda:0') data.h5py: 697 tensor([[697., 698., 699., 700., 701.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([698.], device='cuda:0') data.h5py: 698 tensor([[698., 699., 700., 701., 702.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([699.], device='cuda:0') data.h5py: 699 tensor([[699., 700., 701., 702., 703.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([700.], device='cuda:0') data.h5py: 700 tensor([[700., 701., 702., 703., 704.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([701.], device='cuda:0') data.h5py: 701 tensor([[701., 702., 703., 704., 705.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([702.], device='cuda:0') data.h5py: 702 tensor([[702., 703., 704., 705., 706.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([703.], device='cuda:0') data.h5py: 703 tensor([[703., 704., 705., 706., 707.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([704.], device='cuda:0') data.h5py: 704 tensor([[704., 705., 706., 707., 708.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([705.], device='cuda:0') data.h5py: 705 tensor([[705., 706., 707., 708., 709.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([706.], device='cuda:0') data.h5py: 706 tensor([[706., 707., 708., 709., 710.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([707.], device='cuda:0') data.h5py: 707 tensor([[707., 708., 709., 710., 711.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([708.], device='cuda:0') data.h5py: 708 tensor([[708., 709., 710., 711., 712.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([709.], device='cuda:0') data.h5py: 709 tensor([[709., 710., 711., 712., 713.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([710.], device='cuda:0') data.h5py: 710 tensor([[710., 711., 712., 713., 714.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([711.], device='cuda:0') data.h5py: 711 tensor([[711., 712., 713., 714., 715.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([712.], device='cuda:0') data.h5py: 712 tensor([[712., 713., 714., 715., 716.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([713.], device='cuda:0') data.h5py: 713 tensor([[713., 714., 715., 716., 717.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([714.], device='cuda:0') data.h5py: 714 tensor([[714., 715., 716., 717., 718.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([715.], device='cuda:0') data.h5py: 715 tensor([[715., 716., 717., 718., 719.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([716.], device='cuda:0') data.h5py: 716 tensor([[716., 717., 718., 719., 720.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([717.], device='cuda:0') data.h5py: 717 tensor([[717., 718., 719., 720., 721.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([718.], device='cuda:0') data.h5py: 718 tensor([[718., 719., 720., 721., 722.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([719.], device='cuda:0') data.h5py: 719 tensor([[719., 720., 721., 722., 723.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([720.], device='cuda:0') data.h5py: 720 tensor([[720., 721., 722., 723., 724.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([721.], device='cuda:0') data.h5py: 721 tensor([[721., 722., 723., 724., 725.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([722.], device='cuda:0') data.h5py: 722 tensor([[722., 723., 724., 725., 726.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([723.], device='cuda:0') data.h5py: 723 tensor([[723., 724., 725., 726., 727.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([724.], device='cuda:0') data.h5py: 724 tensor([[724., 725., 726., 727., 728.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([725.], device='cuda:0') data.h5py: 725 tensor([[725., 726., 727., 728., 729.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([726.], device='cuda:0') data.h5py: 726 tensor([[726., 727., 728., 729., 730.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([727.], device='cuda:0') data.h5py: 727 tensor([[727., 728., 729., 730., 731.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([728.], device='cuda:0') data.h5py: 728 tensor([[728., 729., 730., 731., 732.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([729.], device='cuda:0') data.h5py: 729 tensor([[729., 730., 731., 732., 733.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([730.], device='cuda:0') data.h5py: 730 tensor([[730., 731., 732., 733., 734.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([731.], device='cuda:0') data.h5py: 731 tensor([[731., 732., 733., 734., 735.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([732.], device='cuda:0') data.h5py: 732 tensor([[732., 733., 734., 735., 736.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([733.], device='cuda:0') data.h5py: 733 tensor([[733., 734., 735., 736., 737.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([734.], device='cuda:0') data.h5py: 734 tensor([[734., 735., 736., 737., 738.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([735.], device='cuda:0') data.h5py: 735 tensor([[735., 736., 737., 738., 739.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([736.], device='cuda:0') data.h5py: 736 tensor([[736., 737., 738., 739., 740.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([737.], device='cuda:0') data.h5py: 737 tensor([[737., 738., 739., 740., 741.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([738.], device='cuda:0') data.h5py: 738 tensor([[738., 739., 740., 741., 742.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([739.], device='cuda:0') data.h5py: 739 tensor([[739., 740., 741., 742., 743.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([740.], device='cuda:0') data.h5py: 740 tensor([[740., 741., 742., 743., 744.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([741.], device='cuda:0') data.h5py: 741 tensor([[741., 742., 743., 744., 745.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([742.], device='cuda:0') data.h5py: 742 tensor([[742., 743., 744., 745., 746.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([743.], device='cuda:0') data.h5py: 743 tensor([[743., 744., 745., 746., 747.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([744.], device='cuda:0') data.h5py: 744 tensor([[744., 745., 746., 747., 748.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([745.], device='cuda:0') data.h5py: 745 tensor([[745., 746., 747., 748., 749.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([746.], device='cuda:0') data.h5py: 746 tensor([[746., 747., 748., 749., 750.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([747.], device='cuda:0') data.h5py: 747 tensor([[747., 748., 749., 750., 751.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([748.], device='cuda:0') data.h5py: 748 tensor([[748., 749., 750., 751., 752.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([749.], device='cuda:0') data.h5py: 749 tensor([[749., 750., 751., 752., 753.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([750.], device='cuda:0') data.h5py: 750 tensor([[750., 751., 752., 753., 754.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([751.], device='cuda:0') data.h5py: 751 tensor([[751., 752., 753., 754., 755.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([752.], device='cuda:0') data.h5py: 752 tensor([[752., 753., 754., 755., 756.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([753.], device='cuda:0') data.h5py: 753 tensor([[753., 754., 755., 756., 757.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([754.], device='cuda:0') data.h5py: 754 tensor([[754., 755., 756., 757., 758.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([755.], device='cuda:0') data.h5py: 755 tensor([[755., 756., 757., 758., 759.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([756.], device='cuda:0') data.h5py: 756 tensor([[756., 757., 758., 759., 760.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([757.], device='cuda:0') data.h5py: 757 tensor([[757., 758., 759., 760., 761.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([758.], device='cuda:0') data.h5py: 758 tensor([[758., 759., 760., 761., 762.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([759.], device='cuda:0') data.h5py: 759 tensor([[759., 760., 761., 762., 763.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([760.], device='cuda:0') data.h5py: 760 tensor([[760., 761., 762., 763., 764.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([761.], device='cuda:0') data.h5py: 761 tensor([[761., 762., 763., 764., 765.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([762.], device='cuda:0') data.h5py: 762 tensor([[762., 763., 764., 765., 766.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([763.], device='cuda:0') data.h5py: 763 tensor([[763., 764., 765., 766., 767.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([764.], device='cuda:0') data.h5py: 764 tensor([[764., 765., 766., 767., 768.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([765.], device='cuda:0') data.h5py: 765 tensor([[765., 766., 767., 768., 769.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([766.], device='cuda:0') data.h5py: 766 tensor([[766., 767., 768., 769., 770.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([767.], device='cuda:0') data.h5py: 767 tensor([[767., 768., 769., 770., 771.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([768.], device='cuda:0') data.h5py: 768 tensor([[768., 769., 770., 771., 772.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([769.], device='cuda:0') data.h5py: 769 tensor([[769., 770., 771., 772., 773.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([770.], device='cuda:0') data.h5py: 770 tensor([[770., 771., 772., 773., 774.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([771.], device='cuda:0') data.h5py: 771 tensor([[771., 772., 773., 774., 775.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([772.], device='cuda:0') data.h5py: 772 tensor([[772., 773., 774., 775., 776.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([773.], device='cuda:0') data.h5py: 773 tensor([[773., 774., 775., 776., 777.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([774.], device='cuda:0') data.h5py: 774 tensor([[774., 775., 776., 777., 778.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([775.], device='cuda:0') data.h5py: 775 tensor([[775., 776., 777., 778., 779.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([776.], device='cuda:0') data.h5py: 776 tensor([[776., 777., 778., 779., 780.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([777.], device='cuda:0') data.h5py: 777 tensor([[777., 778., 779., 780., 781.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([778.], device='cuda:0') data.h5py: 778 tensor([[778., 779., 780., 781., 782.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([779.], device='cuda:0') data.h5py: 779 tensor([[779., 780., 781., 782., 783.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([780.], device='cuda:0') data.h5py: 780 tensor([[780., 781., 782., 783., 784.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([781.], device='cuda:0') data.h5py: 781 tensor([[781., 782., 783., 784., 785.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([782.], device='cuda:0') data.h5py: 782 tensor([[782., 783., 784., 785., 786.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([783.], device='cuda:0') data.h5py: 783 tensor([[783., 784., 785., 786., 787.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([784.], device='cuda:0') data.h5py: 784 tensor([[784., 785., 786., 787., 788.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([785.], device='cuda:0') data.h5py: 785 tensor([[785., 786., 787., 788., 789.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([786.], device='cuda:0') data.h5py: 786 tensor([[786., 787., 788., 789., 790.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([787.], device='cuda:0') data.h5py: 787 tensor([[787., 788., 789., 790., 791.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([788.], device='cuda:0') data.h5py: 788 tensor([[788., 789., 790., 791., 792.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([789.], device='cuda:0') data.h5py: 789 tensor([[789., 790., 791., 792., 793.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([790.], device='cuda:0') data.h5py: 790 tensor([[790., 791., 792., 793., 794.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([791.], device='cuda:0') data.h5py: 791 tensor([[791., 792., 793., 794., 795.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([792.], device='cuda:0') data.h5py: 792 tensor([[792., 793., 794., 795., 796.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([793.], device='cuda:0') data.h5py: 793 tensor([[793., 794., 795., 796., 797.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([794.], device='cuda:0') data.h5py: 794 tensor([[794., 795., 796., 797., 798.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([795.], device='cuda:0') data.h5py: 795 tensor([[795., 796., 797., 798., 799.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([796.], device='cuda:0') data.h5py: 796 tensor([[796., 797., 798., 799., 800.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([797.], device='cuda:0') data.h5py: 797 tensor([[797., 798., 799., 800., 801.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([798.], device='cuda:0') data.h5py: 798 tensor([[798., 799., 800., 801., 802.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([799.], device='cuda:0') data.h5py: 799 tensor([[799., 800., 801., 802., 803.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([800.], device='cuda:0') data.h5py: 800 tensor([[800., 801., 802., 803., 804.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([801.], device='cuda:0') data.h5py: 801 tensor([[801., 802., 803., 804., 805.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([802.], device='cuda:0') data.h5py: 802 tensor([[802., 803., 804., 805., 806.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([803.], device='cuda:0') data.h5py: 803 tensor([[803., 804., 805., 806., 807.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([804.], device='cuda:0') data.h5py: 804 tensor([[804., 805., 806., 807., 808.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([805.], device='cuda:0') data.h5py: 805 tensor([[805., 806., 807., 808., 809.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([806.], device='cuda:0') data.h5py: 806 tensor([[806., 807., 808., 809., 810.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([807.], device='cuda:0') data.h5py: 807 tensor([[807., 808., 809., 810., 811.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([808.], device='cuda:0') data.h5py: 808 tensor([[808., 809., 810., 811., 812.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([809.], device='cuda:0') data.h5py: 809 tensor([[809., 810., 811., 812., 813.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([810.], device='cuda:0') data.h5py: 810 tensor([[810., 811., 812., 813., 814.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([811.], device='cuda:0') data.h5py: 811 tensor([[811., 812., 813., 814., 815.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([812.], device='cuda:0') data.h5py: 812 tensor([[812., 813., 814., 815., 816.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([813.], device='cuda:0') data.h5py: 813 tensor([[813., 814., 815., 816., 817.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([814.], device='cuda:0') data.h5py: 814 tensor([[814., 815., 816., 817., 818.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([815.], device='cuda:0') data.h5py: 815 tensor([[815., 816., 817., 818., 819.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([816.], device='cuda:0') data.h5py: 816 tensor([[816., 817., 818., 819., 820.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([817.], device='cuda:0') data.h5py: 817 tensor([[817., 818., 819., 820., 821.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([818.], device='cuda:0') data.h5py: 818 tensor([[818., 819., 820., 821., 822.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([819.], device='cuda:0') data.h5py: 819 tensor([[819., 820., 821., 822., 823.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([820.], device='cuda:0') data.h5py: 820 tensor([[820., 821., 822., 823., 824.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([821.], device='cuda:0') data.h5py: 821 tensor([[821., 822., 823., 824., 825.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([822.], device='cuda:0') data.h5py: 822 tensor([[822., 823., 824., 825., 826.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([823.], device='cuda:0') data.h5py: 823 tensor([[823., 824., 825., 826., 827.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([824.], device='cuda:0') data.h5py: 824 tensor([[824., 825., 826., 827., 828.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([825.], device='cuda:0') data.h5py: 825 tensor([[825., 826., 827., 828., 829.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([826.], device='cuda:0') data.h5py: 826 tensor([[826., 827., 828., 829., 830.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([827.], device='cuda:0') data.h5py: 827 tensor([[827., 828., 829., 830., 831.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([828.], device='cuda:0') data.h5py: 828 tensor([[828., 829., 830., 831., 832.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([829.], device='cuda:0') data.h5py: 829 tensor([[829., 830., 831., 832., 833.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([830.], device='cuda:0') data.h5py: 830 tensor([[830., 831., 832., 833., 834.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([831.], device='cuda:0') data.h5py: 831 tensor([[831., 832., 833., 834., 835.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([832.], device='cuda:0') data.h5py: 832 tensor([[832., 833., 834., 835., 836.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([833.], device='cuda:0') data.h5py: 833 tensor([[833., 834., 835., 836., 837.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([834.], device='cuda:0') data.h5py: 834 tensor([[834., 835., 836., 837., 838.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([835.], device='cuda:0') data.h5py: 835 tensor([[835., 836., 837., 838., 839.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([836.], device='cuda:0') data.h5py: 836 tensor([[836., 837., 838., 839., 840.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([837.], device='cuda:0') data.h5py: 837 tensor([[837., 838., 839., 840., 841.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([838.], device='cuda:0') data.h5py: 838 tensor([[838., 839., 840., 841., 842.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([839.], device='cuda:0') data.h5py: 839 tensor([[839., 840., 841., 842., 843.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([840.], device='cuda:0') data.h5py: 840 tensor([[840., 841., 842., 843., 844.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([841.], device='cuda:0') data.h5py: 841 tensor([[841., 842., 843., 844., 845.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([842.], device='cuda:0') data.h5py: 842 tensor([[842., 843., 844., 845., 846.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([843.], device='cuda:0') data.h5py: 843 tensor([[843., 844., 845., 846., 847.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([844.], device='cuda:0') data.h5py: 844 tensor([[844., 845., 846., 847., 848.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([845.], device='cuda:0') data.h5py: 845 tensor([[845., 846., 847., 848., 849.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([846.], device='cuda:0') data.h5py: 846 tensor([[846., 847., 848., 849., 850.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([847.], device='cuda:0') data.h5py: 847 tensor([[847., 848., 849., 850., 851.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([848.], device='cuda:0') data.h5py: 848 tensor([[848., 849., 850., 851., 852.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([849.], device='cuda:0') data.h5py: 849 tensor([[849., 850., 851., 852., 853.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([850.], device='cuda:0') data.h5py: 850 tensor([[850., 851., 852., 853., 854.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([851.], device='cuda:0') data.h5py: 851 tensor([[851., 852., 853., 854., 855.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([852.], device='cuda:0') data.h5py: 852 tensor([[852., 853., 854., 855., 856.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([853.], device='cuda:0') data.h5py: 853 tensor([[853., 854., 855., 856., 857.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([854.], device='cuda:0') data.h5py: 854 tensor([[854., 855., 856., 857., 858.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([855.], device='cuda:0') data.h5py: 855 tensor([[855., 856., 857., 858., 859.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([856.], device='cuda:0') data.h5py: 856 tensor([[856., 857., 858., 859., 860.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([857.], device='cuda:0') data.h5py: 857 tensor([[857., 858., 859., 860., 861.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([858.], device='cuda:0') data.h5py: 858 tensor([[858., 859., 860., 861., 862.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([859.], device='cuda:0') data.h5py: 859 tensor([[859., 860., 861., 862., 863.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([860.], device='cuda:0') data.h5py: 860 tensor([[860., 861., 862., 863., 864.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([861.], device='cuda:0') data.h5py: 861 tensor([[861., 862., 863., 864., 865.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([862.], device='cuda:0') data.h5py: 862 tensor([[862., 863., 864., 865., 866.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([863.], device='cuda:0') data.h5py: 863 tensor([[863., 864., 865., 866., 867.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([864.], device='cuda:0') data.h5py: 864 tensor([[864., 865., 866., 867., 868.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([865.], device='cuda:0') data.h5py: 865 tensor([[865., 866., 867., 868., 869.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([866.], device='cuda:0') data.h5py: 866 tensor([[866., 867., 868., 869., 870.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([867.], device='cuda:0') data.h5py: 867 tensor([[867., 868., 869., 870., 871.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([868.], device='cuda:0') data.h5py: 868 tensor([[868., 869., 870., 871., 872.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([869.], device='cuda:0') data.h5py: 869 tensor([[869., 870., 871., 872., 873.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([870.], device='cuda:0') data.h5py: 870 tensor([[870., 871., 872., 873., 874.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([871.], device='cuda:0') data.h5py: 871 tensor([[871., 872., 873., 874., 875.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([872.], device='cuda:0') data.h5py: 872 tensor([[872., 873., 874., 875., 876.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([873.], device='cuda:0') data.h5py: 873 tensor([[873., 874., 875., 876., 877.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([874.], device='cuda:0') data.h5py: 874 tensor([[874., 875., 876., 877., 878.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([875.], device='cuda:0') data.h5py: 875 tensor([[875., 876., 877., 878., 879.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([876.], device='cuda:0') data.h5py: 876 tensor([[876., 877., 878., 879., 880.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([877.], device='cuda:0') data.h5py: 877 tensor([[877., 878., 879., 880., 881.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([878.], device='cuda:0') data.h5py: 878 tensor([[878., 879., 880., 881., 882.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([879.], device='cuda:0') data.h5py: 879 tensor([[879., 880., 881., 882., 883.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([880.], device='cuda:0') data.h5py: 880 tensor([[880., 881., 882., 883., 884.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([881.], device='cuda:0') data.h5py: 881 tensor([[881., 882., 883., 884., 885.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([882.], device='cuda:0') data.h5py: 882 tensor([[882., 883., 884., 885., 886.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([883.], device='cuda:0') data.h5py: 883 tensor([[883., 884., 885., 886., 887.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([884.], device='cuda:0') data.h5py: 884 tensor([[884., 885., 886., 887., 888.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([885.], device='cuda:0') data.h5py: 885 tensor([[885., 886., 887., 888., 889.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([886.], device='cuda:0') data.h5py: 886 tensor([[886., 887., 888., 889., 890.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([887.], device='cuda:0') data.h5py: 887 tensor([[887., 888., 889., 890., 891.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([888.], device='cuda:0') data.h5py: 888 tensor([[888., 889., 890., 891., 892.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([889.], device='cuda:0') data.h5py: 889 tensor([[889., 890., 891., 892., 893.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([890.], device='cuda:0') data.h5py: 890 tensor([[890., 891., 892., 893., 894.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([891.], device='cuda:0') data.h5py: 891 tensor([[891., 892., 893., 894., 895.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([892.], device='cuda:0') data.h5py: 892 tensor([[892., 893., 894., 895., 896.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([893.], device='cuda:0') data.h5py: 893 tensor([[893., 894., 895., 896., 897.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([894.], device='cuda:0') data.h5py: 894 tensor([[894., 895., 896., 897., 898.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([895.], device='cuda:0') data.h5py: 895 tensor([[895., 896., 897., 898., 899.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([896.], device='cuda:0') data.h5py: 896 tensor([[896., 897., 898., 899., 900.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([897.], device='cuda:0') data.h5py: 897 tensor([[897., 898., 899., 900., 901.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([898.], device='cuda:0') data.h5py: 898 tensor([[898., 899., 900., 901., 902.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([899.], device='cuda:0') data.h5py: 899 tensor([[899., 900., 901., 902., 903.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([900.], device='cuda:0') data.h5py: 900 tensor([[900., 901., 902., 903., 904.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([901.], device='cuda:0') data.h5py: 901 tensor([[901., 902., 903., 904., 905.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([902.], device='cuda:0') data.h5py: 902 tensor([[902., 903., 904., 905., 906.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([903.], device='cuda:0') data.h5py: 903 tensor([[903., 904., 905., 906., 907.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([904.], device='cuda:0') data.h5py: 904 tensor([[904., 905., 906., 907., 908.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([905.], device='cuda:0') data.h5py: 905 tensor([[905., 906., 907., 908., 909.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([906.], device='cuda:0') data.h5py: 906 tensor([[906., 907., 908., 909., 910.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([907.], device='cuda:0') data.h5py: 907 tensor([[907., 908., 909., 910., 911.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([908.], device='cuda:0') data.h5py: 908 tensor([[908., 909., 910., 911., 912.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([909.], device='cuda:0') data.h5py: 909 tensor([[909., 910., 911., 912., 913.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([910.], device='cuda:0') data.h5py: 910 tensor([[910., 911., 912., 913., 914.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([911.], device='cuda:0') data.h5py: 911 tensor([[911., 912., 913., 914., 915.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([912.], device='cuda:0') data.h5py: 912 tensor([[912., 913., 914., 915., 916.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([913.], device='cuda:0') data.h5py: 913 tensor([[913., 914., 915., 916., 917.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([914.], device='cuda:0') data.h5py: 914 tensor([[914., 915., 916., 917., 918.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([915.], device='cuda:0') data.h5py: 915 tensor([[915., 916., 917., 918., 919.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([916.], device='cuda:0') data.h5py: 916 tensor([[916., 917., 918., 919., 920.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([917.], device='cuda:0') data.h5py: 917 tensor([[917., 918., 919., 920., 921.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([918.], device='cuda:0') data.h5py: 918 tensor([[918., 919., 920., 921., 922.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([919.], device='cuda:0') data.h5py: 919 tensor([[919., 920., 921., 922., 923.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([920.], device='cuda:0') data.h5py: 920 tensor([[920., 921., 922., 923., 924.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([921.], device='cuda:0') data.h5py: 921 tensor([[921., 922., 923., 924., 925.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([922.], device='cuda:0') data.h5py: 922 tensor([[922., 923., 924., 925., 926.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([923.], device='cuda:0') data.h5py: 923 tensor([[923., 924., 925., 926., 927.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([924.], device='cuda:0') data.h5py: 924 tensor([[924., 925., 926., 927., 928.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([925.], device='cuda:0') data.h5py: 925 tensor([[925., 926., 927., 928., 929.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([926.], device='cuda:0') data.h5py: 926 tensor([[926., 927., 928., 929., 930.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([927.], device='cuda:0') data.h5py: 927 tensor([[927., 928., 929., 930., 931.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([928.], device='cuda:0') data.h5py: 928 tensor([[928., 929., 930., 931., 932.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([929.], device='cuda:0') data.h5py: 929 tensor([[929., 930., 931., 932., 933.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([930.], device='cuda:0') data.h5py: 930 tensor([[930., 931., 932., 933., 934.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([931.], device='cuda:0') data.h5py: 931 tensor([[931., 932., 933., 934., 935.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([932.], device='cuda:0') data.h5py: 932 tensor([[932., 933., 934., 935., 936.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([933.], device='cuda:0') data.h5py: 933 tensor([[933., 934., 935., 936., 937.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([934.], device='cuda:0') data.h5py: 934 tensor([[934., 935., 936., 937., 938.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([935.], device='cuda:0') data.h5py: 935 tensor([[935., 936., 937., 938., 939.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([936.], device='cuda:0') data.h5py: 936 tensor([[936., 937., 938., 939., 940.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([937.], device='cuda:0') data.h5py: 937 tensor([[937., 938., 939., 940., 941.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([938.], device='cuda:0') data.h5py: 938 tensor([[938., 939., 940., 941., 942.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([939.], device='cuda:0') data.h5py: 939 tensor([[939., 940., 941., 942., 943.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([940.], device='cuda:0') data.h5py: 940 tensor([[940., 941., 942., 943., 944.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([941.], device='cuda:0') data.h5py: 941 tensor([[941., 942., 943., 944., 945.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([942.], device='cuda:0') data.h5py: 942 tensor([[942., 943., 944., 945., 946.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([943.], device='cuda:0') data.h5py: 943 tensor([[943., 944., 945., 946., 947.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([944.], device='cuda:0') data.h5py: 944 tensor([[944., 945., 946., 947., 948.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([945.], device='cuda:0') data.h5py: 945 tensor([[945., 946., 947., 948., 949.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([946.], device='cuda:0') data.h5py: 946 tensor([[946., 947., 948., 949., 950.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([947.], device='cuda:0') data.h5py: 947 tensor([[947., 948., 949., 950., 951.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([948.], device='cuda:0') data.h5py: 948 tensor([[948., 949., 950., 951., 952.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([949.], device='cuda:0') data.h5py: 949 tensor([[949., 950., 951., 952., 953.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([950.], device='cuda:0') data.h5py: 950 tensor([[950., 951., 952., 953., 954.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([951.], device='cuda:0') data.h5py: 951 tensor([[951., 952., 953., 954., 955.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([952.], device='cuda:0') data.h5py: 952 tensor([[952., 953., 954., 955., 956.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([953.], device='cuda:0') data.h5py: 953 tensor([[953., 954., 955., 956., 957.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([954.], device='cuda:0') data.h5py: 954 tensor([[954., 955., 956., 957., 958.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([955.], device='cuda:0') data.h5py: 955 tensor([[955., 956., 957., 958., 959.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([956.], device='cuda:0') data.h5py: 956 tensor([[956., 957., 958., 959., 960.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([957.], device='cuda:0') data.h5py: 957 tensor([[957., 958., 959., 960., 961.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([958.], device='cuda:0') data.h5py: 958 tensor([[958., 959., 960., 961., 962.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([959.], device='cuda:0') data.h5py: 959 tensor([[959., 960., 961., 962., 963.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([960.], device='cuda:0') data.h5py: 960 tensor([[960., 961., 962., 963., 964.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([961.], device='cuda:0') data.h5py: 961 tensor([[961., 962., 963., 964., 965.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([962.], device='cuda:0') data.h5py: 962 tensor([[962., 963., 964., 965., 966.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([963.], device='cuda:0') data.h5py: 963 tensor([[963., 964., 965., 966., 967.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([964.], device='cuda:0') data.h5py: 964 tensor([[964., 965., 966., 967., 968.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([965.], device='cuda:0') data.h5py: 965 tensor([[965., 966., 967., 968., 969.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([966.], device='cuda:0') data.h5py: 966 tensor([[966., 967., 968., 969., 970.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([967.], device='cuda:0') data.h5py: 967 tensor([[967., 968., 969., 970., 971.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([968.], device='cuda:0') data.h5py: 968 tensor([[968., 969., 970., 971., 972.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([969.], device='cuda:0') data.h5py: 969 tensor([[969., 970., 971., 972., 973.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([970.], device='cuda:0') data.h5py: 970 tensor([[970., 971., 972., 973., 974.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([971.], device='cuda:0') data.h5py: 971 tensor([[971., 972., 973., 974., 975.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([972.], device='cuda:0') data.h5py: 972 tensor([[972., 973., 974., 975., 976.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([973.], device='cuda:0') data.h5py: 973 tensor([[973., 974., 975., 976., 977.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([974.], device='cuda:0') data.h5py: 974 tensor([[974., 975., 976., 977., 978.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([975.], device='cuda:0') data.h5py: 975 tensor([[975., 976., 977., 978., 979.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([976.], device='cuda:0') data.h5py: 976 tensor([[976., 977., 978., 979., 980.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([977.], device='cuda:0') data.h5py: 977 tensor([[977., 978., 979., 980., 981.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([978.], device='cuda:0') data.h5py: 978 tensor([[978., 979., 980., 981., 982.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([979.], device='cuda:0') data.h5py: 979 tensor([[979., 980., 981., 982., 983.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([980.], device='cuda:0') data.h5py: 980 tensor([[980., 981., 982., 983., 984.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([981.], device='cuda:0') data.h5py: 981 tensor([[981., 982., 983., 984., 985.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([982.], device='cuda:0') data.h5py: 982 tensor([[982., 983., 984., 985., 986.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([983.], device='cuda:0') data.h5py: 983 tensor([[983., 984., 985., 986., 987.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([984.], device='cuda:0') data.h5py: 984 tensor([[984., 985., 986., 987., 988.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([985.], device='cuda:0') data.h5py: 985 tensor([[985., 986., 987., 988., 989.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([986.], device='cuda:0') data.h5py: 986 tensor([[986., 987., 988., 989., 990.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([987.], device='cuda:0') data.h5py: 987 tensor([[987., 988., 989., 990., 991.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([988.], device='cuda:0') data.h5py: 988 tensor([[988., 989., 990., 991., 992.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([989.], device='cuda:0') data.h5py: 989 tensor([[989., 990., 991., 992., 993.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([990.], device='cuda:0') data.h5py: 990 tensor([[990., 991., 992., 993., 994.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([991.], device='cuda:0') data.h5py: 991 tensor([[991., 992., 993., 994., 995.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([992.], device='cuda:0') data.h5py: 992 tensor([[992., 993., 994., 995., 996.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([993.], device='cuda:0') data.h5py: 993 tensor([[993., 994., 995., 996., 997.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([994.], device='cuda:0') data.h5py: 994 tensor([[994., 995., 996., 997., 998.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([995.], device='cuda:0') data.h5py: 995 tensor([[995., 996., 997., 998., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([996.], device='cuda:0') data.h5py: 996 tensor([[996., 997., 998., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([997.], device='cuda:0') data.h5py: 997 tensor([[997., 998., 999., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([998.], device='cuda:0') data.h5py: 998 tensor([[998., 999., 999., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([999.], device='cuda:0') data.h5py: 999 tensor([[999., 999., 999., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1000.], device='cuda:0') Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5cparser_seq.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5CParser ( os . path . join ( root_folder , 'test_data_h5cparser_seq' ), keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 10 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Output data.webtools: All required datasets are available. data.h5py: 0 tensor([[0., 0., 0., 0., 0.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1.], device='cuda:0') data.h5py: 1 tensor([[0., 0., 0., 0., 1.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([2.], device='cuda:0') data.h5py: 2 tensor([[0., 0., 0., 1., 2.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([3.], device='cuda:0') data.h5py: 3 tensor([[0., 0., 1., 2., 3.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([4.], device='cuda:0') data.h5py: 4 tensor([[0., 1., 2., 3., 4.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([5.], device='cuda:0') data.h5py: 5 tensor([[1., 2., 3., 4., 5.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([6.], device='cuda:0') data.h5py: 6 tensor([[2., 3., 4., 5., 6.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([7.], device='cuda:0') data.h5py: 7 tensor([[3., 4., 5., 6., 7.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([8.], device='cuda:0') data.h5py: 8 tensor([[4., 5., 6., 7., 8.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([9.], device='cuda:0') data.h5py: 9 tensor([[5., 6., 7., 8., 9.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([10.], device='cuda:0') data.h5py: 10 tensor([[ 6., 7., 8., 9., 10.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([11.], device='cuda:0') data.h5py: 11 tensor([[ 7., 8., 9., 10., 11.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([12.], device='cuda:0') data.h5py: 12 tensor([[ 8., 9., 10., 11., 12.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([13.], device='cuda:0') data.h5py: 13 tensor([[ 9., 10., 11., 12., 13.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([14.], device='cuda:0') data.h5py: 14 tensor([[10., 11., 12., 13., 14.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([15.], device='cuda:0') data.h5py: 15 tensor([[11., 12., 13., 14., 15.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([16.], device='cuda:0') data.h5py: 16 tensor([[12., 13., 14., 15., 16.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([17.], device='cuda:0') data.h5py: 17 tensor([[13., 14., 15., 16., 17.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([18.], device='cuda:0') data.h5py: 18 tensor([[14., 15., 16., 17., 18.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([19.], device='cuda:0') data.h5py: 19 tensor([[15., 16., 17., 18., 19.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([20.], device='cuda:0') data.h5py: 20 tensor([[20., 20., 20., 20., 20.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([21.], device='cuda:0') data.h5py: 21 tensor([[20., 20., 20., 20., 21.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([22.], device='cuda:0') data.h5py: 22 tensor([[20., 20., 20., 21., 22.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([23.], device='cuda:0') data.h5py: 23 tensor([[20., 20., 21., 22., 23.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([24.], device='cuda:0') data.h5py: 24 tensor([[20., 21., 22., 23., 24.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([25.], device='cuda:0') data.h5py: 25 tensor([[21., 22., 23., 24., 25.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([26.], device='cuda:0') data.h5py: 26 tensor([[22., 23., 24., 25., 26.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([27.], device='cuda:0') data.h5py: 27 tensor([[23., 24., 25., 26., 27.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([28.], device='cuda:0') data.h5py: 28 tensor([[24., 25., 26., 27., 28.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([29.], device='cuda:0') data.h5py: 29 tensor([[25., 26., 27., 28., 29.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([30.], device='cuda:0') data.h5py: 30 tensor([[26., 27., 28., 29., 30.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([31.], device='cuda:0') data.h5py: 31 tensor([[27., 28., 29., 30., 31.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([32.], device='cuda:0') data.h5py: 32 tensor([[28., 29., 30., 31., 32.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([33.], device='cuda:0') data.h5py: 33 tensor([[29., 30., 31., 32., 33.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([34.], device='cuda:0') data.h5py: 34 tensor([[30., 31., 32., 33., 34.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([35.], device='cuda:0') data.h5py: 35 tensor([[31., 32., 33., 34., 35.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([36.], device='cuda:0') data.h5py: 36 tensor([[32., 33., 34., 35., 36.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([37.], device='cuda:0') data.h5py: 37 tensor([[33., 34., 35., 36., 37.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([38.], device='cuda:0') data.h5py: 38 tensor([[38., 38., 38., 38., 38.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([39.], device='cuda:0') data.h5py: 39 tensor([[38., 38., 38., 38., 39.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([40.], device='cuda:0') data.h5py: 40 tensor([[38., 38., 38., 39., 40.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([41.], device='cuda:0') data.h5py: 41 tensor([[38., 38., 39., 40., 41.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([42.], device='cuda:0') data.h5py: 42 tensor([[38., 39., 40., 41., 42.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([43.], device='cuda:0') data.h5py: 43 tensor([[39., 40., 41., 42., 43.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([44.], device='cuda:0') data.h5py: 44 tensor([[40., 41., 42., 43., 44.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([45.], device='cuda:0') data.h5py: 45 tensor([[41., 42., 43., 44., 45.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([46.], device='cuda:0') data.h5py: 46 tensor([[42., 43., 44., 45., 46.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([47.], device='cuda:0') data.h5py: 47 tensor([[43., 44., 45., 46., 47.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([48.], device='cuda:0') data.h5py: 48 tensor([[44., 45., 46., 47., 48.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([49.], device='cuda:0') data.h5py: 49 tensor([[45., 46., 47., 48., 49.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([50.], device='cuda:0') data.h5py: 50 tensor([[46., 47., 48., 49., 50.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([51.], device='cuda:0') data.h5py: 51 tensor([[51., 51., 51., 51., 51.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([52.], device='cuda:0') data.h5py: 52 tensor([[51., 51., 51., 51., 52.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([53.], device='cuda:0') data.h5py: 53 tensor([[51., 51., 51., 52., 53.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([54.], device='cuda:0') data.h5py: 54 tensor([[51., 51., 52., 53., 54.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([55.], device='cuda:0') data.h5py: 55 tensor([[51., 52., 53., 54., 55.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([56.], device='cuda:0') data.h5py: 56 tensor([[52., 53., 54., 55., 56.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([57.], device='cuda:0') data.h5py: 57 tensor([[53., 54., 55., 56., 57.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([58.], device='cuda:0') data.h5py: 58 tensor([[54., 55., 56., 57., 58.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([59.], device='cuda:0') data.h5py: 59 tensor([[55., 56., 57., 58., 59.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([60.], device='cuda:0') data.h5py: 60 tensor([[56., 57., 58., 59., 60.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([61.], device='cuda:0') data.h5py: 61 tensor([[57., 58., 59., 60., 61.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([62.], device='cuda:0') data.h5py: 62 tensor([[58., 59., 60., 61., 62.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([63.], device='cuda:0') data.h5py: 63 tensor([[59., 60., 61., 62., 63.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([64.], device='cuda:0') data.h5py: 64 tensor([[60., 61., 62., 63., 64.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([65.], device='cuda:0') data.h5py: 65 tensor([[61., 62., 63., 64., 65.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([66.], device='cuda:0') data.h5py: 66 tensor([[62., 63., 64., 65., 66.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([67.], device='cuda:0') data.h5py: 67 tensor([[63., 64., 65., 66., 67.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([68.], device='cuda:0') data.h5py: 68 tensor([[64., 65., 66., 67., 68.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([69.], device='cuda:0') data.h5py: 69 tensor([[65., 66., 67., 68., 69.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([70.], device='cuda:0') data.h5py: 70 tensor([[70., 70., 70., 70., 70.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([71.], device='cuda:0') data.h5py: 71 tensor([[70., 70., 70., 70., 71.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([72.], device='cuda:0') data.h5py: 72 tensor([[70., 70., 70., 71., 72.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([73.], device='cuda:0') data.h5py: 73 tensor([[70., 70., 71., 72., 73.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([74.], device='cuda:0') data.h5py: 74 tensor([[70., 71., 72., 73., 74.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([75.], device='cuda:0') data.h5py: 75 tensor([[71., 72., 73., 74., 75.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([76.], device='cuda:0') data.h5py: 76 tensor([[72., 73., 74., 75., 76.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([77.], device='cuda:0') data.h5py: 77 tensor([[73., 74., 75., 76., 77.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([78.], device='cuda:0') data.h5py: 78 tensor([[74., 75., 76., 77., 78.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([79.], device='cuda:0') data.h5py: 79 tensor([[75., 76., 77., 78., 79.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([80.], device='cuda:0') data.h5py: 80 tensor([[76., 77., 78., 79., 80.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([81.], device='cuda:0') data.h5py: 81 tensor([[77., 78., 79., 80., 81.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([82.], device='cuda:0') data.h5py: 82 tensor([[82., 82., 82., 82., 82.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([83.], device='cuda:0') data.h5py: 83 tensor([[82., 82., 82., 82., 83.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([84.], device='cuda:0') data.h5py: 84 tensor([[82., 82., 82., 83., 84.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([85.], device='cuda:0') data.h5py: 85 tensor([[82., 82., 83., 84., 85.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([86.], device='cuda:0') data.h5py: 86 tensor([[82., 83., 84., 85., 86.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([87.], device='cuda:0') data.h5py: 87 tensor([[83., 84., 85., 86., 87.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([88.], device='cuda:0') data.h5py: 88 tensor([[84., 85., 86., 87., 88.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([89.], device='cuda:0') data.h5py: 89 tensor([[85., 86., 87., 88., 89.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([90.], device='cuda:0') data.h5py: 90 tensor([[86., 87., 88., 89., 90.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([91.], device='cuda:0') data.h5py: 91 tensor([[87., 88., 89., 90., 91.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([92.], device='cuda:0') data.h5py: 92 tensor([[88., 89., 90., 91., 92.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([93.], device='cuda:0') data.h5py: 93 tensor([[89., 90., 91., 92., 93.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([94.], device='cuda:0') data.h5py: 94 tensor([[90., 91., 92., 93., 94.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([95.], device='cuda:0') data.h5py: 95 tensor([[91., 92., 93., 94., 95.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([96.], device='cuda:0') data.h5py: 96 tensor([[92., 93., 94., 95., 96.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([97.], device='cuda:0') data.h5py: 97 tensor([[93., 94., 95., 96., 97.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([98.], device='cuda:0') data.h5py: 98 tensor([[94., 95., 96., 97., 98.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([99.], device='cuda:0') data.h5py: 99 tensor([[95., 96., 97., 98., 99.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([100.], device='cuda:0') data.h5py: 100 tensor([[100., 100., 100., 100., 100.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([101.], device='cuda:0') data.h5py: 101 tensor([[100., 100., 100., 100., 101.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([102.], device='cuda:0') data.h5py: 102 tensor([[100., 100., 100., 101., 102.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([103.], device='cuda:0') data.h5py: 103 tensor([[100., 100., 101., 102., 103.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([104.], device='cuda:0') data.h5py: 104 tensor([[100., 101., 102., 103., 104.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([105.], device='cuda:0') data.h5py: 105 tensor([[101., 102., 103., 104., 105.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([106.], device='cuda:0') data.h5py: 106 tensor([[102., 103., 104., 105., 106.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([107.], device='cuda:0') data.h5py: 107 tensor([[103., 104., 105., 106., 107.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([108.], device='cuda:0') data.h5py: 108 tensor([[104., 105., 106., 107., 108.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([109.], device='cuda:0') data.h5py: 109 tensor([[105., 106., 107., 108., 109.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([110.], device='cuda:0') data.h5py: 110 tensor([[106., 107., 108., 109., 110.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([111.], device='cuda:0') data.h5py: 111 tensor([[107., 108., 109., 110., 111.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([112.], device='cuda:0') data.h5py: 112 tensor([[108., 109., 110., 111., 112.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([113.], device='cuda:0') data.h5py: 113 tensor([[109., 110., 111., 112., 113.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([114.], device='cuda:0') data.h5py: 114 tensor([[110., 111., 112., 113., 114.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([115.], device='cuda:0') data.h5py: 115 tensor([[111., 112., 113., 114., 115.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([116.], device='cuda:0') data.h5py: 116 tensor([[116., 116., 116., 116., 116.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([117.], device='cuda:0') data.h5py: 117 tensor([[116., 116., 116., 116., 117.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([118.], device='cuda:0') data.h5py: 118 tensor([[116., 116., 116., 117., 118.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([119.], device='cuda:0') data.h5py: 119 tensor([[116., 116., 117., 118., 119.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([120.], device='cuda:0') data.h5py: 120 tensor([[116., 117., 118., 119., 120.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([121.], device='cuda:0') data.h5py: 121 tensor([[117., 118., 119., 120., 121.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([122.], device='cuda:0') data.h5py: 122 tensor([[118., 119., 120., 121., 122.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([123.], device='cuda:0') data.h5py: 123 tensor([[119., 120., 121., 122., 123.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([124.], device='cuda:0') data.h5py: 124 tensor([[120., 121., 122., 123., 124.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([125.], device='cuda:0') data.h5py: 125 tensor([[121., 122., 123., 124., 125.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([126.], device='cuda:0') data.h5py: 126 tensor([[122., 123., 124., 125., 126.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([127.], device='cuda:0') data.h5py: 127 tensor([[123., 124., 125., 126., 127.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([128.], device='cuda:0') data.h5py: 128 tensor([[124., 125., 126., 127., 128.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([129.], device='cuda:0') data.h5py: 129 tensor([[125., 126., 127., 128., 129.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([130.], device='cuda:0') data.h5py: 130 tensor([[126., 127., 128., 129., 130.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([131.], device='cuda:0') data.h5py: 131 tensor([[127., 128., 129., 130., 131.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([132.], device='cuda:0') data.h5py: 132 tensor([[128., 129., 130., 131., 132.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([133.], device='cuda:0') data.h5py: 133 tensor([[129., 130., 131., 132., 133.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([134.], device='cuda:0') data.h5py: 134 tensor([[130., 131., 132., 133., 134.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([135.], device='cuda:0') data.h5py: 135 tensor([[131., 132., 133., 134., 135.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([136.], device='cuda:0') data.h5py: 136 tensor([[136., 136., 136., 136., 136.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([137.], device='cuda:0') data.h5py: 137 tensor([[136., 136., 136., 136., 137.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([138.], device='cuda:0') data.h5py: 138 tensor([[136., 136., 136., 137., 138.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([139.], device='cuda:0') data.h5py: 139 tensor([[136., 136., 137., 138., 139.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([140.], device='cuda:0') data.h5py: 140 tensor([[136., 137., 138., 139., 140.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([141.], device='cuda:0') data.h5py: 141 tensor([[137., 138., 139., 140., 141.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([142.], device='cuda:0') data.h5py: 142 tensor([[138., 139., 140., 141., 142.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([143.], device='cuda:0') data.h5py: 143 tensor([[139., 140., 141., 142., 143.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([144.], device='cuda:0') data.h5py: 144 tensor([[140., 141., 142., 143., 144.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([145.], device='cuda:0') data.h5py: 145 tensor([[141., 142., 143., 144., 145.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([146.], device='cuda:0') data.h5py: 146 tensor([[142., 143., 144., 145., 146.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([147.], device='cuda:0') data.h5py: 147 tensor([[143., 144., 145., 146., 147.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([148.], device='cuda:0') data.h5py: 148 tensor([[144., 145., 146., 147., 148.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([149.], device='cuda:0') data.h5py: 149 tensor([[145., 146., 147., 148., 149.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([150.], device='cuda:0') data.h5py: 150 tensor([[146., 147., 148., 149., 150.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([151.], device='cuda:0') data.h5py: 151 tensor([[147., 148., 149., 150., 151.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([152.], device='cuda:0') data.h5py: 152 tensor([[148., 149., 150., 151., 152.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([153.], device='cuda:0') data.h5py: 153 tensor([[153., 153., 153., 153., 153.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([154.], device='cuda:0') data.h5py: 154 tensor([[153., 153., 153., 153., 154.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([155.], device='cuda:0') data.h5py: 155 tensor([[153., 153., 153., 154., 155.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([156.], device='cuda:0') data.h5py: 156 tensor([[153., 153., 154., 155., 156.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([157.], device='cuda:0') data.h5py: 157 tensor([[153., 154., 155., 156., 157.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([158.], device='cuda:0') data.h5py: 158 tensor([[154., 155., 156., 157., 158.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([159.], device='cuda:0') data.h5py: 159 tensor([[155., 156., 157., 158., 159.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([160.], device='cuda:0') data.h5py: 160 tensor([[156., 157., 158., 159., 160.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([161.], device='cuda:0') data.h5py: 161 tensor([[157., 158., 159., 160., 161.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([162.], device='cuda:0') data.h5py: 162 tensor([[158., 159., 160., 161., 162.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([163.], device='cuda:0') data.h5py: 163 tensor([[159., 160., 161., 162., 163.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([164.], device='cuda:0') data.h5py: 164 tensor([[160., 161., 162., 163., 164.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([165.], device='cuda:0') data.h5py: 165 tensor([[161., 162., 163., 164., 165.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([166.], device='cuda:0') data.h5py: 166 tensor([[162., 163., 164., 165., 166.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([167.], device='cuda:0') data.h5py: 167 tensor([[163., 164., 165., 166., 167.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([168.], device='cuda:0') data.h5py: 168 tensor([[164., 165., 166., 167., 168.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([169.], device='cuda:0') data.h5py: 169 tensor([[165., 166., 167., 168., 169.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([170.], device='cuda:0') data.h5py: 170 tensor([[166., 167., 168., 169., 170.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([171.], device='cuda:0') data.h5py: 171 tensor([[167., 168., 169., 170., 171.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([172.], device='cuda:0') data.h5py: 172 tensor([[168., 169., 170., 171., 172.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([173.], device='cuda:0') data.h5py: 173 tensor([[173., 173., 173., 173., 173.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([174.], device='cuda:0') data.h5py: 174 tensor([[173., 173., 173., 173., 174.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([175.], device='cuda:0') data.h5py: 175 tensor([[173., 173., 173., 174., 175.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([176.], device='cuda:0') data.h5py: 176 tensor([[173., 173., 174., 175., 176.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([177.], device='cuda:0') data.h5py: 177 tensor([[173., 174., 175., 176., 177.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([178.], device='cuda:0') data.h5py: 178 tensor([[174., 175., 176., 177., 178.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([179.], device='cuda:0') data.h5py: 179 tensor([[175., 176., 177., 178., 179.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([180.], device='cuda:0') data.h5py: 180 tensor([[176., 177., 178., 179., 180.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([181.], device='cuda:0') data.h5py: 181 tensor([[177., 178., 179., 180., 181.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([182.], device='cuda:0') data.h5py: 182 tensor([[178., 179., 180., 181., 182.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([183.], device='cuda:0') data.h5py: 183 tensor([[179., 180., 181., 182., 183.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([184.], device='cuda:0') data.h5py: 184 tensor([[180., 181., 182., 183., 184.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([185.], device='cuda:0') data.h5py: 185 tensor([[181., 182., 183., 184., 185.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([186.], device='cuda:0') data.h5py: 186 tensor([[186., 186., 186., 186., 186.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([187.], device='cuda:0') data.h5py: 187 tensor([[186., 186., 186., 186., 187.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([188.], device='cuda:0') data.h5py: 188 tensor([[186., 186., 186., 187., 188.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([189.], device='cuda:0') data.h5py: 189 tensor([[186., 186., 187., 188., 189.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([190.], device='cuda:0') data.h5py: 190 tensor([[186., 187., 188., 189., 190.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([191.], device='cuda:0') data.h5py: 191 tensor([[187., 188., 189., 190., 191.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([192.], device='cuda:0') data.h5py: 192 tensor([[188., 189., 190., 191., 192.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([193.], device='cuda:0') data.h5py: 193 tensor([[189., 190., 191., 192., 193.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([194.], device='cuda:0') data.h5py: 194 tensor([[190., 191., 192., 193., 194.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([195.], device='cuda:0') data.h5py: 195 tensor([[191., 192., 193., 194., 195.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([196.], device='cuda:0') data.h5py: 196 tensor([[196., 196., 196., 196., 196.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([197.], device='cuda:0') data.h5py: 197 tensor([[196., 196., 196., 196., 197.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([198.], device='cuda:0') data.h5py: 198 tensor([[196., 196., 196., 197., 198.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([199.], device='cuda:0') data.h5py: 199 tensor([[196., 196., 197., 198., 199.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([200.], device='cuda:0') data.h5py: 200 tensor([[196., 197., 198., 199., 200.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([201.], device='cuda:0') data.h5py: 201 tensor([[197., 198., 199., 200., 201.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([202.], device='cuda:0') data.h5py: 202 tensor([[198., 199., 200., 201., 202.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([203.], device='cuda:0') data.h5py: 203 tensor([[199., 200., 201., 202., 203.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([204.], device='cuda:0') data.h5py: 204 tensor([[200., 201., 202., 203., 204.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([205.], device='cuda:0') data.h5py: 205 tensor([[201., 202., 203., 204., 205.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([206.], device='cuda:0') data.h5py: 206 tensor([[206., 206., 206., 206., 206.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([207.], device='cuda:0') data.h5py: 207 tensor([[206., 206., 206., 206., 207.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([208.], device='cuda:0') data.h5py: 208 tensor([[206., 206., 206., 207., 208.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([209.], device='cuda:0') data.h5py: 209 tensor([[206., 206., 207., 208., 209.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([210.], device='cuda:0') data.h5py: 210 tensor([[206., 207., 208., 209., 210.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([211.], device='cuda:0') data.h5py: 211 tensor([[207., 208., 209., 210., 211.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([212.], device='cuda:0') data.h5py: 212 tensor([[208., 209., 210., 211., 212.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([213.], device='cuda:0') data.h5py: 213 tensor([[209., 210., 211., 212., 213.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([214.], device='cuda:0') data.h5py: 214 tensor([[210., 211., 212., 213., 214.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([215.], device='cuda:0') data.h5py: 215 tensor([[211., 212., 213., 214., 215.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([216.], device='cuda:0') data.h5py: 216 tensor([[212., 213., 214., 215., 216.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([217.], device='cuda:0') data.h5py: 217 tensor([[217., 217., 217., 217., 217.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([218.], device='cuda:0') data.h5py: 218 tensor([[217., 217., 217., 217., 218.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([219.], device='cuda:0') data.h5py: 219 tensor([[217., 217., 217., 218., 219.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([220.], device='cuda:0') data.h5py: 220 tensor([[217., 217., 218., 219., 220.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([221.], device='cuda:0') data.h5py: 221 tensor([[217., 218., 219., 220., 221.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([222.], device='cuda:0') data.h5py: 222 tensor([[218., 219., 220., 221., 222.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([223.], device='cuda:0') data.h5py: 223 tensor([[219., 220., 221., 222., 223.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([224.], device='cuda:0') data.h5py: 224 tensor([[220., 221., 222., 223., 224.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([225.], device='cuda:0') data.h5py: 225 tensor([[221., 222., 223., 224., 225.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([226.], device='cuda:0') data.h5py: 226 tensor([[222., 223., 224., 225., 226.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([227.], device='cuda:0') data.h5py: 227 tensor([[223., 224., 225., 226., 227.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([228.], device='cuda:0') data.h5py: 228 tensor([[224., 225., 226., 227., 228.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([229.], device='cuda:0') data.h5py: 229 tensor([[225., 226., 227., 228., 229.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([230.], device='cuda:0') data.h5py: 230 tensor([[226., 227., 228., 229., 230.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([231.], device='cuda:0') data.h5py: 231 tensor([[227., 228., 229., 230., 231.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([232.], device='cuda:0') data.h5py: 232 tensor([[228., 229., 230., 231., 232.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([233.], device='cuda:0') data.h5py: 233 tensor([[233., 233., 233., 233., 233.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([234.], device='cuda:0') data.h5py: 234 tensor([[233., 233., 233., 233., 234.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([235.], device='cuda:0') data.h5py: 235 tensor([[233., 233., 233., 234., 235.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([236.], device='cuda:0') data.h5py: 236 tensor([[233., 233., 234., 235., 236.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([237.], device='cuda:0') data.h5py: 237 tensor([[233., 234., 235., 236., 237.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([238.], device='cuda:0') data.h5py: 238 tensor([[234., 235., 236., 237., 238.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([239.], device='cuda:0') data.h5py: 239 tensor([[235., 236., 237., 238., 239.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([240.], device='cuda:0') data.h5py: 240 tensor([[236., 237., 238., 239., 240.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([241.], device='cuda:0') data.h5py: 241 tensor([[237., 238., 239., 240., 241.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([242.], device='cuda:0') data.h5py: 242 tensor([[238., 239., 240., 241., 242.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([243.], device='cuda:0') data.h5py: 243 tensor([[239., 240., 241., 242., 243.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([244.], device='cuda:0') data.h5py: 244 tensor([[240., 241., 242., 243., 244.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([245.], device='cuda:0') data.h5py: 245 tensor([[241., 242., 243., 244., 245.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([246.], device='cuda:0') data.h5py: 246 tensor([[242., 243., 244., 245., 246.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([247.], device='cuda:0') data.h5py: 247 tensor([[243., 244., 245., 246., 247.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([248.], device='cuda:0') data.h5py: 248 tensor([[244., 245., 246., 247., 248.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([249.], device='cuda:0') data.h5py: 249 tensor([[245., 246., 247., 248., 249.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([250.], device='cuda:0') data.h5py: 250 tensor([[246., 247., 248., 249., 250.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([251.], device='cuda:0') data.h5py: 251 tensor([[247., 248., 249., 250., 251.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([252.], device='cuda:0') data.h5py: 252 tensor([[248., 249., 250., 251., 252.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([253.], device='cuda:0') data.h5py: 253 tensor([[253., 253., 253., 253., 253.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([254.], device='cuda:0') data.h5py: 254 tensor([[253., 253., 253., 253., 254.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([255.], device='cuda:0') data.h5py: 255 tensor([[253., 253., 253., 254., 255.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([256.], device='cuda:0') data.h5py: 256 tensor([[253., 253., 254., 255., 256.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([257.], device='cuda:0') data.h5py: 257 tensor([[253., 254., 255., 256., 257.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([258.], device='cuda:0') data.h5py: 258 tensor([[254., 255., 256., 257., 258.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([259.], device='cuda:0') data.h5py: 259 tensor([[255., 256., 257., 258., 259.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([260.], device='cuda:0') data.h5py: 260 tensor([[256., 257., 258., 259., 260.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([261.], device='cuda:0') data.h5py: 261 tensor([[257., 258., 259., 260., 261.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([262.], device='cuda:0') data.h5py: 262 tensor([[258., 259., 260., 261., 262.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([263.], device='cuda:0') data.h5py: 263 tensor([[259., 260., 261., 262., 263.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([264.], device='cuda:0') data.h5py: 264 tensor([[260., 261., 262., 263., 264.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([265.], device='cuda:0') data.h5py: 265 tensor([[261., 262., 263., 264., 265.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([266.], device='cuda:0') data.h5py: 266 tensor([[262., 263., 264., 265., 266.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([267.], device='cuda:0') data.h5py: 267 tensor([[263., 264., 265., 266., 267.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([268.], device='cuda:0') data.h5py: 268 tensor([[264., 265., 266., 267., 268.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([269.], device='cuda:0') data.h5py: 269 tensor([[265., 266., 267., 268., 269.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([270.], device='cuda:0') data.h5py: 270 tensor([[266., 267., 268., 269., 270.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([271.], device='cuda:0') data.h5py: 271 tensor([[271., 271., 271., 271., 271.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([272.], device='cuda:0') data.h5py: 272 tensor([[271., 271., 271., 271., 272.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([273.], device='cuda:0') data.h5py: 273 tensor([[271., 271., 271., 272., 273.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([274.], device='cuda:0') data.h5py: 274 tensor([[271., 271., 272., 273., 274.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([275.], device='cuda:0') data.h5py: 275 tensor([[271., 272., 273., 274., 275.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([276.], device='cuda:0') data.h5py: 276 tensor([[272., 273., 274., 275., 276.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([277.], device='cuda:0') data.h5py: 277 tensor([[273., 274., 275., 276., 277.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([278.], device='cuda:0') data.h5py: 278 tensor([[274., 275., 276., 277., 278.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([279.], device='cuda:0') data.h5py: 279 tensor([[275., 276., 277., 278., 279.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([280.], device='cuda:0') data.h5py: 280 tensor([[276., 277., 278., 279., 280.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([281.], device='cuda:0') data.h5py: 281 tensor([[277., 278., 279., 280., 281.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([282.], device='cuda:0') data.h5py: 282 tensor([[278., 279., 280., 281., 282.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([283.], device='cuda:0') data.h5py: 283 tensor([[279., 280., 281., 282., 283.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([284.], device='cuda:0') data.h5py: 284 tensor([[280., 281., 282., 283., 284.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([285.], device='cuda:0') data.h5py: 285 tensor([[281., 282., 283., 284., 285.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([286.], device='cuda:0') data.h5py: 286 tensor([[282., 283., 284., 285., 286.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([287.], device='cuda:0') data.h5py: 287 tensor([[283., 284., 285., 286., 287.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([288.], device='cuda:0') data.h5py: 288 tensor([[288., 288., 288., 288., 288.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([289.], device='cuda:0') data.h5py: 289 tensor([[288., 288., 288., 288., 289.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([290.], device='cuda:0') data.h5py: 290 tensor([[288., 288., 288., 289., 290.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([291.], device='cuda:0') data.h5py: 291 tensor([[288., 288., 289., 290., 291.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([292.], device='cuda:0') data.h5py: 292 tensor([[288., 289., 290., 291., 292.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([293.], device='cuda:0') data.h5py: 293 tensor([[289., 290., 291., 292., 293.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([294.], device='cuda:0') data.h5py: 294 tensor([[290., 291., 292., 293., 294.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([295.], device='cuda:0') data.h5py: 295 tensor([[291., 292., 293., 294., 295.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([296.], device='cuda:0') data.h5py: 296 tensor([[292., 293., 294., 295., 296.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([297.], device='cuda:0') data.h5py: 297 tensor([[293., 294., 295., 296., 297.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([298.], device='cuda:0') data.h5py: 298 tensor([[298., 298., 298., 298., 298.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([299.], device='cuda:0') data.h5py: 299 tensor([[298., 298., 298., 298., 299.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([300.], device='cuda:0') data.h5py: 300 tensor([[298., 298., 298., 299., 300.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([301.], device='cuda:0') data.h5py: 301 tensor([[298., 298., 299., 300., 301.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([302.], device='cuda:0') data.h5py: 302 tensor([[298., 299., 300., 301., 302.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([303.], device='cuda:0') data.h5py: 303 tensor([[299., 300., 301., 302., 303.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([304.], device='cuda:0') data.h5py: 304 tensor([[300., 301., 302., 303., 304.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([305.], device='cuda:0') data.h5py: 305 tensor([[301., 302., 303., 304., 305.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([306.], device='cuda:0') data.h5py: 306 tensor([[302., 303., 304., 305., 306.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([307.], device='cuda:0') data.h5py: 307 tensor([[303., 304., 305., 306., 307.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([308.], device='cuda:0') data.h5py: 308 tensor([[304., 305., 306., 307., 308.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([309.], device='cuda:0') data.h5py: 309 tensor([[305., 306., 307., 308., 309.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([310.], device='cuda:0') data.h5py: 310 tensor([[310., 310., 310., 310., 310.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([311.], device='cuda:0') data.h5py: 311 tensor([[310., 310., 310., 310., 311.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([312.], device='cuda:0') data.h5py: 312 tensor([[310., 310., 310., 311., 312.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([313.], device='cuda:0') data.h5py: 313 tensor([[310., 310., 311., 312., 313.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([314.], device='cuda:0') data.h5py: 314 tensor([[310., 311., 312., 313., 314.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([315.], device='cuda:0') data.h5py: 315 tensor([[311., 312., 313., 314., 315.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([316.], device='cuda:0') data.h5py: 316 tensor([[312., 313., 314., 315., 316.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([317.], device='cuda:0') data.h5py: 317 tensor([[313., 314., 315., 316., 317.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([318.], device='cuda:0') data.h5py: 318 tensor([[314., 315., 316., 317., 318.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([319.], device='cuda:0') data.h5py: 319 tensor([[315., 316., 317., 318., 319.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([320.], device='cuda:0') data.h5py: 320 tensor([[316., 317., 318., 319., 320.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([321.], device='cuda:0') data.h5py: 321 tensor([[317., 318., 319., 320., 321.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([322.], device='cuda:0') data.h5py: 322 tensor([[318., 319., 320., 321., 322.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([323.], device='cuda:0') data.h5py: 323 tensor([[319., 320., 321., 322., 323.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([324.], device='cuda:0') data.h5py: 324 tensor([[324., 324., 324., 324., 324.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([325.], device='cuda:0') data.h5py: 325 tensor([[324., 324., 324., 324., 325.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([326.], device='cuda:0') data.h5py: 326 tensor([[324., 324., 324., 325., 326.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([327.], device='cuda:0') data.h5py: 327 tensor([[324., 324., 325., 326., 327.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([328.], device='cuda:0') data.h5py: 328 tensor([[324., 325., 326., 327., 328.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([329.], device='cuda:0') data.h5py: 329 tensor([[325., 326., 327., 328., 329.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([330.], device='cuda:0') data.h5py: 330 tensor([[326., 327., 328., 329., 330.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([331.], device='cuda:0') data.h5py: 331 tensor([[327., 328., 329., 330., 331.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([332.], device='cuda:0') data.h5py: 332 tensor([[328., 329., 330., 331., 332.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([333.], device='cuda:0') data.h5py: 333 tensor([[329., 330., 331., 332., 333.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([334.], device='cuda:0') data.h5py: 334 tensor([[334., 334., 334., 334., 334.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([335.], device='cuda:0') data.h5py: 335 tensor([[334., 334., 334., 334., 335.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([336.], device='cuda:0') data.h5py: 336 tensor([[334., 334., 334., 335., 336.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([337.], device='cuda:0') data.h5py: 337 tensor([[334., 334., 335., 336., 337.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([338.], device='cuda:0') data.h5py: 338 tensor([[334., 335., 336., 337., 338.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([339.], device='cuda:0') data.h5py: 339 tensor([[335., 336., 337., 338., 339.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([340.], device='cuda:0') data.h5py: 340 tensor([[336., 337., 338., 339., 340.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([341.], device='cuda:0') data.h5py: 341 tensor([[337., 338., 339., 340., 341.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([342.], device='cuda:0') data.h5py: 342 tensor([[338., 339., 340., 341., 342.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([343.], device='cuda:0') data.h5py: 343 tensor([[339., 340., 341., 342., 343.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([344.], device='cuda:0') data.h5py: 344 tensor([[340., 341., 342., 343., 344.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([345.], device='cuda:0') data.h5py: 345 tensor([[341., 342., 343., 344., 345.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([346.], device='cuda:0') data.h5py: 346 tensor([[342., 343., 344., 345., 346.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([347.], device='cuda:0') data.h5py: 347 tensor([[343., 344., 345., 346., 347.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([348.], device='cuda:0') data.h5py: 348 tensor([[344., 345., 346., 347., 348.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([349.], device='cuda:0') data.h5py: 349 tensor([[345., 346., 347., 348., 349.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([350.], device='cuda:0') data.h5py: 350 tensor([[346., 347., 348., 349., 350.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([351.], device='cuda:0') data.h5py: 351 tensor([[347., 348., 349., 350., 351.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([352.], device='cuda:0') data.h5py: 352 tensor([[348., 349., 350., 351., 352.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([353.], device='cuda:0') data.h5py: 353 tensor([[349., 350., 351., 352., 353.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([354.], device='cuda:0') data.h5py: 354 tensor([[354., 354., 354., 354., 354.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([355.], device='cuda:0') data.h5py: 355 tensor([[354., 354., 354., 354., 355.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([356.], device='cuda:0') data.h5py: 356 tensor([[354., 354., 354., 355., 356.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([357.], device='cuda:0') data.h5py: 357 tensor([[354., 354., 355., 356., 357.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([358.], device='cuda:0') data.h5py: 358 tensor([[354., 355., 356., 357., 358.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([359.], device='cuda:0') data.h5py: 359 tensor([[355., 356., 357., 358., 359.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([360.], device='cuda:0') data.h5py: 360 tensor([[356., 357., 358., 359., 360.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([361.], device='cuda:0') data.h5py: 361 tensor([[357., 358., 359., 360., 361.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([362.], device='cuda:0') data.h5py: 362 tensor([[358., 359., 360., 361., 362.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([363.], device='cuda:0') data.h5py: 363 tensor([[359., 360., 361., 362., 363.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([364.], device='cuda:0') data.h5py: 364 tensor([[360., 361., 362., 363., 364.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([365.], device='cuda:0') data.h5py: 365 tensor([[361., 362., 363., 364., 365.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([366.], device='cuda:0') data.h5py: 366 tensor([[362., 363., 364., 365., 366.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([367.], device='cuda:0') data.h5py: 367 tensor([[363., 364., 365., 366., 367.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([368.], device='cuda:0') data.h5py: 368 tensor([[364., 365., 366., 367., 368.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([369.], device='cuda:0') data.h5py: 369 tensor([[365., 366., 367., 368., 369.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([370.], device='cuda:0') data.h5py: 370 tensor([[370., 370., 370., 370., 370.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([371.], device='cuda:0') data.h5py: 371 tensor([[370., 370., 370., 370., 371.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([372.], device='cuda:0') data.h5py: 372 tensor([[370., 370., 370., 371., 372.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([373.], device='cuda:0') data.h5py: 373 tensor([[370., 370., 371., 372., 373.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([374.], device='cuda:0') data.h5py: 374 tensor([[370., 371., 372., 373., 374.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([375.], device='cuda:0') data.h5py: 375 tensor([[371., 372., 373., 374., 375.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([376.], device='cuda:0') data.h5py: 376 tensor([[372., 373., 374., 375., 376.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([377.], device='cuda:0') data.h5py: 377 tensor([[373., 374., 375., 376., 377.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([378.], device='cuda:0') data.h5py: 378 tensor([[374., 375., 376., 377., 378.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([379.], device='cuda:0') data.h5py: 379 tensor([[375., 376., 377., 378., 379.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([380.], device='cuda:0') data.h5py: 380 tensor([[380., 380., 380., 380., 380.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([381.], device='cuda:0') data.h5py: 381 tensor([[380., 380., 380., 380., 381.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([382.], device='cuda:0') data.h5py: 382 tensor([[380., 380., 380., 381., 382.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([383.], device='cuda:0') data.h5py: 383 tensor([[380., 380., 381., 382., 383.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([384.], device='cuda:0') data.h5py: 384 tensor([[380., 381., 382., 383., 384.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([385.], device='cuda:0') data.h5py: 385 tensor([[381., 382., 383., 384., 385.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([386.], device='cuda:0') data.h5py: 386 tensor([[382., 383., 384., 385., 386.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([387.], device='cuda:0') data.h5py: 387 tensor([[383., 384., 385., 386., 387.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([388.], device='cuda:0') data.h5py: 388 tensor([[384., 385., 386., 387., 388.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([389.], device='cuda:0') data.h5py: 389 tensor([[385., 386., 387., 388., 389.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([390.], device='cuda:0') data.h5py: 390 tensor([[386., 387., 388., 389., 390.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([391.], device='cuda:0') data.h5py: 391 tensor([[387., 388., 389., 390., 391.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([392.], device='cuda:0') data.h5py: 392 tensor([[388., 389., 390., 391., 392.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([393.], device='cuda:0') data.h5py: 393 tensor([[389., 390., 391., 392., 393.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([394.], device='cuda:0') data.h5py: 394 tensor([[390., 391., 392., 393., 394.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([395.], device='cuda:0') data.h5py: 395 tensor([[395., 395., 395., 395., 395.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([396.], device='cuda:0') data.h5py: 396 tensor([[395., 395., 395., 395., 396.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([397.], device='cuda:0') data.h5py: 397 tensor([[395., 395., 395., 396., 397.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([398.], device='cuda:0') data.h5py: 398 tensor([[395., 395., 396., 397., 398.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([399.], device='cuda:0') data.h5py: 399 tensor([[395., 396., 397., 398., 399.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([400.], device='cuda:0') data.h5py: 400 tensor([[396., 397., 398., 399., 400.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([401.], device='cuda:0') data.h5py: 401 tensor([[397., 398., 399., 400., 401.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([402.], device='cuda:0') data.h5py: 402 tensor([[398., 399., 400., 401., 402.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([403.], device='cuda:0') data.h5py: 403 tensor([[399., 400., 401., 402., 403.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([404.], device='cuda:0') data.h5py: 404 tensor([[400., 401., 402., 403., 404.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([405.], device='cuda:0') data.h5py: 405 tensor([[401., 402., 403., 404., 405.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([406.], device='cuda:0') data.h5py: 406 tensor([[402., 403., 404., 405., 406.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([407.], device='cuda:0') data.h5py: 407 tensor([[403., 404., 405., 406., 407.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([408.], device='cuda:0') data.h5py: 408 tensor([[408., 408., 408., 408., 408.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([409.], device='cuda:0') data.h5py: 409 tensor([[408., 408., 408., 408., 409.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([410.], device='cuda:0') data.h5py: 410 tensor([[408., 408., 408., 409., 410.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([411.], device='cuda:0') data.h5py: 411 tensor([[408., 408., 409., 410., 411.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([412.], device='cuda:0') data.h5py: 412 tensor([[408., 409., 410., 411., 412.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([413.], device='cuda:0') data.h5py: 413 tensor([[409., 410., 411., 412., 413.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([414.], device='cuda:0') data.h5py: 414 tensor([[410., 411., 412., 413., 414.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([415.], device='cuda:0') data.h5py: 415 tensor([[411., 412., 413., 414., 415.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([416.], device='cuda:0') data.h5py: 416 tensor([[412., 413., 414., 415., 416.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([417.], device='cuda:0') data.h5py: 417 tensor([[413., 414., 415., 416., 417.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([418.], device='cuda:0') data.h5py: 418 tensor([[414., 415., 416., 417., 418.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([419.], device='cuda:0') data.h5py: 419 tensor([[415., 416., 417., 418., 419.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([420.], device='cuda:0') data.h5py: 420 tensor([[416., 417., 418., 419., 420.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([421.], device='cuda:0') data.h5py: 421 tensor([[417., 418., 419., 420., 421.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([422.], device='cuda:0') data.h5py: 422 tensor([[418., 419., 420., 421., 422.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([423.], device='cuda:0') data.h5py: 423 tensor([[419., 420., 421., 422., 423.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([424.], device='cuda:0') data.h5py: 424 tensor([[420., 421., 422., 423., 424.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([425.], device='cuda:0') data.h5py: 425 tensor([[421., 422., 423., 424., 425.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([426.], device='cuda:0') data.h5py: 426 tensor([[422., 423., 424., 425., 426.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([427.], device='cuda:0') data.h5py: 427 tensor([[423., 424., 425., 426., 427.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([428.], device='cuda:0') data.h5py: 428 tensor([[428., 428., 428., 428., 428.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([429.], device='cuda:0') data.h5py: 429 tensor([[428., 428., 428., 428., 429.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([430.], device='cuda:0') data.h5py: 430 tensor([[428., 428., 428., 429., 430.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([431.], device='cuda:0') data.h5py: 431 tensor([[428., 428., 429., 430., 431.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([432.], device='cuda:0') data.h5py: 432 tensor([[428., 429., 430., 431., 432.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([433.], device='cuda:0') data.h5py: 433 tensor([[429., 430., 431., 432., 433.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([434.], device='cuda:0') data.h5py: 434 tensor([[430., 431., 432., 433., 434.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([435.], device='cuda:0') data.h5py: 435 tensor([[431., 432., 433., 434., 435.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([436.], device='cuda:0') data.h5py: 436 tensor([[432., 433., 434., 435., 436.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([437.], device='cuda:0') data.h5py: 437 tensor([[433., 434., 435., 436., 437.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([438.], device='cuda:0') data.h5py: 438 tensor([[434., 435., 436., 437., 438.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([439.], device='cuda:0') data.h5py: 439 tensor([[435., 436., 437., 438., 439.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([440.], device='cuda:0') data.h5py: 440 tensor([[436., 437., 438., 439., 440.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([441.], device='cuda:0') data.h5py: 441 tensor([[437., 438., 439., 440., 441.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([442.], device='cuda:0') data.h5py: 442 tensor([[438., 439., 440., 441., 442.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([443.], device='cuda:0') data.h5py: 443 tensor([[439., 440., 441., 442., 443.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([444.], device='cuda:0') data.h5py: 444 tensor([[440., 441., 442., 443., 444.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([445.], device='cuda:0') data.h5py: 445 tensor([[441., 442., 443., 444., 445.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([446.], device='cuda:0') data.h5py: 446 tensor([[442., 443., 444., 445., 446.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([447.], device='cuda:0') data.h5py: 447 tensor([[443., 444., 445., 446., 447.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([448.], device='cuda:0') data.h5py: 448 tensor([[448., 448., 448., 448., 448.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([449.], device='cuda:0') data.h5py: 449 tensor([[448., 448., 448., 448., 449.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([450.], device='cuda:0') data.h5py: 450 tensor([[448., 448., 448., 449., 450.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([451.], device='cuda:0') data.h5py: 451 tensor([[448., 448., 449., 450., 451.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([452.], device='cuda:0') data.h5py: 452 tensor([[448., 449., 450., 451., 452.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([453.], device='cuda:0') data.h5py: 453 tensor([[449., 450., 451., 452., 453.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([454.], device='cuda:0') data.h5py: 454 tensor([[450., 451., 452., 453., 454.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([455.], device='cuda:0') data.h5py: 455 tensor([[451., 452., 453., 454., 455.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([456.], device='cuda:0') data.h5py: 456 tensor([[452., 453., 454., 455., 456.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([457.], device='cuda:0') data.h5py: 457 tensor([[453., 454., 455., 456., 457.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([458.], device='cuda:0') data.h5py: 458 tensor([[454., 455., 456., 457., 458.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([459.], device='cuda:0') data.h5py: 459 tensor([[455., 456., 457., 458., 459.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([460.], device='cuda:0') data.h5py: 460 tensor([[456., 457., 458., 459., 460.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([461.], device='cuda:0') data.h5py: 461 tensor([[457., 458., 459., 460., 461.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([462.], device='cuda:0') data.h5py: 462 tensor([[458., 459., 460., 461., 462.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([463.], device='cuda:0') data.h5py: 463 tensor([[459., 460., 461., 462., 463.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([464.], device='cuda:0') data.h5py: 464 tensor([[460., 461., 462., 463., 464.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([465.], device='cuda:0') data.h5py: 465 tensor([[461., 462., 463., 464., 465.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([466.], device='cuda:0') data.h5py: 466 tensor([[462., 463., 464., 465., 466.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([467.], device='cuda:0') data.h5py: 467 tensor([[463., 464., 465., 466., 467.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([468.], device='cuda:0') data.h5py: 468 tensor([[468., 468., 468., 468., 468.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([469.], device='cuda:0') data.h5py: 469 tensor([[468., 468., 468., 468., 469.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([470.], device='cuda:0') data.h5py: 470 tensor([[468., 468., 468., 469., 470.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([471.], device='cuda:0') data.h5py: 471 tensor([[468., 468., 469., 470., 471.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([472.], device='cuda:0') data.h5py: 472 tensor([[468., 469., 470., 471., 472.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([473.], device='cuda:0') data.h5py: 473 tensor([[469., 470., 471., 472., 473.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([474.], device='cuda:0') data.h5py: 474 tensor([[470., 471., 472., 473., 474.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([475.], device='cuda:0') data.h5py: 475 tensor([[471., 472., 473., 474., 475.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([476.], device='cuda:0') data.h5py: 476 tensor([[472., 473., 474., 475., 476.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([477.], device='cuda:0') data.h5py: 477 tensor([[473., 474., 475., 476., 477.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([478.], device='cuda:0') data.h5py: 478 tensor([[474., 475., 476., 477., 478.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([479.], device='cuda:0') data.h5py: 479 tensor([[475., 476., 477., 478., 479.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([480.], device='cuda:0') data.h5py: 480 tensor([[476., 477., 478., 479., 480.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([481.], device='cuda:0') data.h5py: 481 tensor([[477., 478., 479., 480., 481.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([482.], device='cuda:0') data.h5py: 482 tensor([[478., 479., 480., 481., 482.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([483.], device='cuda:0') data.h5py: 483 tensor([[479., 480., 481., 482., 483.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([484.], device='cuda:0') data.h5py: 484 tensor([[480., 481., 482., 483., 484.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([485.], device='cuda:0') data.h5py: 485 tensor([[481., 482., 483., 484., 485.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([486.], device='cuda:0') data.h5py: 486 tensor([[486., 486., 486., 486., 486.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([487.], device='cuda:0') data.h5py: 487 tensor([[486., 486., 486., 486., 487.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([488.], device='cuda:0') data.h5py: 488 tensor([[486., 486., 486., 487., 488.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([489.], device='cuda:0') data.h5py: 489 tensor([[486., 486., 487., 488., 489.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([490.], device='cuda:0') data.h5py: 490 tensor([[486., 487., 488., 489., 490.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([491.], device='cuda:0') data.h5py: 491 tensor([[487., 488., 489., 490., 491.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([492.], device='cuda:0') data.h5py: 492 tensor([[488., 489., 490., 491., 492.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([493.], device='cuda:0') data.h5py: 493 tensor([[489., 490., 491., 492., 493.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([494.], device='cuda:0') data.h5py: 494 tensor([[490., 491., 492., 493., 494.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([495.], device='cuda:0') data.h5py: 495 tensor([[491., 492., 493., 494., 495.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([496.], device='cuda:0') data.h5py: 496 tensor([[492., 493., 494., 495., 496.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([497.], device='cuda:0') data.h5py: 497 tensor([[493., 494., 495., 496., 497.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([498.], device='cuda:0') data.h5py: 498 tensor([[494., 495., 496., 497., 498.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([499.], device='cuda:0') data.h5py: 499 tensor([[495., 496., 497., 498., 499.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([500.], device='cuda:0') data.h5py: 500 tensor([[500., 500., 500., 500., 500.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([501.], device='cuda:0') data.h5py: 501 tensor([[500., 500., 500., 500., 501.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([502.], device='cuda:0') data.h5py: 502 tensor([[500., 500., 500., 501., 502.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([503.], device='cuda:0') data.h5py: 503 tensor([[500., 500., 501., 502., 503.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([504.], device='cuda:0') data.h5py: 504 tensor([[500., 501., 502., 503., 504.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([505.], device='cuda:0') data.h5py: 505 tensor([[501., 502., 503., 504., 505.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([506.], device='cuda:0') data.h5py: 506 tensor([[502., 503., 504., 505., 506.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([507.], device='cuda:0') data.h5py: 507 tensor([[503., 504., 505., 506., 507.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([508.], device='cuda:0') data.h5py: 508 tensor([[504., 505., 506., 507., 508.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([509.], device='cuda:0') data.h5py: 509 tensor([[505., 506., 507., 508., 509.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([510.], device='cuda:0') data.h5py: 510 tensor([[510., 510., 510., 510., 510.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([511.], device='cuda:0') data.h5py: 511 tensor([[510., 510., 510., 510., 511.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([512.], device='cuda:0') data.h5py: 512 tensor([[510., 510., 510., 511., 512.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([513.], device='cuda:0') data.h5py: 513 tensor([[510., 510., 511., 512., 513.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([514.], device='cuda:0') data.h5py: 514 tensor([[510., 511., 512., 513., 514.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([515.], device='cuda:0') data.h5py: 515 tensor([[511., 512., 513., 514., 515.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([516.], device='cuda:0') data.h5py: 516 tensor([[512., 513., 514., 515., 516.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([517.], device='cuda:0') data.h5py: 517 tensor([[513., 514., 515., 516., 517.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([518.], device='cuda:0') data.h5py: 518 tensor([[514., 515., 516., 517., 518.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([519.], device='cuda:0') data.h5py: 519 tensor([[515., 516., 517., 518., 519.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([520.], device='cuda:0') data.h5py: 520 tensor([[516., 517., 518., 519., 520.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([521.], device='cuda:0') data.h5py: 521 tensor([[517., 518., 519., 520., 521.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([522.], device='cuda:0') data.h5py: 522 tensor([[518., 519., 520., 521., 522.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([523.], device='cuda:0') data.h5py: 523 tensor([[519., 520., 521., 522., 523.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([524.], device='cuda:0') data.h5py: 524 tensor([[520., 521., 522., 523., 524.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([525.], device='cuda:0') data.h5py: 525 tensor([[521., 522., 523., 524., 525.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([526.], device='cuda:0') data.h5py: 526 tensor([[522., 523., 524., 525., 526.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([527.], device='cuda:0') data.h5py: 527 tensor([[523., 524., 525., 526., 527.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([528.], device='cuda:0') data.h5py: 528 tensor([[528., 528., 528., 528., 528.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([529.], device='cuda:0') data.h5py: 529 tensor([[528., 528., 528., 528., 529.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([530.], device='cuda:0') data.h5py: 530 tensor([[528., 528., 528., 529., 530.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([531.], device='cuda:0') data.h5py: 531 tensor([[528., 528., 529., 530., 531.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([532.], device='cuda:0') data.h5py: 532 tensor([[528., 529., 530., 531., 532.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([533.], device='cuda:0') data.h5py: 533 tensor([[529., 530., 531., 532., 533.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([534.], device='cuda:0') data.h5py: 534 tensor([[530., 531., 532., 533., 534.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([535.], device='cuda:0') data.h5py: 535 tensor([[531., 532., 533., 534., 535.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([536.], device='cuda:0') data.h5py: 536 tensor([[532., 533., 534., 535., 536.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([537.], device='cuda:0') data.h5py: 537 tensor([[533., 534., 535., 536., 537.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([538.], device='cuda:0') data.h5py: 538 tensor([[534., 535., 536., 537., 538.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([539.], device='cuda:0') data.h5py: 539 tensor([[535., 536., 537., 538., 539.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([540.], device='cuda:0') data.h5py: 540 tensor([[536., 537., 538., 539., 540.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([541.], device='cuda:0') data.h5py: 541 tensor([[541., 541., 541., 541., 541.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([542.], device='cuda:0') data.h5py: 542 tensor([[541., 541., 541., 541., 542.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([543.], device='cuda:0') data.h5py: 543 tensor([[541., 541., 541., 542., 543.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([544.], device='cuda:0') data.h5py: 544 tensor([[541., 541., 542., 543., 544.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([545.], device='cuda:0') data.h5py: 545 tensor([[541., 542., 543., 544., 545.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([546.], device='cuda:0') data.h5py: 546 tensor([[542., 543., 544., 545., 546.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([547.], device='cuda:0') data.h5py: 547 tensor([[543., 544., 545., 546., 547.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([548.], device='cuda:0') data.h5py: 548 tensor([[544., 545., 546., 547., 548.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([549.], device='cuda:0') data.h5py: 549 tensor([[545., 546., 547., 548., 549.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([550.], device='cuda:0') data.h5py: 550 tensor([[546., 547., 548., 549., 550.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([551.], device='cuda:0') data.h5py: 551 tensor([[547., 548., 549., 550., 551.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([552.], device='cuda:0') data.h5py: 552 tensor([[548., 549., 550., 551., 552.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([553.], device='cuda:0') data.h5py: 553 tensor([[549., 550., 551., 552., 553.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([554.], device='cuda:0') data.h5py: 554 tensor([[550., 551., 552., 553., 554.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([555.], device='cuda:0') data.h5py: 555 tensor([[555., 555., 555., 555., 555.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([556.], device='cuda:0') data.h5py: 556 tensor([[555., 555., 555., 555., 556.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([557.], device='cuda:0') data.h5py: 557 tensor([[555., 555., 555., 556., 557.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([558.], device='cuda:0') data.h5py: 558 tensor([[555., 555., 556., 557., 558.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([559.], device='cuda:0') data.h5py: 559 tensor([[555., 556., 557., 558., 559.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([560.], device='cuda:0') data.h5py: 560 tensor([[556., 557., 558., 559., 560.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([561.], device='cuda:0') data.h5py: 561 tensor([[557., 558., 559., 560., 561.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([562.], device='cuda:0') data.h5py: 562 tensor([[558., 559., 560., 561., 562.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([563.], device='cuda:0') data.h5py: 563 tensor([[559., 560., 561., 562., 563.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([564.], device='cuda:0') data.h5py: 564 tensor([[560., 561., 562., 563., 564.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([565.], device='cuda:0') data.h5py: 565 tensor([[561., 562., 563., 564., 565.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([566.], device='cuda:0') data.h5py: 566 tensor([[562., 563., 564., 565., 566.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([567.], device='cuda:0') data.h5py: 567 tensor([[563., 564., 565., 566., 567.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([568.], device='cuda:0') data.h5py: 568 tensor([[564., 565., 566., 567., 568.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([569.], device='cuda:0') data.h5py: 569 tensor([[565., 566., 567., 568., 569.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([570.], device='cuda:0') data.h5py: 570 tensor([[566., 567., 568., 569., 570.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([571.], device='cuda:0') data.h5py: 571 tensor([[567., 568., 569., 570., 571.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([572.], device='cuda:0') data.h5py: 572 tensor([[568., 569., 570., 571., 572.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([573.], device='cuda:0') data.h5py: 573 tensor([[573., 573., 573., 573., 573.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([574.], device='cuda:0') data.h5py: 574 tensor([[573., 573., 573., 573., 574.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([575.], device='cuda:0') data.h5py: 575 tensor([[573., 573., 573., 574., 575.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([576.], device='cuda:0') data.h5py: 576 tensor([[573., 573., 574., 575., 576.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([577.], device='cuda:0') data.h5py: 577 tensor([[573., 574., 575., 576., 577.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([578.], device='cuda:0') data.h5py: 578 tensor([[574., 575., 576., 577., 578.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([579.], device='cuda:0') data.h5py: 579 tensor([[575., 576., 577., 578., 579.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([580.], device='cuda:0') data.h5py: 580 tensor([[576., 577., 578., 579., 580.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([581.], device='cuda:0') data.h5py: 581 tensor([[577., 578., 579., 580., 581.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([582.], device='cuda:0') data.h5py: 582 tensor([[578., 579., 580., 581., 582.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([583.], device='cuda:0') data.h5py: 583 tensor([[579., 580., 581., 582., 583.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([584.], device='cuda:0') data.h5py: 584 tensor([[580., 581., 582., 583., 584.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([585.], device='cuda:0') data.h5py: 585 tensor([[581., 582., 583., 584., 585.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([586.], device='cuda:0') data.h5py: 586 tensor([[582., 583., 584., 585., 586.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([587.], device='cuda:0') data.h5py: 587 tensor([[583., 584., 585., 586., 587.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([588.], device='cuda:0') data.h5py: 588 tensor([[584., 585., 586., 587., 588.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([589.], device='cuda:0') data.h5py: 589 tensor([[589., 589., 589., 589., 589.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([590.], device='cuda:0') data.h5py: 590 tensor([[589., 589., 589., 589., 590.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([591.], device='cuda:0') data.h5py: 591 tensor([[589., 589., 589., 590., 591.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([592.], device='cuda:0') data.h5py: 592 tensor([[589., 589., 590., 591., 592.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([593.], device='cuda:0') data.h5py: 593 tensor([[589., 590., 591., 592., 593.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([594.], device='cuda:0') data.h5py: 594 tensor([[590., 591., 592., 593., 594.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([595.], device='cuda:0') data.h5py: 595 tensor([[591., 592., 593., 594., 595.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([596.], device='cuda:0') data.h5py: 596 tensor([[592., 593., 594., 595., 596.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([597.], device='cuda:0') data.h5py: 597 tensor([[593., 594., 595., 596., 597.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([598.], device='cuda:0') data.h5py: 598 tensor([[594., 595., 596., 597., 598.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([599.], device='cuda:0') data.h5py: 599 tensor([[595., 596., 597., 598., 599.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([600.], device='cuda:0') data.h5py: 600 tensor([[596., 597., 598., 599., 600.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([601.], device='cuda:0') data.h5py: 601 tensor([[601., 601., 601., 601., 601.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([602.], device='cuda:0') data.h5py: 602 tensor([[601., 601., 601., 601., 602.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([603.], device='cuda:0') data.h5py: 603 tensor([[601., 601., 601., 602., 603.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([604.], device='cuda:0') data.h5py: 604 tensor([[601., 601., 602., 603., 604.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([605.], device='cuda:0') data.h5py: 605 tensor([[601., 602., 603., 604., 605.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([606.], device='cuda:0') data.h5py: 606 tensor([[602., 603., 604., 605., 606.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([607.], device='cuda:0') data.h5py: 607 tensor([[603., 604., 605., 606., 607.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([608.], device='cuda:0') data.h5py: 608 tensor([[604., 605., 606., 607., 608.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([609.], device='cuda:0') data.h5py: 609 tensor([[605., 606., 607., 608., 609.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([610.], device='cuda:0') data.h5py: 610 tensor([[606., 607., 608., 609., 610.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([611.], device='cuda:0') data.h5py: 611 tensor([[607., 608., 609., 610., 611.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([612.], device='cuda:0') data.h5py: 612 tensor([[608., 609., 610., 611., 612.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([613.], device='cuda:0') data.h5py: 613 tensor([[609., 610., 611., 612., 613.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([614.], device='cuda:0') data.h5py: 614 tensor([[610., 611., 612., 613., 614.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([615.], device='cuda:0') data.h5py: 615 tensor([[615., 615., 615., 615., 615.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([616.], device='cuda:0') data.h5py: 616 tensor([[615., 615., 615., 615., 616.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([617.], device='cuda:0') data.h5py: 617 tensor([[615., 615., 615., 616., 617.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([618.], device='cuda:0') data.h5py: 618 tensor([[615., 615., 616., 617., 618.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([619.], device='cuda:0') data.h5py: 619 tensor([[615., 616., 617., 618., 619.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([620.], device='cuda:0') data.h5py: 620 tensor([[616., 617., 618., 619., 620.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([621.], device='cuda:0') data.h5py: 621 tensor([[617., 618., 619., 620., 621.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([622.], device='cuda:0') data.h5py: 622 tensor([[618., 619., 620., 621., 622.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([623.], device='cuda:0') data.h5py: 623 tensor([[619., 620., 621., 622., 623.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([624.], device='cuda:0') data.h5py: 624 tensor([[620., 621., 622., 623., 624.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([625.], device='cuda:0') data.h5py: 625 tensor([[621., 622., 623., 624., 625.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([626.], device='cuda:0') data.h5py: 626 tensor([[622., 623., 624., 625., 626.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([627.], device='cuda:0') data.h5py: 627 tensor([[623., 624., 625., 626., 627.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([628.], device='cuda:0') data.h5py: 628 tensor([[624., 625., 626., 627., 628.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([629.], device='cuda:0') data.h5py: 629 tensor([[625., 626., 627., 628., 629.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([630.], device='cuda:0') data.h5py: 630 tensor([[626., 627., 628., 629., 630.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([631.], device='cuda:0') data.h5py: 631 tensor([[631., 631., 631., 631., 631.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([632.], device='cuda:0') data.h5py: 632 tensor([[631., 631., 631., 631., 632.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([633.], device='cuda:0') data.h5py: 633 tensor([[631., 631., 631., 632., 633.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([634.], device='cuda:0') data.h5py: 634 tensor([[631., 631., 632., 633., 634.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([635.], device='cuda:0') data.h5py: 635 tensor([[631., 632., 633., 634., 635.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([636.], device='cuda:0') data.h5py: 636 tensor([[632., 633., 634., 635., 636.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([637.], device='cuda:0') data.h5py: 637 tensor([[633., 634., 635., 636., 637.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([638.], device='cuda:0') data.h5py: 638 tensor([[634., 635., 636., 637., 638.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([639.], device='cuda:0') data.h5py: 639 tensor([[635., 636., 637., 638., 639.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([640.], device='cuda:0') data.h5py: 640 tensor([[636., 637., 638., 639., 640.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([641.], device='cuda:0') data.h5py: 641 tensor([[637., 638., 639., 640., 641.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([642.], device='cuda:0') data.h5py: 642 tensor([[638., 639., 640., 641., 642.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([643.], device='cuda:0') data.h5py: 643 tensor([[639., 640., 641., 642., 643.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([644.], device='cuda:0') data.h5py: 644 tensor([[640., 641., 642., 643., 644.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([645.], device='cuda:0') data.h5py: 645 tensor([[641., 642., 643., 644., 645.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([646.], device='cuda:0') data.h5py: 646 tensor([[642., 643., 644., 645., 646.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([647.], device='cuda:0') data.h5py: 647 tensor([[643., 644., 645., 646., 647.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([648.], device='cuda:0') data.h5py: 648 tensor([[644., 645., 646., 647., 648.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([649.], device='cuda:0') data.h5py: 649 tensor([[645., 646., 647., 648., 649.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([650.], device='cuda:0') data.h5py: 650 tensor([[646., 647., 648., 649., 650.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([651.], device='cuda:0') data.h5py: 651 tensor([[651., 651., 651., 651., 651.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([652.], device='cuda:0') data.h5py: 652 tensor([[651., 651., 651., 651., 652.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([653.], device='cuda:0') data.h5py: 653 tensor([[651., 651., 651., 652., 653.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([654.], device='cuda:0') data.h5py: 654 tensor([[651., 651., 652., 653., 654.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([655.], device='cuda:0') data.h5py: 655 tensor([[651., 652., 653., 654., 655.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([656.], device='cuda:0') data.h5py: 656 tensor([[652., 653., 654., 655., 656.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([657.], device='cuda:0') data.h5py: 657 tensor([[653., 654., 655., 656., 657.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([658.], device='cuda:0') data.h5py: 658 tensor([[654., 655., 656., 657., 658.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([659.], device='cuda:0') data.h5py: 659 tensor([[655., 656., 657., 658., 659.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([660.], device='cuda:0') data.h5py: 660 tensor([[656., 657., 658., 659., 660.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([661.], device='cuda:0') data.h5py: 661 tensor([[657., 658., 659., 660., 661.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([662.], device='cuda:0') data.h5py: 662 tensor([[658., 659., 660., 661., 662.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([663.], device='cuda:0') data.h5py: 663 tensor([[659., 660., 661., 662., 663.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([664.], device='cuda:0') data.h5py: 664 tensor([[660., 661., 662., 663., 664.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([665.], device='cuda:0') data.h5py: 665 tensor([[661., 662., 663., 664., 665.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([666.], device='cuda:0') data.h5py: 666 tensor([[662., 663., 664., 665., 666.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([667.], device='cuda:0') data.h5py: 667 tensor([[663., 664., 665., 666., 667.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([668.], device='cuda:0') data.h5py: 668 tensor([[664., 665., 666., 667., 668.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([669.], device='cuda:0') data.h5py: 669 tensor([[669., 669., 669., 669., 669.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([670.], device='cuda:0') data.h5py: 670 tensor([[669., 669., 669., 669., 670.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([671.], device='cuda:0') data.h5py: 671 tensor([[669., 669., 669., 670., 671.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([672.], device='cuda:0') data.h5py: 672 tensor([[669., 669., 670., 671., 672.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([673.], device='cuda:0') data.h5py: 673 tensor([[669., 670., 671., 672., 673.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([674.], device='cuda:0') data.h5py: 674 tensor([[670., 671., 672., 673., 674.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([675.], device='cuda:0') data.h5py: 675 tensor([[671., 672., 673., 674., 675.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([676.], device='cuda:0') data.h5py: 676 tensor([[672., 673., 674., 675., 676.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([677.], device='cuda:0') data.h5py: 677 tensor([[673., 674., 675., 676., 677.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([678.], device='cuda:0') data.h5py: 678 tensor([[674., 675., 676., 677., 678.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([679.], device='cuda:0') data.h5py: 679 tensor([[675., 676., 677., 678., 679.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([680.], device='cuda:0') data.h5py: 680 tensor([[676., 677., 678., 679., 680.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([681.], device='cuda:0') data.h5py: 681 tensor([[677., 678., 679., 680., 681.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([682.], device='cuda:0') data.h5py: 682 tensor([[678., 679., 680., 681., 682.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([683.], device='cuda:0') data.h5py: 683 tensor([[679., 680., 681., 682., 683.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([684.], device='cuda:0') data.h5py: 684 tensor([[680., 681., 682., 683., 684.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([685.], device='cuda:0') data.h5py: 685 tensor([[685., 685., 685., 685., 685.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([686.], device='cuda:0') data.h5py: 686 tensor([[685., 685., 685., 685., 686.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([687.], device='cuda:0') data.h5py: 687 tensor([[685., 685., 685., 686., 687.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([688.], device='cuda:0') data.h5py: 688 tensor([[685., 685., 686., 687., 688.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([689.], device='cuda:0') data.h5py: 689 tensor([[685., 686., 687., 688., 689.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([690.], device='cuda:0') data.h5py: 690 tensor([[686., 687., 688., 689., 690.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([691.], device='cuda:0') data.h5py: 691 tensor([[687., 688., 689., 690., 691.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([692.], device='cuda:0') data.h5py: 692 tensor([[688., 689., 690., 691., 692.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([693.], device='cuda:0') data.h5py: 693 tensor([[689., 690., 691., 692., 693.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([694.], device='cuda:0') data.h5py: 694 tensor([[690., 691., 692., 693., 694.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([695.], device='cuda:0') data.h5py: 695 tensor([[691., 692., 693., 694., 695.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([696.], device='cuda:0') data.h5py: 696 tensor([[692., 693., 694., 695., 696.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([697.], device='cuda:0') data.h5py: 697 tensor([[693., 694., 695., 696., 697.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([698.], device='cuda:0') data.h5py: 698 tensor([[694., 695., 696., 697., 698.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([699.], device='cuda:0') data.h5py: 699 tensor([[695., 696., 697., 698., 699.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([700.], device='cuda:0') data.h5py: 700 tensor([[696., 697., 698., 699., 700.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([701.], device='cuda:0') data.h5py: 701 tensor([[697., 698., 699., 700., 701.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([702.], device='cuda:0') data.h5py: 702 tensor([[698., 699., 700., 701., 702.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([703.], device='cuda:0') data.h5py: 703 tensor([[699., 700., 701., 702., 703.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([704.], device='cuda:0') data.h5py: 704 tensor([[704., 704., 704., 704., 704.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([705.], device='cuda:0') data.h5py: 705 tensor([[704., 704., 704., 704., 705.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([706.], device='cuda:0') data.h5py: 706 tensor([[704., 704., 704., 705., 706.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([707.], device='cuda:0') data.h5py: 707 tensor([[704., 704., 705., 706., 707.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([708.], device='cuda:0') data.h5py: 708 tensor([[704., 705., 706., 707., 708.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([709.], device='cuda:0') data.h5py: 709 tensor([[705., 706., 707., 708., 709.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([710.], device='cuda:0') data.h5py: 710 tensor([[706., 707., 708., 709., 710.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([711.], device='cuda:0') data.h5py: 711 tensor([[707., 708., 709., 710., 711.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([712.], device='cuda:0') data.h5py: 712 tensor([[708., 709., 710., 711., 712.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([713.], device='cuda:0') data.h5py: 713 tensor([[709., 710., 711., 712., 713.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([714.], device='cuda:0') data.h5py: 714 tensor([[710., 711., 712., 713., 714.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([715.], device='cuda:0') data.h5py: 715 tensor([[711., 712., 713., 714., 715.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([716.], device='cuda:0') data.h5py: 716 tensor([[712., 713., 714., 715., 716.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([717.], device='cuda:0') data.h5py: 717 tensor([[713., 714., 715., 716., 717.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([718.], device='cuda:0') data.h5py: 718 tensor([[714., 715., 716., 717., 718.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([719.], device='cuda:0') data.h5py: 719 tensor([[715., 716., 717., 718., 719.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([720.], device='cuda:0') data.h5py: 720 tensor([[716., 717., 718., 719., 720.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([721.], device='cuda:0') data.h5py: 721 tensor([[721., 721., 721., 721., 721.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([722.], device='cuda:0') data.h5py: 722 tensor([[721., 721., 721., 721., 722.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([723.], device='cuda:0') data.h5py: 723 tensor([[721., 721., 721., 722., 723.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([724.], device='cuda:0') data.h5py: 724 tensor([[721., 721., 722., 723., 724.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([725.], device='cuda:0') data.h5py: 725 tensor([[721., 722., 723., 724., 725.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([726.], device='cuda:0') data.h5py: 726 tensor([[722., 723., 724., 725., 726.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([727.], device='cuda:0') data.h5py: 727 tensor([[723., 724., 725., 726., 727.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([728.], device='cuda:0') data.h5py: 728 tensor([[724., 725., 726., 727., 728.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([729.], device='cuda:0') data.h5py: 729 tensor([[725., 726., 727., 728., 729.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([730.], device='cuda:0') data.h5py: 730 tensor([[726., 727., 728., 729., 730.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([731.], device='cuda:0') data.h5py: 731 tensor([[727., 728., 729., 730., 731.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([732.], device='cuda:0') data.h5py: 732 tensor([[728., 729., 730., 731., 732.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([733.], device='cuda:0') data.h5py: 733 tensor([[729., 730., 731., 732., 733.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([734.], device='cuda:0') data.h5py: 734 tensor([[730., 731., 732., 733., 734.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([735.], device='cuda:0') data.h5py: 735 tensor([[735., 735., 735., 735., 735.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([736.], device='cuda:0') data.h5py: 736 tensor([[735., 735., 735., 735., 736.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([737.], device='cuda:0') data.h5py: 737 tensor([[735., 735., 735., 736., 737.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([738.], device='cuda:0') data.h5py: 738 tensor([[735., 735., 736., 737., 738.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([739.], device='cuda:0') data.h5py: 739 tensor([[735., 736., 737., 738., 739.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([740.], device='cuda:0') data.h5py: 740 tensor([[736., 737., 738., 739., 740.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([741.], device='cuda:0') data.h5py: 741 tensor([[737., 738., 739., 740., 741.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([742.], device='cuda:0') data.h5py: 742 tensor([[738., 739., 740., 741., 742.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([743.], device='cuda:0') data.h5py: 743 tensor([[739., 740., 741., 742., 743.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([744.], device='cuda:0') data.h5py: 744 tensor([[740., 741., 742., 743., 744.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([745.], device='cuda:0') data.h5py: 745 tensor([[741., 742., 743., 744., 745.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([746.], device='cuda:0') data.h5py: 746 tensor([[742., 743., 744., 745., 746.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([747.], device='cuda:0') data.h5py: 747 tensor([[743., 744., 745., 746., 747.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([748.], device='cuda:0') data.h5py: 748 tensor([[744., 745., 746., 747., 748.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([749.], device='cuda:0') data.h5py: 749 tensor([[745., 746., 747., 748., 749.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([750.], device='cuda:0') data.h5py: 750 tensor([[746., 747., 748., 749., 750.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([751.], device='cuda:0') data.h5py: 751 tensor([[747., 748., 749., 750., 751.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([752.], device='cuda:0') data.h5py: 752 tensor([[748., 749., 750., 751., 752.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([753.], device='cuda:0') data.h5py: 753 tensor([[749., 750., 751., 752., 753.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([754.], device='cuda:0') data.h5py: 754 tensor([[750., 751., 752., 753., 754.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([755.], device='cuda:0') data.h5py: 755 tensor([[755., 755., 755., 755., 755.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([756.], device='cuda:0') data.h5py: 756 tensor([[755., 755., 755., 755., 756.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([757.], device='cuda:0') data.h5py: 757 tensor([[755., 755., 755., 756., 757.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([758.], device='cuda:0') data.h5py: 758 tensor([[755., 755., 756., 757., 758.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([759.], device='cuda:0') data.h5py: 759 tensor([[755., 756., 757., 758., 759.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([760.], device='cuda:0') data.h5py: 760 tensor([[756., 757., 758., 759., 760.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([761.], device='cuda:0') data.h5py: 761 tensor([[757., 758., 759., 760., 761.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([762.], device='cuda:0') data.h5py: 762 tensor([[758., 759., 760., 761., 762.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([763.], device='cuda:0') data.h5py: 763 tensor([[759., 760., 761., 762., 763.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([764.], device='cuda:0') data.h5py: 764 tensor([[760., 761., 762., 763., 764.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([765.], device='cuda:0') data.h5py: 765 tensor([[765., 765., 765., 765., 765.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([766.], device='cuda:0') data.h5py: 766 tensor([[765., 765., 765., 765., 766.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([767.], device='cuda:0') data.h5py: 767 tensor([[765., 765., 765., 766., 767.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([768.], device='cuda:0') data.h5py: 768 tensor([[765., 765., 766., 767., 768.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([769.], device='cuda:0') data.h5py: 769 tensor([[765., 766., 767., 768., 769.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([770.], device='cuda:0') data.h5py: 770 tensor([[766., 767., 768., 769., 770.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([771.], device='cuda:0') data.h5py: 771 tensor([[767., 768., 769., 770., 771.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([772.], device='cuda:0') data.h5py: 772 tensor([[768., 769., 770., 771., 772.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([773.], device='cuda:0') data.h5py: 773 tensor([[769., 770., 771., 772., 773.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([774.], device='cuda:0') data.h5py: 774 tensor([[770., 771., 772., 773., 774.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([775.], device='cuda:0') data.h5py: 775 tensor([[771., 772., 773., 774., 775.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([776.], device='cuda:0') data.h5py: 776 tensor([[772., 773., 774., 775., 776.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([777.], device='cuda:0') data.h5py: 777 tensor([[773., 774., 775., 776., 777.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([778.], device='cuda:0') data.h5py: 778 tensor([[774., 775., 776., 777., 778.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([779.], device='cuda:0') data.h5py: 779 tensor([[779., 779., 779., 779., 779.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([780.], device='cuda:0') data.h5py: 780 tensor([[779., 779., 779., 779., 780.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([781.], device='cuda:0') data.h5py: 781 tensor([[779., 779., 779., 780., 781.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([782.], device='cuda:0') data.h5py: 782 tensor([[779., 779., 780., 781., 782.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([783.], device='cuda:0') data.h5py: 783 tensor([[779., 780., 781., 782., 783.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([784.], device='cuda:0') data.h5py: 784 tensor([[780., 781., 782., 783., 784.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([785.], device='cuda:0') data.h5py: 785 tensor([[781., 782., 783., 784., 785.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([786.], device='cuda:0') data.h5py: 786 tensor([[782., 783., 784., 785., 786.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([787.], device='cuda:0') data.h5py: 787 tensor([[783., 784., 785., 786., 787.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([788.], device='cuda:0') data.h5py: 788 tensor([[784., 785., 786., 787., 788.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([789.], device='cuda:0') data.h5py: 789 tensor([[785., 786., 787., 788., 789.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([790.], device='cuda:0') data.h5py: 790 tensor([[786., 787., 788., 789., 790.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([791.], device='cuda:0') data.h5py: 791 tensor([[787., 788., 789., 790., 791.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([792.], device='cuda:0') data.h5py: 792 tensor([[788., 789., 790., 791., 792.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([793.], device='cuda:0') data.h5py: 793 tensor([[789., 790., 791., 792., 793.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([794.], device='cuda:0') data.h5py: 794 tensor([[790., 791., 792., 793., 794.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([795.], device='cuda:0') data.h5py: 795 tensor([[791., 792., 793., 794., 795.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([796.], device='cuda:0') data.h5py: 796 tensor([[792., 793., 794., 795., 796.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([797.], device='cuda:0') data.h5py: 797 tensor([[793., 794., 795., 796., 797.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([798.], device='cuda:0') data.h5py: 798 tensor([[798., 798., 798., 798., 798.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([799.], device='cuda:0') data.h5py: 799 tensor([[798., 798., 798., 798., 799.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([800.], device='cuda:0') data.h5py: 800 tensor([[798., 798., 798., 799., 800.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([801.], device='cuda:0') data.h5py: 801 tensor([[798., 798., 799., 800., 801.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([802.], device='cuda:0') data.h5py: 802 tensor([[798., 799., 800., 801., 802.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([803.], device='cuda:0') data.h5py: 803 tensor([[799., 800., 801., 802., 803.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([804.], device='cuda:0') data.h5py: 804 tensor([[800., 801., 802., 803., 804.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([805.], device='cuda:0') data.h5py: 805 tensor([[801., 802., 803., 804., 805.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([806.], device='cuda:0') data.h5py: 806 tensor([[802., 803., 804., 805., 806.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([807.], device='cuda:0') data.h5py: 807 tensor([[803., 804., 805., 806., 807.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([808.], device='cuda:0') data.h5py: 808 tensor([[804., 805., 806., 807., 808.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([809.], device='cuda:0') data.h5py: 809 tensor([[805., 806., 807., 808., 809.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([810.], device='cuda:0') data.h5py: 810 tensor([[806., 807., 808., 809., 810.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([811.], device='cuda:0') data.h5py: 811 tensor([[807., 808., 809., 810., 811.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([812.], device='cuda:0') data.h5py: 812 tensor([[808., 809., 810., 811., 812.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([813.], device='cuda:0') data.h5py: 813 tensor([[813., 813., 813., 813., 813.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([814.], device='cuda:0') data.h5py: 814 tensor([[813., 813., 813., 813., 814.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([815.], device='cuda:0') data.h5py: 815 tensor([[813., 813., 813., 814., 815.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([816.], device='cuda:0') data.h5py: 816 tensor([[813., 813., 814., 815., 816.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([817.], device='cuda:0') data.h5py: 817 tensor([[813., 814., 815., 816., 817.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([818.], device='cuda:0') data.h5py: 818 tensor([[814., 815., 816., 817., 818.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([819.], device='cuda:0') data.h5py: 819 tensor([[815., 816., 817., 818., 819.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([820.], device='cuda:0') data.h5py: 820 tensor([[816., 817., 818., 819., 820.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([821.], device='cuda:0') data.h5py: 821 tensor([[817., 818., 819., 820., 821.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([822.], device='cuda:0') data.h5py: 822 tensor([[818., 819., 820., 821., 822.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([823.], device='cuda:0') data.h5py: 823 tensor([[819., 820., 821., 822., 823.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([824.], device='cuda:0') data.h5py: 824 tensor([[820., 821., 822., 823., 824.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([825.], device='cuda:0') data.h5py: 825 tensor([[821., 822., 823., 824., 825.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([826.], device='cuda:0') data.h5py: 826 tensor([[822., 823., 824., 825., 826.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([827.], device='cuda:0') data.h5py: 827 tensor([[823., 824., 825., 826., 827.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([828.], device='cuda:0') data.h5py: 828 tensor([[824., 825., 826., 827., 828.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([829.], device='cuda:0') data.h5py: 829 tensor([[825., 826., 827., 828., 829.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([830.], device='cuda:0') data.h5py: 830 tensor([[826., 827., 828., 829., 830.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([831.], device='cuda:0') data.h5py: 831 tensor([[831., 831., 831., 831., 831.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([832.], device='cuda:0') data.h5py: 832 tensor([[831., 831., 831., 831., 832.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([833.], device='cuda:0') data.h5py: 833 tensor([[831., 831., 831., 832., 833.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([834.], device='cuda:0') data.h5py: 834 tensor([[831., 831., 832., 833., 834.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([835.], device='cuda:0') data.h5py: 835 tensor([[831., 832., 833., 834., 835.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([836.], device='cuda:0') data.h5py: 836 tensor([[832., 833., 834., 835., 836.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([837.], device='cuda:0') data.h5py: 837 tensor([[833., 834., 835., 836., 837.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([838.], device='cuda:0') data.h5py: 838 tensor([[834., 835., 836., 837., 838.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([839.], device='cuda:0') data.h5py: 839 tensor([[835., 836., 837., 838., 839.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([840.], device='cuda:0') data.h5py: 840 tensor([[836., 837., 838., 839., 840.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([841.], device='cuda:0') data.h5py: 841 tensor([[837., 838., 839., 840., 841.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([842.], device='cuda:0') data.h5py: 842 tensor([[842., 842., 842., 842., 842.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([843.], device='cuda:0') data.h5py: 843 tensor([[842., 842., 842., 842., 843.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([844.], device='cuda:0') data.h5py: 844 tensor([[842., 842., 842., 843., 844.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([845.], device='cuda:0') data.h5py: 845 tensor([[842., 842., 843., 844., 845.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([846.], device='cuda:0') data.h5py: 846 tensor([[842., 843., 844., 845., 846.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([847.], device='cuda:0') data.h5py: 847 tensor([[843., 844., 845., 846., 847.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([848.], device='cuda:0') data.h5py: 848 tensor([[844., 845., 846., 847., 848.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([849.], device='cuda:0') data.h5py: 849 tensor([[845., 846., 847., 848., 849.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([850.], device='cuda:0') data.h5py: 850 tensor([[846., 847., 848., 849., 850.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([851.], device='cuda:0') data.h5py: 851 tensor([[847., 848., 849., 850., 851.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([852.], device='cuda:0') data.h5py: 852 tensor([[848., 849., 850., 851., 852.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([853.], device='cuda:0') data.h5py: 853 tensor([[849., 850., 851., 852., 853.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([854.], device='cuda:0') data.h5py: 854 tensor([[850., 851., 852., 853., 854.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([855.], device='cuda:0') data.h5py: 855 tensor([[851., 852., 853., 854., 855.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([856.], device='cuda:0') data.h5py: 856 tensor([[852., 853., 854., 855., 856.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([857.], device='cuda:0') data.h5py: 857 tensor([[853., 854., 855., 856., 857.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([858.], device='cuda:0') data.h5py: 858 tensor([[858., 858., 858., 858., 858.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([859.], device='cuda:0') data.h5py: 859 tensor([[858., 858., 858., 858., 859.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([860.], device='cuda:0') data.h5py: 860 tensor([[858., 858., 858., 859., 860.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([861.], device='cuda:0') data.h5py: 861 tensor([[858., 858., 859., 860., 861.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([862.], device='cuda:0') data.h5py: 862 tensor([[858., 859., 860., 861., 862.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([863.], device='cuda:0') data.h5py: 863 tensor([[859., 860., 861., 862., 863.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([864.], device='cuda:0') data.h5py: 864 tensor([[860., 861., 862., 863., 864.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([865.], device='cuda:0') data.h5py: 865 tensor([[861., 862., 863., 864., 865.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([866.], device='cuda:0') data.h5py: 866 tensor([[862., 863., 864., 865., 866.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([867.], device='cuda:0') data.h5py: 867 tensor([[863., 864., 865., 866., 867.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([868.], device='cuda:0') data.h5py: 868 tensor([[864., 865., 866., 867., 868.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([869.], device='cuda:0') data.h5py: 869 tensor([[865., 866., 867., 868., 869.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([870.], device='cuda:0') data.h5py: 870 tensor([[866., 867., 868., 869., 870.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([871.], device='cuda:0') data.h5py: 871 tensor([[867., 868., 869., 870., 871.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([872.], device='cuda:0') data.h5py: 872 tensor([[868., 869., 870., 871., 872.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([873.], device='cuda:0') data.h5py: 873 tensor([[869., 870., 871., 872., 873.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([874.], device='cuda:0') data.h5py: 874 tensor([[870., 871., 872., 873., 874.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([875.], device='cuda:0') data.h5py: 875 tensor([[871., 872., 873., 874., 875.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([876.], device='cuda:0') data.h5py: 876 tensor([[872., 873., 874., 875., 876.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([877.], device='cuda:0') data.h5py: 877 tensor([[877., 877., 877., 877., 877.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([878.], device='cuda:0') data.h5py: 878 tensor([[877., 877., 877., 877., 878.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([879.], device='cuda:0') data.h5py: 879 tensor([[877., 877., 877., 878., 879.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([880.], device='cuda:0') data.h5py: 880 tensor([[877., 877., 878., 879., 880.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([881.], device='cuda:0') data.h5py: 881 tensor([[877., 878., 879., 880., 881.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([882.], device='cuda:0') data.h5py: 882 tensor([[878., 879., 880., 881., 882.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([883.], device='cuda:0') data.h5py: 883 tensor([[879., 880., 881., 882., 883.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([884.], device='cuda:0') data.h5py: 884 tensor([[880., 881., 882., 883., 884.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([885.], device='cuda:0') data.h5py: 885 tensor([[881., 882., 883., 884., 885.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([886.], device='cuda:0') data.h5py: 886 tensor([[882., 883., 884., 885., 886.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([887.], device='cuda:0') data.h5py: 887 tensor([[883., 884., 885., 886., 887.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([888.], device='cuda:0') data.h5py: 888 tensor([[884., 885., 886., 887., 888.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([889.], device='cuda:0') data.h5py: 889 tensor([[885., 886., 887., 888., 889.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([890.], device='cuda:0') data.h5py: 890 tensor([[890., 890., 890., 890., 890.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([891.], device='cuda:0') data.h5py: 891 tensor([[890., 890., 890., 890., 891.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([892.], device='cuda:0') data.h5py: 892 tensor([[890., 890., 890., 891., 892.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([893.], device='cuda:0') data.h5py: 893 tensor([[890., 890., 891., 892., 893.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([894.], device='cuda:0') data.h5py: 894 tensor([[890., 891., 892., 893., 894.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([895.], device='cuda:0') data.h5py: 895 tensor([[891., 892., 893., 894., 895.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([896.], device='cuda:0') data.h5py: 896 tensor([[892., 893., 894., 895., 896.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([897.], device='cuda:0') data.h5py: 897 tensor([[893., 894., 895., 896., 897.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([898.], device='cuda:0') data.h5py: 898 tensor([[894., 895., 896., 897., 898.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([899.], device='cuda:0') data.h5py: 899 tensor([[895., 896., 897., 898., 899.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([900.], device='cuda:0') data.h5py: 900 tensor([[896., 897., 898., 899., 900.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([901.], device='cuda:0') data.h5py: 901 tensor([[897., 898., 899., 900., 901.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([902.], device='cuda:0') data.h5py: 902 tensor([[898., 899., 900., 901., 902.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([903.], device='cuda:0') data.h5py: 903 tensor([[899., 900., 901., 902., 903.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([904.], device='cuda:0') data.h5py: 904 tensor([[900., 901., 902., 903., 904.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([905.], device='cuda:0') data.h5py: 905 tensor([[901., 902., 903., 904., 905.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([906.], device='cuda:0') data.h5py: 906 tensor([[902., 903., 904., 905., 906.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([907.], device='cuda:0') data.h5py: 907 tensor([[907., 907., 907., 907., 907.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([908.], device='cuda:0') data.h5py: 908 tensor([[907., 907., 907., 907., 908.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([909.], device='cuda:0') data.h5py: 909 tensor([[907., 907., 907., 908., 909.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([910.], device='cuda:0') data.h5py: 910 tensor([[907., 907., 908., 909., 910.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([911.], device='cuda:0') data.h5py: 911 tensor([[907., 908., 909., 910., 911.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([912.], device='cuda:0') data.h5py: 912 tensor([[908., 909., 910., 911., 912.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([913.], device='cuda:0') data.h5py: 913 tensor([[909., 910., 911., 912., 913.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([914.], device='cuda:0') data.h5py: 914 tensor([[910., 911., 912., 913., 914.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([915.], device='cuda:0') data.h5py: 915 tensor([[911., 912., 913., 914., 915.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([916.], device='cuda:0') data.h5py: 916 tensor([[912., 913., 914., 915., 916.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([917.], device='cuda:0') data.h5py: 917 tensor([[913., 914., 915., 916., 917.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([918.], device='cuda:0') data.h5py: 918 tensor([[914., 915., 916., 917., 918.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([919.], device='cuda:0') data.h5py: 919 tensor([[915., 916., 917., 918., 919.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([920.], device='cuda:0') data.h5py: 920 tensor([[920., 920., 920., 920., 920.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([921.], device='cuda:0') data.h5py: 921 tensor([[920., 920., 920., 920., 921.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([922.], device='cuda:0') data.h5py: 922 tensor([[920., 920., 920., 921., 922.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([923.], device='cuda:0') data.h5py: 923 tensor([[920., 920., 921., 922., 923.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([924.], device='cuda:0') data.h5py: 924 tensor([[920., 921., 922., 923., 924.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([925.], device='cuda:0') data.h5py: 925 tensor([[921., 922., 923., 924., 925.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([926.], device='cuda:0') data.h5py: 926 tensor([[922., 923., 924., 925., 926.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([927.], device='cuda:0') data.h5py: 927 tensor([[923., 924., 925., 926., 927.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([928.], device='cuda:0') data.h5py: 928 tensor([[924., 925., 926., 927., 928.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([929.], device='cuda:0') data.h5py: 929 tensor([[925., 926., 927., 928., 929.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([930.], device='cuda:0') data.h5py: 930 tensor([[926., 927., 928., 929., 930.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([931.], device='cuda:0') data.h5py: 931 tensor([[927., 928., 929., 930., 931.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([932.], device='cuda:0') data.h5py: 932 tensor([[928., 929., 930., 931., 932.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([933.], device='cuda:0') data.h5py: 933 tensor([[929., 930., 931., 932., 933.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([934.], device='cuda:0') data.h5py: 934 tensor([[930., 931., 932., 933., 934.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([935.], device='cuda:0') data.h5py: 935 tensor([[931., 932., 933., 934., 935.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([936.], device='cuda:0') data.h5py: 936 tensor([[932., 933., 934., 935., 936.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([937.], device='cuda:0') data.h5py: 937 tensor([[937., 937., 937., 937., 937.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([938.], device='cuda:0') data.h5py: 938 tensor([[937., 937., 937., 937., 938.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([939.], device='cuda:0') data.h5py: 939 tensor([[937., 937., 937., 938., 939.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([940.], device='cuda:0') data.h5py: 940 tensor([[937., 937., 938., 939., 940.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([941.], device='cuda:0') data.h5py: 941 tensor([[937., 938., 939., 940., 941.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([942.], device='cuda:0') data.h5py: 942 tensor([[938., 939., 940., 941., 942.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([943.], device='cuda:0') data.h5py: 943 tensor([[939., 940., 941., 942., 943.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([944.], device='cuda:0') data.h5py: 944 tensor([[940., 941., 942., 943., 944.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([945.], device='cuda:0') data.h5py: 945 tensor([[941., 942., 943., 944., 945.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([946.], device='cuda:0') data.h5py: 946 tensor([[942., 943., 944., 945., 946.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([947.], device='cuda:0') data.h5py: 947 tensor([[943., 944., 945., 946., 947.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([948.], device='cuda:0') data.h5py: 948 tensor([[944., 945., 946., 947., 948.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([949.], device='cuda:0') data.h5py: 949 tensor([[945., 946., 947., 948., 949.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([950.], device='cuda:0') data.h5py: 950 tensor([[946., 947., 948., 949., 950.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([951.], device='cuda:0') data.h5py: 951 tensor([[947., 948., 949., 950., 951.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([952.], device='cuda:0') data.h5py: 952 tensor([[948., 949., 950., 951., 952.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([953.], device='cuda:0') data.h5py: 953 tensor([[953., 953., 953., 953., 953.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([954.], device='cuda:0') data.h5py: 954 tensor([[953., 953., 953., 953., 954.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([955.], device='cuda:0') data.h5py: 955 tensor([[953., 953., 953., 954., 955.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([956.], device='cuda:0') data.h5py: 956 tensor([[953., 953., 954., 955., 956.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([957.], device='cuda:0') data.h5py: 957 tensor([[953., 954., 955., 956., 957.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([958.], device='cuda:0') data.h5py: 958 tensor([[954., 955., 956., 957., 958.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([959.], device='cuda:0') data.h5py: 959 tensor([[955., 956., 957., 958., 959.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([960.], device='cuda:0') data.h5py: 960 tensor([[956., 957., 958., 959., 960.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([961.], device='cuda:0') data.h5py: 961 tensor([[957., 958., 959., 960., 961.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([962.], device='cuda:0') data.h5py: 962 tensor([[958., 959., 960., 961., 962.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([963.], device='cuda:0') data.h5py: 963 tensor([[963., 963., 963., 963., 963.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([964.], device='cuda:0') data.h5py: 964 tensor([[963., 963., 963., 963., 964.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([965.], device='cuda:0') data.h5py: 965 tensor([[963., 963., 963., 964., 965.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([966.], device='cuda:0') data.h5py: 966 tensor([[963., 963., 964., 965., 966.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([967.], device='cuda:0') data.h5py: 967 tensor([[963., 964., 965., 966., 967.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([968.], device='cuda:0') data.h5py: 968 tensor([[964., 965., 966., 967., 968.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([969.], device='cuda:0') data.h5py: 969 tensor([[965., 966., 967., 968., 969.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([970.], device='cuda:0') data.h5py: 970 tensor([[966., 967., 968., 969., 970.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([971.], device='cuda:0') data.h5py: 971 tensor([[967., 968., 969., 970., 971.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([972.], device='cuda:0') data.h5py: 972 tensor([[968., 969., 970., 971., 972.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([973.], device='cuda:0') data.h5py: 973 tensor([[969., 970., 971., 972., 973.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([974.], device='cuda:0') data.h5py: 974 tensor([[970., 971., 972., 973., 974.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([975.], device='cuda:0') data.h5py: 975 tensor([[971., 972., 973., 974., 975.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([976.], device='cuda:0') data.h5py: 976 tensor([[972., 973., 974., 975., 976.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([977.], device='cuda:0') data.h5py: 977 tensor([[973., 974., 975., 976., 977.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([978.], device='cuda:0') data.h5py: 978 tensor([[974., 975., 976., 977., 978.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([979.], device='cuda:0') data.h5py: 979 tensor([[975., 976., 977., 978., 979.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([980.], device='cuda:0') data.h5py: 980 tensor([[980., 980., 980., 980., 980.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([981.], device='cuda:0') data.h5py: 981 tensor([[980., 980., 980., 980., 981.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([982.], device='cuda:0') data.h5py: 982 tensor([[980., 980., 980., 981., 982.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([983.], device='cuda:0') data.h5py: 983 tensor([[980., 980., 981., 982., 983.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([984.], device='cuda:0') data.h5py: 984 tensor([[980., 981., 982., 983., 984.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([985.], device='cuda:0') data.h5py: 985 tensor([[981., 982., 983., 984., 985.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([986.], device='cuda:0') data.h5py: 986 tensor([[982., 983., 984., 985., 986.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([987.], device='cuda:0') data.h5py: 987 tensor([[983., 984., 985., 986., 987.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([988.], device='cuda:0') data.h5py: 988 tensor([[984., 985., 986., 987., 988.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([989.], device='cuda:0') data.h5py: 989 tensor([[985., 986., 987., 988., 989.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([990.], device='cuda:0') data.h5py: 990 tensor([[986., 987., 988., 989., 990.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([991.], device='cuda:0') data.h5py: 991 tensor([[987., 988., 989., 990., 991.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([992.], device='cuda:0') data.h5py: 992 tensor([[988., 989., 990., 991., 992.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([993.], device='cuda:0') data.h5py: 993 tensor([[989., 990., 991., 992., 993.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([994.], device='cuda:0') data.h5py: 994 tensor([[990., 991., 992., 993., 994.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([995.], device='cuda:0') data.h5py: 995 tensor([[991., 992., 993., 994., 995.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([996.], device='cuda:0') data.h5py: 996 tensor([[992., 993., 994., 995., 996.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([997.], device='cuda:0') data.h5py: 997 tensor([[993., 994., 995., 996., 997.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([998.], device='cuda:0') data.h5py: 998 tensor([[994., 995., 996., 997., 998.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([999.], device='cuda:0') data.h5py: 999 tensor([[995., 996., 997., 998., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1000.], device='cuda:0')","title":"<span class='magic-codeicon-class'>H5CParser</span>"},{"location":"apis/data/h5py/H5CParser/#datah5pyh5cparser","text":"Class \u00b7 Context \u00b7 Source dset = mdnc . data . h5py . H5CParser ( file_name , keywords_sequence , keywords_single , batch_size = 32 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and parse it by mdnc.data.sequence.MPSequence . The realization could be described as: This parser is the upgraded version of mdnc.data.h5py.H5GParser , it is specially designed for parsing data to LSTM/ConvLSTM. A sequence dimension would be inserted between batches and channels . In each batch, the sequence is continuously extracted in the order of the batches. During each epoch, a sliding window would iterate the first axis (samples). The number of batches would be the same as using mdnc.data.h5py.H5GParser . For each variable specified by keywords_sequence , each sample in the mini-batch is a sequence. This parser could also read the dataset converted by mdnc.data.h5py.H5SeqConverter . The workflow is shown in the following figure:","title":"data.h5py.H5CParser"},{"location":"apis/data/h5py/H5CParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords_sequence ( str , ) The keyword of sequence data. The keywords in this list would be parsed as (B, S, C1, C2, ...) , where B and S are the sample number and sequence length (given by the argument sequence_size ) respectively. It should be a list of keywords (or a single keyword). keyword_single ( str , ) The keyword of single values. The keywords in this list would be parsed as (B, C1, C2, ...) , where B is the sample number. It should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. sequence_size int The size of each sequence. It represents S of (B, S, C1, C2, ...) . sequence_position int The aligned position between the single values and the sequence values. It should be in the range of >= 0 and < sequence_size . sequence_padding int The padding method for each epoch, it will influence the first or the final samples in the dataset. Could be 'same' , 'zero' or 'none' . If set None , the number of batches of each epoch would be a little bit smaller than the actual number. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip At least one keyword requires to be given in keywords_sequence or keyword_single . In some cases, we need to use both kinds of keywords. For example, the input could be a sequence, and the label may be a scalar. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5CParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5CParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5CParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5CParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5CParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5CParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5CParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5CParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5CParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5CParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5CParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5CParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5CParser/#batch_size","text":"dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/h5py/H5CParser/#sequence_size","text":"dset . sequence_size The length of each sequence. This value is given by the argument sequence_size during the initialization.","title":" sequence_size"},{"location":"apis/data/h5py/H5CParser/#sequence_position","text":"dset . sequence_position The alignment between keywords_sequence and keyword_single . This value is given by the argument sequence_position during the initialization.","title":" sequence_position"},{"location":"apis/data/h5py/H5CParser/#sequence_padding","text":"dset . sequence_position The padding method of each sequence. This value is given by the argument sequence_padding during the initialization.","title":" sequence_padding"},{"location":"apis/data/h5py/H5CParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5CParser/#examples","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5cparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5CParser ( os . path . join ( root_folder , 'test_data_h5cparser' ), keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position = 0 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 1 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Output data.webtools: All required datasets are available. data.h5py: 0 tensor([[0., 1., 2., 3., 4.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1.], device='cuda:0') data.h5py: 1 tensor([[1., 2., 3., 4., 5.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([2.], device='cuda:0') data.h5py: 2 tensor([[2., 3., 4., 5., 6.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([3.], device='cuda:0') data.h5py: 3 tensor([[3., 4., 5., 6., 7.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([4.], device='cuda:0') data.h5py: 4 tensor([[4., 5., 6., 7., 8.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([5.], device='cuda:0') data.h5py: 5 tensor([[5., 6., 7., 8., 9.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([6.], device='cuda:0') data.h5py: 6 tensor([[ 6., 7., 8., 9., 10.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([7.], device='cuda:0') data.h5py: 7 tensor([[ 7., 8., 9., 10., 11.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([8.], device='cuda:0') data.h5py: 8 tensor([[ 8., 9., 10., 11., 12.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([9.], device='cuda:0') data.h5py: 9 tensor([[ 9., 10., 11., 12., 13.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([10.], device='cuda:0') data.h5py: 10 tensor([[10., 11., 12., 13., 14.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([11.], device='cuda:0') data.h5py: 11 tensor([[11., 12., 13., 14., 15.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([12.], device='cuda:0') data.h5py: 12 tensor([[12., 13., 14., 15., 16.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([13.], device='cuda:0') data.h5py: 13 tensor([[13., 14., 15., 16., 17.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([14.], device='cuda:0') data.h5py: 14 tensor([[14., 15., 16., 17., 18.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([15.], device='cuda:0') data.h5py: 15 tensor([[15., 16., 17., 18., 19.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([16.], device='cuda:0') data.h5py: 16 tensor([[16., 17., 18., 19., 20.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([17.], device='cuda:0') data.h5py: 17 tensor([[17., 18., 19., 20., 21.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([18.], device='cuda:0') data.h5py: 18 tensor([[18., 19., 20., 21., 22.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([19.], device='cuda:0') data.h5py: 19 tensor([[19., 20., 21., 22., 23.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([20.], device='cuda:0') data.h5py: 20 tensor([[20., 21., 22., 23., 24.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([21.], device='cuda:0') data.h5py: 21 tensor([[21., 22., 23., 24., 25.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([22.], device='cuda:0') data.h5py: 22 tensor([[22., 23., 24., 25., 26.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([23.], device='cuda:0') data.h5py: 23 tensor([[23., 24., 25., 26., 27.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([24.], device='cuda:0') data.h5py: 24 tensor([[24., 25., 26., 27., 28.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([25.], device='cuda:0') data.h5py: 25 tensor([[25., 26., 27., 28., 29.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([26.], device='cuda:0') data.h5py: 26 tensor([[26., 27., 28., 29., 30.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([27.], device='cuda:0') data.h5py: 27 tensor([[27., 28., 29., 30., 31.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([28.], device='cuda:0') data.h5py: 28 tensor([[28., 29., 30., 31., 32.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([29.], device='cuda:0') data.h5py: 29 tensor([[29., 30., 31., 32., 33.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([30.], device='cuda:0') data.h5py: 30 tensor([[30., 31., 32., 33., 34.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([31.], device='cuda:0') data.h5py: 31 tensor([[31., 32., 33., 34., 35.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([32.], device='cuda:0') data.h5py: 32 tensor([[32., 33., 34., 35., 36.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([33.], device='cuda:0') data.h5py: 33 tensor([[33., 34., 35., 36., 37.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([34.], device='cuda:0') data.h5py: 34 tensor([[34., 35., 36., 37., 38.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([35.], device='cuda:0') data.h5py: 35 tensor([[35., 36., 37., 38., 39.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([36.], device='cuda:0') data.h5py: 36 tensor([[36., 37., 38., 39., 40.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([37.], device='cuda:0') data.h5py: 37 tensor([[37., 38., 39., 40., 41.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([38.], device='cuda:0') data.h5py: 38 tensor([[38., 39., 40., 41., 42.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([39.], device='cuda:0') data.h5py: 39 tensor([[39., 40., 41., 42., 43.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([40.], device='cuda:0') data.h5py: 40 tensor([[40., 41., 42., 43., 44.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([41.], device='cuda:0') data.h5py: 41 tensor([[41., 42., 43., 44., 45.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([42.], device='cuda:0') data.h5py: 42 tensor([[42., 43., 44., 45., 46.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([43.], device='cuda:0') data.h5py: 43 tensor([[43., 44., 45., 46., 47.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([44.], device='cuda:0') data.h5py: 44 tensor([[44., 45., 46., 47., 48.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([45.], device='cuda:0') data.h5py: 45 tensor([[45., 46., 47., 48., 49.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([46.], device='cuda:0') data.h5py: 46 tensor([[46., 47., 48., 49., 50.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([47.], device='cuda:0') data.h5py: 47 tensor([[47., 48., 49., 50., 51.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([48.], device='cuda:0') data.h5py: 48 tensor([[48., 49., 50., 51., 52.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([49.], device='cuda:0') data.h5py: 49 tensor([[49., 50., 51., 52., 53.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([50.], device='cuda:0') data.h5py: 50 tensor([[50., 51., 52., 53., 54.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([51.], device='cuda:0') data.h5py: 51 tensor([[51., 52., 53., 54., 55.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([52.], device='cuda:0') data.h5py: 52 tensor([[52., 53., 54., 55., 56.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([53.], device='cuda:0') data.h5py: 53 tensor([[53., 54., 55., 56., 57.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([54.], device='cuda:0') data.h5py: 54 tensor([[54., 55., 56., 57., 58.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([55.], device='cuda:0') data.h5py: 55 tensor([[55., 56., 57., 58., 59.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([56.], device='cuda:0') data.h5py: 56 tensor([[56., 57., 58., 59., 60.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([57.], device='cuda:0') data.h5py: 57 tensor([[57., 58., 59., 60., 61.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([58.], device='cuda:0') data.h5py: 58 tensor([[58., 59., 60., 61., 62.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([59.], device='cuda:0') data.h5py: 59 tensor([[59., 60., 61., 62., 63.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([60.], device='cuda:0') data.h5py: 60 tensor([[60., 61., 62., 63., 64.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([61.], device='cuda:0') data.h5py: 61 tensor([[61., 62., 63., 64., 65.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([62.], device='cuda:0') data.h5py: 62 tensor([[62., 63., 64., 65., 66.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([63.], device='cuda:0') data.h5py: 63 tensor([[63., 64., 65., 66., 67.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([64.], device='cuda:0') data.h5py: 64 tensor([[64., 65., 66., 67., 68.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([65.], device='cuda:0') data.h5py: 65 tensor([[65., 66., 67., 68., 69.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([66.], device='cuda:0') data.h5py: 66 tensor([[66., 67., 68., 69., 70.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([67.], device='cuda:0') data.h5py: 67 tensor([[67., 68., 69., 70., 71.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([68.], device='cuda:0') data.h5py: 68 tensor([[68., 69., 70., 71., 72.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([69.], device='cuda:0') data.h5py: 69 tensor([[69., 70., 71., 72., 73.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([70.], device='cuda:0') data.h5py: 70 tensor([[70., 71., 72., 73., 74.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([71.], device='cuda:0') data.h5py: 71 tensor([[71., 72., 73., 74., 75.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([72.], device='cuda:0') data.h5py: 72 tensor([[72., 73., 74., 75., 76.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([73.], device='cuda:0') data.h5py: 73 tensor([[73., 74., 75., 76., 77.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([74.], device='cuda:0') data.h5py: 74 tensor([[74., 75., 76., 77., 78.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([75.], device='cuda:0') data.h5py: 75 tensor([[75., 76., 77., 78., 79.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([76.], device='cuda:0') data.h5py: 76 tensor([[76., 77., 78., 79., 80.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([77.], device='cuda:0') data.h5py: 77 tensor([[77., 78., 79., 80., 81.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([78.], device='cuda:0') data.h5py: 78 tensor([[78., 79., 80., 81., 82.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([79.], device='cuda:0') data.h5py: 79 tensor([[79., 80., 81., 82., 83.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([80.], device='cuda:0') data.h5py: 80 tensor([[80., 81., 82., 83., 84.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([81.], device='cuda:0') data.h5py: 81 tensor([[81., 82., 83., 84., 85.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([82.], device='cuda:0') data.h5py: 82 tensor([[82., 83., 84., 85., 86.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([83.], device='cuda:0') data.h5py: 83 tensor([[83., 84., 85., 86., 87.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([84.], device='cuda:0') data.h5py: 84 tensor([[84., 85., 86., 87., 88.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([85.], device='cuda:0') data.h5py: 85 tensor([[85., 86., 87., 88., 89.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([86.], device='cuda:0') data.h5py: 86 tensor([[86., 87., 88., 89., 90.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([87.], device='cuda:0') data.h5py: 87 tensor([[87., 88., 89., 90., 91.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([88.], device='cuda:0') data.h5py: 88 tensor([[88., 89., 90., 91., 92.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([89.], device='cuda:0') data.h5py: 89 tensor([[89., 90., 91., 92., 93.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([90.], device='cuda:0') data.h5py: 90 tensor([[90., 91., 92., 93., 94.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([91.], device='cuda:0') data.h5py: 91 tensor([[91., 92., 93., 94., 95.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([92.], device='cuda:0') data.h5py: 92 tensor([[92., 93., 94., 95., 96.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([93.], device='cuda:0') data.h5py: 93 tensor([[93., 94., 95., 96., 97.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([94.], device='cuda:0') data.h5py: 94 tensor([[94., 95., 96., 97., 98.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([95.], device='cuda:0') data.h5py: 95 tensor([[95., 96., 97., 98., 99.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([96.], device='cuda:0') data.h5py: 96 tensor([[ 96., 97., 98., 99., 100.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([97.], device='cuda:0') data.h5py: 97 tensor([[ 97., 98., 99., 100., 101.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([98.], device='cuda:0') data.h5py: 98 tensor([[ 98., 99., 100., 101., 102.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([99.], device='cuda:0') data.h5py: 99 tensor([[ 99., 100., 101., 102., 103.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([100.], device='cuda:0') data.h5py: 100 tensor([[100., 101., 102., 103., 104.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([101.], device='cuda:0') data.h5py: 101 tensor([[101., 102., 103., 104., 105.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([102.], device='cuda:0') data.h5py: 102 tensor([[102., 103., 104., 105., 106.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([103.], device='cuda:0') data.h5py: 103 tensor([[103., 104., 105., 106., 107.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([104.], device='cuda:0') data.h5py: 104 tensor([[104., 105., 106., 107., 108.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([105.], device='cuda:0') data.h5py: 105 tensor([[105., 106., 107., 108., 109.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([106.], device='cuda:0') data.h5py: 106 tensor([[106., 107., 108., 109., 110.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([107.], device='cuda:0') data.h5py: 107 tensor([[107., 108., 109., 110., 111.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([108.], device='cuda:0') data.h5py: 108 tensor([[108., 109., 110., 111., 112.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([109.], device='cuda:0') data.h5py: 109 tensor([[109., 110., 111., 112., 113.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([110.], device='cuda:0') data.h5py: 110 tensor([[110., 111., 112., 113., 114.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([111.], device='cuda:0') data.h5py: 111 tensor([[111., 112., 113., 114., 115.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([112.], device='cuda:0') data.h5py: 112 tensor([[112., 113., 114., 115., 116.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([113.], device='cuda:0') data.h5py: 113 tensor([[113., 114., 115., 116., 117.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([114.], device='cuda:0') data.h5py: 114 tensor([[114., 115., 116., 117., 118.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([115.], device='cuda:0') data.h5py: 115 tensor([[115., 116., 117., 118., 119.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([116.], device='cuda:0') data.h5py: 116 tensor([[116., 117., 118., 119., 120.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([117.], device='cuda:0') data.h5py: 117 tensor([[117., 118., 119., 120., 121.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([118.], device='cuda:0') data.h5py: 118 tensor([[118., 119., 120., 121., 122.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([119.], device='cuda:0') data.h5py: 119 tensor([[119., 120., 121., 122., 123.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([120.], device='cuda:0') data.h5py: 120 tensor([[120., 121., 122., 123., 124.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([121.], device='cuda:0') data.h5py: 121 tensor([[121., 122., 123., 124., 125.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([122.], device='cuda:0') data.h5py: 122 tensor([[122., 123., 124., 125., 126.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([123.], device='cuda:0') data.h5py: 123 tensor([[123., 124., 125., 126., 127.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([124.], device='cuda:0') data.h5py: 124 tensor([[124., 125., 126., 127., 128.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([125.], device='cuda:0') data.h5py: 125 tensor([[125., 126., 127., 128., 129.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([126.], device='cuda:0') data.h5py: 126 tensor([[126., 127., 128., 129., 130.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([127.], device='cuda:0') data.h5py: 127 tensor([[127., 128., 129., 130., 131.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([128.], device='cuda:0') data.h5py: 128 tensor([[128., 129., 130., 131., 132.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([129.], device='cuda:0') data.h5py: 129 tensor([[129., 130., 131., 132., 133.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([130.], device='cuda:0') data.h5py: 130 tensor([[130., 131., 132., 133., 134.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([131.], device='cuda:0') data.h5py: 131 tensor([[131., 132., 133., 134., 135.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([132.], device='cuda:0') data.h5py: 132 tensor([[132., 133., 134., 135., 136.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([133.], device='cuda:0') data.h5py: 133 tensor([[133., 134., 135., 136., 137.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([134.], device='cuda:0') data.h5py: 134 tensor([[134., 135., 136., 137., 138.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([135.], device='cuda:0') data.h5py: 135 tensor([[135., 136., 137., 138., 139.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([136.], device='cuda:0') data.h5py: 136 tensor([[136., 137., 138., 139., 140.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([137.], device='cuda:0') data.h5py: 137 tensor([[137., 138., 139., 140., 141.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([138.], device='cuda:0') data.h5py: 138 tensor([[138., 139., 140., 141., 142.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([139.], device='cuda:0') data.h5py: 139 tensor([[139., 140., 141., 142., 143.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([140.], device='cuda:0') data.h5py: 140 tensor([[140., 141., 142., 143., 144.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([141.], device='cuda:0') data.h5py: 141 tensor([[141., 142., 143., 144., 145.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([142.], device='cuda:0') data.h5py: 142 tensor([[142., 143., 144., 145., 146.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([143.], device='cuda:0') data.h5py: 143 tensor([[143., 144., 145., 146., 147.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([144.], device='cuda:0') data.h5py: 144 tensor([[144., 145., 146., 147., 148.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([145.], device='cuda:0') data.h5py: 145 tensor([[145., 146., 147., 148., 149.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([146.], device='cuda:0') data.h5py: 146 tensor([[146., 147., 148., 149., 150.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([147.], device='cuda:0') data.h5py: 147 tensor([[147., 148., 149., 150., 151.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([148.], device='cuda:0') data.h5py: 148 tensor([[148., 149., 150., 151., 152.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([149.], device='cuda:0') data.h5py: 149 tensor([[149., 150., 151., 152., 153.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([150.], device='cuda:0') data.h5py: 150 tensor([[150., 151., 152., 153., 154.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([151.], device='cuda:0') data.h5py: 151 tensor([[151., 152., 153., 154., 155.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([152.], device='cuda:0') data.h5py: 152 tensor([[152., 153., 154., 155., 156.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([153.], device='cuda:0') data.h5py: 153 tensor([[153., 154., 155., 156., 157.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([154.], device='cuda:0') data.h5py: 154 tensor([[154., 155., 156., 157., 158.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([155.], device='cuda:0') data.h5py: 155 tensor([[155., 156., 157., 158., 159.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([156.], device='cuda:0') data.h5py: 156 tensor([[156., 157., 158., 159., 160.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([157.], device='cuda:0') data.h5py: 157 tensor([[157., 158., 159., 160., 161.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([158.], device='cuda:0') data.h5py: 158 tensor([[158., 159., 160., 161., 162.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([159.], device='cuda:0') data.h5py: 159 tensor([[159., 160., 161., 162., 163.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([160.], device='cuda:0') data.h5py: 160 tensor([[160., 161., 162., 163., 164.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([161.], device='cuda:0') data.h5py: 161 tensor([[161., 162., 163., 164., 165.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([162.], device='cuda:0') data.h5py: 162 tensor([[162., 163., 164., 165., 166.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([163.], device='cuda:0') data.h5py: 163 tensor([[163., 164., 165., 166., 167.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([164.], device='cuda:0') data.h5py: 164 tensor([[164., 165., 166., 167., 168.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([165.], device='cuda:0') data.h5py: 165 tensor([[165., 166., 167., 168., 169.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([166.], device='cuda:0') data.h5py: 166 tensor([[166., 167., 168., 169., 170.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([167.], device='cuda:0') data.h5py: 167 tensor([[167., 168., 169., 170., 171.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([168.], device='cuda:0') data.h5py: 168 tensor([[168., 169., 170., 171., 172.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([169.], device='cuda:0') data.h5py: 169 tensor([[169., 170., 171., 172., 173.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([170.], device='cuda:0') data.h5py: 170 tensor([[170., 171., 172., 173., 174.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([171.], device='cuda:0') data.h5py: 171 tensor([[171., 172., 173., 174., 175.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([172.], device='cuda:0') data.h5py: 172 tensor([[172., 173., 174., 175., 176.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([173.], device='cuda:0') data.h5py: 173 tensor([[173., 174., 175., 176., 177.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([174.], device='cuda:0') data.h5py: 174 tensor([[174., 175., 176., 177., 178.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([175.], device='cuda:0') data.h5py: 175 tensor([[175., 176., 177., 178., 179.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([176.], device='cuda:0') data.h5py: 176 tensor([[176., 177., 178., 179., 180.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([177.], device='cuda:0') data.h5py: 177 tensor([[177., 178., 179., 180., 181.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([178.], device='cuda:0') data.h5py: 178 tensor([[178., 179., 180., 181., 182.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([179.], device='cuda:0') data.h5py: 179 tensor([[179., 180., 181., 182., 183.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([180.], device='cuda:0') data.h5py: 180 tensor([[180., 181., 182., 183., 184.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([181.], device='cuda:0') data.h5py: 181 tensor([[181., 182., 183., 184., 185.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([182.], device='cuda:0') data.h5py: 182 tensor([[182., 183., 184., 185., 186.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([183.], device='cuda:0') data.h5py: 183 tensor([[183., 184., 185., 186., 187.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([184.], device='cuda:0') data.h5py: 184 tensor([[184., 185., 186., 187., 188.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([185.], device='cuda:0') data.h5py: 185 tensor([[185., 186., 187., 188., 189.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([186.], device='cuda:0') data.h5py: 186 tensor([[186., 187., 188., 189., 190.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([187.], device='cuda:0') data.h5py: 187 tensor([[187., 188., 189., 190., 191.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([188.], device='cuda:0') data.h5py: 188 tensor([[188., 189., 190., 191., 192.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([189.], device='cuda:0') data.h5py: 189 tensor([[189., 190., 191., 192., 193.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([190.], device='cuda:0') data.h5py: 190 tensor([[190., 191., 192., 193., 194.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([191.], device='cuda:0') data.h5py: 191 tensor([[191., 192., 193., 194., 195.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([192.], device='cuda:0') data.h5py: 192 tensor([[192., 193., 194., 195., 196.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([193.], device='cuda:0') data.h5py: 193 tensor([[193., 194., 195., 196., 197.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([194.], device='cuda:0') data.h5py: 194 tensor([[194., 195., 196., 197., 198.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([195.], device='cuda:0') data.h5py: 195 tensor([[195., 196., 197., 198., 199.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([196.], device='cuda:0') data.h5py: 196 tensor([[196., 197., 198., 199., 200.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([197.], device='cuda:0') data.h5py: 197 tensor([[197., 198., 199., 200., 201.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([198.], device='cuda:0') data.h5py: 198 tensor([[198., 199., 200., 201., 202.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([199.], device='cuda:0') data.h5py: 199 tensor([[199., 200., 201., 202., 203.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([200.], device='cuda:0') data.h5py: 200 tensor([[200., 201., 202., 203., 204.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([201.], device='cuda:0') data.h5py: 201 tensor([[201., 202., 203., 204., 205.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([202.], device='cuda:0') data.h5py: 202 tensor([[202., 203., 204., 205., 206.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([203.], device='cuda:0') data.h5py: 203 tensor([[203., 204., 205., 206., 207.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([204.], device='cuda:0') data.h5py: 204 tensor([[204., 205., 206., 207., 208.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([205.], device='cuda:0') data.h5py: 205 tensor([[205., 206., 207., 208., 209.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([206.], device='cuda:0') data.h5py: 206 tensor([[206., 207., 208., 209., 210.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([207.], device='cuda:0') data.h5py: 207 tensor([[207., 208., 209., 210., 211.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([208.], device='cuda:0') data.h5py: 208 tensor([[208., 209., 210., 211., 212.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([209.], device='cuda:0') data.h5py: 209 tensor([[209., 210., 211., 212., 213.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([210.], device='cuda:0') data.h5py: 210 tensor([[210., 211., 212., 213., 214.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([211.], device='cuda:0') data.h5py: 211 tensor([[211., 212., 213., 214., 215.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([212.], device='cuda:0') data.h5py: 212 tensor([[212., 213., 214., 215., 216.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([213.], device='cuda:0') data.h5py: 213 tensor([[213., 214., 215., 216., 217.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([214.], device='cuda:0') data.h5py: 214 tensor([[214., 215., 216., 217., 218.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([215.], device='cuda:0') data.h5py: 215 tensor([[215., 216., 217., 218., 219.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([216.], device='cuda:0') data.h5py: 216 tensor([[216., 217., 218., 219., 220.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([217.], device='cuda:0') data.h5py: 217 tensor([[217., 218., 219., 220., 221.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([218.], device='cuda:0') data.h5py: 218 tensor([[218., 219., 220., 221., 222.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([219.], device='cuda:0') data.h5py: 219 tensor([[219., 220., 221., 222., 223.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([220.], device='cuda:0') data.h5py: 220 tensor([[220., 221., 222., 223., 224.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([221.], device='cuda:0') data.h5py: 221 tensor([[221., 222., 223., 224., 225.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([222.], device='cuda:0') data.h5py: 222 tensor([[222., 223., 224., 225., 226.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([223.], device='cuda:0') data.h5py: 223 tensor([[223., 224., 225., 226., 227.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([224.], device='cuda:0') data.h5py: 224 tensor([[224., 225., 226., 227., 228.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([225.], device='cuda:0') data.h5py: 225 tensor([[225., 226., 227., 228., 229.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([226.], device='cuda:0') data.h5py: 226 tensor([[226., 227., 228., 229., 230.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([227.], device='cuda:0') data.h5py: 227 tensor([[227., 228., 229., 230., 231.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([228.], device='cuda:0') data.h5py: 228 tensor([[228., 229., 230., 231., 232.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([229.], device='cuda:0') data.h5py: 229 tensor([[229., 230., 231., 232., 233.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([230.], device='cuda:0') data.h5py: 230 tensor([[230., 231., 232., 233., 234.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([231.], device='cuda:0') data.h5py: 231 tensor([[231., 232., 233., 234., 235.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([232.], device='cuda:0') data.h5py: 232 tensor([[232., 233., 234., 235., 236.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([233.], device='cuda:0') data.h5py: 233 tensor([[233., 234., 235., 236., 237.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([234.], device='cuda:0') data.h5py: 234 tensor([[234., 235., 236., 237., 238.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([235.], device='cuda:0') data.h5py: 235 tensor([[235., 236., 237., 238., 239.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([236.], device='cuda:0') data.h5py: 236 tensor([[236., 237., 238., 239., 240.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([237.], device='cuda:0') data.h5py: 237 tensor([[237., 238., 239., 240., 241.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([238.], device='cuda:0') data.h5py: 238 tensor([[238., 239., 240., 241., 242.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([239.], device='cuda:0') data.h5py: 239 tensor([[239., 240., 241., 242., 243.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([240.], device='cuda:0') data.h5py: 240 tensor([[240., 241., 242., 243., 244.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([241.], device='cuda:0') data.h5py: 241 tensor([[241., 242., 243., 244., 245.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([242.], device='cuda:0') data.h5py: 242 tensor([[242., 243., 244., 245., 246.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([243.], device='cuda:0') data.h5py: 243 tensor([[243., 244., 245., 246., 247.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([244.], device='cuda:0') data.h5py: 244 tensor([[244., 245., 246., 247., 248.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([245.], device='cuda:0') data.h5py: 245 tensor([[245., 246., 247., 248., 249.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([246.], device='cuda:0') data.h5py: 246 tensor([[246., 247., 248., 249., 250.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([247.], device='cuda:0') data.h5py: 247 tensor([[247., 248., 249., 250., 251.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([248.], device='cuda:0') data.h5py: 248 tensor([[248., 249., 250., 251., 252.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([249.], device='cuda:0') data.h5py: 249 tensor([[249., 250., 251., 252., 253.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([250.], device='cuda:0') data.h5py: 250 tensor([[250., 251., 252., 253., 254.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([251.], device='cuda:0') data.h5py: 251 tensor([[251., 252., 253., 254., 255.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([252.], device='cuda:0') data.h5py: 252 tensor([[252., 253., 254., 255., 256.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([253.], device='cuda:0') data.h5py: 253 tensor([[253., 254., 255., 256., 257.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([254.], device='cuda:0') data.h5py: 254 tensor([[254., 255., 256., 257., 258.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([255.], device='cuda:0') data.h5py: 255 tensor([[255., 256., 257., 258., 259.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([256.], device='cuda:0') data.h5py: 256 tensor([[256., 257., 258., 259., 260.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([257.], device='cuda:0') data.h5py: 257 tensor([[257., 258., 259., 260., 261.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([258.], device='cuda:0') data.h5py: 258 tensor([[258., 259., 260., 261., 262.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([259.], device='cuda:0') data.h5py: 259 tensor([[259., 260., 261., 262., 263.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([260.], device='cuda:0') data.h5py: 260 tensor([[260., 261., 262., 263., 264.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([261.], device='cuda:0') data.h5py: 261 tensor([[261., 262., 263., 264., 265.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([262.], device='cuda:0') data.h5py: 262 tensor([[262., 263., 264., 265., 266.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([263.], device='cuda:0') data.h5py: 263 tensor([[263., 264., 265., 266., 267.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([264.], device='cuda:0') data.h5py: 264 tensor([[264., 265., 266., 267., 268.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([265.], device='cuda:0') data.h5py: 265 tensor([[265., 266., 267., 268., 269.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([266.], device='cuda:0') data.h5py: 266 tensor([[266., 267., 268., 269., 270.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([267.], device='cuda:0') data.h5py: 267 tensor([[267., 268., 269., 270., 271.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([268.], device='cuda:0') data.h5py: 268 tensor([[268., 269., 270., 271., 272.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([269.], device='cuda:0') data.h5py: 269 tensor([[269., 270., 271., 272., 273.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([270.], device='cuda:0') data.h5py: 270 tensor([[270., 271., 272., 273., 274.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([271.], device='cuda:0') data.h5py: 271 tensor([[271., 272., 273., 274., 275.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([272.], device='cuda:0') data.h5py: 272 tensor([[272., 273., 274., 275., 276.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([273.], device='cuda:0') data.h5py: 273 tensor([[273., 274., 275., 276., 277.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([274.], device='cuda:0') data.h5py: 274 tensor([[274., 275., 276., 277., 278.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([275.], device='cuda:0') data.h5py: 275 tensor([[275., 276., 277., 278., 279.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([276.], device='cuda:0') data.h5py: 276 tensor([[276., 277., 278., 279., 280.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([277.], device='cuda:0') data.h5py: 277 tensor([[277., 278., 279., 280., 281.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([278.], device='cuda:0') data.h5py: 278 tensor([[278., 279., 280., 281., 282.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([279.], device='cuda:0') data.h5py: 279 tensor([[279., 280., 281., 282., 283.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([280.], device='cuda:0') data.h5py: 280 tensor([[280., 281., 282., 283., 284.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([281.], device='cuda:0') data.h5py: 281 tensor([[281., 282., 283., 284., 285.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([282.], device='cuda:0') data.h5py: 282 tensor([[282., 283., 284., 285., 286.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([283.], device='cuda:0') data.h5py: 283 tensor([[283., 284., 285., 286., 287.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([284.], device='cuda:0') data.h5py: 284 tensor([[284., 285., 286., 287., 288.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([285.], device='cuda:0') data.h5py: 285 tensor([[285., 286., 287., 288., 289.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([286.], device='cuda:0') data.h5py: 286 tensor([[286., 287., 288., 289., 290.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([287.], device='cuda:0') data.h5py: 287 tensor([[287., 288., 289., 290., 291.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([288.], device='cuda:0') data.h5py: 288 tensor([[288., 289., 290., 291., 292.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([289.], device='cuda:0') data.h5py: 289 tensor([[289., 290., 291., 292., 293.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([290.], device='cuda:0') data.h5py: 290 tensor([[290., 291., 292., 293., 294.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([291.], device='cuda:0') data.h5py: 291 tensor([[291., 292., 293., 294., 295.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([292.], device='cuda:0') data.h5py: 292 tensor([[292., 293., 294., 295., 296.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([293.], device='cuda:0') data.h5py: 293 tensor([[293., 294., 295., 296., 297.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([294.], device='cuda:0') data.h5py: 294 tensor([[294., 295., 296., 297., 298.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([295.], device='cuda:0') data.h5py: 295 tensor([[295., 296., 297., 298., 299.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([296.], device='cuda:0') data.h5py: 296 tensor([[296., 297., 298., 299., 300.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([297.], device='cuda:0') data.h5py: 297 tensor([[297., 298., 299., 300., 301.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([298.], device='cuda:0') data.h5py: 298 tensor([[298., 299., 300., 301., 302.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([299.], device='cuda:0') data.h5py: 299 tensor([[299., 300., 301., 302., 303.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([300.], device='cuda:0') data.h5py: 300 tensor([[300., 301., 302., 303., 304.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([301.], device='cuda:0') data.h5py: 301 tensor([[301., 302., 303., 304., 305.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([302.], device='cuda:0') data.h5py: 302 tensor([[302., 303., 304., 305., 306.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([303.], device='cuda:0') data.h5py: 303 tensor([[303., 304., 305., 306., 307.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([304.], device='cuda:0') data.h5py: 304 tensor([[304., 305., 306., 307., 308.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([305.], device='cuda:0') data.h5py: 305 tensor([[305., 306., 307., 308., 309.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([306.], device='cuda:0') data.h5py: 306 tensor([[306., 307., 308., 309., 310.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([307.], device='cuda:0') data.h5py: 307 tensor([[307., 308., 309., 310., 311.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([308.], device='cuda:0') data.h5py: 308 tensor([[308., 309., 310., 311., 312.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([309.], device='cuda:0') data.h5py: 309 tensor([[309., 310., 311., 312., 313.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([310.], device='cuda:0') data.h5py: 310 tensor([[310., 311., 312., 313., 314.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([311.], device='cuda:0') data.h5py: 311 tensor([[311., 312., 313., 314., 315.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([312.], device='cuda:0') data.h5py: 312 tensor([[312., 313., 314., 315., 316.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([313.], device='cuda:0') data.h5py: 313 tensor([[313., 314., 315., 316., 317.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([314.], device='cuda:0') data.h5py: 314 tensor([[314., 315., 316., 317., 318.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([315.], device='cuda:0') data.h5py: 315 tensor([[315., 316., 317., 318., 319.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([316.], device='cuda:0') data.h5py: 316 tensor([[316., 317., 318., 319., 320.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([317.], device='cuda:0') data.h5py: 317 tensor([[317., 318., 319., 320., 321.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([318.], device='cuda:0') data.h5py: 318 tensor([[318., 319., 320., 321., 322.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([319.], device='cuda:0') data.h5py: 319 tensor([[319., 320., 321., 322., 323.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([320.], device='cuda:0') data.h5py: 320 tensor([[320., 321., 322., 323., 324.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([321.], device='cuda:0') data.h5py: 321 tensor([[321., 322., 323., 324., 325.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([322.], device='cuda:0') data.h5py: 322 tensor([[322., 323., 324., 325., 326.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([323.], device='cuda:0') data.h5py: 323 tensor([[323., 324., 325., 326., 327.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([324.], device='cuda:0') data.h5py: 324 tensor([[324., 325., 326., 327., 328.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([325.], device='cuda:0') data.h5py: 325 tensor([[325., 326., 327., 328., 329.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([326.], device='cuda:0') data.h5py: 326 tensor([[326., 327., 328., 329., 330.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([327.], device='cuda:0') data.h5py: 327 tensor([[327., 328., 329., 330., 331.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([328.], device='cuda:0') data.h5py: 328 tensor([[328., 329., 330., 331., 332.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([329.], device='cuda:0') data.h5py: 329 tensor([[329., 330., 331., 332., 333.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([330.], device='cuda:0') data.h5py: 330 tensor([[330., 331., 332., 333., 334.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([331.], device='cuda:0') data.h5py: 331 tensor([[331., 332., 333., 334., 335.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([332.], device='cuda:0') data.h5py: 332 tensor([[332., 333., 334., 335., 336.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([333.], device='cuda:0') data.h5py: 333 tensor([[333., 334., 335., 336., 337.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([334.], device='cuda:0') data.h5py: 334 tensor([[334., 335., 336., 337., 338.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([335.], device='cuda:0') data.h5py: 335 tensor([[335., 336., 337., 338., 339.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([336.], device='cuda:0') data.h5py: 336 tensor([[336., 337., 338., 339., 340.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([337.], device='cuda:0') data.h5py: 337 tensor([[337., 338., 339., 340., 341.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([338.], device='cuda:0') data.h5py: 338 tensor([[338., 339., 340., 341., 342.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([339.], device='cuda:0') data.h5py: 339 tensor([[339., 340., 341., 342., 343.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([340.], device='cuda:0') data.h5py: 340 tensor([[340., 341., 342., 343., 344.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([341.], device='cuda:0') data.h5py: 341 tensor([[341., 342., 343., 344., 345.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([342.], device='cuda:0') data.h5py: 342 tensor([[342., 343., 344., 345., 346.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([343.], device='cuda:0') data.h5py: 343 tensor([[343., 344., 345., 346., 347.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([344.], device='cuda:0') data.h5py: 344 tensor([[344., 345., 346., 347., 348.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([345.], device='cuda:0') data.h5py: 345 tensor([[345., 346., 347., 348., 349.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([346.], device='cuda:0') data.h5py: 346 tensor([[346., 347., 348., 349., 350.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([347.], device='cuda:0') data.h5py: 347 tensor([[347., 348., 349., 350., 351.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([348.], device='cuda:0') data.h5py: 348 tensor([[348., 349., 350., 351., 352.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([349.], device='cuda:0') data.h5py: 349 tensor([[349., 350., 351., 352., 353.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([350.], device='cuda:0') data.h5py: 350 tensor([[350., 351., 352., 353., 354.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([351.], device='cuda:0') data.h5py: 351 tensor([[351., 352., 353., 354., 355.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([352.], device='cuda:0') data.h5py: 352 tensor([[352., 353., 354., 355., 356.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([353.], device='cuda:0') data.h5py: 353 tensor([[353., 354., 355., 356., 357.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([354.], device='cuda:0') data.h5py: 354 tensor([[354., 355., 356., 357., 358.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([355.], device='cuda:0') data.h5py: 355 tensor([[355., 356., 357., 358., 359.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([356.], device='cuda:0') data.h5py: 356 tensor([[356., 357., 358., 359., 360.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([357.], device='cuda:0') data.h5py: 357 tensor([[357., 358., 359., 360., 361.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([358.], device='cuda:0') data.h5py: 358 tensor([[358., 359., 360., 361., 362.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([359.], device='cuda:0') data.h5py: 359 tensor([[359., 360., 361., 362., 363.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([360.], device='cuda:0') data.h5py: 360 tensor([[360., 361., 362., 363., 364.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([361.], device='cuda:0') data.h5py: 361 tensor([[361., 362., 363., 364., 365.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([362.], device='cuda:0') data.h5py: 362 tensor([[362., 363., 364., 365., 366.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([363.], device='cuda:0') data.h5py: 363 tensor([[363., 364., 365., 366., 367.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([364.], device='cuda:0') data.h5py: 364 tensor([[364., 365., 366., 367., 368.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([365.], device='cuda:0') data.h5py: 365 tensor([[365., 366., 367., 368., 369.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([366.], device='cuda:0') data.h5py: 366 tensor([[366., 367., 368., 369., 370.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([367.], device='cuda:0') data.h5py: 367 tensor([[367., 368., 369., 370., 371.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([368.], device='cuda:0') data.h5py: 368 tensor([[368., 369., 370., 371., 372.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([369.], device='cuda:0') data.h5py: 369 tensor([[369., 370., 371., 372., 373.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([370.], device='cuda:0') data.h5py: 370 tensor([[370., 371., 372., 373., 374.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([371.], device='cuda:0') data.h5py: 371 tensor([[371., 372., 373., 374., 375.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([372.], device='cuda:0') data.h5py: 372 tensor([[372., 373., 374., 375., 376.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([373.], device='cuda:0') data.h5py: 373 tensor([[373., 374., 375., 376., 377.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([374.], device='cuda:0') data.h5py: 374 tensor([[374., 375., 376., 377., 378.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([375.], device='cuda:0') data.h5py: 375 tensor([[375., 376., 377., 378., 379.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([376.], device='cuda:0') data.h5py: 376 tensor([[376., 377., 378., 379., 380.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([377.], device='cuda:0') data.h5py: 377 tensor([[377., 378., 379., 380., 381.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([378.], device='cuda:0') data.h5py: 378 tensor([[378., 379., 380., 381., 382.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([379.], device='cuda:0') data.h5py: 379 tensor([[379., 380., 381., 382., 383.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([380.], device='cuda:0') data.h5py: 380 tensor([[380., 381., 382., 383., 384.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([381.], device='cuda:0') data.h5py: 381 tensor([[381., 382., 383., 384., 385.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([382.], device='cuda:0') data.h5py: 382 tensor([[382., 383., 384., 385., 386.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([383.], device='cuda:0') data.h5py: 383 tensor([[383., 384., 385., 386., 387.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([384.], device='cuda:0') data.h5py: 384 tensor([[384., 385., 386., 387., 388.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([385.], device='cuda:0') data.h5py: 385 tensor([[385., 386., 387., 388., 389.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([386.], device='cuda:0') data.h5py: 386 tensor([[386., 387., 388., 389., 390.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([387.], device='cuda:0') data.h5py: 387 tensor([[387., 388., 389., 390., 391.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([388.], device='cuda:0') data.h5py: 388 tensor([[388., 389., 390., 391., 392.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([389.], device='cuda:0') data.h5py: 389 tensor([[389., 390., 391., 392., 393.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([390.], device='cuda:0') data.h5py: 390 tensor([[390., 391., 392., 393., 394.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([391.], device='cuda:0') data.h5py: 391 tensor([[391., 392., 393., 394., 395.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([392.], device='cuda:0') data.h5py: 392 tensor([[392., 393., 394., 395., 396.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([393.], device='cuda:0') data.h5py: 393 tensor([[393., 394., 395., 396., 397.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([394.], device='cuda:0') data.h5py: 394 tensor([[394., 395., 396., 397., 398.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([395.], device='cuda:0') data.h5py: 395 tensor([[395., 396., 397., 398., 399.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([396.], device='cuda:0') data.h5py: 396 tensor([[396., 397., 398., 399., 400.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([397.], device='cuda:0') data.h5py: 397 tensor([[397., 398., 399., 400., 401.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([398.], device='cuda:0') data.h5py: 398 tensor([[398., 399., 400., 401., 402.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([399.], device='cuda:0') data.h5py: 399 tensor([[399., 400., 401., 402., 403.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([400.], device='cuda:0') data.h5py: 400 tensor([[400., 401., 402., 403., 404.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([401.], device='cuda:0') data.h5py: 401 tensor([[401., 402., 403., 404., 405.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([402.], device='cuda:0') data.h5py: 402 tensor([[402., 403., 404., 405., 406.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([403.], device='cuda:0') data.h5py: 403 tensor([[403., 404., 405., 406., 407.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([404.], device='cuda:0') data.h5py: 404 tensor([[404., 405., 406., 407., 408.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([405.], device='cuda:0') data.h5py: 405 tensor([[405., 406., 407., 408., 409.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([406.], device='cuda:0') data.h5py: 406 tensor([[406., 407., 408., 409., 410.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([407.], device='cuda:0') data.h5py: 407 tensor([[407., 408., 409., 410., 411.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([408.], device='cuda:0') data.h5py: 408 tensor([[408., 409., 410., 411., 412.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([409.], device='cuda:0') data.h5py: 409 tensor([[409., 410., 411., 412., 413.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([410.], device='cuda:0') data.h5py: 410 tensor([[410., 411., 412., 413., 414.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([411.], device='cuda:0') data.h5py: 411 tensor([[411., 412., 413., 414., 415.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([412.], device='cuda:0') data.h5py: 412 tensor([[412., 413., 414., 415., 416.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([413.], device='cuda:0') data.h5py: 413 tensor([[413., 414., 415., 416., 417.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([414.], device='cuda:0') data.h5py: 414 tensor([[414., 415., 416., 417., 418.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([415.], device='cuda:0') data.h5py: 415 tensor([[415., 416., 417., 418., 419.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([416.], device='cuda:0') data.h5py: 416 tensor([[416., 417., 418., 419., 420.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([417.], device='cuda:0') data.h5py: 417 tensor([[417., 418., 419., 420., 421.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([418.], device='cuda:0') data.h5py: 418 tensor([[418., 419., 420., 421., 422.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([419.], device='cuda:0') data.h5py: 419 tensor([[419., 420., 421., 422., 423.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([420.], device='cuda:0') data.h5py: 420 tensor([[420., 421., 422., 423., 424.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([421.], device='cuda:0') data.h5py: 421 tensor([[421., 422., 423., 424., 425.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([422.], device='cuda:0') data.h5py: 422 tensor([[422., 423., 424., 425., 426.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([423.], device='cuda:0') data.h5py: 423 tensor([[423., 424., 425., 426., 427.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([424.], device='cuda:0') data.h5py: 424 tensor([[424., 425., 426., 427., 428.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([425.], device='cuda:0') data.h5py: 425 tensor([[425., 426., 427., 428., 429.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([426.], device='cuda:0') data.h5py: 426 tensor([[426., 427., 428., 429., 430.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([427.], device='cuda:0') data.h5py: 427 tensor([[427., 428., 429., 430., 431.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([428.], device='cuda:0') data.h5py: 428 tensor([[428., 429., 430., 431., 432.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([429.], device='cuda:0') data.h5py: 429 tensor([[429., 430., 431., 432., 433.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([430.], device='cuda:0') data.h5py: 430 tensor([[430., 431., 432., 433., 434.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([431.], device='cuda:0') data.h5py: 431 tensor([[431., 432., 433., 434., 435.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([432.], device='cuda:0') data.h5py: 432 tensor([[432., 433., 434., 435., 436.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([433.], device='cuda:0') data.h5py: 433 tensor([[433., 434., 435., 436., 437.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([434.], device='cuda:0') data.h5py: 434 tensor([[434., 435., 436., 437., 438.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([435.], device='cuda:0') data.h5py: 435 tensor([[435., 436., 437., 438., 439.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([436.], device='cuda:0') data.h5py: 436 tensor([[436., 437., 438., 439., 440.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([437.], device='cuda:0') data.h5py: 437 tensor([[437., 438., 439., 440., 441.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([438.], device='cuda:0') data.h5py: 438 tensor([[438., 439., 440., 441., 442.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([439.], device='cuda:0') data.h5py: 439 tensor([[439., 440., 441., 442., 443.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([440.], device='cuda:0') data.h5py: 440 tensor([[440., 441., 442., 443., 444.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([441.], device='cuda:0') data.h5py: 441 tensor([[441., 442., 443., 444., 445.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([442.], device='cuda:0') data.h5py: 442 tensor([[442., 443., 444., 445., 446.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([443.], device='cuda:0') data.h5py: 443 tensor([[443., 444., 445., 446., 447.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([444.], device='cuda:0') data.h5py: 444 tensor([[444., 445., 446., 447., 448.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([445.], device='cuda:0') data.h5py: 445 tensor([[445., 446., 447., 448., 449.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([446.], device='cuda:0') data.h5py: 446 tensor([[446., 447., 448., 449., 450.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([447.], device='cuda:0') data.h5py: 447 tensor([[447., 448., 449., 450., 451.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([448.], device='cuda:0') data.h5py: 448 tensor([[448., 449., 450., 451., 452.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([449.], device='cuda:0') data.h5py: 449 tensor([[449., 450., 451., 452., 453.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([450.], device='cuda:0') data.h5py: 450 tensor([[450., 451., 452., 453., 454.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([451.], device='cuda:0') data.h5py: 451 tensor([[451., 452., 453., 454., 455.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([452.], device='cuda:0') data.h5py: 452 tensor([[452., 453., 454., 455., 456.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([453.], device='cuda:0') data.h5py: 453 tensor([[453., 454., 455., 456., 457.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([454.], device='cuda:0') data.h5py: 454 tensor([[454., 455., 456., 457., 458.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([455.], device='cuda:0') data.h5py: 455 tensor([[455., 456., 457., 458., 459.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([456.], device='cuda:0') data.h5py: 456 tensor([[456., 457., 458., 459., 460.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([457.], device='cuda:0') data.h5py: 457 tensor([[457., 458., 459., 460., 461.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([458.], device='cuda:0') data.h5py: 458 tensor([[458., 459., 460., 461., 462.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([459.], device='cuda:0') data.h5py: 459 tensor([[459., 460., 461., 462., 463.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([460.], device='cuda:0') data.h5py: 460 tensor([[460., 461., 462., 463., 464.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([461.], device='cuda:0') data.h5py: 461 tensor([[461., 462., 463., 464., 465.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([462.], device='cuda:0') data.h5py: 462 tensor([[462., 463., 464., 465., 466.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([463.], device='cuda:0') data.h5py: 463 tensor([[463., 464., 465., 466., 467.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([464.], device='cuda:0') data.h5py: 464 tensor([[464., 465., 466., 467., 468.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([465.], device='cuda:0') data.h5py: 465 tensor([[465., 466., 467., 468., 469.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([466.], device='cuda:0') data.h5py: 466 tensor([[466., 467., 468., 469., 470.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([467.], device='cuda:0') data.h5py: 467 tensor([[467., 468., 469., 470., 471.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([468.], device='cuda:0') data.h5py: 468 tensor([[468., 469., 470., 471., 472.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([469.], device='cuda:0') data.h5py: 469 tensor([[469., 470., 471., 472., 473.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([470.], device='cuda:0') data.h5py: 470 tensor([[470., 471., 472., 473., 474.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([471.], device='cuda:0') data.h5py: 471 tensor([[471., 472., 473., 474., 475.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([472.], device='cuda:0') data.h5py: 472 tensor([[472., 473., 474., 475., 476.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([473.], device='cuda:0') data.h5py: 473 tensor([[473., 474., 475., 476., 477.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([474.], device='cuda:0') data.h5py: 474 tensor([[474., 475., 476., 477., 478.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([475.], device='cuda:0') data.h5py: 475 tensor([[475., 476., 477., 478., 479.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([476.], device='cuda:0') data.h5py: 476 tensor([[476., 477., 478., 479., 480.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([477.], device='cuda:0') data.h5py: 477 tensor([[477., 478., 479., 480., 481.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([478.], device='cuda:0') data.h5py: 478 tensor([[478., 479., 480., 481., 482.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([479.], device='cuda:0') data.h5py: 479 tensor([[479., 480., 481., 482., 483.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([480.], device='cuda:0') data.h5py: 480 tensor([[480., 481., 482., 483., 484.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([481.], device='cuda:0') data.h5py: 481 tensor([[481., 482., 483., 484., 485.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([482.], device='cuda:0') data.h5py: 482 tensor([[482., 483., 484., 485., 486.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([483.], device='cuda:0') data.h5py: 483 tensor([[483., 484., 485., 486., 487.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([484.], device='cuda:0') data.h5py: 484 tensor([[484., 485., 486., 487., 488.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([485.], device='cuda:0') data.h5py: 485 tensor([[485., 486., 487., 488., 489.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([486.], device='cuda:0') data.h5py: 486 tensor([[486., 487., 488., 489., 490.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([487.], device='cuda:0') data.h5py: 487 tensor([[487., 488., 489., 490., 491.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([488.], device='cuda:0') data.h5py: 488 tensor([[488., 489., 490., 491., 492.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([489.], device='cuda:0') data.h5py: 489 tensor([[489., 490., 491., 492., 493.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([490.], device='cuda:0') data.h5py: 490 tensor([[490., 491., 492., 493., 494.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([491.], device='cuda:0') data.h5py: 491 tensor([[491., 492., 493., 494., 495.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([492.], device='cuda:0') data.h5py: 492 tensor([[492., 493., 494., 495., 496.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([493.], device='cuda:0') data.h5py: 493 tensor([[493., 494., 495., 496., 497.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([494.], device='cuda:0') data.h5py: 494 tensor([[494., 495., 496., 497., 498.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([495.], device='cuda:0') data.h5py: 495 tensor([[495., 496., 497., 498., 499.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([496.], device='cuda:0') data.h5py: 496 tensor([[496., 497., 498., 499., 500.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([497.], device='cuda:0') data.h5py: 497 tensor([[497., 498., 499., 500., 501.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([498.], device='cuda:0') data.h5py: 498 tensor([[498., 499., 500., 501., 502.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([499.], device='cuda:0') data.h5py: 499 tensor([[499., 500., 501., 502., 503.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([500.], device='cuda:0') data.h5py: 500 tensor([[500., 501., 502., 503., 504.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([501.], device='cuda:0') data.h5py: 501 tensor([[501., 502., 503., 504., 505.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([502.], device='cuda:0') data.h5py: 502 tensor([[502., 503., 504., 505., 506.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([503.], device='cuda:0') data.h5py: 503 tensor([[503., 504., 505., 506., 507.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([504.], device='cuda:0') data.h5py: 504 tensor([[504., 505., 506., 507., 508.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([505.], device='cuda:0') data.h5py: 505 tensor([[505., 506., 507., 508., 509.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([506.], device='cuda:0') data.h5py: 506 tensor([[506., 507., 508., 509., 510.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([507.], device='cuda:0') data.h5py: 507 tensor([[507., 508., 509., 510., 511.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([508.], device='cuda:0') data.h5py: 508 tensor([[508., 509., 510., 511., 512.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([509.], device='cuda:0') data.h5py: 509 tensor([[509., 510., 511., 512., 513.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([510.], device='cuda:0') data.h5py: 510 tensor([[510., 511., 512., 513., 514.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([511.], device='cuda:0') data.h5py: 511 tensor([[511., 512., 513., 514., 515.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([512.], device='cuda:0') data.h5py: 512 tensor([[512., 513., 514., 515., 516.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([513.], device='cuda:0') data.h5py: 513 tensor([[513., 514., 515., 516., 517.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([514.], device='cuda:0') data.h5py: 514 tensor([[514., 515., 516., 517., 518.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([515.], device='cuda:0') data.h5py: 515 tensor([[515., 516., 517., 518., 519.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([516.], device='cuda:0') data.h5py: 516 tensor([[516., 517., 518., 519., 520.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([517.], device='cuda:0') data.h5py: 517 tensor([[517., 518., 519., 520., 521.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([518.], device='cuda:0') data.h5py: 518 tensor([[518., 519., 520., 521., 522.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([519.], device='cuda:0') data.h5py: 519 tensor([[519., 520., 521., 522., 523.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([520.], device='cuda:0') data.h5py: 520 tensor([[520., 521., 522., 523., 524.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([521.], device='cuda:0') data.h5py: 521 tensor([[521., 522., 523., 524., 525.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([522.], device='cuda:0') data.h5py: 522 tensor([[522., 523., 524., 525., 526.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([523.], device='cuda:0') data.h5py: 523 tensor([[523., 524., 525., 526., 527.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([524.], device='cuda:0') data.h5py: 524 tensor([[524., 525., 526., 527., 528.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([525.], device='cuda:0') data.h5py: 525 tensor([[525., 526., 527., 528., 529.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([526.], device='cuda:0') data.h5py: 526 tensor([[526., 527., 528., 529., 530.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([527.], device='cuda:0') data.h5py: 527 tensor([[527., 528., 529., 530., 531.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([528.], device='cuda:0') data.h5py: 528 tensor([[528., 529., 530., 531., 532.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([529.], device='cuda:0') data.h5py: 529 tensor([[529., 530., 531., 532., 533.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([530.], device='cuda:0') data.h5py: 530 tensor([[530., 531., 532., 533., 534.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([531.], device='cuda:0') data.h5py: 531 tensor([[531., 532., 533., 534., 535.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([532.], device='cuda:0') data.h5py: 532 tensor([[532., 533., 534., 535., 536.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([533.], device='cuda:0') data.h5py: 533 tensor([[533., 534., 535., 536., 537.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([534.], device='cuda:0') data.h5py: 534 tensor([[534., 535., 536., 537., 538.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([535.], device='cuda:0') data.h5py: 535 tensor([[535., 536., 537., 538., 539.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([536.], device='cuda:0') data.h5py: 536 tensor([[536., 537., 538., 539., 540.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([537.], device='cuda:0') data.h5py: 537 tensor([[537., 538., 539., 540., 541.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([538.], device='cuda:0') data.h5py: 538 tensor([[538., 539., 540., 541., 542.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([539.], device='cuda:0') data.h5py: 539 tensor([[539., 540., 541., 542., 543.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([540.], device='cuda:0') data.h5py: 540 tensor([[540., 541., 542., 543., 544.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([541.], device='cuda:0') data.h5py: 541 tensor([[541., 542., 543., 544., 545.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([542.], device='cuda:0') data.h5py: 542 tensor([[542., 543., 544., 545., 546.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([543.], device='cuda:0') data.h5py: 543 tensor([[543., 544., 545., 546., 547.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([544.], device='cuda:0') data.h5py: 544 tensor([[544., 545., 546., 547., 548.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([545.], device='cuda:0') data.h5py: 545 tensor([[545., 546., 547., 548., 549.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([546.], device='cuda:0') data.h5py: 546 tensor([[546., 547., 548., 549., 550.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([547.], device='cuda:0') data.h5py: 547 tensor([[547., 548., 549., 550., 551.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([548.], device='cuda:0') data.h5py: 548 tensor([[548., 549., 550., 551., 552.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([549.], device='cuda:0') data.h5py: 549 tensor([[549., 550., 551., 552., 553.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([550.], device='cuda:0') data.h5py: 550 tensor([[550., 551., 552., 553., 554.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([551.], device='cuda:0') data.h5py: 551 tensor([[551., 552., 553., 554., 555.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([552.], device='cuda:0') data.h5py: 552 tensor([[552., 553., 554., 555., 556.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([553.], device='cuda:0') data.h5py: 553 tensor([[553., 554., 555., 556., 557.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([554.], device='cuda:0') data.h5py: 554 tensor([[554., 555., 556., 557., 558.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([555.], device='cuda:0') data.h5py: 555 tensor([[555., 556., 557., 558., 559.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([556.], device='cuda:0') data.h5py: 556 tensor([[556., 557., 558., 559., 560.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([557.], device='cuda:0') data.h5py: 557 tensor([[557., 558., 559., 560., 561.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([558.], device='cuda:0') data.h5py: 558 tensor([[558., 559., 560., 561., 562.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([559.], device='cuda:0') data.h5py: 559 tensor([[559., 560., 561., 562., 563.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([560.], device='cuda:0') data.h5py: 560 tensor([[560., 561., 562., 563., 564.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([561.], device='cuda:0') data.h5py: 561 tensor([[561., 562., 563., 564., 565.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([562.], device='cuda:0') data.h5py: 562 tensor([[562., 563., 564., 565., 566.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([563.], device='cuda:0') data.h5py: 563 tensor([[563., 564., 565., 566., 567.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([564.], device='cuda:0') data.h5py: 564 tensor([[564., 565., 566., 567., 568.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([565.], device='cuda:0') data.h5py: 565 tensor([[565., 566., 567., 568., 569.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([566.], device='cuda:0') data.h5py: 566 tensor([[566., 567., 568., 569., 570.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([567.], device='cuda:0') data.h5py: 567 tensor([[567., 568., 569., 570., 571.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([568.], device='cuda:0') data.h5py: 568 tensor([[568., 569., 570., 571., 572.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([569.], device='cuda:0') data.h5py: 569 tensor([[569., 570., 571., 572., 573.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([570.], device='cuda:0') data.h5py: 570 tensor([[570., 571., 572., 573., 574.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([571.], device='cuda:0') data.h5py: 571 tensor([[571., 572., 573., 574., 575.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([572.], device='cuda:0') data.h5py: 572 tensor([[572., 573., 574., 575., 576.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([573.], device='cuda:0') data.h5py: 573 tensor([[573., 574., 575., 576., 577.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([574.], device='cuda:0') data.h5py: 574 tensor([[574., 575., 576., 577., 578.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([575.], device='cuda:0') data.h5py: 575 tensor([[575., 576., 577., 578., 579.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([576.], device='cuda:0') data.h5py: 576 tensor([[576., 577., 578., 579., 580.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([577.], device='cuda:0') data.h5py: 577 tensor([[577., 578., 579., 580., 581.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([578.], device='cuda:0') data.h5py: 578 tensor([[578., 579., 580., 581., 582.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([579.], device='cuda:0') data.h5py: 579 tensor([[579., 580., 581., 582., 583.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([580.], device='cuda:0') data.h5py: 580 tensor([[580., 581., 582., 583., 584.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([581.], device='cuda:0') data.h5py: 581 tensor([[581., 582., 583., 584., 585.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([582.], device='cuda:0') data.h5py: 582 tensor([[582., 583., 584., 585., 586.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([583.], device='cuda:0') data.h5py: 583 tensor([[583., 584., 585., 586., 587.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([584.], device='cuda:0') data.h5py: 584 tensor([[584., 585., 586., 587., 588.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([585.], device='cuda:0') data.h5py: 585 tensor([[585., 586., 587., 588., 589.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([586.], device='cuda:0') data.h5py: 586 tensor([[586., 587., 588., 589., 590.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([587.], device='cuda:0') data.h5py: 587 tensor([[587., 588., 589., 590., 591.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([588.], device='cuda:0') data.h5py: 588 tensor([[588., 589., 590., 591., 592.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([589.], device='cuda:0') data.h5py: 589 tensor([[589., 590., 591., 592., 593.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([590.], device='cuda:0') data.h5py: 590 tensor([[590., 591., 592., 593., 594.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([591.], device='cuda:0') data.h5py: 591 tensor([[591., 592., 593., 594., 595.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([592.], device='cuda:0') data.h5py: 592 tensor([[592., 593., 594., 595., 596.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([593.], device='cuda:0') data.h5py: 593 tensor([[593., 594., 595., 596., 597.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([594.], device='cuda:0') data.h5py: 594 tensor([[594., 595., 596., 597., 598.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([595.], device='cuda:0') data.h5py: 595 tensor([[595., 596., 597., 598., 599.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([596.], device='cuda:0') data.h5py: 596 tensor([[596., 597., 598., 599., 600.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([597.], device='cuda:0') data.h5py: 597 tensor([[597., 598., 599., 600., 601.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([598.], device='cuda:0') data.h5py: 598 tensor([[598., 599., 600., 601., 602.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([599.], device='cuda:0') data.h5py: 599 tensor([[599., 600., 601., 602., 603.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([600.], device='cuda:0') data.h5py: 600 tensor([[600., 601., 602., 603., 604.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([601.], device='cuda:0') data.h5py: 601 tensor([[601., 602., 603., 604., 605.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([602.], device='cuda:0') data.h5py: 602 tensor([[602., 603., 604., 605., 606.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([603.], device='cuda:0') data.h5py: 603 tensor([[603., 604., 605., 606., 607.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([604.], device='cuda:0') data.h5py: 604 tensor([[604., 605., 606., 607., 608.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([605.], device='cuda:0') data.h5py: 605 tensor([[605., 606., 607., 608., 609.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([606.], device='cuda:0') data.h5py: 606 tensor([[606., 607., 608., 609., 610.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([607.], device='cuda:0') data.h5py: 607 tensor([[607., 608., 609., 610., 611.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([608.], device='cuda:0') data.h5py: 608 tensor([[608., 609., 610., 611., 612.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([609.], device='cuda:0') data.h5py: 609 tensor([[609., 610., 611., 612., 613.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([610.], device='cuda:0') data.h5py: 610 tensor([[610., 611., 612., 613., 614.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([611.], device='cuda:0') data.h5py: 611 tensor([[611., 612., 613., 614., 615.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([612.], device='cuda:0') data.h5py: 612 tensor([[612., 613., 614., 615., 616.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([613.], device='cuda:0') data.h5py: 613 tensor([[613., 614., 615., 616., 617.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([614.], device='cuda:0') data.h5py: 614 tensor([[614., 615., 616., 617., 618.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([615.], device='cuda:0') data.h5py: 615 tensor([[615., 616., 617., 618., 619.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([616.], device='cuda:0') data.h5py: 616 tensor([[616., 617., 618., 619., 620.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([617.], device='cuda:0') data.h5py: 617 tensor([[617., 618., 619., 620., 621.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([618.], device='cuda:0') data.h5py: 618 tensor([[618., 619., 620., 621., 622.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([619.], device='cuda:0') data.h5py: 619 tensor([[619., 620., 621., 622., 623.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([620.], device='cuda:0') data.h5py: 620 tensor([[620., 621., 622., 623., 624.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([621.], device='cuda:0') data.h5py: 621 tensor([[621., 622., 623., 624., 625.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([622.], device='cuda:0') data.h5py: 622 tensor([[622., 623., 624., 625., 626.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([623.], device='cuda:0') data.h5py: 623 tensor([[623., 624., 625., 626., 627.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([624.], device='cuda:0') data.h5py: 624 tensor([[624., 625., 626., 627., 628.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([625.], device='cuda:0') data.h5py: 625 tensor([[625., 626., 627., 628., 629.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([626.], device='cuda:0') data.h5py: 626 tensor([[626., 627., 628., 629., 630.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([627.], device='cuda:0') data.h5py: 627 tensor([[627., 628., 629., 630., 631.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([628.], device='cuda:0') data.h5py: 628 tensor([[628., 629., 630., 631., 632.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([629.], device='cuda:0') data.h5py: 629 tensor([[629., 630., 631., 632., 633.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([630.], device='cuda:0') data.h5py: 630 tensor([[630., 631., 632., 633., 634.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([631.], device='cuda:0') data.h5py: 631 tensor([[631., 632., 633., 634., 635.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([632.], device='cuda:0') data.h5py: 632 tensor([[632., 633., 634., 635., 636.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([633.], device='cuda:0') data.h5py: 633 tensor([[633., 634., 635., 636., 637.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([634.], device='cuda:0') data.h5py: 634 tensor([[634., 635., 636., 637., 638.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([635.], device='cuda:0') data.h5py: 635 tensor([[635., 636., 637., 638., 639.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([636.], device='cuda:0') data.h5py: 636 tensor([[636., 637., 638., 639., 640.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([637.], device='cuda:0') data.h5py: 637 tensor([[637., 638., 639., 640., 641.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([638.], device='cuda:0') data.h5py: 638 tensor([[638., 639., 640., 641., 642.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([639.], device='cuda:0') data.h5py: 639 tensor([[639., 640., 641., 642., 643.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([640.], device='cuda:0') data.h5py: 640 tensor([[640., 641., 642., 643., 644.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([641.], device='cuda:0') data.h5py: 641 tensor([[641., 642., 643., 644., 645.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([642.], device='cuda:0') data.h5py: 642 tensor([[642., 643., 644., 645., 646.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([643.], device='cuda:0') data.h5py: 643 tensor([[643., 644., 645., 646., 647.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([644.], device='cuda:0') data.h5py: 644 tensor([[644., 645., 646., 647., 648.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([645.], device='cuda:0') data.h5py: 645 tensor([[645., 646., 647., 648., 649.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([646.], device='cuda:0') data.h5py: 646 tensor([[646., 647., 648., 649., 650.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([647.], device='cuda:0') data.h5py: 647 tensor([[647., 648., 649., 650., 651.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([648.], device='cuda:0') data.h5py: 648 tensor([[648., 649., 650., 651., 652.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([649.], device='cuda:0') data.h5py: 649 tensor([[649., 650., 651., 652., 653.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([650.], device='cuda:0') data.h5py: 650 tensor([[650., 651., 652., 653., 654.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([651.], device='cuda:0') data.h5py: 651 tensor([[651., 652., 653., 654., 655.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([652.], device='cuda:0') data.h5py: 652 tensor([[652., 653., 654., 655., 656.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([653.], device='cuda:0') data.h5py: 653 tensor([[653., 654., 655., 656., 657.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([654.], device='cuda:0') data.h5py: 654 tensor([[654., 655., 656., 657., 658.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([655.], device='cuda:0') data.h5py: 655 tensor([[655., 656., 657., 658., 659.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([656.], device='cuda:0') data.h5py: 656 tensor([[656., 657., 658., 659., 660.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([657.], device='cuda:0') data.h5py: 657 tensor([[657., 658., 659., 660., 661.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([658.], device='cuda:0') data.h5py: 658 tensor([[658., 659., 660., 661., 662.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([659.], device='cuda:0') data.h5py: 659 tensor([[659., 660., 661., 662., 663.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([660.], device='cuda:0') data.h5py: 660 tensor([[660., 661., 662., 663., 664.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([661.], device='cuda:0') data.h5py: 661 tensor([[661., 662., 663., 664., 665.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([662.], device='cuda:0') data.h5py: 662 tensor([[662., 663., 664., 665., 666.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([663.], device='cuda:0') data.h5py: 663 tensor([[663., 664., 665., 666., 667.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([664.], device='cuda:0') data.h5py: 664 tensor([[664., 665., 666., 667., 668.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([665.], device='cuda:0') data.h5py: 665 tensor([[665., 666., 667., 668., 669.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([666.], device='cuda:0') data.h5py: 666 tensor([[666., 667., 668., 669., 670.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([667.], device='cuda:0') data.h5py: 667 tensor([[667., 668., 669., 670., 671.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([668.], device='cuda:0') data.h5py: 668 tensor([[668., 669., 670., 671., 672.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([669.], device='cuda:0') data.h5py: 669 tensor([[669., 670., 671., 672., 673.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([670.], device='cuda:0') data.h5py: 670 tensor([[670., 671., 672., 673., 674.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([671.], device='cuda:0') data.h5py: 671 tensor([[671., 672., 673., 674., 675.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([672.], device='cuda:0') data.h5py: 672 tensor([[672., 673., 674., 675., 676.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([673.], device='cuda:0') data.h5py: 673 tensor([[673., 674., 675., 676., 677.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([674.], device='cuda:0') data.h5py: 674 tensor([[674., 675., 676., 677., 678.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([675.], device='cuda:0') data.h5py: 675 tensor([[675., 676., 677., 678., 679.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([676.], device='cuda:0') data.h5py: 676 tensor([[676., 677., 678., 679., 680.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([677.], device='cuda:0') data.h5py: 677 tensor([[677., 678., 679., 680., 681.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([678.], device='cuda:0') data.h5py: 678 tensor([[678., 679., 680., 681., 682.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([679.], device='cuda:0') data.h5py: 679 tensor([[679., 680., 681., 682., 683.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([680.], device='cuda:0') data.h5py: 680 tensor([[680., 681., 682., 683., 684.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([681.], device='cuda:0') data.h5py: 681 tensor([[681., 682., 683., 684., 685.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([682.], device='cuda:0') data.h5py: 682 tensor([[682., 683., 684., 685., 686.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([683.], device='cuda:0') data.h5py: 683 tensor([[683., 684., 685., 686., 687.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([684.], device='cuda:0') data.h5py: 684 tensor([[684., 685., 686., 687., 688.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([685.], device='cuda:0') data.h5py: 685 tensor([[685., 686., 687., 688., 689.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([686.], device='cuda:0') data.h5py: 686 tensor([[686., 687., 688., 689., 690.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([687.], device='cuda:0') data.h5py: 687 tensor([[687., 688., 689., 690., 691.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([688.], device='cuda:0') data.h5py: 688 tensor([[688., 689., 690., 691., 692.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([689.], device='cuda:0') data.h5py: 689 tensor([[689., 690., 691., 692., 693.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([690.], device='cuda:0') data.h5py: 690 tensor([[690., 691., 692., 693., 694.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([691.], device='cuda:0') data.h5py: 691 tensor([[691., 692., 693., 694., 695.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([692.], device='cuda:0') data.h5py: 692 tensor([[692., 693., 694., 695., 696.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([693.], device='cuda:0') data.h5py: 693 tensor([[693., 694., 695., 696., 697.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([694.], device='cuda:0') data.h5py: 694 tensor([[694., 695., 696., 697., 698.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([695.], device='cuda:0') data.h5py: 695 tensor([[695., 696., 697., 698., 699.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([696.], device='cuda:0') data.h5py: 696 tensor([[696., 697., 698., 699., 700.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([697.], device='cuda:0') data.h5py: 697 tensor([[697., 698., 699., 700., 701.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([698.], device='cuda:0') data.h5py: 698 tensor([[698., 699., 700., 701., 702.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([699.], device='cuda:0') data.h5py: 699 tensor([[699., 700., 701., 702., 703.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([700.], device='cuda:0') data.h5py: 700 tensor([[700., 701., 702., 703., 704.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([701.], device='cuda:0') data.h5py: 701 tensor([[701., 702., 703., 704., 705.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([702.], device='cuda:0') data.h5py: 702 tensor([[702., 703., 704., 705., 706.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([703.], device='cuda:0') data.h5py: 703 tensor([[703., 704., 705., 706., 707.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([704.], device='cuda:0') data.h5py: 704 tensor([[704., 705., 706., 707., 708.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([705.], device='cuda:0') data.h5py: 705 tensor([[705., 706., 707., 708., 709.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([706.], device='cuda:0') data.h5py: 706 tensor([[706., 707., 708., 709., 710.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([707.], device='cuda:0') data.h5py: 707 tensor([[707., 708., 709., 710., 711.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([708.], device='cuda:0') data.h5py: 708 tensor([[708., 709., 710., 711., 712.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([709.], device='cuda:0') data.h5py: 709 tensor([[709., 710., 711., 712., 713.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([710.], device='cuda:0') data.h5py: 710 tensor([[710., 711., 712., 713., 714.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([711.], device='cuda:0') data.h5py: 711 tensor([[711., 712., 713., 714., 715.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([712.], device='cuda:0') data.h5py: 712 tensor([[712., 713., 714., 715., 716.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([713.], device='cuda:0') data.h5py: 713 tensor([[713., 714., 715., 716., 717.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([714.], device='cuda:0') data.h5py: 714 tensor([[714., 715., 716., 717., 718.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([715.], device='cuda:0') data.h5py: 715 tensor([[715., 716., 717., 718., 719.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([716.], device='cuda:0') data.h5py: 716 tensor([[716., 717., 718., 719., 720.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([717.], device='cuda:0') data.h5py: 717 tensor([[717., 718., 719., 720., 721.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([718.], device='cuda:0') data.h5py: 718 tensor([[718., 719., 720., 721., 722.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([719.], device='cuda:0') data.h5py: 719 tensor([[719., 720., 721., 722., 723.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([720.], device='cuda:0') data.h5py: 720 tensor([[720., 721., 722., 723., 724.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([721.], device='cuda:0') data.h5py: 721 tensor([[721., 722., 723., 724., 725.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([722.], device='cuda:0') data.h5py: 722 tensor([[722., 723., 724., 725., 726.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([723.], device='cuda:0') data.h5py: 723 tensor([[723., 724., 725., 726., 727.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([724.], device='cuda:0') data.h5py: 724 tensor([[724., 725., 726., 727., 728.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([725.], device='cuda:0') data.h5py: 725 tensor([[725., 726., 727., 728., 729.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([726.], device='cuda:0') data.h5py: 726 tensor([[726., 727., 728., 729., 730.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([727.], device='cuda:0') data.h5py: 727 tensor([[727., 728., 729., 730., 731.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([728.], device='cuda:0') data.h5py: 728 tensor([[728., 729., 730., 731., 732.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([729.], device='cuda:0') data.h5py: 729 tensor([[729., 730., 731., 732., 733.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([730.], device='cuda:0') data.h5py: 730 tensor([[730., 731., 732., 733., 734.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([731.], device='cuda:0') data.h5py: 731 tensor([[731., 732., 733., 734., 735.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([732.], device='cuda:0') data.h5py: 732 tensor([[732., 733., 734., 735., 736.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([733.], device='cuda:0') data.h5py: 733 tensor([[733., 734., 735., 736., 737.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([734.], device='cuda:0') data.h5py: 734 tensor([[734., 735., 736., 737., 738.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([735.], device='cuda:0') data.h5py: 735 tensor([[735., 736., 737., 738., 739.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([736.], device='cuda:0') data.h5py: 736 tensor([[736., 737., 738., 739., 740.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([737.], device='cuda:0') data.h5py: 737 tensor([[737., 738., 739., 740., 741.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([738.], device='cuda:0') data.h5py: 738 tensor([[738., 739., 740., 741., 742.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([739.], device='cuda:0') data.h5py: 739 tensor([[739., 740., 741., 742., 743.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([740.], device='cuda:0') data.h5py: 740 tensor([[740., 741., 742., 743., 744.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([741.], device='cuda:0') data.h5py: 741 tensor([[741., 742., 743., 744., 745.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([742.], device='cuda:0') data.h5py: 742 tensor([[742., 743., 744., 745., 746.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([743.], device='cuda:0') data.h5py: 743 tensor([[743., 744., 745., 746., 747.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([744.], device='cuda:0') data.h5py: 744 tensor([[744., 745., 746., 747., 748.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([745.], device='cuda:0') data.h5py: 745 tensor([[745., 746., 747., 748., 749.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([746.], device='cuda:0') data.h5py: 746 tensor([[746., 747., 748., 749., 750.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([747.], device='cuda:0') data.h5py: 747 tensor([[747., 748., 749., 750., 751.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([748.], device='cuda:0') data.h5py: 748 tensor([[748., 749., 750., 751., 752.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([749.], device='cuda:0') data.h5py: 749 tensor([[749., 750., 751., 752., 753.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([750.], device='cuda:0') data.h5py: 750 tensor([[750., 751., 752., 753., 754.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([751.], device='cuda:0') data.h5py: 751 tensor([[751., 752., 753., 754., 755.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([752.], device='cuda:0') data.h5py: 752 tensor([[752., 753., 754., 755., 756.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([753.], device='cuda:0') data.h5py: 753 tensor([[753., 754., 755., 756., 757.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([754.], device='cuda:0') data.h5py: 754 tensor([[754., 755., 756., 757., 758.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([755.], device='cuda:0') data.h5py: 755 tensor([[755., 756., 757., 758., 759.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([756.], device='cuda:0') data.h5py: 756 tensor([[756., 757., 758., 759., 760.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([757.], device='cuda:0') data.h5py: 757 tensor([[757., 758., 759., 760., 761.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([758.], device='cuda:0') data.h5py: 758 tensor([[758., 759., 760., 761., 762.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([759.], device='cuda:0') data.h5py: 759 tensor([[759., 760., 761., 762., 763.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([760.], device='cuda:0') data.h5py: 760 tensor([[760., 761., 762., 763., 764.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([761.], device='cuda:0') data.h5py: 761 tensor([[761., 762., 763., 764., 765.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([762.], device='cuda:0') data.h5py: 762 tensor([[762., 763., 764., 765., 766.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([763.], device='cuda:0') data.h5py: 763 tensor([[763., 764., 765., 766., 767.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([764.], device='cuda:0') data.h5py: 764 tensor([[764., 765., 766., 767., 768.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([765.], device='cuda:0') data.h5py: 765 tensor([[765., 766., 767., 768., 769.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([766.], device='cuda:0') data.h5py: 766 tensor([[766., 767., 768., 769., 770.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([767.], device='cuda:0') data.h5py: 767 tensor([[767., 768., 769., 770., 771.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([768.], device='cuda:0') data.h5py: 768 tensor([[768., 769., 770., 771., 772.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([769.], device='cuda:0') data.h5py: 769 tensor([[769., 770., 771., 772., 773.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([770.], device='cuda:0') data.h5py: 770 tensor([[770., 771., 772., 773., 774.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([771.], device='cuda:0') data.h5py: 771 tensor([[771., 772., 773., 774., 775.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([772.], device='cuda:0') data.h5py: 772 tensor([[772., 773., 774., 775., 776.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([773.], device='cuda:0') data.h5py: 773 tensor([[773., 774., 775., 776., 777.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([774.], device='cuda:0') data.h5py: 774 tensor([[774., 775., 776., 777., 778.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([775.], device='cuda:0') data.h5py: 775 tensor([[775., 776., 777., 778., 779.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([776.], device='cuda:0') data.h5py: 776 tensor([[776., 777., 778., 779., 780.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([777.], device='cuda:0') data.h5py: 777 tensor([[777., 778., 779., 780., 781.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([778.], device='cuda:0') data.h5py: 778 tensor([[778., 779., 780., 781., 782.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([779.], device='cuda:0') data.h5py: 779 tensor([[779., 780., 781., 782., 783.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([780.], device='cuda:0') data.h5py: 780 tensor([[780., 781., 782., 783., 784.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([781.], device='cuda:0') data.h5py: 781 tensor([[781., 782., 783., 784., 785.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([782.], device='cuda:0') data.h5py: 782 tensor([[782., 783., 784., 785., 786.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([783.], device='cuda:0') data.h5py: 783 tensor([[783., 784., 785., 786., 787.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([784.], device='cuda:0') data.h5py: 784 tensor([[784., 785., 786., 787., 788.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([785.], device='cuda:0') data.h5py: 785 tensor([[785., 786., 787., 788., 789.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([786.], device='cuda:0') data.h5py: 786 tensor([[786., 787., 788., 789., 790.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([787.], device='cuda:0') data.h5py: 787 tensor([[787., 788., 789., 790., 791.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([788.], device='cuda:0') data.h5py: 788 tensor([[788., 789., 790., 791., 792.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([789.], device='cuda:0') data.h5py: 789 tensor([[789., 790., 791., 792., 793.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([790.], device='cuda:0') data.h5py: 790 tensor([[790., 791., 792., 793., 794.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([791.], device='cuda:0') data.h5py: 791 tensor([[791., 792., 793., 794., 795.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([792.], device='cuda:0') data.h5py: 792 tensor([[792., 793., 794., 795., 796.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([793.], device='cuda:0') data.h5py: 793 tensor([[793., 794., 795., 796., 797.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([794.], device='cuda:0') data.h5py: 794 tensor([[794., 795., 796., 797., 798.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([795.], device='cuda:0') data.h5py: 795 tensor([[795., 796., 797., 798., 799.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([796.], device='cuda:0') data.h5py: 796 tensor([[796., 797., 798., 799., 800.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([797.], device='cuda:0') data.h5py: 797 tensor([[797., 798., 799., 800., 801.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([798.], device='cuda:0') data.h5py: 798 tensor([[798., 799., 800., 801., 802.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([799.], device='cuda:0') data.h5py: 799 tensor([[799., 800., 801., 802., 803.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([800.], device='cuda:0') data.h5py: 800 tensor([[800., 801., 802., 803., 804.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([801.], device='cuda:0') data.h5py: 801 tensor([[801., 802., 803., 804., 805.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([802.], device='cuda:0') data.h5py: 802 tensor([[802., 803., 804., 805., 806.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([803.], device='cuda:0') data.h5py: 803 tensor([[803., 804., 805., 806., 807.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([804.], device='cuda:0') data.h5py: 804 tensor([[804., 805., 806., 807., 808.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([805.], device='cuda:0') data.h5py: 805 tensor([[805., 806., 807., 808., 809.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([806.], device='cuda:0') data.h5py: 806 tensor([[806., 807., 808., 809., 810.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([807.], device='cuda:0') data.h5py: 807 tensor([[807., 808., 809., 810., 811.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([808.], device='cuda:0') data.h5py: 808 tensor([[808., 809., 810., 811., 812.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([809.], device='cuda:0') data.h5py: 809 tensor([[809., 810., 811., 812., 813.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([810.], device='cuda:0') data.h5py: 810 tensor([[810., 811., 812., 813., 814.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([811.], device='cuda:0') data.h5py: 811 tensor([[811., 812., 813., 814., 815.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([812.], device='cuda:0') data.h5py: 812 tensor([[812., 813., 814., 815., 816.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([813.], device='cuda:0') data.h5py: 813 tensor([[813., 814., 815., 816., 817.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([814.], device='cuda:0') data.h5py: 814 tensor([[814., 815., 816., 817., 818.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([815.], device='cuda:0') data.h5py: 815 tensor([[815., 816., 817., 818., 819.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([816.], device='cuda:0') data.h5py: 816 tensor([[816., 817., 818., 819., 820.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([817.], device='cuda:0') data.h5py: 817 tensor([[817., 818., 819., 820., 821.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([818.], device='cuda:0') data.h5py: 818 tensor([[818., 819., 820., 821., 822.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([819.], device='cuda:0') data.h5py: 819 tensor([[819., 820., 821., 822., 823.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([820.], device='cuda:0') data.h5py: 820 tensor([[820., 821., 822., 823., 824.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([821.], device='cuda:0') data.h5py: 821 tensor([[821., 822., 823., 824., 825.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([822.], device='cuda:0') data.h5py: 822 tensor([[822., 823., 824., 825., 826.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([823.], device='cuda:0') data.h5py: 823 tensor([[823., 824., 825., 826., 827.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([824.], device='cuda:0') data.h5py: 824 tensor([[824., 825., 826., 827., 828.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([825.], device='cuda:0') data.h5py: 825 tensor([[825., 826., 827., 828., 829.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([826.], device='cuda:0') data.h5py: 826 tensor([[826., 827., 828., 829., 830.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([827.], device='cuda:0') data.h5py: 827 tensor([[827., 828., 829., 830., 831.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([828.], device='cuda:0') data.h5py: 828 tensor([[828., 829., 830., 831., 832.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([829.], device='cuda:0') data.h5py: 829 tensor([[829., 830., 831., 832., 833.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([830.], device='cuda:0') data.h5py: 830 tensor([[830., 831., 832., 833., 834.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([831.], device='cuda:0') data.h5py: 831 tensor([[831., 832., 833., 834., 835.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([832.], device='cuda:0') data.h5py: 832 tensor([[832., 833., 834., 835., 836.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([833.], device='cuda:0') data.h5py: 833 tensor([[833., 834., 835., 836., 837.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([834.], device='cuda:0') data.h5py: 834 tensor([[834., 835., 836., 837., 838.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([835.], device='cuda:0') data.h5py: 835 tensor([[835., 836., 837., 838., 839.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([836.], device='cuda:0') data.h5py: 836 tensor([[836., 837., 838., 839., 840.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([837.], device='cuda:0') data.h5py: 837 tensor([[837., 838., 839., 840., 841.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([838.], device='cuda:0') data.h5py: 838 tensor([[838., 839., 840., 841., 842.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([839.], device='cuda:0') data.h5py: 839 tensor([[839., 840., 841., 842., 843.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([840.], device='cuda:0') data.h5py: 840 tensor([[840., 841., 842., 843., 844.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([841.], device='cuda:0') data.h5py: 841 tensor([[841., 842., 843., 844., 845.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([842.], device='cuda:0') data.h5py: 842 tensor([[842., 843., 844., 845., 846.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([843.], device='cuda:0') data.h5py: 843 tensor([[843., 844., 845., 846., 847.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([844.], device='cuda:0') data.h5py: 844 tensor([[844., 845., 846., 847., 848.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([845.], device='cuda:0') data.h5py: 845 tensor([[845., 846., 847., 848., 849.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([846.], device='cuda:0') data.h5py: 846 tensor([[846., 847., 848., 849., 850.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([847.], device='cuda:0') data.h5py: 847 tensor([[847., 848., 849., 850., 851.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([848.], device='cuda:0') data.h5py: 848 tensor([[848., 849., 850., 851., 852.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([849.], device='cuda:0') data.h5py: 849 tensor([[849., 850., 851., 852., 853.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([850.], device='cuda:0') data.h5py: 850 tensor([[850., 851., 852., 853., 854.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([851.], device='cuda:0') data.h5py: 851 tensor([[851., 852., 853., 854., 855.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([852.], device='cuda:0') data.h5py: 852 tensor([[852., 853., 854., 855., 856.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([853.], device='cuda:0') data.h5py: 853 tensor([[853., 854., 855., 856., 857.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([854.], device='cuda:0') data.h5py: 854 tensor([[854., 855., 856., 857., 858.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([855.], device='cuda:0') data.h5py: 855 tensor([[855., 856., 857., 858., 859.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([856.], device='cuda:0') data.h5py: 856 tensor([[856., 857., 858., 859., 860.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([857.], device='cuda:0') data.h5py: 857 tensor([[857., 858., 859., 860., 861.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([858.], device='cuda:0') data.h5py: 858 tensor([[858., 859., 860., 861., 862.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([859.], device='cuda:0') data.h5py: 859 tensor([[859., 860., 861., 862., 863.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([860.], device='cuda:0') data.h5py: 860 tensor([[860., 861., 862., 863., 864.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([861.], device='cuda:0') data.h5py: 861 tensor([[861., 862., 863., 864., 865.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([862.], device='cuda:0') data.h5py: 862 tensor([[862., 863., 864., 865., 866.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([863.], device='cuda:0') data.h5py: 863 tensor([[863., 864., 865., 866., 867.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([864.], device='cuda:0') data.h5py: 864 tensor([[864., 865., 866., 867., 868.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([865.], device='cuda:0') data.h5py: 865 tensor([[865., 866., 867., 868., 869.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([866.], device='cuda:0') data.h5py: 866 tensor([[866., 867., 868., 869., 870.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([867.], device='cuda:0') data.h5py: 867 tensor([[867., 868., 869., 870., 871.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([868.], device='cuda:0') data.h5py: 868 tensor([[868., 869., 870., 871., 872.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([869.], device='cuda:0') data.h5py: 869 tensor([[869., 870., 871., 872., 873.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([870.], device='cuda:0') data.h5py: 870 tensor([[870., 871., 872., 873., 874.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([871.], device='cuda:0') data.h5py: 871 tensor([[871., 872., 873., 874., 875.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([872.], device='cuda:0') data.h5py: 872 tensor([[872., 873., 874., 875., 876.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([873.], device='cuda:0') data.h5py: 873 tensor([[873., 874., 875., 876., 877.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([874.], device='cuda:0') data.h5py: 874 tensor([[874., 875., 876., 877., 878.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([875.], device='cuda:0') data.h5py: 875 tensor([[875., 876., 877., 878., 879.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([876.], device='cuda:0') data.h5py: 876 tensor([[876., 877., 878., 879., 880.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([877.], device='cuda:0') data.h5py: 877 tensor([[877., 878., 879., 880., 881.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([878.], device='cuda:0') data.h5py: 878 tensor([[878., 879., 880., 881., 882.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([879.], device='cuda:0') data.h5py: 879 tensor([[879., 880., 881., 882., 883.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([880.], device='cuda:0') data.h5py: 880 tensor([[880., 881., 882., 883., 884.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([881.], device='cuda:0') data.h5py: 881 tensor([[881., 882., 883., 884., 885.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([882.], device='cuda:0') data.h5py: 882 tensor([[882., 883., 884., 885., 886.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([883.], device='cuda:0') data.h5py: 883 tensor([[883., 884., 885., 886., 887.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([884.], device='cuda:0') data.h5py: 884 tensor([[884., 885., 886., 887., 888.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([885.], device='cuda:0') data.h5py: 885 tensor([[885., 886., 887., 888., 889.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([886.], device='cuda:0') data.h5py: 886 tensor([[886., 887., 888., 889., 890.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([887.], device='cuda:0') data.h5py: 887 tensor([[887., 888., 889., 890., 891.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([888.], device='cuda:0') data.h5py: 888 tensor([[888., 889., 890., 891., 892.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([889.], device='cuda:0') data.h5py: 889 tensor([[889., 890., 891., 892., 893.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([890.], device='cuda:0') data.h5py: 890 tensor([[890., 891., 892., 893., 894.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([891.], device='cuda:0') data.h5py: 891 tensor([[891., 892., 893., 894., 895.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([892.], device='cuda:0') data.h5py: 892 tensor([[892., 893., 894., 895., 896.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([893.], device='cuda:0') data.h5py: 893 tensor([[893., 894., 895., 896., 897.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([894.], device='cuda:0') data.h5py: 894 tensor([[894., 895., 896., 897., 898.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([895.], device='cuda:0') data.h5py: 895 tensor([[895., 896., 897., 898., 899.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([896.], device='cuda:0') data.h5py: 896 tensor([[896., 897., 898., 899., 900.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([897.], device='cuda:0') data.h5py: 897 tensor([[897., 898., 899., 900., 901.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([898.], device='cuda:0') data.h5py: 898 tensor([[898., 899., 900., 901., 902.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([899.], device='cuda:0') data.h5py: 899 tensor([[899., 900., 901., 902., 903.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([900.], device='cuda:0') data.h5py: 900 tensor([[900., 901., 902., 903., 904.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([901.], device='cuda:0') data.h5py: 901 tensor([[901., 902., 903., 904., 905.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([902.], device='cuda:0') data.h5py: 902 tensor([[902., 903., 904., 905., 906.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([903.], device='cuda:0') data.h5py: 903 tensor([[903., 904., 905., 906., 907.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([904.], device='cuda:0') data.h5py: 904 tensor([[904., 905., 906., 907., 908.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([905.], device='cuda:0') data.h5py: 905 tensor([[905., 906., 907., 908., 909.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([906.], device='cuda:0') data.h5py: 906 tensor([[906., 907., 908., 909., 910.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([907.], device='cuda:0') data.h5py: 907 tensor([[907., 908., 909., 910., 911.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([908.], device='cuda:0') data.h5py: 908 tensor([[908., 909., 910., 911., 912.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([909.], device='cuda:0') data.h5py: 909 tensor([[909., 910., 911., 912., 913.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([910.], device='cuda:0') data.h5py: 910 tensor([[910., 911., 912., 913., 914.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([911.], device='cuda:0') data.h5py: 911 tensor([[911., 912., 913., 914., 915.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([912.], device='cuda:0') data.h5py: 912 tensor([[912., 913., 914., 915., 916.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([913.], device='cuda:0') data.h5py: 913 tensor([[913., 914., 915., 916., 917.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([914.], device='cuda:0') data.h5py: 914 tensor([[914., 915., 916., 917., 918.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([915.], device='cuda:0') data.h5py: 915 tensor([[915., 916., 917., 918., 919.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([916.], device='cuda:0') data.h5py: 916 tensor([[916., 917., 918., 919., 920.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([917.], device='cuda:0') data.h5py: 917 tensor([[917., 918., 919., 920., 921.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([918.], device='cuda:0') data.h5py: 918 tensor([[918., 919., 920., 921., 922.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([919.], device='cuda:0') data.h5py: 919 tensor([[919., 920., 921., 922., 923.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([920.], device='cuda:0') data.h5py: 920 tensor([[920., 921., 922., 923., 924.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([921.], device='cuda:0') data.h5py: 921 tensor([[921., 922., 923., 924., 925.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([922.], device='cuda:0') data.h5py: 922 tensor([[922., 923., 924., 925., 926.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([923.], device='cuda:0') data.h5py: 923 tensor([[923., 924., 925., 926., 927.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([924.], device='cuda:0') data.h5py: 924 tensor([[924., 925., 926., 927., 928.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([925.], device='cuda:0') data.h5py: 925 tensor([[925., 926., 927., 928., 929.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([926.], device='cuda:0') data.h5py: 926 tensor([[926., 927., 928., 929., 930.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([927.], device='cuda:0') data.h5py: 927 tensor([[927., 928., 929., 930., 931.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([928.], device='cuda:0') data.h5py: 928 tensor([[928., 929., 930., 931., 932.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([929.], device='cuda:0') data.h5py: 929 tensor([[929., 930., 931., 932., 933.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([930.], device='cuda:0') data.h5py: 930 tensor([[930., 931., 932., 933., 934.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([931.], device='cuda:0') data.h5py: 931 tensor([[931., 932., 933., 934., 935.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([932.], device='cuda:0') data.h5py: 932 tensor([[932., 933., 934., 935., 936.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([933.], device='cuda:0') data.h5py: 933 tensor([[933., 934., 935., 936., 937.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([934.], device='cuda:0') data.h5py: 934 tensor([[934., 935., 936., 937., 938.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([935.], device='cuda:0') data.h5py: 935 tensor([[935., 936., 937., 938., 939.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([936.], device='cuda:0') data.h5py: 936 tensor([[936., 937., 938., 939., 940.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([937.], device='cuda:0') data.h5py: 937 tensor([[937., 938., 939., 940., 941.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([938.], device='cuda:0') data.h5py: 938 tensor([[938., 939., 940., 941., 942.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([939.], device='cuda:0') data.h5py: 939 tensor([[939., 940., 941., 942., 943.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([940.], device='cuda:0') data.h5py: 940 tensor([[940., 941., 942., 943., 944.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([941.], device='cuda:0') data.h5py: 941 tensor([[941., 942., 943., 944., 945.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([942.], device='cuda:0') data.h5py: 942 tensor([[942., 943., 944., 945., 946.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([943.], device='cuda:0') data.h5py: 943 tensor([[943., 944., 945., 946., 947.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([944.], device='cuda:0') data.h5py: 944 tensor([[944., 945., 946., 947., 948.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([945.], device='cuda:0') data.h5py: 945 tensor([[945., 946., 947., 948., 949.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([946.], device='cuda:0') data.h5py: 946 tensor([[946., 947., 948., 949., 950.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([947.], device='cuda:0') data.h5py: 947 tensor([[947., 948., 949., 950., 951.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([948.], device='cuda:0') data.h5py: 948 tensor([[948., 949., 950., 951., 952.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([949.], device='cuda:0') data.h5py: 949 tensor([[949., 950., 951., 952., 953.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([950.], device='cuda:0') data.h5py: 950 tensor([[950., 951., 952., 953., 954.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([951.], device='cuda:0') data.h5py: 951 tensor([[951., 952., 953., 954., 955.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([952.], device='cuda:0') data.h5py: 952 tensor([[952., 953., 954., 955., 956.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([953.], device='cuda:0') data.h5py: 953 tensor([[953., 954., 955., 956., 957.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([954.], device='cuda:0') data.h5py: 954 tensor([[954., 955., 956., 957., 958.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([955.], device='cuda:0') data.h5py: 955 tensor([[955., 956., 957., 958., 959.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([956.], device='cuda:0') data.h5py: 956 tensor([[956., 957., 958., 959., 960.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([957.], device='cuda:0') data.h5py: 957 tensor([[957., 958., 959., 960., 961.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([958.], device='cuda:0') data.h5py: 958 tensor([[958., 959., 960., 961., 962.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([959.], device='cuda:0') data.h5py: 959 tensor([[959., 960., 961., 962., 963.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([960.], device='cuda:0') data.h5py: 960 tensor([[960., 961., 962., 963., 964.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([961.], device='cuda:0') data.h5py: 961 tensor([[961., 962., 963., 964., 965.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([962.], device='cuda:0') data.h5py: 962 tensor([[962., 963., 964., 965., 966.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([963.], device='cuda:0') data.h5py: 963 tensor([[963., 964., 965., 966., 967.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([964.], device='cuda:0') data.h5py: 964 tensor([[964., 965., 966., 967., 968.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([965.], device='cuda:0') data.h5py: 965 tensor([[965., 966., 967., 968., 969.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([966.], device='cuda:0') data.h5py: 966 tensor([[966., 967., 968., 969., 970.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([967.], device='cuda:0') data.h5py: 967 tensor([[967., 968., 969., 970., 971.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([968.], device='cuda:0') data.h5py: 968 tensor([[968., 969., 970., 971., 972.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([969.], device='cuda:0') data.h5py: 969 tensor([[969., 970., 971., 972., 973.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([970.], device='cuda:0') data.h5py: 970 tensor([[970., 971., 972., 973., 974.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([971.], device='cuda:0') data.h5py: 971 tensor([[971., 972., 973., 974., 975.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([972.], device='cuda:0') data.h5py: 972 tensor([[972., 973., 974., 975., 976.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([973.], device='cuda:0') data.h5py: 973 tensor([[973., 974., 975., 976., 977.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([974.], device='cuda:0') data.h5py: 974 tensor([[974., 975., 976., 977., 978.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([975.], device='cuda:0') data.h5py: 975 tensor([[975., 976., 977., 978., 979.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([976.], device='cuda:0') data.h5py: 976 tensor([[976., 977., 978., 979., 980.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([977.], device='cuda:0') data.h5py: 977 tensor([[977., 978., 979., 980., 981.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([978.], device='cuda:0') data.h5py: 978 tensor([[978., 979., 980., 981., 982.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([979.], device='cuda:0') data.h5py: 979 tensor([[979., 980., 981., 982., 983.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([980.], device='cuda:0') data.h5py: 980 tensor([[980., 981., 982., 983., 984.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([981.], device='cuda:0') data.h5py: 981 tensor([[981., 982., 983., 984., 985.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([982.], device='cuda:0') data.h5py: 982 tensor([[982., 983., 984., 985., 986.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([983.], device='cuda:0') data.h5py: 983 tensor([[983., 984., 985., 986., 987.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([984.], device='cuda:0') data.h5py: 984 tensor([[984., 985., 986., 987., 988.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([985.], device='cuda:0') data.h5py: 985 tensor([[985., 986., 987., 988., 989.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([986.], device='cuda:0') data.h5py: 986 tensor([[986., 987., 988., 989., 990.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([987.], device='cuda:0') data.h5py: 987 tensor([[987., 988., 989., 990., 991.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([988.], device='cuda:0') data.h5py: 988 tensor([[988., 989., 990., 991., 992.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([989.], device='cuda:0') data.h5py: 989 tensor([[989., 990., 991., 992., 993.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([990.], device='cuda:0') data.h5py: 990 tensor([[990., 991., 992., 993., 994.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([991.], device='cuda:0') data.h5py: 991 tensor([[991., 992., 993., 994., 995.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([992.], device='cuda:0') data.h5py: 992 tensor([[992., 993., 994., 995., 996.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([993.], device='cuda:0') data.h5py: 993 tensor([[993., 994., 995., 996., 997.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([994.], device='cuda:0') data.h5py: 994 tensor([[994., 995., 996., 997., 998.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([995.], device='cuda:0') data.h5py: 995 tensor([[995., 996., 997., 998., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([996.], device='cuda:0') data.h5py: 996 tensor([[996., 997., 998., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([997.], device='cuda:0') data.h5py: 997 tensor([[997., 998., 999., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([998.], device='cuda:0') data.h5py: 998 tensor([[998., 999., 999., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([999.], device='cuda:0') data.h5py: 999 tensor([[999., 999., 999., 999., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1000.], device='cuda:0') Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5cparser_seq.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5CParser ( os . path . join ( root_folder , 'test_data_h5cparser_seq' ), keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 10 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Output data.webtools: All required datasets are available. data.h5py: 0 tensor([[0., 0., 0., 0., 0.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1.], device='cuda:0') data.h5py: 1 tensor([[0., 0., 0., 0., 1.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([2.], device='cuda:0') data.h5py: 2 tensor([[0., 0., 0., 1., 2.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([3.], device='cuda:0') data.h5py: 3 tensor([[0., 0., 1., 2., 3.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([4.], device='cuda:0') data.h5py: 4 tensor([[0., 1., 2., 3., 4.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([5.], device='cuda:0') data.h5py: 5 tensor([[1., 2., 3., 4., 5.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([6.], device='cuda:0') data.h5py: 6 tensor([[2., 3., 4., 5., 6.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([7.], device='cuda:0') data.h5py: 7 tensor([[3., 4., 5., 6., 7.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([8.], device='cuda:0') data.h5py: 8 tensor([[4., 5., 6., 7., 8.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([9.], device='cuda:0') data.h5py: 9 tensor([[5., 6., 7., 8., 9.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([10.], device='cuda:0') data.h5py: 10 tensor([[ 6., 7., 8., 9., 10.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([11.], device='cuda:0') data.h5py: 11 tensor([[ 7., 8., 9., 10., 11.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([12.], device='cuda:0') data.h5py: 12 tensor([[ 8., 9., 10., 11., 12.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([13.], device='cuda:0') data.h5py: 13 tensor([[ 9., 10., 11., 12., 13.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([14.], device='cuda:0') data.h5py: 14 tensor([[10., 11., 12., 13., 14.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([15.], device='cuda:0') data.h5py: 15 tensor([[11., 12., 13., 14., 15.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([16.], device='cuda:0') data.h5py: 16 tensor([[12., 13., 14., 15., 16.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([17.], device='cuda:0') data.h5py: 17 tensor([[13., 14., 15., 16., 17.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([18.], device='cuda:0') data.h5py: 18 tensor([[14., 15., 16., 17., 18.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([19.], device='cuda:0') data.h5py: 19 tensor([[15., 16., 17., 18., 19.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([20.], device='cuda:0') data.h5py: 20 tensor([[20., 20., 20., 20., 20.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([21.], device='cuda:0') data.h5py: 21 tensor([[20., 20., 20., 20., 21.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([22.], device='cuda:0') data.h5py: 22 tensor([[20., 20., 20., 21., 22.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([23.], device='cuda:0') data.h5py: 23 tensor([[20., 20., 21., 22., 23.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([24.], device='cuda:0') data.h5py: 24 tensor([[20., 21., 22., 23., 24.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([25.], device='cuda:0') data.h5py: 25 tensor([[21., 22., 23., 24., 25.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([26.], device='cuda:0') data.h5py: 26 tensor([[22., 23., 24., 25., 26.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([27.], device='cuda:0') data.h5py: 27 tensor([[23., 24., 25., 26., 27.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([28.], device='cuda:0') data.h5py: 28 tensor([[24., 25., 26., 27., 28.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([29.], device='cuda:0') data.h5py: 29 tensor([[25., 26., 27., 28., 29.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([30.], device='cuda:0') data.h5py: 30 tensor([[26., 27., 28., 29., 30.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([31.], device='cuda:0') data.h5py: 31 tensor([[27., 28., 29., 30., 31.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([32.], device='cuda:0') data.h5py: 32 tensor([[28., 29., 30., 31., 32.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([33.], device='cuda:0') data.h5py: 33 tensor([[29., 30., 31., 32., 33.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([34.], device='cuda:0') data.h5py: 34 tensor([[30., 31., 32., 33., 34.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([35.], device='cuda:0') data.h5py: 35 tensor([[31., 32., 33., 34., 35.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([36.], device='cuda:0') data.h5py: 36 tensor([[32., 33., 34., 35., 36.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([37.], device='cuda:0') data.h5py: 37 tensor([[33., 34., 35., 36., 37.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([38.], device='cuda:0') data.h5py: 38 tensor([[38., 38., 38., 38., 38.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([39.], device='cuda:0') data.h5py: 39 tensor([[38., 38., 38., 38., 39.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([40.], device='cuda:0') data.h5py: 40 tensor([[38., 38., 38., 39., 40.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([41.], device='cuda:0') data.h5py: 41 tensor([[38., 38., 39., 40., 41.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([42.], device='cuda:0') data.h5py: 42 tensor([[38., 39., 40., 41., 42.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([43.], device='cuda:0') data.h5py: 43 tensor([[39., 40., 41., 42., 43.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([44.], device='cuda:0') data.h5py: 44 tensor([[40., 41., 42., 43., 44.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([45.], device='cuda:0') data.h5py: 45 tensor([[41., 42., 43., 44., 45.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([46.], device='cuda:0') data.h5py: 46 tensor([[42., 43., 44., 45., 46.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([47.], device='cuda:0') data.h5py: 47 tensor([[43., 44., 45., 46., 47.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([48.], device='cuda:0') data.h5py: 48 tensor([[44., 45., 46., 47., 48.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([49.], device='cuda:0') data.h5py: 49 tensor([[45., 46., 47., 48., 49.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([50.], device='cuda:0') data.h5py: 50 tensor([[46., 47., 48., 49., 50.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([51.], device='cuda:0') data.h5py: 51 tensor([[51., 51., 51., 51., 51.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([52.], device='cuda:0') data.h5py: 52 tensor([[51., 51., 51., 51., 52.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([53.], device='cuda:0') data.h5py: 53 tensor([[51., 51., 51., 52., 53.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([54.], device='cuda:0') data.h5py: 54 tensor([[51., 51., 52., 53., 54.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([55.], device='cuda:0') data.h5py: 55 tensor([[51., 52., 53., 54., 55.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([56.], device='cuda:0') data.h5py: 56 tensor([[52., 53., 54., 55., 56.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([57.], device='cuda:0') data.h5py: 57 tensor([[53., 54., 55., 56., 57.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([58.], device='cuda:0') data.h5py: 58 tensor([[54., 55., 56., 57., 58.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([59.], device='cuda:0') data.h5py: 59 tensor([[55., 56., 57., 58., 59.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([60.], device='cuda:0') data.h5py: 60 tensor([[56., 57., 58., 59., 60.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([61.], device='cuda:0') data.h5py: 61 tensor([[57., 58., 59., 60., 61.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([62.], device='cuda:0') data.h5py: 62 tensor([[58., 59., 60., 61., 62.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([63.], device='cuda:0') data.h5py: 63 tensor([[59., 60., 61., 62., 63.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([64.], device='cuda:0') data.h5py: 64 tensor([[60., 61., 62., 63., 64.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([65.], device='cuda:0') data.h5py: 65 tensor([[61., 62., 63., 64., 65.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([66.], device='cuda:0') data.h5py: 66 tensor([[62., 63., 64., 65., 66.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([67.], device='cuda:0') data.h5py: 67 tensor([[63., 64., 65., 66., 67.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([68.], device='cuda:0') data.h5py: 68 tensor([[64., 65., 66., 67., 68.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([69.], device='cuda:0') data.h5py: 69 tensor([[65., 66., 67., 68., 69.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([70.], device='cuda:0') data.h5py: 70 tensor([[70., 70., 70., 70., 70.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([71.], device='cuda:0') data.h5py: 71 tensor([[70., 70., 70., 70., 71.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([72.], device='cuda:0') data.h5py: 72 tensor([[70., 70., 70., 71., 72.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([73.], device='cuda:0') data.h5py: 73 tensor([[70., 70., 71., 72., 73.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([74.], device='cuda:0') data.h5py: 74 tensor([[70., 71., 72., 73., 74.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([75.], device='cuda:0') data.h5py: 75 tensor([[71., 72., 73., 74., 75.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([76.], device='cuda:0') data.h5py: 76 tensor([[72., 73., 74., 75., 76.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([77.], device='cuda:0') data.h5py: 77 tensor([[73., 74., 75., 76., 77.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([78.], device='cuda:0') data.h5py: 78 tensor([[74., 75., 76., 77., 78.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([79.], device='cuda:0') data.h5py: 79 tensor([[75., 76., 77., 78., 79.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([80.], device='cuda:0') data.h5py: 80 tensor([[76., 77., 78., 79., 80.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([81.], device='cuda:0') data.h5py: 81 tensor([[77., 78., 79., 80., 81.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([82.], device='cuda:0') data.h5py: 82 tensor([[82., 82., 82., 82., 82.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([83.], device='cuda:0') data.h5py: 83 tensor([[82., 82., 82., 82., 83.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([84.], device='cuda:0') data.h5py: 84 tensor([[82., 82., 82., 83., 84.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([85.], device='cuda:0') data.h5py: 85 tensor([[82., 82., 83., 84., 85.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([86.], device='cuda:0') data.h5py: 86 tensor([[82., 83., 84., 85., 86.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([87.], device='cuda:0') data.h5py: 87 tensor([[83., 84., 85., 86., 87.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([88.], device='cuda:0') data.h5py: 88 tensor([[84., 85., 86., 87., 88.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([89.], device='cuda:0') data.h5py: 89 tensor([[85., 86., 87., 88., 89.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([90.], device='cuda:0') data.h5py: 90 tensor([[86., 87., 88., 89., 90.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([91.], device='cuda:0') data.h5py: 91 tensor([[87., 88., 89., 90., 91.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([92.], device='cuda:0') data.h5py: 92 tensor([[88., 89., 90., 91., 92.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([93.], device='cuda:0') data.h5py: 93 tensor([[89., 90., 91., 92., 93.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([94.], device='cuda:0') data.h5py: 94 tensor([[90., 91., 92., 93., 94.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([95.], device='cuda:0') data.h5py: 95 tensor([[91., 92., 93., 94., 95.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([96.], device='cuda:0') data.h5py: 96 tensor([[92., 93., 94., 95., 96.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([97.], device='cuda:0') data.h5py: 97 tensor([[93., 94., 95., 96., 97.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([98.], device='cuda:0') data.h5py: 98 tensor([[94., 95., 96., 97., 98.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([99.], device='cuda:0') data.h5py: 99 tensor([[95., 96., 97., 98., 99.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([100.], device='cuda:0') data.h5py: 100 tensor([[100., 100., 100., 100., 100.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([101.], device='cuda:0') data.h5py: 101 tensor([[100., 100., 100., 100., 101.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([102.], device='cuda:0') data.h5py: 102 tensor([[100., 100., 100., 101., 102.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([103.], device='cuda:0') data.h5py: 103 tensor([[100., 100., 101., 102., 103.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([104.], device='cuda:0') data.h5py: 104 tensor([[100., 101., 102., 103., 104.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([105.], device='cuda:0') data.h5py: 105 tensor([[101., 102., 103., 104., 105.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([106.], device='cuda:0') data.h5py: 106 tensor([[102., 103., 104., 105., 106.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([107.], device='cuda:0') data.h5py: 107 tensor([[103., 104., 105., 106., 107.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([108.], device='cuda:0') data.h5py: 108 tensor([[104., 105., 106., 107., 108.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([109.], device='cuda:0') data.h5py: 109 tensor([[105., 106., 107., 108., 109.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([110.], device='cuda:0') data.h5py: 110 tensor([[106., 107., 108., 109., 110.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([111.], device='cuda:0') data.h5py: 111 tensor([[107., 108., 109., 110., 111.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([112.], device='cuda:0') data.h5py: 112 tensor([[108., 109., 110., 111., 112.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([113.], device='cuda:0') data.h5py: 113 tensor([[109., 110., 111., 112., 113.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([114.], device='cuda:0') data.h5py: 114 tensor([[110., 111., 112., 113., 114.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([115.], device='cuda:0') data.h5py: 115 tensor([[111., 112., 113., 114., 115.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([116.], device='cuda:0') data.h5py: 116 tensor([[116., 116., 116., 116., 116.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([117.], device='cuda:0') data.h5py: 117 tensor([[116., 116., 116., 116., 117.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([118.], device='cuda:0') data.h5py: 118 tensor([[116., 116., 116., 117., 118.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([119.], device='cuda:0') data.h5py: 119 tensor([[116., 116., 117., 118., 119.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([120.], device='cuda:0') data.h5py: 120 tensor([[116., 117., 118., 119., 120.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([121.], device='cuda:0') data.h5py: 121 tensor([[117., 118., 119., 120., 121.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([122.], device='cuda:0') data.h5py: 122 tensor([[118., 119., 120., 121., 122.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([123.], device='cuda:0') data.h5py: 123 tensor([[119., 120., 121., 122., 123.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([124.], device='cuda:0') data.h5py: 124 tensor([[120., 121., 122., 123., 124.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([125.], device='cuda:0') data.h5py: 125 tensor([[121., 122., 123., 124., 125.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([126.], device='cuda:0') data.h5py: 126 tensor([[122., 123., 124., 125., 126.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([127.], device='cuda:0') data.h5py: 127 tensor([[123., 124., 125., 126., 127.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([128.], device='cuda:0') data.h5py: 128 tensor([[124., 125., 126., 127., 128.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([129.], device='cuda:0') data.h5py: 129 tensor([[125., 126., 127., 128., 129.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([130.], device='cuda:0') data.h5py: 130 tensor([[126., 127., 128., 129., 130.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([131.], device='cuda:0') data.h5py: 131 tensor([[127., 128., 129., 130., 131.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([132.], device='cuda:0') data.h5py: 132 tensor([[128., 129., 130., 131., 132.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([133.], device='cuda:0') data.h5py: 133 tensor([[129., 130., 131., 132., 133.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([134.], device='cuda:0') data.h5py: 134 tensor([[130., 131., 132., 133., 134.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([135.], device='cuda:0') data.h5py: 135 tensor([[131., 132., 133., 134., 135.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([136.], device='cuda:0') data.h5py: 136 tensor([[136., 136., 136., 136., 136.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([137.], device='cuda:0') data.h5py: 137 tensor([[136., 136., 136., 136., 137.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([138.], device='cuda:0') data.h5py: 138 tensor([[136., 136., 136., 137., 138.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([139.], device='cuda:0') data.h5py: 139 tensor([[136., 136., 137., 138., 139.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([140.], device='cuda:0') data.h5py: 140 tensor([[136., 137., 138., 139., 140.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([141.], device='cuda:0') data.h5py: 141 tensor([[137., 138., 139., 140., 141.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([142.], device='cuda:0') data.h5py: 142 tensor([[138., 139., 140., 141., 142.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([143.], device='cuda:0') data.h5py: 143 tensor([[139., 140., 141., 142., 143.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([144.], device='cuda:0') data.h5py: 144 tensor([[140., 141., 142., 143., 144.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([145.], device='cuda:0') data.h5py: 145 tensor([[141., 142., 143., 144., 145.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([146.], device='cuda:0') data.h5py: 146 tensor([[142., 143., 144., 145., 146.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([147.], device='cuda:0') data.h5py: 147 tensor([[143., 144., 145., 146., 147.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([148.], device='cuda:0') data.h5py: 148 tensor([[144., 145., 146., 147., 148.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([149.], device='cuda:0') data.h5py: 149 tensor([[145., 146., 147., 148., 149.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([150.], device='cuda:0') data.h5py: 150 tensor([[146., 147., 148., 149., 150.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([151.], device='cuda:0') data.h5py: 151 tensor([[147., 148., 149., 150., 151.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([152.], device='cuda:0') data.h5py: 152 tensor([[148., 149., 150., 151., 152.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([153.], device='cuda:0') data.h5py: 153 tensor([[153., 153., 153., 153., 153.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([154.], device='cuda:0') data.h5py: 154 tensor([[153., 153., 153., 153., 154.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([155.], device='cuda:0') data.h5py: 155 tensor([[153., 153., 153., 154., 155.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([156.], device='cuda:0') data.h5py: 156 tensor([[153., 153., 154., 155., 156.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([157.], device='cuda:0') data.h5py: 157 tensor([[153., 154., 155., 156., 157.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([158.], device='cuda:0') data.h5py: 158 tensor([[154., 155., 156., 157., 158.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([159.], device='cuda:0') data.h5py: 159 tensor([[155., 156., 157., 158., 159.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([160.], device='cuda:0') data.h5py: 160 tensor([[156., 157., 158., 159., 160.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([161.], device='cuda:0') data.h5py: 161 tensor([[157., 158., 159., 160., 161.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([162.], device='cuda:0') data.h5py: 162 tensor([[158., 159., 160., 161., 162.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([163.], device='cuda:0') data.h5py: 163 tensor([[159., 160., 161., 162., 163.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([164.], device='cuda:0') data.h5py: 164 tensor([[160., 161., 162., 163., 164.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([165.], device='cuda:0') data.h5py: 165 tensor([[161., 162., 163., 164., 165.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([166.], device='cuda:0') data.h5py: 166 tensor([[162., 163., 164., 165., 166.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([167.], device='cuda:0') data.h5py: 167 tensor([[163., 164., 165., 166., 167.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([168.], device='cuda:0') data.h5py: 168 tensor([[164., 165., 166., 167., 168.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([169.], device='cuda:0') data.h5py: 169 tensor([[165., 166., 167., 168., 169.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([170.], device='cuda:0') data.h5py: 170 tensor([[166., 167., 168., 169., 170.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([171.], device='cuda:0') data.h5py: 171 tensor([[167., 168., 169., 170., 171.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([172.], device='cuda:0') data.h5py: 172 tensor([[168., 169., 170., 171., 172.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([173.], device='cuda:0') data.h5py: 173 tensor([[173., 173., 173., 173., 173.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([174.], device='cuda:0') data.h5py: 174 tensor([[173., 173., 173., 173., 174.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([175.], device='cuda:0') data.h5py: 175 tensor([[173., 173., 173., 174., 175.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([176.], device='cuda:0') data.h5py: 176 tensor([[173., 173., 174., 175., 176.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([177.], device='cuda:0') data.h5py: 177 tensor([[173., 174., 175., 176., 177.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([178.], device='cuda:0') data.h5py: 178 tensor([[174., 175., 176., 177., 178.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([179.], device='cuda:0') data.h5py: 179 tensor([[175., 176., 177., 178., 179.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([180.], device='cuda:0') data.h5py: 180 tensor([[176., 177., 178., 179., 180.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([181.], device='cuda:0') data.h5py: 181 tensor([[177., 178., 179., 180., 181.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([182.], device='cuda:0') data.h5py: 182 tensor([[178., 179., 180., 181., 182.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([183.], device='cuda:0') data.h5py: 183 tensor([[179., 180., 181., 182., 183.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([184.], device='cuda:0') data.h5py: 184 tensor([[180., 181., 182., 183., 184.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([185.], device='cuda:0') data.h5py: 185 tensor([[181., 182., 183., 184., 185.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([186.], device='cuda:0') data.h5py: 186 tensor([[186., 186., 186., 186., 186.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([187.], device='cuda:0') data.h5py: 187 tensor([[186., 186., 186., 186., 187.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([188.], device='cuda:0') data.h5py: 188 tensor([[186., 186., 186., 187., 188.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([189.], device='cuda:0') data.h5py: 189 tensor([[186., 186., 187., 188., 189.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([190.], device='cuda:0') data.h5py: 190 tensor([[186., 187., 188., 189., 190.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([191.], device='cuda:0') data.h5py: 191 tensor([[187., 188., 189., 190., 191.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([192.], device='cuda:0') data.h5py: 192 tensor([[188., 189., 190., 191., 192.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([193.], device='cuda:0') data.h5py: 193 tensor([[189., 190., 191., 192., 193.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([194.], device='cuda:0') data.h5py: 194 tensor([[190., 191., 192., 193., 194.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([195.], device='cuda:0') data.h5py: 195 tensor([[191., 192., 193., 194., 195.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([196.], device='cuda:0') data.h5py: 196 tensor([[196., 196., 196., 196., 196.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([197.], device='cuda:0') data.h5py: 197 tensor([[196., 196., 196., 196., 197.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([198.], device='cuda:0') data.h5py: 198 tensor([[196., 196., 196., 197., 198.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([199.], device='cuda:0') data.h5py: 199 tensor([[196., 196., 197., 198., 199.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([200.], device='cuda:0') data.h5py: 200 tensor([[196., 197., 198., 199., 200.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([201.], device='cuda:0') data.h5py: 201 tensor([[197., 198., 199., 200., 201.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([202.], device='cuda:0') data.h5py: 202 tensor([[198., 199., 200., 201., 202.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([203.], device='cuda:0') data.h5py: 203 tensor([[199., 200., 201., 202., 203.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([204.], device='cuda:0') data.h5py: 204 tensor([[200., 201., 202., 203., 204.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([205.], device='cuda:0') data.h5py: 205 tensor([[201., 202., 203., 204., 205.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([206.], device='cuda:0') data.h5py: 206 tensor([[206., 206., 206., 206., 206.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([207.], device='cuda:0') data.h5py: 207 tensor([[206., 206., 206., 206., 207.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([208.], device='cuda:0') data.h5py: 208 tensor([[206., 206., 206., 207., 208.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([209.], device='cuda:0') data.h5py: 209 tensor([[206., 206., 207., 208., 209.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([210.], device='cuda:0') data.h5py: 210 tensor([[206., 207., 208., 209., 210.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([211.], device='cuda:0') data.h5py: 211 tensor([[207., 208., 209., 210., 211.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([212.], device='cuda:0') data.h5py: 212 tensor([[208., 209., 210., 211., 212.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([213.], device='cuda:0') data.h5py: 213 tensor([[209., 210., 211., 212., 213.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([214.], device='cuda:0') data.h5py: 214 tensor([[210., 211., 212., 213., 214.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([215.], device='cuda:0') data.h5py: 215 tensor([[211., 212., 213., 214., 215.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([216.], device='cuda:0') data.h5py: 216 tensor([[212., 213., 214., 215., 216.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([217.], device='cuda:0') data.h5py: 217 tensor([[217., 217., 217., 217., 217.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([218.], device='cuda:0') data.h5py: 218 tensor([[217., 217., 217., 217., 218.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([219.], device='cuda:0') data.h5py: 219 tensor([[217., 217., 217., 218., 219.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([220.], device='cuda:0') data.h5py: 220 tensor([[217., 217., 218., 219., 220.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([221.], device='cuda:0') data.h5py: 221 tensor([[217., 218., 219., 220., 221.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([222.], device='cuda:0') data.h5py: 222 tensor([[218., 219., 220., 221., 222.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([223.], device='cuda:0') data.h5py: 223 tensor([[219., 220., 221., 222., 223.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([224.], device='cuda:0') data.h5py: 224 tensor([[220., 221., 222., 223., 224.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([225.], device='cuda:0') data.h5py: 225 tensor([[221., 222., 223., 224., 225.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([226.], device='cuda:0') data.h5py: 226 tensor([[222., 223., 224., 225., 226.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([227.], device='cuda:0') data.h5py: 227 tensor([[223., 224., 225., 226., 227.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([228.], device='cuda:0') data.h5py: 228 tensor([[224., 225., 226., 227., 228.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([229.], device='cuda:0') data.h5py: 229 tensor([[225., 226., 227., 228., 229.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([230.], device='cuda:0') data.h5py: 230 tensor([[226., 227., 228., 229., 230.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([231.], device='cuda:0') data.h5py: 231 tensor([[227., 228., 229., 230., 231.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([232.], device='cuda:0') data.h5py: 232 tensor([[228., 229., 230., 231., 232.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([233.], device='cuda:0') data.h5py: 233 tensor([[233., 233., 233., 233., 233.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([234.], device='cuda:0') data.h5py: 234 tensor([[233., 233., 233., 233., 234.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([235.], device='cuda:0') data.h5py: 235 tensor([[233., 233., 233., 234., 235.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([236.], device='cuda:0') data.h5py: 236 tensor([[233., 233., 234., 235., 236.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([237.], device='cuda:0') data.h5py: 237 tensor([[233., 234., 235., 236., 237.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([238.], device='cuda:0') data.h5py: 238 tensor([[234., 235., 236., 237., 238.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([239.], device='cuda:0') data.h5py: 239 tensor([[235., 236., 237., 238., 239.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([240.], device='cuda:0') data.h5py: 240 tensor([[236., 237., 238., 239., 240.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([241.], device='cuda:0') data.h5py: 241 tensor([[237., 238., 239., 240., 241.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([242.], device='cuda:0') data.h5py: 242 tensor([[238., 239., 240., 241., 242.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([243.], device='cuda:0') data.h5py: 243 tensor([[239., 240., 241., 242., 243.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([244.], device='cuda:0') data.h5py: 244 tensor([[240., 241., 242., 243., 244.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([245.], device='cuda:0') data.h5py: 245 tensor([[241., 242., 243., 244., 245.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([246.], device='cuda:0') data.h5py: 246 tensor([[242., 243., 244., 245., 246.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([247.], device='cuda:0') data.h5py: 247 tensor([[243., 244., 245., 246., 247.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([248.], device='cuda:0') data.h5py: 248 tensor([[244., 245., 246., 247., 248.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([249.], device='cuda:0') data.h5py: 249 tensor([[245., 246., 247., 248., 249.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([250.], device='cuda:0') data.h5py: 250 tensor([[246., 247., 248., 249., 250.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([251.], device='cuda:0') data.h5py: 251 tensor([[247., 248., 249., 250., 251.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([252.], device='cuda:0') data.h5py: 252 tensor([[248., 249., 250., 251., 252.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([253.], device='cuda:0') data.h5py: 253 tensor([[253., 253., 253., 253., 253.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([254.], device='cuda:0') data.h5py: 254 tensor([[253., 253., 253., 253., 254.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([255.], device='cuda:0') data.h5py: 255 tensor([[253., 253., 253., 254., 255.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([256.], device='cuda:0') data.h5py: 256 tensor([[253., 253., 254., 255., 256.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([257.], device='cuda:0') data.h5py: 257 tensor([[253., 254., 255., 256., 257.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([258.], device='cuda:0') data.h5py: 258 tensor([[254., 255., 256., 257., 258.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([259.], device='cuda:0') data.h5py: 259 tensor([[255., 256., 257., 258., 259.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([260.], device='cuda:0') data.h5py: 260 tensor([[256., 257., 258., 259., 260.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([261.], device='cuda:0') data.h5py: 261 tensor([[257., 258., 259., 260., 261.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([262.], device='cuda:0') data.h5py: 262 tensor([[258., 259., 260., 261., 262.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([263.], device='cuda:0') data.h5py: 263 tensor([[259., 260., 261., 262., 263.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([264.], device='cuda:0') data.h5py: 264 tensor([[260., 261., 262., 263., 264.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([265.], device='cuda:0') data.h5py: 265 tensor([[261., 262., 263., 264., 265.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([266.], device='cuda:0') data.h5py: 266 tensor([[262., 263., 264., 265., 266.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([267.], device='cuda:0') data.h5py: 267 tensor([[263., 264., 265., 266., 267.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([268.], device='cuda:0') data.h5py: 268 tensor([[264., 265., 266., 267., 268.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([269.], device='cuda:0') data.h5py: 269 tensor([[265., 266., 267., 268., 269.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([270.], device='cuda:0') data.h5py: 270 tensor([[266., 267., 268., 269., 270.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([271.], device='cuda:0') data.h5py: 271 tensor([[271., 271., 271., 271., 271.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([272.], device='cuda:0') data.h5py: 272 tensor([[271., 271., 271., 271., 272.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([273.], device='cuda:0') data.h5py: 273 tensor([[271., 271., 271., 272., 273.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([274.], device='cuda:0') data.h5py: 274 tensor([[271., 271., 272., 273., 274.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([275.], device='cuda:0') data.h5py: 275 tensor([[271., 272., 273., 274., 275.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([276.], device='cuda:0') data.h5py: 276 tensor([[272., 273., 274., 275., 276.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([277.], device='cuda:0') data.h5py: 277 tensor([[273., 274., 275., 276., 277.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([278.], device='cuda:0') data.h5py: 278 tensor([[274., 275., 276., 277., 278.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([279.], device='cuda:0') data.h5py: 279 tensor([[275., 276., 277., 278., 279.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([280.], device='cuda:0') data.h5py: 280 tensor([[276., 277., 278., 279., 280.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([281.], device='cuda:0') data.h5py: 281 tensor([[277., 278., 279., 280., 281.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([282.], device='cuda:0') data.h5py: 282 tensor([[278., 279., 280., 281., 282.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([283.], device='cuda:0') data.h5py: 283 tensor([[279., 280., 281., 282., 283.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([284.], device='cuda:0') data.h5py: 284 tensor([[280., 281., 282., 283., 284.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([285.], device='cuda:0') data.h5py: 285 tensor([[281., 282., 283., 284., 285.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([286.], device='cuda:0') data.h5py: 286 tensor([[282., 283., 284., 285., 286.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([287.], device='cuda:0') data.h5py: 287 tensor([[283., 284., 285., 286., 287.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([288.], device='cuda:0') data.h5py: 288 tensor([[288., 288., 288., 288., 288.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([289.], device='cuda:0') data.h5py: 289 tensor([[288., 288., 288., 288., 289.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([290.], device='cuda:0') data.h5py: 290 tensor([[288., 288., 288., 289., 290.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([291.], device='cuda:0') data.h5py: 291 tensor([[288., 288., 289., 290., 291.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([292.], device='cuda:0') data.h5py: 292 tensor([[288., 289., 290., 291., 292.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([293.], device='cuda:0') data.h5py: 293 tensor([[289., 290., 291., 292., 293.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([294.], device='cuda:0') data.h5py: 294 tensor([[290., 291., 292., 293., 294.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([295.], device='cuda:0') data.h5py: 295 tensor([[291., 292., 293., 294., 295.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([296.], device='cuda:0') data.h5py: 296 tensor([[292., 293., 294., 295., 296.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([297.], device='cuda:0') data.h5py: 297 tensor([[293., 294., 295., 296., 297.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([298.], device='cuda:0') data.h5py: 298 tensor([[298., 298., 298., 298., 298.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([299.], device='cuda:0') data.h5py: 299 tensor([[298., 298., 298., 298., 299.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([300.], device='cuda:0') data.h5py: 300 tensor([[298., 298., 298., 299., 300.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([301.], device='cuda:0') data.h5py: 301 tensor([[298., 298., 299., 300., 301.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([302.], device='cuda:0') data.h5py: 302 tensor([[298., 299., 300., 301., 302.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([303.], device='cuda:0') data.h5py: 303 tensor([[299., 300., 301., 302., 303.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([304.], device='cuda:0') data.h5py: 304 tensor([[300., 301., 302., 303., 304.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([305.], device='cuda:0') data.h5py: 305 tensor([[301., 302., 303., 304., 305.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([306.], device='cuda:0') data.h5py: 306 tensor([[302., 303., 304., 305., 306.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([307.], device='cuda:0') data.h5py: 307 tensor([[303., 304., 305., 306., 307.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([308.], device='cuda:0') data.h5py: 308 tensor([[304., 305., 306., 307., 308.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([309.], device='cuda:0') data.h5py: 309 tensor([[305., 306., 307., 308., 309.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([310.], device='cuda:0') data.h5py: 310 tensor([[310., 310., 310., 310., 310.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([311.], device='cuda:0') data.h5py: 311 tensor([[310., 310., 310., 310., 311.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([312.], device='cuda:0') data.h5py: 312 tensor([[310., 310., 310., 311., 312.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([313.], device='cuda:0') data.h5py: 313 tensor([[310., 310., 311., 312., 313.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([314.], device='cuda:0') data.h5py: 314 tensor([[310., 311., 312., 313., 314.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([315.], device='cuda:0') data.h5py: 315 tensor([[311., 312., 313., 314., 315.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([316.], device='cuda:0') data.h5py: 316 tensor([[312., 313., 314., 315., 316.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([317.], device='cuda:0') data.h5py: 317 tensor([[313., 314., 315., 316., 317.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([318.], device='cuda:0') data.h5py: 318 tensor([[314., 315., 316., 317., 318.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([319.], device='cuda:0') data.h5py: 319 tensor([[315., 316., 317., 318., 319.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([320.], device='cuda:0') data.h5py: 320 tensor([[316., 317., 318., 319., 320.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([321.], device='cuda:0') data.h5py: 321 tensor([[317., 318., 319., 320., 321.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([322.], device='cuda:0') data.h5py: 322 tensor([[318., 319., 320., 321., 322.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([323.], device='cuda:0') data.h5py: 323 tensor([[319., 320., 321., 322., 323.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([324.], device='cuda:0') data.h5py: 324 tensor([[324., 324., 324., 324., 324.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([325.], device='cuda:0') data.h5py: 325 tensor([[324., 324., 324., 324., 325.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([326.], device='cuda:0') data.h5py: 326 tensor([[324., 324., 324., 325., 326.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([327.], device='cuda:0') data.h5py: 327 tensor([[324., 324., 325., 326., 327.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([328.], device='cuda:0') data.h5py: 328 tensor([[324., 325., 326., 327., 328.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([329.], device='cuda:0') data.h5py: 329 tensor([[325., 326., 327., 328., 329.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([330.], device='cuda:0') data.h5py: 330 tensor([[326., 327., 328., 329., 330.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([331.], device='cuda:0') data.h5py: 331 tensor([[327., 328., 329., 330., 331.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([332.], device='cuda:0') data.h5py: 332 tensor([[328., 329., 330., 331., 332.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([333.], device='cuda:0') data.h5py: 333 tensor([[329., 330., 331., 332., 333.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([334.], device='cuda:0') data.h5py: 334 tensor([[334., 334., 334., 334., 334.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([335.], device='cuda:0') data.h5py: 335 tensor([[334., 334., 334., 334., 335.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([336.], device='cuda:0') data.h5py: 336 tensor([[334., 334., 334., 335., 336.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([337.], device='cuda:0') data.h5py: 337 tensor([[334., 334., 335., 336., 337.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([338.], device='cuda:0') data.h5py: 338 tensor([[334., 335., 336., 337., 338.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([339.], device='cuda:0') data.h5py: 339 tensor([[335., 336., 337., 338., 339.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([340.], device='cuda:0') data.h5py: 340 tensor([[336., 337., 338., 339., 340.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([341.], device='cuda:0') data.h5py: 341 tensor([[337., 338., 339., 340., 341.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([342.], device='cuda:0') data.h5py: 342 tensor([[338., 339., 340., 341., 342.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([343.], device='cuda:0') data.h5py: 343 tensor([[339., 340., 341., 342., 343.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([344.], device='cuda:0') data.h5py: 344 tensor([[340., 341., 342., 343., 344.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([345.], device='cuda:0') data.h5py: 345 tensor([[341., 342., 343., 344., 345.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([346.], device='cuda:0') data.h5py: 346 tensor([[342., 343., 344., 345., 346.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([347.], device='cuda:0') data.h5py: 347 tensor([[343., 344., 345., 346., 347.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([348.], device='cuda:0') data.h5py: 348 tensor([[344., 345., 346., 347., 348.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([349.], device='cuda:0') data.h5py: 349 tensor([[345., 346., 347., 348., 349.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([350.], device='cuda:0') data.h5py: 350 tensor([[346., 347., 348., 349., 350.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([351.], device='cuda:0') data.h5py: 351 tensor([[347., 348., 349., 350., 351.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([352.], device='cuda:0') data.h5py: 352 tensor([[348., 349., 350., 351., 352.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([353.], device='cuda:0') data.h5py: 353 tensor([[349., 350., 351., 352., 353.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([354.], device='cuda:0') data.h5py: 354 tensor([[354., 354., 354., 354., 354.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([355.], device='cuda:0') data.h5py: 355 tensor([[354., 354., 354., 354., 355.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([356.], device='cuda:0') data.h5py: 356 tensor([[354., 354., 354., 355., 356.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([357.], device='cuda:0') data.h5py: 357 tensor([[354., 354., 355., 356., 357.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([358.], device='cuda:0') data.h5py: 358 tensor([[354., 355., 356., 357., 358.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([359.], device='cuda:0') data.h5py: 359 tensor([[355., 356., 357., 358., 359.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([360.], device='cuda:0') data.h5py: 360 tensor([[356., 357., 358., 359., 360.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([361.], device='cuda:0') data.h5py: 361 tensor([[357., 358., 359., 360., 361.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([362.], device='cuda:0') data.h5py: 362 tensor([[358., 359., 360., 361., 362.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([363.], device='cuda:0') data.h5py: 363 tensor([[359., 360., 361., 362., 363.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([364.], device='cuda:0') data.h5py: 364 tensor([[360., 361., 362., 363., 364.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([365.], device='cuda:0') data.h5py: 365 tensor([[361., 362., 363., 364., 365.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([366.], device='cuda:0') data.h5py: 366 tensor([[362., 363., 364., 365., 366.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([367.], device='cuda:0') data.h5py: 367 tensor([[363., 364., 365., 366., 367.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([368.], device='cuda:0') data.h5py: 368 tensor([[364., 365., 366., 367., 368.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([369.], device='cuda:0') data.h5py: 369 tensor([[365., 366., 367., 368., 369.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([370.], device='cuda:0') data.h5py: 370 tensor([[370., 370., 370., 370., 370.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([371.], device='cuda:0') data.h5py: 371 tensor([[370., 370., 370., 370., 371.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([372.], device='cuda:0') data.h5py: 372 tensor([[370., 370., 370., 371., 372.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([373.], device='cuda:0') data.h5py: 373 tensor([[370., 370., 371., 372., 373.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([374.], device='cuda:0') data.h5py: 374 tensor([[370., 371., 372., 373., 374.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([375.], device='cuda:0') data.h5py: 375 tensor([[371., 372., 373., 374., 375.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([376.], device='cuda:0') data.h5py: 376 tensor([[372., 373., 374., 375., 376.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([377.], device='cuda:0') data.h5py: 377 tensor([[373., 374., 375., 376., 377.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([378.], device='cuda:0') data.h5py: 378 tensor([[374., 375., 376., 377., 378.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([379.], device='cuda:0') data.h5py: 379 tensor([[375., 376., 377., 378., 379.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([380.], device='cuda:0') data.h5py: 380 tensor([[380., 380., 380., 380., 380.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([381.], device='cuda:0') data.h5py: 381 tensor([[380., 380., 380., 380., 381.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([382.], device='cuda:0') data.h5py: 382 tensor([[380., 380., 380., 381., 382.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([383.], device='cuda:0') data.h5py: 383 tensor([[380., 380., 381., 382., 383.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([384.], device='cuda:0') data.h5py: 384 tensor([[380., 381., 382., 383., 384.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([385.], device='cuda:0') data.h5py: 385 tensor([[381., 382., 383., 384., 385.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([386.], device='cuda:0') data.h5py: 386 tensor([[382., 383., 384., 385., 386.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([387.], device='cuda:0') data.h5py: 387 tensor([[383., 384., 385., 386., 387.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([388.], device='cuda:0') data.h5py: 388 tensor([[384., 385., 386., 387., 388.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([389.], device='cuda:0') data.h5py: 389 tensor([[385., 386., 387., 388., 389.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([390.], device='cuda:0') data.h5py: 390 tensor([[386., 387., 388., 389., 390.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([391.], device='cuda:0') data.h5py: 391 tensor([[387., 388., 389., 390., 391.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([392.], device='cuda:0') data.h5py: 392 tensor([[388., 389., 390., 391., 392.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([393.], device='cuda:0') data.h5py: 393 tensor([[389., 390., 391., 392., 393.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([394.], device='cuda:0') data.h5py: 394 tensor([[390., 391., 392., 393., 394.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([395.], device='cuda:0') data.h5py: 395 tensor([[395., 395., 395., 395., 395.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([396.], device='cuda:0') data.h5py: 396 tensor([[395., 395., 395., 395., 396.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([397.], device='cuda:0') data.h5py: 397 tensor([[395., 395., 395., 396., 397.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([398.], device='cuda:0') data.h5py: 398 tensor([[395., 395., 396., 397., 398.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([399.], device='cuda:0') data.h5py: 399 tensor([[395., 396., 397., 398., 399.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([400.], device='cuda:0') data.h5py: 400 tensor([[396., 397., 398., 399., 400.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([401.], device='cuda:0') data.h5py: 401 tensor([[397., 398., 399., 400., 401.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([402.], device='cuda:0') data.h5py: 402 tensor([[398., 399., 400., 401., 402.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([403.], device='cuda:0') data.h5py: 403 tensor([[399., 400., 401., 402., 403.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([404.], device='cuda:0') data.h5py: 404 tensor([[400., 401., 402., 403., 404.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([405.], device='cuda:0') data.h5py: 405 tensor([[401., 402., 403., 404., 405.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([406.], device='cuda:0') data.h5py: 406 tensor([[402., 403., 404., 405., 406.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([407.], device='cuda:0') data.h5py: 407 tensor([[403., 404., 405., 406., 407.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([408.], device='cuda:0') data.h5py: 408 tensor([[408., 408., 408., 408., 408.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([409.], device='cuda:0') data.h5py: 409 tensor([[408., 408., 408., 408., 409.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([410.], device='cuda:0') data.h5py: 410 tensor([[408., 408., 408., 409., 410.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([411.], device='cuda:0') data.h5py: 411 tensor([[408., 408., 409., 410., 411.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([412.], device='cuda:0') data.h5py: 412 tensor([[408., 409., 410., 411., 412.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([413.], device='cuda:0') data.h5py: 413 tensor([[409., 410., 411., 412., 413.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([414.], device='cuda:0') data.h5py: 414 tensor([[410., 411., 412., 413., 414.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([415.], device='cuda:0') data.h5py: 415 tensor([[411., 412., 413., 414., 415.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([416.], device='cuda:0') data.h5py: 416 tensor([[412., 413., 414., 415., 416.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([417.], device='cuda:0') data.h5py: 417 tensor([[413., 414., 415., 416., 417.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([418.], device='cuda:0') data.h5py: 418 tensor([[414., 415., 416., 417., 418.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([419.], device='cuda:0') data.h5py: 419 tensor([[415., 416., 417., 418., 419.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([420.], device='cuda:0') data.h5py: 420 tensor([[416., 417., 418., 419., 420.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([421.], device='cuda:0') data.h5py: 421 tensor([[417., 418., 419., 420., 421.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([422.], device='cuda:0') data.h5py: 422 tensor([[418., 419., 420., 421., 422.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([423.], device='cuda:0') data.h5py: 423 tensor([[419., 420., 421., 422., 423.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([424.], device='cuda:0') data.h5py: 424 tensor([[420., 421., 422., 423., 424.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([425.], device='cuda:0') data.h5py: 425 tensor([[421., 422., 423., 424., 425.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([426.], device='cuda:0') data.h5py: 426 tensor([[422., 423., 424., 425., 426.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([427.], device='cuda:0') data.h5py: 427 tensor([[423., 424., 425., 426., 427.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([428.], device='cuda:0') data.h5py: 428 tensor([[428., 428., 428., 428., 428.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([429.], device='cuda:0') data.h5py: 429 tensor([[428., 428., 428., 428., 429.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([430.], device='cuda:0') data.h5py: 430 tensor([[428., 428., 428., 429., 430.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([431.], device='cuda:0') data.h5py: 431 tensor([[428., 428., 429., 430., 431.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([432.], device='cuda:0') data.h5py: 432 tensor([[428., 429., 430., 431., 432.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([433.], device='cuda:0') data.h5py: 433 tensor([[429., 430., 431., 432., 433.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([434.], device='cuda:0') data.h5py: 434 tensor([[430., 431., 432., 433., 434.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([435.], device='cuda:0') data.h5py: 435 tensor([[431., 432., 433., 434., 435.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([436.], device='cuda:0') data.h5py: 436 tensor([[432., 433., 434., 435., 436.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([437.], device='cuda:0') data.h5py: 437 tensor([[433., 434., 435., 436., 437.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([438.], device='cuda:0') data.h5py: 438 tensor([[434., 435., 436., 437., 438.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([439.], device='cuda:0') data.h5py: 439 tensor([[435., 436., 437., 438., 439.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([440.], device='cuda:0') data.h5py: 440 tensor([[436., 437., 438., 439., 440.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([441.], device='cuda:0') data.h5py: 441 tensor([[437., 438., 439., 440., 441.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([442.], device='cuda:0') data.h5py: 442 tensor([[438., 439., 440., 441., 442.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([443.], device='cuda:0') data.h5py: 443 tensor([[439., 440., 441., 442., 443.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([444.], device='cuda:0') data.h5py: 444 tensor([[440., 441., 442., 443., 444.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([445.], device='cuda:0') data.h5py: 445 tensor([[441., 442., 443., 444., 445.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([446.], device='cuda:0') data.h5py: 446 tensor([[442., 443., 444., 445., 446.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([447.], device='cuda:0') data.h5py: 447 tensor([[443., 444., 445., 446., 447.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([448.], device='cuda:0') data.h5py: 448 tensor([[448., 448., 448., 448., 448.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([449.], device='cuda:0') data.h5py: 449 tensor([[448., 448., 448., 448., 449.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([450.], device='cuda:0') data.h5py: 450 tensor([[448., 448., 448., 449., 450.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([451.], device='cuda:0') data.h5py: 451 tensor([[448., 448., 449., 450., 451.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([452.], device='cuda:0') data.h5py: 452 tensor([[448., 449., 450., 451., 452.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([453.], device='cuda:0') data.h5py: 453 tensor([[449., 450., 451., 452., 453.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([454.], device='cuda:0') data.h5py: 454 tensor([[450., 451., 452., 453., 454.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([455.], device='cuda:0') data.h5py: 455 tensor([[451., 452., 453., 454., 455.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([456.], device='cuda:0') data.h5py: 456 tensor([[452., 453., 454., 455., 456.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([457.], device='cuda:0') data.h5py: 457 tensor([[453., 454., 455., 456., 457.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([458.], device='cuda:0') data.h5py: 458 tensor([[454., 455., 456., 457., 458.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([459.], device='cuda:0') data.h5py: 459 tensor([[455., 456., 457., 458., 459.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([460.], device='cuda:0') data.h5py: 460 tensor([[456., 457., 458., 459., 460.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([461.], device='cuda:0') data.h5py: 461 tensor([[457., 458., 459., 460., 461.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([462.], device='cuda:0') data.h5py: 462 tensor([[458., 459., 460., 461., 462.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([463.], device='cuda:0') data.h5py: 463 tensor([[459., 460., 461., 462., 463.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([464.], device='cuda:0') data.h5py: 464 tensor([[460., 461., 462., 463., 464.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([465.], device='cuda:0') data.h5py: 465 tensor([[461., 462., 463., 464., 465.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([466.], device='cuda:0') data.h5py: 466 tensor([[462., 463., 464., 465., 466.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([467.], device='cuda:0') data.h5py: 467 tensor([[463., 464., 465., 466., 467.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([468.], device='cuda:0') data.h5py: 468 tensor([[468., 468., 468., 468., 468.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([469.], device='cuda:0') data.h5py: 469 tensor([[468., 468., 468., 468., 469.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([470.], device='cuda:0') data.h5py: 470 tensor([[468., 468., 468., 469., 470.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([471.], device='cuda:0') data.h5py: 471 tensor([[468., 468., 469., 470., 471.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([472.], device='cuda:0') data.h5py: 472 tensor([[468., 469., 470., 471., 472.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([473.], device='cuda:0') data.h5py: 473 tensor([[469., 470., 471., 472., 473.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([474.], device='cuda:0') data.h5py: 474 tensor([[470., 471., 472., 473., 474.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([475.], device='cuda:0') data.h5py: 475 tensor([[471., 472., 473., 474., 475.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([476.], device='cuda:0') data.h5py: 476 tensor([[472., 473., 474., 475., 476.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([477.], device='cuda:0') data.h5py: 477 tensor([[473., 474., 475., 476., 477.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([478.], device='cuda:0') data.h5py: 478 tensor([[474., 475., 476., 477., 478.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([479.], device='cuda:0') data.h5py: 479 tensor([[475., 476., 477., 478., 479.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([480.], device='cuda:0') data.h5py: 480 tensor([[476., 477., 478., 479., 480.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([481.], device='cuda:0') data.h5py: 481 tensor([[477., 478., 479., 480., 481.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([482.], device='cuda:0') data.h5py: 482 tensor([[478., 479., 480., 481., 482.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([483.], device='cuda:0') data.h5py: 483 tensor([[479., 480., 481., 482., 483.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([484.], device='cuda:0') data.h5py: 484 tensor([[480., 481., 482., 483., 484.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([485.], device='cuda:0') data.h5py: 485 tensor([[481., 482., 483., 484., 485.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([486.], device='cuda:0') data.h5py: 486 tensor([[486., 486., 486., 486., 486.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([487.], device='cuda:0') data.h5py: 487 tensor([[486., 486., 486., 486., 487.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([488.], device='cuda:0') data.h5py: 488 tensor([[486., 486., 486., 487., 488.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([489.], device='cuda:0') data.h5py: 489 tensor([[486., 486., 487., 488., 489.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([490.], device='cuda:0') data.h5py: 490 tensor([[486., 487., 488., 489., 490.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([491.], device='cuda:0') data.h5py: 491 tensor([[487., 488., 489., 490., 491.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([492.], device='cuda:0') data.h5py: 492 tensor([[488., 489., 490., 491., 492.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([493.], device='cuda:0') data.h5py: 493 tensor([[489., 490., 491., 492., 493.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([494.], device='cuda:0') data.h5py: 494 tensor([[490., 491., 492., 493., 494.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([495.], device='cuda:0') data.h5py: 495 tensor([[491., 492., 493., 494., 495.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([496.], device='cuda:0') data.h5py: 496 tensor([[492., 493., 494., 495., 496.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([497.], device='cuda:0') data.h5py: 497 tensor([[493., 494., 495., 496., 497.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([498.], device='cuda:0') data.h5py: 498 tensor([[494., 495., 496., 497., 498.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([499.], device='cuda:0') data.h5py: 499 tensor([[495., 496., 497., 498., 499.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([500.], device='cuda:0') data.h5py: 500 tensor([[500., 500., 500., 500., 500.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([501.], device='cuda:0') data.h5py: 501 tensor([[500., 500., 500., 500., 501.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([502.], device='cuda:0') data.h5py: 502 tensor([[500., 500., 500., 501., 502.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([503.], device='cuda:0') data.h5py: 503 tensor([[500., 500., 501., 502., 503.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([504.], device='cuda:0') data.h5py: 504 tensor([[500., 501., 502., 503., 504.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([505.], device='cuda:0') data.h5py: 505 tensor([[501., 502., 503., 504., 505.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([506.], device='cuda:0') data.h5py: 506 tensor([[502., 503., 504., 505., 506.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([507.], device='cuda:0') data.h5py: 507 tensor([[503., 504., 505., 506., 507.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([508.], device='cuda:0') data.h5py: 508 tensor([[504., 505., 506., 507., 508.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([509.], device='cuda:0') data.h5py: 509 tensor([[505., 506., 507., 508., 509.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([510.], device='cuda:0') data.h5py: 510 tensor([[510., 510., 510., 510., 510.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([511.], device='cuda:0') data.h5py: 511 tensor([[510., 510., 510., 510., 511.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([512.], device='cuda:0') data.h5py: 512 tensor([[510., 510., 510., 511., 512.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([513.], device='cuda:0') data.h5py: 513 tensor([[510., 510., 511., 512., 513.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([514.], device='cuda:0') data.h5py: 514 tensor([[510., 511., 512., 513., 514.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([515.], device='cuda:0') data.h5py: 515 tensor([[511., 512., 513., 514., 515.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([516.], device='cuda:0') data.h5py: 516 tensor([[512., 513., 514., 515., 516.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([517.], device='cuda:0') data.h5py: 517 tensor([[513., 514., 515., 516., 517.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([518.], device='cuda:0') data.h5py: 518 tensor([[514., 515., 516., 517., 518.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([519.], device='cuda:0') data.h5py: 519 tensor([[515., 516., 517., 518., 519.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([520.], device='cuda:0') data.h5py: 520 tensor([[516., 517., 518., 519., 520.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([521.], device='cuda:0') data.h5py: 521 tensor([[517., 518., 519., 520., 521.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([522.], device='cuda:0') data.h5py: 522 tensor([[518., 519., 520., 521., 522.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([523.], device='cuda:0') data.h5py: 523 tensor([[519., 520., 521., 522., 523.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([524.], device='cuda:0') data.h5py: 524 tensor([[520., 521., 522., 523., 524.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([525.], device='cuda:0') data.h5py: 525 tensor([[521., 522., 523., 524., 525.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([526.], device='cuda:0') data.h5py: 526 tensor([[522., 523., 524., 525., 526.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([527.], device='cuda:0') data.h5py: 527 tensor([[523., 524., 525., 526., 527.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([528.], device='cuda:0') data.h5py: 528 tensor([[528., 528., 528., 528., 528.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([529.], device='cuda:0') data.h5py: 529 tensor([[528., 528., 528., 528., 529.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([530.], device='cuda:0') data.h5py: 530 tensor([[528., 528., 528., 529., 530.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([531.], device='cuda:0') data.h5py: 531 tensor([[528., 528., 529., 530., 531.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([532.], device='cuda:0') data.h5py: 532 tensor([[528., 529., 530., 531., 532.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([533.], device='cuda:0') data.h5py: 533 tensor([[529., 530., 531., 532., 533.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([534.], device='cuda:0') data.h5py: 534 tensor([[530., 531., 532., 533., 534.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([535.], device='cuda:0') data.h5py: 535 tensor([[531., 532., 533., 534., 535.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([536.], device='cuda:0') data.h5py: 536 tensor([[532., 533., 534., 535., 536.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([537.], device='cuda:0') data.h5py: 537 tensor([[533., 534., 535., 536., 537.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([538.], device='cuda:0') data.h5py: 538 tensor([[534., 535., 536., 537., 538.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([539.], device='cuda:0') data.h5py: 539 tensor([[535., 536., 537., 538., 539.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([540.], device='cuda:0') data.h5py: 540 tensor([[536., 537., 538., 539., 540.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([541.], device='cuda:0') data.h5py: 541 tensor([[541., 541., 541., 541., 541.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([542.], device='cuda:0') data.h5py: 542 tensor([[541., 541., 541., 541., 542.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([543.], device='cuda:0') data.h5py: 543 tensor([[541., 541., 541., 542., 543.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([544.], device='cuda:0') data.h5py: 544 tensor([[541., 541., 542., 543., 544.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([545.], device='cuda:0') data.h5py: 545 tensor([[541., 542., 543., 544., 545.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([546.], device='cuda:0') data.h5py: 546 tensor([[542., 543., 544., 545., 546.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([547.], device='cuda:0') data.h5py: 547 tensor([[543., 544., 545., 546., 547.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([548.], device='cuda:0') data.h5py: 548 tensor([[544., 545., 546., 547., 548.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([549.], device='cuda:0') data.h5py: 549 tensor([[545., 546., 547., 548., 549.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([550.], device='cuda:0') data.h5py: 550 tensor([[546., 547., 548., 549., 550.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([551.], device='cuda:0') data.h5py: 551 tensor([[547., 548., 549., 550., 551.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([552.], device='cuda:0') data.h5py: 552 tensor([[548., 549., 550., 551., 552.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([553.], device='cuda:0') data.h5py: 553 tensor([[549., 550., 551., 552., 553.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([554.], device='cuda:0') data.h5py: 554 tensor([[550., 551., 552., 553., 554.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([555.], device='cuda:0') data.h5py: 555 tensor([[555., 555., 555., 555., 555.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([556.], device='cuda:0') data.h5py: 556 tensor([[555., 555., 555., 555., 556.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([557.], device='cuda:0') data.h5py: 557 tensor([[555., 555., 555., 556., 557.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([558.], device='cuda:0') data.h5py: 558 tensor([[555., 555., 556., 557., 558.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([559.], device='cuda:0') data.h5py: 559 tensor([[555., 556., 557., 558., 559.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([560.], device='cuda:0') data.h5py: 560 tensor([[556., 557., 558., 559., 560.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([561.], device='cuda:0') data.h5py: 561 tensor([[557., 558., 559., 560., 561.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([562.], device='cuda:0') data.h5py: 562 tensor([[558., 559., 560., 561., 562.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([563.], device='cuda:0') data.h5py: 563 tensor([[559., 560., 561., 562., 563.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([564.], device='cuda:0') data.h5py: 564 tensor([[560., 561., 562., 563., 564.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([565.], device='cuda:0') data.h5py: 565 tensor([[561., 562., 563., 564., 565.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([566.], device='cuda:0') data.h5py: 566 tensor([[562., 563., 564., 565., 566.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([567.], device='cuda:0') data.h5py: 567 tensor([[563., 564., 565., 566., 567.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([568.], device='cuda:0') data.h5py: 568 tensor([[564., 565., 566., 567., 568.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([569.], device='cuda:0') data.h5py: 569 tensor([[565., 566., 567., 568., 569.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([570.], device='cuda:0') data.h5py: 570 tensor([[566., 567., 568., 569., 570.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([571.], device='cuda:0') data.h5py: 571 tensor([[567., 568., 569., 570., 571.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([572.], device='cuda:0') data.h5py: 572 tensor([[568., 569., 570., 571., 572.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([573.], device='cuda:0') data.h5py: 573 tensor([[573., 573., 573., 573., 573.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([574.], device='cuda:0') data.h5py: 574 tensor([[573., 573., 573., 573., 574.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([575.], device='cuda:0') data.h5py: 575 tensor([[573., 573., 573., 574., 575.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([576.], device='cuda:0') data.h5py: 576 tensor([[573., 573., 574., 575., 576.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([577.], device='cuda:0') data.h5py: 577 tensor([[573., 574., 575., 576., 577.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([578.], device='cuda:0') data.h5py: 578 tensor([[574., 575., 576., 577., 578.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([579.], device='cuda:0') data.h5py: 579 tensor([[575., 576., 577., 578., 579.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([580.], device='cuda:0') data.h5py: 580 tensor([[576., 577., 578., 579., 580.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([581.], device='cuda:0') data.h5py: 581 tensor([[577., 578., 579., 580., 581.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([582.], device='cuda:0') data.h5py: 582 tensor([[578., 579., 580., 581., 582.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([583.], device='cuda:0') data.h5py: 583 tensor([[579., 580., 581., 582., 583.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([584.], device='cuda:0') data.h5py: 584 tensor([[580., 581., 582., 583., 584.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([585.], device='cuda:0') data.h5py: 585 tensor([[581., 582., 583., 584., 585.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([586.], device='cuda:0') data.h5py: 586 tensor([[582., 583., 584., 585., 586.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([587.], device='cuda:0') data.h5py: 587 tensor([[583., 584., 585., 586., 587.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([588.], device='cuda:0') data.h5py: 588 tensor([[584., 585., 586., 587., 588.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([589.], device='cuda:0') data.h5py: 589 tensor([[589., 589., 589., 589., 589.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([590.], device='cuda:0') data.h5py: 590 tensor([[589., 589., 589., 589., 590.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([591.], device='cuda:0') data.h5py: 591 tensor([[589., 589., 589., 590., 591.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([592.], device='cuda:0') data.h5py: 592 tensor([[589., 589., 590., 591., 592.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([593.], device='cuda:0') data.h5py: 593 tensor([[589., 590., 591., 592., 593.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([594.], device='cuda:0') data.h5py: 594 tensor([[590., 591., 592., 593., 594.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([595.], device='cuda:0') data.h5py: 595 tensor([[591., 592., 593., 594., 595.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([596.], device='cuda:0') data.h5py: 596 tensor([[592., 593., 594., 595., 596.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([597.], device='cuda:0') data.h5py: 597 tensor([[593., 594., 595., 596., 597.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([598.], device='cuda:0') data.h5py: 598 tensor([[594., 595., 596., 597., 598.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([599.], device='cuda:0') data.h5py: 599 tensor([[595., 596., 597., 598., 599.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([600.], device='cuda:0') data.h5py: 600 tensor([[596., 597., 598., 599., 600.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([601.], device='cuda:0') data.h5py: 601 tensor([[601., 601., 601., 601., 601.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([602.], device='cuda:0') data.h5py: 602 tensor([[601., 601., 601., 601., 602.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([603.], device='cuda:0') data.h5py: 603 tensor([[601., 601., 601., 602., 603.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([604.], device='cuda:0') data.h5py: 604 tensor([[601., 601., 602., 603., 604.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([605.], device='cuda:0') data.h5py: 605 tensor([[601., 602., 603., 604., 605.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([606.], device='cuda:0') data.h5py: 606 tensor([[602., 603., 604., 605., 606.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([607.], device='cuda:0') data.h5py: 607 tensor([[603., 604., 605., 606., 607.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([608.], device='cuda:0') data.h5py: 608 tensor([[604., 605., 606., 607., 608.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([609.], device='cuda:0') data.h5py: 609 tensor([[605., 606., 607., 608., 609.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([610.], device='cuda:0') data.h5py: 610 tensor([[606., 607., 608., 609., 610.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([611.], device='cuda:0') data.h5py: 611 tensor([[607., 608., 609., 610., 611.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([612.], device='cuda:0') data.h5py: 612 tensor([[608., 609., 610., 611., 612.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([613.], device='cuda:0') data.h5py: 613 tensor([[609., 610., 611., 612., 613.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([614.], device='cuda:0') data.h5py: 614 tensor([[610., 611., 612., 613., 614.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([615.], device='cuda:0') data.h5py: 615 tensor([[615., 615., 615., 615., 615.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([616.], device='cuda:0') data.h5py: 616 tensor([[615., 615., 615., 615., 616.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([617.], device='cuda:0') data.h5py: 617 tensor([[615., 615., 615., 616., 617.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([618.], device='cuda:0') data.h5py: 618 tensor([[615., 615., 616., 617., 618.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([619.], device='cuda:0') data.h5py: 619 tensor([[615., 616., 617., 618., 619.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([620.], device='cuda:0') data.h5py: 620 tensor([[616., 617., 618., 619., 620.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([621.], device='cuda:0') data.h5py: 621 tensor([[617., 618., 619., 620., 621.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([622.], device='cuda:0') data.h5py: 622 tensor([[618., 619., 620., 621., 622.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([623.], device='cuda:0') data.h5py: 623 tensor([[619., 620., 621., 622., 623.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([624.], device='cuda:0') data.h5py: 624 tensor([[620., 621., 622., 623., 624.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([625.], device='cuda:0') data.h5py: 625 tensor([[621., 622., 623., 624., 625.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([626.], device='cuda:0') data.h5py: 626 tensor([[622., 623., 624., 625., 626.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([627.], device='cuda:0') data.h5py: 627 tensor([[623., 624., 625., 626., 627.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([628.], device='cuda:0') data.h5py: 628 tensor([[624., 625., 626., 627., 628.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([629.], device='cuda:0') data.h5py: 629 tensor([[625., 626., 627., 628., 629.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([630.], device='cuda:0') data.h5py: 630 tensor([[626., 627., 628., 629., 630.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([631.], device='cuda:0') data.h5py: 631 tensor([[631., 631., 631., 631., 631.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([632.], device='cuda:0') data.h5py: 632 tensor([[631., 631., 631., 631., 632.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([633.], device='cuda:0') data.h5py: 633 tensor([[631., 631., 631., 632., 633.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([634.], device='cuda:0') data.h5py: 634 tensor([[631., 631., 632., 633., 634.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([635.], device='cuda:0') data.h5py: 635 tensor([[631., 632., 633., 634., 635.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([636.], device='cuda:0') data.h5py: 636 tensor([[632., 633., 634., 635., 636.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([637.], device='cuda:0') data.h5py: 637 tensor([[633., 634., 635., 636., 637.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([638.], device='cuda:0') data.h5py: 638 tensor([[634., 635., 636., 637., 638.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([639.], device='cuda:0') data.h5py: 639 tensor([[635., 636., 637., 638., 639.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([640.], device='cuda:0') data.h5py: 640 tensor([[636., 637., 638., 639., 640.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([641.], device='cuda:0') data.h5py: 641 tensor([[637., 638., 639., 640., 641.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([642.], device='cuda:0') data.h5py: 642 tensor([[638., 639., 640., 641., 642.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([643.], device='cuda:0') data.h5py: 643 tensor([[639., 640., 641., 642., 643.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([644.], device='cuda:0') data.h5py: 644 tensor([[640., 641., 642., 643., 644.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([645.], device='cuda:0') data.h5py: 645 tensor([[641., 642., 643., 644., 645.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([646.], device='cuda:0') data.h5py: 646 tensor([[642., 643., 644., 645., 646.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([647.], device='cuda:0') data.h5py: 647 tensor([[643., 644., 645., 646., 647.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([648.], device='cuda:0') data.h5py: 648 tensor([[644., 645., 646., 647., 648.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([649.], device='cuda:0') data.h5py: 649 tensor([[645., 646., 647., 648., 649.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([650.], device='cuda:0') data.h5py: 650 tensor([[646., 647., 648., 649., 650.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([651.], device='cuda:0') data.h5py: 651 tensor([[651., 651., 651., 651., 651.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([652.], device='cuda:0') data.h5py: 652 tensor([[651., 651., 651., 651., 652.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([653.], device='cuda:0') data.h5py: 653 tensor([[651., 651., 651., 652., 653.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([654.], device='cuda:0') data.h5py: 654 tensor([[651., 651., 652., 653., 654.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([655.], device='cuda:0') data.h5py: 655 tensor([[651., 652., 653., 654., 655.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([656.], device='cuda:0') data.h5py: 656 tensor([[652., 653., 654., 655., 656.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([657.], device='cuda:0') data.h5py: 657 tensor([[653., 654., 655., 656., 657.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([658.], device='cuda:0') data.h5py: 658 tensor([[654., 655., 656., 657., 658.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([659.], device='cuda:0') data.h5py: 659 tensor([[655., 656., 657., 658., 659.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([660.], device='cuda:0') data.h5py: 660 tensor([[656., 657., 658., 659., 660.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([661.], device='cuda:0') data.h5py: 661 tensor([[657., 658., 659., 660., 661.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([662.], device='cuda:0') data.h5py: 662 tensor([[658., 659., 660., 661., 662.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([663.], device='cuda:0') data.h5py: 663 tensor([[659., 660., 661., 662., 663.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([664.], device='cuda:0') data.h5py: 664 tensor([[660., 661., 662., 663., 664.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([665.], device='cuda:0') data.h5py: 665 tensor([[661., 662., 663., 664., 665.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([666.], device='cuda:0') data.h5py: 666 tensor([[662., 663., 664., 665., 666.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([667.], device='cuda:0') data.h5py: 667 tensor([[663., 664., 665., 666., 667.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([668.], device='cuda:0') data.h5py: 668 tensor([[664., 665., 666., 667., 668.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([669.], device='cuda:0') data.h5py: 669 tensor([[669., 669., 669., 669., 669.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([670.], device='cuda:0') data.h5py: 670 tensor([[669., 669., 669., 669., 670.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([671.], device='cuda:0') data.h5py: 671 tensor([[669., 669., 669., 670., 671.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([672.], device='cuda:0') data.h5py: 672 tensor([[669., 669., 670., 671., 672.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([673.], device='cuda:0') data.h5py: 673 tensor([[669., 670., 671., 672., 673.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([674.], device='cuda:0') data.h5py: 674 tensor([[670., 671., 672., 673., 674.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([675.], device='cuda:0') data.h5py: 675 tensor([[671., 672., 673., 674., 675.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([676.], device='cuda:0') data.h5py: 676 tensor([[672., 673., 674., 675., 676.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([677.], device='cuda:0') data.h5py: 677 tensor([[673., 674., 675., 676., 677.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([678.], device='cuda:0') data.h5py: 678 tensor([[674., 675., 676., 677., 678.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([679.], device='cuda:0') data.h5py: 679 tensor([[675., 676., 677., 678., 679.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([680.], device='cuda:0') data.h5py: 680 tensor([[676., 677., 678., 679., 680.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([681.], device='cuda:0') data.h5py: 681 tensor([[677., 678., 679., 680., 681.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([682.], device='cuda:0') data.h5py: 682 tensor([[678., 679., 680., 681., 682.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([683.], device='cuda:0') data.h5py: 683 tensor([[679., 680., 681., 682., 683.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([684.], device='cuda:0') data.h5py: 684 tensor([[680., 681., 682., 683., 684.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([685.], device='cuda:0') data.h5py: 685 tensor([[685., 685., 685., 685., 685.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([686.], device='cuda:0') data.h5py: 686 tensor([[685., 685., 685., 685., 686.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([687.], device='cuda:0') data.h5py: 687 tensor([[685., 685., 685., 686., 687.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([688.], device='cuda:0') data.h5py: 688 tensor([[685., 685., 686., 687., 688.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([689.], device='cuda:0') data.h5py: 689 tensor([[685., 686., 687., 688., 689.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([690.], device='cuda:0') data.h5py: 690 tensor([[686., 687., 688., 689., 690.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([691.], device='cuda:0') data.h5py: 691 tensor([[687., 688., 689., 690., 691.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([692.], device='cuda:0') data.h5py: 692 tensor([[688., 689., 690., 691., 692.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([693.], device='cuda:0') data.h5py: 693 tensor([[689., 690., 691., 692., 693.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([694.], device='cuda:0') data.h5py: 694 tensor([[690., 691., 692., 693., 694.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([695.], device='cuda:0') data.h5py: 695 tensor([[691., 692., 693., 694., 695.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([696.], device='cuda:0') data.h5py: 696 tensor([[692., 693., 694., 695., 696.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([697.], device='cuda:0') data.h5py: 697 tensor([[693., 694., 695., 696., 697.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([698.], device='cuda:0') data.h5py: 698 tensor([[694., 695., 696., 697., 698.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([699.], device='cuda:0') data.h5py: 699 tensor([[695., 696., 697., 698., 699.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([700.], device='cuda:0') data.h5py: 700 tensor([[696., 697., 698., 699., 700.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([701.], device='cuda:0') data.h5py: 701 tensor([[697., 698., 699., 700., 701.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([702.], device='cuda:0') data.h5py: 702 tensor([[698., 699., 700., 701., 702.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([703.], device='cuda:0') data.h5py: 703 tensor([[699., 700., 701., 702., 703.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([704.], device='cuda:0') data.h5py: 704 tensor([[704., 704., 704., 704., 704.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([705.], device='cuda:0') data.h5py: 705 tensor([[704., 704., 704., 704., 705.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([706.], device='cuda:0') data.h5py: 706 tensor([[704., 704., 704., 705., 706.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([707.], device='cuda:0') data.h5py: 707 tensor([[704., 704., 705., 706., 707.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([708.], device='cuda:0') data.h5py: 708 tensor([[704., 705., 706., 707., 708.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([709.], device='cuda:0') data.h5py: 709 tensor([[705., 706., 707., 708., 709.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([710.], device='cuda:0') data.h5py: 710 tensor([[706., 707., 708., 709., 710.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([711.], device='cuda:0') data.h5py: 711 tensor([[707., 708., 709., 710., 711.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([712.], device='cuda:0') data.h5py: 712 tensor([[708., 709., 710., 711., 712.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([713.], device='cuda:0') data.h5py: 713 tensor([[709., 710., 711., 712., 713.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([714.], device='cuda:0') data.h5py: 714 tensor([[710., 711., 712., 713., 714.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([715.], device='cuda:0') data.h5py: 715 tensor([[711., 712., 713., 714., 715.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([716.], device='cuda:0') data.h5py: 716 tensor([[712., 713., 714., 715., 716.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([717.], device='cuda:0') data.h5py: 717 tensor([[713., 714., 715., 716., 717.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([718.], device='cuda:0') data.h5py: 718 tensor([[714., 715., 716., 717., 718.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([719.], device='cuda:0') data.h5py: 719 tensor([[715., 716., 717., 718., 719.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([720.], device='cuda:0') data.h5py: 720 tensor([[716., 717., 718., 719., 720.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([721.], device='cuda:0') data.h5py: 721 tensor([[721., 721., 721., 721., 721.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([722.], device='cuda:0') data.h5py: 722 tensor([[721., 721., 721., 721., 722.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([723.], device='cuda:0') data.h5py: 723 tensor([[721., 721., 721., 722., 723.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([724.], device='cuda:0') data.h5py: 724 tensor([[721., 721., 722., 723., 724.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([725.], device='cuda:0') data.h5py: 725 tensor([[721., 722., 723., 724., 725.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([726.], device='cuda:0') data.h5py: 726 tensor([[722., 723., 724., 725., 726.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([727.], device='cuda:0') data.h5py: 727 tensor([[723., 724., 725., 726., 727.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([728.], device='cuda:0') data.h5py: 728 tensor([[724., 725., 726., 727., 728.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([729.], device='cuda:0') data.h5py: 729 tensor([[725., 726., 727., 728., 729.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([730.], device='cuda:0') data.h5py: 730 tensor([[726., 727., 728., 729., 730.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([731.], device='cuda:0') data.h5py: 731 tensor([[727., 728., 729., 730., 731.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([732.], device='cuda:0') data.h5py: 732 tensor([[728., 729., 730., 731., 732.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([733.], device='cuda:0') data.h5py: 733 tensor([[729., 730., 731., 732., 733.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([734.], device='cuda:0') data.h5py: 734 tensor([[730., 731., 732., 733., 734.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([735.], device='cuda:0') data.h5py: 735 tensor([[735., 735., 735., 735., 735.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([736.], device='cuda:0') data.h5py: 736 tensor([[735., 735., 735., 735., 736.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([737.], device='cuda:0') data.h5py: 737 tensor([[735., 735., 735., 736., 737.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([738.], device='cuda:0') data.h5py: 738 tensor([[735., 735., 736., 737., 738.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([739.], device='cuda:0') data.h5py: 739 tensor([[735., 736., 737., 738., 739.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([740.], device='cuda:0') data.h5py: 740 tensor([[736., 737., 738., 739., 740.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([741.], device='cuda:0') data.h5py: 741 tensor([[737., 738., 739., 740., 741.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([742.], device='cuda:0') data.h5py: 742 tensor([[738., 739., 740., 741., 742.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([743.], device='cuda:0') data.h5py: 743 tensor([[739., 740., 741., 742., 743.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([744.], device='cuda:0') data.h5py: 744 tensor([[740., 741., 742., 743., 744.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([745.], device='cuda:0') data.h5py: 745 tensor([[741., 742., 743., 744., 745.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([746.], device='cuda:0') data.h5py: 746 tensor([[742., 743., 744., 745., 746.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([747.], device='cuda:0') data.h5py: 747 tensor([[743., 744., 745., 746., 747.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([748.], device='cuda:0') data.h5py: 748 tensor([[744., 745., 746., 747., 748.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([749.], device='cuda:0') data.h5py: 749 tensor([[745., 746., 747., 748., 749.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([750.], device='cuda:0') data.h5py: 750 tensor([[746., 747., 748., 749., 750.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([751.], device='cuda:0') data.h5py: 751 tensor([[747., 748., 749., 750., 751.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([752.], device='cuda:0') data.h5py: 752 tensor([[748., 749., 750., 751., 752.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([753.], device='cuda:0') data.h5py: 753 tensor([[749., 750., 751., 752., 753.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([754.], device='cuda:0') data.h5py: 754 tensor([[750., 751., 752., 753., 754.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([755.], device='cuda:0') data.h5py: 755 tensor([[755., 755., 755., 755., 755.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([756.], device='cuda:0') data.h5py: 756 tensor([[755., 755., 755., 755., 756.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([757.], device='cuda:0') data.h5py: 757 tensor([[755., 755., 755., 756., 757.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([758.], device='cuda:0') data.h5py: 758 tensor([[755., 755., 756., 757., 758.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([759.], device='cuda:0') data.h5py: 759 tensor([[755., 756., 757., 758., 759.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([760.], device='cuda:0') data.h5py: 760 tensor([[756., 757., 758., 759., 760.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([761.], device='cuda:0') data.h5py: 761 tensor([[757., 758., 759., 760., 761.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([762.], device='cuda:0') data.h5py: 762 tensor([[758., 759., 760., 761., 762.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([763.], device='cuda:0') data.h5py: 763 tensor([[759., 760., 761., 762., 763.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([764.], device='cuda:0') data.h5py: 764 tensor([[760., 761., 762., 763., 764.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([765.], device='cuda:0') data.h5py: 765 tensor([[765., 765., 765., 765., 765.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([766.], device='cuda:0') data.h5py: 766 tensor([[765., 765., 765., 765., 766.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([767.], device='cuda:0') data.h5py: 767 tensor([[765., 765., 765., 766., 767.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([768.], device='cuda:0') data.h5py: 768 tensor([[765., 765., 766., 767., 768.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([769.], device='cuda:0') data.h5py: 769 tensor([[765., 766., 767., 768., 769.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([770.], device='cuda:0') data.h5py: 770 tensor([[766., 767., 768., 769., 770.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([771.], device='cuda:0') data.h5py: 771 tensor([[767., 768., 769., 770., 771.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([772.], device='cuda:0') data.h5py: 772 tensor([[768., 769., 770., 771., 772.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([773.], device='cuda:0') data.h5py: 773 tensor([[769., 770., 771., 772., 773.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([774.], device='cuda:0') data.h5py: 774 tensor([[770., 771., 772., 773., 774.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([775.], device='cuda:0') data.h5py: 775 tensor([[771., 772., 773., 774., 775.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([776.], device='cuda:0') data.h5py: 776 tensor([[772., 773., 774., 775., 776.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([777.], device='cuda:0') data.h5py: 777 tensor([[773., 774., 775., 776., 777.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([778.], device='cuda:0') data.h5py: 778 tensor([[774., 775., 776., 777., 778.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([779.], device='cuda:0') data.h5py: 779 tensor([[779., 779., 779., 779., 779.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([780.], device='cuda:0') data.h5py: 780 tensor([[779., 779., 779., 779., 780.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([781.], device='cuda:0') data.h5py: 781 tensor([[779., 779., 779., 780., 781.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([782.], device='cuda:0') data.h5py: 782 tensor([[779., 779., 780., 781., 782.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([783.], device='cuda:0') data.h5py: 783 tensor([[779., 780., 781., 782., 783.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([784.], device='cuda:0') data.h5py: 784 tensor([[780., 781., 782., 783., 784.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([785.], device='cuda:0') data.h5py: 785 tensor([[781., 782., 783., 784., 785.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([786.], device='cuda:0') data.h5py: 786 tensor([[782., 783., 784., 785., 786.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([787.], device='cuda:0') data.h5py: 787 tensor([[783., 784., 785., 786., 787.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([788.], device='cuda:0') data.h5py: 788 tensor([[784., 785., 786., 787., 788.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([789.], device='cuda:0') data.h5py: 789 tensor([[785., 786., 787., 788., 789.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([790.], device='cuda:0') data.h5py: 790 tensor([[786., 787., 788., 789., 790.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([791.], device='cuda:0') data.h5py: 791 tensor([[787., 788., 789., 790., 791.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([792.], device='cuda:0') data.h5py: 792 tensor([[788., 789., 790., 791., 792.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([793.], device='cuda:0') data.h5py: 793 tensor([[789., 790., 791., 792., 793.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([794.], device='cuda:0') data.h5py: 794 tensor([[790., 791., 792., 793., 794.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([795.], device='cuda:0') data.h5py: 795 tensor([[791., 792., 793., 794., 795.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([796.], device='cuda:0') data.h5py: 796 tensor([[792., 793., 794., 795., 796.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([797.], device='cuda:0') data.h5py: 797 tensor([[793., 794., 795., 796., 797.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([798.], device='cuda:0') data.h5py: 798 tensor([[798., 798., 798., 798., 798.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([799.], device='cuda:0') data.h5py: 799 tensor([[798., 798., 798., 798., 799.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([800.], device='cuda:0') data.h5py: 800 tensor([[798., 798., 798., 799., 800.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([801.], device='cuda:0') data.h5py: 801 tensor([[798., 798., 799., 800., 801.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([802.], device='cuda:0') data.h5py: 802 tensor([[798., 799., 800., 801., 802.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([803.], device='cuda:0') data.h5py: 803 tensor([[799., 800., 801., 802., 803.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([804.], device='cuda:0') data.h5py: 804 tensor([[800., 801., 802., 803., 804.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([805.], device='cuda:0') data.h5py: 805 tensor([[801., 802., 803., 804., 805.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([806.], device='cuda:0') data.h5py: 806 tensor([[802., 803., 804., 805., 806.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([807.], device='cuda:0') data.h5py: 807 tensor([[803., 804., 805., 806., 807.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([808.], device='cuda:0') data.h5py: 808 tensor([[804., 805., 806., 807., 808.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([809.], device='cuda:0') data.h5py: 809 tensor([[805., 806., 807., 808., 809.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([810.], device='cuda:0') data.h5py: 810 tensor([[806., 807., 808., 809., 810.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([811.], device='cuda:0') data.h5py: 811 tensor([[807., 808., 809., 810., 811.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([812.], device='cuda:0') data.h5py: 812 tensor([[808., 809., 810., 811., 812.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([813.], device='cuda:0') data.h5py: 813 tensor([[813., 813., 813., 813., 813.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([814.], device='cuda:0') data.h5py: 814 tensor([[813., 813., 813., 813., 814.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([815.], device='cuda:0') data.h5py: 815 tensor([[813., 813., 813., 814., 815.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([816.], device='cuda:0') data.h5py: 816 tensor([[813., 813., 814., 815., 816.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([817.], device='cuda:0') data.h5py: 817 tensor([[813., 814., 815., 816., 817.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([818.], device='cuda:0') data.h5py: 818 tensor([[814., 815., 816., 817., 818.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([819.], device='cuda:0') data.h5py: 819 tensor([[815., 816., 817., 818., 819.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([820.], device='cuda:0') data.h5py: 820 tensor([[816., 817., 818., 819., 820.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([821.], device='cuda:0') data.h5py: 821 tensor([[817., 818., 819., 820., 821.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([822.], device='cuda:0') data.h5py: 822 tensor([[818., 819., 820., 821., 822.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([823.], device='cuda:0') data.h5py: 823 tensor([[819., 820., 821., 822., 823.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([824.], device='cuda:0') data.h5py: 824 tensor([[820., 821., 822., 823., 824.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([825.], device='cuda:0') data.h5py: 825 tensor([[821., 822., 823., 824., 825.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([826.], device='cuda:0') data.h5py: 826 tensor([[822., 823., 824., 825., 826.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([827.], device='cuda:0') data.h5py: 827 tensor([[823., 824., 825., 826., 827.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([828.], device='cuda:0') data.h5py: 828 tensor([[824., 825., 826., 827., 828.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([829.], device='cuda:0') data.h5py: 829 tensor([[825., 826., 827., 828., 829.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([830.], device='cuda:0') data.h5py: 830 tensor([[826., 827., 828., 829., 830.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([831.], device='cuda:0') data.h5py: 831 tensor([[831., 831., 831., 831., 831.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([832.], device='cuda:0') data.h5py: 832 tensor([[831., 831., 831., 831., 832.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([833.], device='cuda:0') data.h5py: 833 tensor([[831., 831., 831., 832., 833.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([834.], device='cuda:0') data.h5py: 834 tensor([[831., 831., 832., 833., 834.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([835.], device='cuda:0') data.h5py: 835 tensor([[831., 832., 833., 834., 835.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([836.], device='cuda:0') data.h5py: 836 tensor([[832., 833., 834., 835., 836.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([837.], device='cuda:0') data.h5py: 837 tensor([[833., 834., 835., 836., 837.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([838.], device='cuda:0') data.h5py: 838 tensor([[834., 835., 836., 837., 838.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([839.], device='cuda:0') data.h5py: 839 tensor([[835., 836., 837., 838., 839.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([840.], device='cuda:0') data.h5py: 840 tensor([[836., 837., 838., 839., 840.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([841.], device='cuda:0') data.h5py: 841 tensor([[837., 838., 839., 840., 841.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([842.], device='cuda:0') data.h5py: 842 tensor([[842., 842., 842., 842., 842.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([843.], device='cuda:0') data.h5py: 843 tensor([[842., 842., 842., 842., 843.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([844.], device='cuda:0') data.h5py: 844 tensor([[842., 842., 842., 843., 844.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([845.], device='cuda:0') data.h5py: 845 tensor([[842., 842., 843., 844., 845.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([846.], device='cuda:0') data.h5py: 846 tensor([[842., 843., 844., 845., 846.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([847.], device='cuda:0') data.h5py: 847 tensor([[843., 844., 845., 846., 847.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([848.], device='cuda:0') data.h5py: 848 tensor([[844., 845., 846., 847., 848.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([849.], device='cuda:0') data.h5py: 849 tensor([[845., 846., 847., 848., 849.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([850.], device='cuda:0') data.h5py: 850 tensor([[846., 847., 848., 849., 850.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([851.], device='cuda:0') data.h5py: 851 tensor([[847., 848., 849., 850., 851.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([852.], device='cuda:0') data.h5py: 852 tensor([[848., 849., 850., 851., 852.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([853.], device='cuda:0') data.h5py: 853 tensor([[849., 850., 851., 852., 853.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([854.], device='cuda:0') data.h5py: 854 tensor([[850., 851., 852., 853., 854.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([855.], device='cuda:0') data.h5py: 855 tensor([[851., 852., 853., 854., 855.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([856.], device='cuda:0') data.h5py: 856 tensor([[852., 853., 854., 855., 856.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([857.], device='cuda:0') data.h5py: 857 tensor([[853., 854., 855., 856., 857.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([858.], device='cuda:0') data.h5py: 858 tensor([[858., 858., 858., 858., 858.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([859.], device='cuda:0') data.h5py: 859 tensor([[858., 858., 858., 858., 859.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([860.], device='cuda:0') data.h5py: 860 tensor([[858., 858., 858., 859., 860.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([861.], device='cuda:0') data.h5py: 861 tensor([[858., 858., 859., 860., 861.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([862.], device='cuda:0') data.h5py: 862 tensor([[858., 859., 860., 861., 862.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([863.], device='cuda:0') data.h5py: 863 tensor([[859., 860., 861., 862., 863.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([864.], device='cuda:0') data.h5py: 864 tensor([[860., 861., 862., 863., 864.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([865.], device='cuda:0') data.h5py: 865 tensor([[861., 862., 863., 864., 865.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([866.], device='cuda:0') data.h5py: 866 tensor([[862., 863., 864., 865., 866.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([867.], device='cuda:0') data.h5py: 867 tensor([[863., 864., 865., 866., 867.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([868.], device='cuda:0') data.h5py: 868 tensor([[864., 865., 866., 867., 868.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([869.], device='cuda:0') data.h5py: 869 tensor([[865., 866., 867., 868., 869.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([870.], device='cuda:0') data.h5py: 870 tensor([[866., 867., 868., 869., 870.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([871.], device='cuda:0') data.h5py: 871 tensor([[867., 868., 869., 870., 871.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([872.], device='cuda:0') data.h5py: 872 tensor([[868., 869., 870., 871., 872.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([873.], device='cuda:0') data.h5py: 873 tensor([[869., 870., 871., 872., 873.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([874.], device='cuda:0') data.h5py: 874 tensor([[870., 871., 872., 873., 874.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([875.], device='cuda:0') data.h5py: 875 tensor([[871., 872., 873., 874., 875.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([876.], device='cuda:0') data.h5py: 876 tensor([[872., 873., 874., 875., 876.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([877.], device='cuda:0') data.h5py: 877 tensor([[877., 877., 877., 877., 877.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([878.], device='cuda:0') data.h5py: 878 tensor([[877., 877., 877., 877., 878.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([879.], device='cuda:0') data.h5py: 879 tensor([[877., 877., 877., 878., 879.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([880.], device='cuda:0') data.h5py: 880 tensor([[877., 877., 878., 879., 880.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([881.], device='cuda:0') data.h5py: 881 tensor([[877., 878., 879., 880., 881.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([882.], device='cuda:0') data.h5py: 882 tensor([[878., 879., 880., 881., 882.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([883.], device='cuda:0') data.h5py: 883 tensor([[879., 880., 881., 882., 883.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([884.], device='cuda:0') data.h5py: 884 tensor([[880., 881., 882., 883., 884.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([885.], device='cuda:0') data.h5py: 885 tensor([[881., 882., 883., 884., 885.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([886.], device='cuda:0') data.h5py: 886 tensor([[882., 883., 884., 885., 886.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([887.], device='cuda:0') data.h5py: 887 tensor([[883., 884., 885., 886., 887.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([888.], device='cuda:0') data.h5py: 888 tensor([[884., 885., 886., 887., 888.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([889.], device='cuda:0') data.h5py: 889 tensor([[885., 886., 887., 888., 889.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([890.], device='cuda:0') data.h5py: 890 tensor([[890., 890., 890., 890., 890.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([891.], device='cuda:0') data.h5py: 891 tensor([[890., 890., 890., 890., 891.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([892.], device='cuda:0') data.h5py: 892 tensor([[890., 890., 890., 891., 892.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([893.], device='cuda:0') data.h5py: 893 tensor([[890., 890., 891., 892., 893.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([894.], device='cuda:0') data.h5py: 894 tensor([[890., 891., 892., 893., 894.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([895.], device='cuda:0') data.h5py: 895 tensor([[891., 892., 893., 894., 895.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([896.], device='cuda:0') data.h5py: 896 tensor([[892., 893., 894., 895., 896.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([897.], device='cuda:0') data.h5py: 897 tensor([[893., 894., 895., 896., 897.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([898.], device='cuda:0') data.h5py: 898 tensor([[894., 895., 896., 897., 898.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([899.], device='cuda:0') data.h5py: 899 tensor([[895., 896., 897., 898., 899.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([900.], device='cuda:0') data.h5py: 900 tensor([[896., 897., 898., 899., 900.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([901.], device='cuda:0') data.h5py: 901 tensor([[897., 898., 899., 900., 901.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([902.], device='cuda:0') data.h5py: 902 tensor([[898., 899., 900., 901., 902.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([903.], device='cuda:0') data.h5py: 903 tensor([[899., 900., 901., 902., 903.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([904.], device='cuda:0') data.h5py: 904 tensor([[900., 901., 902., 903., 904.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([905.], device='cuda:0') data.h5py: 905 tensor([[901., 902., 903., 904., 905.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([906.], device='cuda:0') data.h5py: 906 tensor([[902., 903., 904., 905., 906.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([907.], device='cuda:0') data.h5py: 907 tensor([[907., 907., 907., 907., 907.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([908.], device='cuda:0') data.h5py: 908 tensor([[907., 907., 907., 907., 908.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([909.], device='cuda:0') data.h5py: 909 tensor([[907., 907., 907., 908., 909.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([910.], device='cuda:0') data.h5py: 910 tensor([[907., 907., 908., 909., 910.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([911.], device='cuda:0') data.h5py: 911 tensor([[907., 908., 909., 910., 911.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([912.], device='cuda:0') data.h5py: 912 tensor([[908., 909., 910., 911., 912.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([913.], device='cuda:0') data.h5py: 913 tensor([[909., 910., 911., 912., 913.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([914.], device='cuda:0') data.h5py: 914 tensor([[910., 911., 912., 913., 914.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([915.], device='cuda:0') data.h5py: 915 tensor([[911., 912., 913., 914., 915.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([916.], device='cuda:0') data.h5py: 916 tensor([[912., 913., 914., 915., 916.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([917.], device='cuda:0') data.h5py: 917 tensor([[913., 914., 915., 916., 917.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([918.], device='cuda:0') data.h5py: 918 tensor([[914., 915., 916., 917., 918.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([919.], device='cuda:0') data.h5py: 919 tensor([[915., 916., 917., 918., 919.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([920.], device='cuda:0') data.h5py: 920 tensor([[920., 920., 920., 920., 920.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([921.], device='cuda:0') data.h5py: 921 tensor([[920., 920., 920., 920., 921.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([922.], device='cuda:0') data.h5py: 922 tensor([[920., 920., 920., 921., 922.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([923.], device='cuda:0') data.h5py: 923 tensor([[920., 920., 921., 922., 923.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([924.], device='cuda:0') data.h5py: 924 tensor([[920., 921., 922., 923., 924.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([925.], device='cuda:0') data.h5py: 925 tensor([[921., 922., 923., 924., 925.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([926.], device='cuda:0') data.h5py: 926 tensor([[922., 923., 924., 925., 926.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([927.], device='cuda:0') data.h5py: 927 tensor([[923., 924., 925., 926., 927.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([928.], device='cuda:0') data.h5py: 928 tensor([[924., 925., 926., 927., 928.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([929.], device='cuda:0') data.h5py: 929 tensor([[925., 926., 927., 928., 929.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([930.], device='cuda:0') data.h5py: 930 tensor([[926., 927., 928., 929., 930.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([931.], device='cuda:0') data.h5py: 931 tensor([[927., 928., 929., 930., 931.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([932.], device='cuda:0') data.h5py: 932 tensor([[928., 929., 930., 931., 932.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([933.], device='cuda:0') data.h5py: 933 tensor([[929., 930., 931., 932., 933.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([934.], device='cuda:0') data.h5py: 934 tensor([[930., 931., 932., 933., 934.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([935.], device='cuda:0') data.h5py: 935 tensor([[931., 932., 933., 934., 935.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([936.], device='cuda:0') data.h5py: 936 tensor([[932., 933., 934., 935., 936.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([937.], device='cuda:0') data.h5py: 937 tensor([[937., 937., 937., 937., 937.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([938.], device='cuda:0') data.h5py: 938 tensor([[937., 937., 937., 937., 938.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([939.], device='cuda:0') data.h5py: 939 tensor([[937., 937., 937., 938., 939.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([940.], device='cuda:0') data.h5py: 940 tensor([[937., 937., 938., 939., 940.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([941.], device='cuda:0') data.h5py: 941 tensor([[937., 938., 939., 940., 941.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([942.], device='cuda:0') data.h5py: 942 tensor([[938., 939., 940., 941., 942.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([943.], device='cuda:0') data.h5py: 943 tensor([[939., 940., 941., 942., 943.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([944.], device='cuda:0') data.h5py: 944 tensor([[940., 941., 942., 943., 944.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([945.], device='cuda:0') data.h5py: 945 tensor([[941., 942., 943., 944., 945.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([946.], device='cuda:0') data.h5py: 946 tensor([[942., 943., 944., 945., 946.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([947.], device='cuda:0') data.h5py: 947 tensor([[943., 944., 945., 946., 947.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([948.], device='cuda:0') data.h5py: 948 tensor([[944., 945., 946., 947., 948.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([949.], device='cuda:0') data.h5py: 949 tensor([[945., 946., 947., 948., 949.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([950.], device='cuda:0') data.h5py: 950 tensor([[946., 947., 948., 949., 950.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([951.], device='cuda:0') data.h5py: 951 tensor([[947., 948., 949., 950., 951.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([952.], device='cuda:0') data.h5py: 952 tensor([[948., 949., 950., 951., 952.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([953.], device='cuda:0') data.h5py: 953 tensor([[953., 953., 953., 953., 953.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([954.], device='cuda:0') data.h5py: 954 tensor([[953., 953., 953., 953., 954.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([955.], device='cuda:0') data.h5py: 955 tensor([[953., 953., 953., 954., 955.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([956.], device='cuda:0') data.h5py: 956 tensor([[953., 953., 954., 955., 956.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([957.], device='cuda:0') data.h5py: 957 tensor([[953., 954., 955., 956., 957.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([958.], device='cuda:0') data.h5py: 958 tensor([[954., 955., 956., 957., 958.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([959.], device='cuda:0') data.h5py: 959 tensor([[955., 956., 957., 958., 959.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([960.], device='cuda:0') data.h5py: 960 tensor([[956., 957., 958., 959., 960.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([961.], device='cuda:0') data.h5py: 961 tensor([[957., 958., 959., 960., 961.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([962.], device='cuda:0') data.h5py: 962 tensor([[958., 959., 960., 961., 962.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([963.], device='cuda:0') data.h5py: 963 tensor([[963., 963., 963., 963., 963.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([964.], device='cuda:0') data.h5py: 964 tensor([[963., 963., 963., 963., 964.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([965.], device='cuda:0') data.h5py: 965 tensor([[963., 963., 963., 964., 965.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([966.], device='cuda:0') data.h5py: 966 tensor([[963., 963., 964., 965., 966.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([967.], device='cuda:0') data.h5py: 967 tensor([[963., 964., 965., 966., 967.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([968.], device='cuda:0') data.h5py: 968 tensor([[964., 965., 966., 967., 968.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([969.], device='cuda:0') data.h5py: 969 tensor([[965., 966., 967., 968., 969.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([970.], device='cuda:0') data.h5py: 970 tensor([[966., 967., 968., 969., 970.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([971.], device='cuda:0') data.h5py: 971 tensor([[967., 968., 969., 970., 971.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([972.], device='cuda:0') data.h5py: 972 tensor([[968., 969., 970., 971., 972.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([973.], device='cuda:0') data.h5py: 973 tensor([[969., 970., 971., 972., 973.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([974.], device='cuda:0') data.h5py: 974 tensor([[970., 971., 972., 973., 974.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([975.], device='cuda:0') data.h5py: 975 tensor([[971., 972., 973., 974., 975.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([976.], device='cuda:0') data.h5py: 976 tensor([[972., 973., 974., 975., 976.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([977.], device='cuda:0') data.h5py: 977 tensor([[973., 974., 975., 976., 977.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([978.], device='cuda:0') data.h5py: 978 tensor([[974., 975., 976., 977., 978.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([979.], device='cuda:0') data.h5py: 979 tensor([[975., 976., 977., 978., 979.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([980.], device='cuda:0') data.h5py: 980 tensor([[980., 980., 980., 980., 980.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([981.], device='cuda:0') data.h5py: 981 tensor([[980., 980., 980., 980., 981.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([982.], device='cuda:0') data.h5py: 982 tensor([[980., 980., 980., 981., 982.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([983.], device='cuda:0') data.h5py: 983 tensor([[980., 980., 981., 982., 983.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([984.], device='cuda:0') data.h5py: 984 tensor([[980., 981., 982., 983., 984.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([985.], device='cuda:0') data.h5py: 985 tensor([[981., 982., 983., 984., 985.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([986.], device='cuda:0') data.h5py: 986 tensor([[982., 983., 984., 985., 986.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([987.], device='cuda:0') data.h5py: 987 tensor([[983., 984., 985., 986., 987.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([988.], device='cuda:0') data.h5py: 988 tensor([[984., 985., 986., 987., 988.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([989.], device='cuda:0') data.h5py: 989 tensor([[985., 986., 987., 988., 989.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([990.], device='cuda:0') data.h5py: 990 tensor([[986., 987., 988., 989., 990.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([991.], device='cuda:0') data.h5py: 991 tensor([[987., 988., 989., 990., 991.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([992.], device='cuda:0') data.h5py: 992 tensor([[988., 989., 990., 991., 992.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([993.], device='cuda:0') data.h5py: 993 tensor([[989., 990., 991., 992., 993.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([994.], device='cuda:0') data.h5py: 994 tensor([[990., 991., 992., 993., 994.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([995.], device='cuda:0') data.h5py: 995 tensor([[991., 992., 993., 994., 995.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([996.], device='cuda:0') data.h5py: 996 tensor([[992., 993., 994., 995., 996.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([997.], device='cuda:0') data.h5py: 997 tensor([[993., 994., 995., 996., 997.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([998.], device='cuda:0') data.h5py: 998 tensor([[994., 995., 996., 997., 998.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([999.], device='cuda:0') data.h5py: 999 tensor([[995., 996., 997., 998., 999.]], device='cuda:0') torch.Size([1, 5, 10]) tensor([1000.], device='cuda:0')","title":"Examples"},{"location":"apis/data/h5py/H5Converter/","text":"data.h5py.H5Converter \u00b6 Class \u00b7 Source converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Arguments \u00b6 Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () ) Methods \u00b6 convert \u00b6 converter . convert () Perform the data conversion. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5converter.h5' ) dc . query () # Perform test. cvt_o = mdnc . data . h5py . H5Converter ( os . path . join ( root_folder , 'test_data_h5converter' ), 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. os . rename ( os . path . join ( root_folder , 'test_data_h5converter' ), os . path . join ( root_folder , 'test_data_h5converter_cvt' )) cvt_i = mdnc . data . h5py . H5Converter ( os . path . join ( root_folder , 'test_data_h5converter_cvt' ), 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset. Output data.webtools: All required datasets are available. data.h5py: Have dumped /group1/x data.h5py: Have dumped /group1/y data.h5py: Have dumped /group2/group3/x data.h5py: Have dumped /group2/group3/y data.h5py: Have dumped /group2/x data.h5py: Have dumped /group2/y data.h5py: Have dumped /vds data.h5py: Have dumped /x data.h5py: Have dumped /y data.h5py: Have dumped /group1/x data.h5py: Have dumped /group1/y data.h5py: Have dumped /group2/group3/x data.h5py: Have dumped /group2/group3/y data.h5py: Have dumped /group2/x data.h5py: Have dumped /group2/y data.h5py: Have dumped /vds data.h5py: Have dumped /x data.h5py: Have dumped /y","title":"<span class='magic-codeicon-class'>H5Converter</span>"},{"location":"apis/data/h5py/H5Converter/#datah5pyh5converter","text":"Class \u00b7 Source converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets .","title":"data.h5py.H5Converter"},{"location":"apis/data/h5py/H5Converter/#arguments","text":"Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () )","title":"Arguments"},{"location":"apis/data/h5py/H5Converter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5Converter/#convert","text":"converter . convert () Perform the data conversion.","title":" convert"},{"location":"apis/data/h5py/H5Converter/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5converter.h5' ) dc . query () # Perform test. cvt_o = mdnc . data . h5py . H5Converter ( os . path . join ( root_folder , 'test_data_h5converter' ), 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. os . rename ( os . path . join ( root_folder , 'test_data_h5converter' ), os . path . join ( root_folder , 'test_data_h5converter_cvt' )) cvt_i = mdnc . data . h5py . H5Converter ( os . path . join ( root_folder , 'test_data_h5converter_cvt' ), 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset. Output data.webtools: All required datasets are available. data.h5py: Have dumped /group1/x data.h5py: Have dumped /group1/y data.h5py: Have dumped /group2/group3/x data.h5py: Have dumped /group2/group3/y data.h5py: Have dumped /group2/x data.h5py: Have dumped /group2/y data.h5py: Have dumped /vds data.h5py: Have dumped /x data.h5py: Have dumped /y data.h5py: Have dumped /group1/x data.h5py: Have dumped /group1/y data.h5py: Have dumped /group2/group3/x data.h5py: Have dumped /group2/group3/y data.h5py: Have dumped /group2/x data.h5py: Have dumped /group2/y data.h5py: Have dumped /vds data.h5py: Have dumped /x data.h5py: Have dumped /y","title":"Examples"},{"location":"apis/data/h5py/H5GParser/","text":"data.h5py.H5GParser \u00b6 Class \u00b7 Context \u00b7 Source dset = mdnc . data . h5py . H5GParser ( file_name , keywords , batch_size = 32 , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) Grouply parsing dataset. This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file indexer, this indexer would be initialized by sequence.MPSequence . It would use the user defined keywords to get a group of h5py . Dataset s. Estimate the h5py . Dataset sizes, each dataset should share the same size (but could have different shapes). Use the dataset size to create a sequence.MPSequence , and allows it to randomly shuffle the indices in each epoch. Invoke the sequence.MPSequence APIs to serve the parallel dataset parsing. Certainly, you could use this parser to load a single h5py . Dataset . To find details about the parallel parsing workflow, please check mdnc.data.sequence.MPSequence . Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Exampless \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5GParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = None ) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 2, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Output data.webtools: All required datasets are available. data.h5py: Epoch 1, Batch 0 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 1 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 2 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 3 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 4 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 5 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 6 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 7 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 8 torch.Size([1, 20]) torch.Size([1, 10]) data.h5py: Epoch 2, Batch 0 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 1 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 2 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 3 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 4 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 5 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 6 torch.Size([1, 20]) torch.Size([1, 10]) data.h5py: Epoch 2, Batch 7 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 8 torch.Size([3, 20]) torch.Size([3, 10]) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5GParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = mdnc . data . preprocs . ProcScaler ()) with dset . start_test () as p : for i , ( d_one , d_two ) in enumerate ( p ): d_one , d_two = d_one . cpu () . numpy (), d_two . cpu () . numpy () std_one , std_two = np . std ( d_one ), np . std ( d_two ) d_one , d_two = p . preproc . postprocess ( d_one , d_two ) std_one_ , std_two_ = np . std ( d_one ), np . std ( d_two ) print ( 'Before: {0} , {1} ; After: {2} , {3} .' . format ( std_one , std_two , std_one_ , std_two_ )) Output data.webtools: All required datasets are available. Before: 0.4213927686214447, 0.5810447931289673; After: 3.4976863861083984, 4.269893169403076. Before: 0.47204485535621643, 0.5270004868507385; After: 3.2560627460479736, 5.232884407043457. Before: 0.380888432264328, 0.5548458099365234; After: 2.69606876373291, 4.5017008781433105. Before: 0.555243968963623, 0.5082056522369385; After: 3.231991767883301, 5.085717678070068. Before: 0.39406657218933105, 0.5630286931991577; After: 2.8078441619873047, 5.10365629196167. Before: 0.49584802985191345, 0.5255910754203796; After: 2.706739664077759, 5.646749019622803. Before: 0.4346843361854553, 0.5725106000900269; After: 2.7871317863464355, 4.466533660888672. Before: 0.5043540000915527, 0.5292088389396667; After: 2.373351573944092, 4.446733474731445. Before: 0.46324262022972107, 0.6497944593429565; After: 2.350776433944702, 5.593009948730469.","title":"<span class='magic-codeicon-class'>H5GParser</span>"},{"location":"apis/data/h5py/H5GParser/#datah5pyh5gparser","text":"Class \u00b7 Context \u00b7 Source dset = mdnc . data . h5py . H5GParser ( file_name , keywords , batch_size = 32 , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) Grouply parsing dataset. This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file indexer, this indexer would be initialized by sequence.MPSequence . It would use the user defined keywords to get a group of h5py . Dataset s. Estimate the h5py . Dataset sizes, each dataset should share the same size (but could have different shapes). Use the dataset size to create a sequence.MPSequence , and allows it to randomly shuffle the indices in each epoch. Invoke the sequence.MPSequence APIs to serve the parallel dataset parsing. Certainly, you could use this parser to load a single h5py . Dataset . To find details about the parallel parsing workflow, please check mdnc.data.sequence.MPSequence .","title":"data.h5py.H5GParser"},{"location":"apis/data/h5py/H5GParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5GParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5GParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5GParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5GParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5GParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5GParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5GParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5GParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5GParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5GParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5GParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5GParser/#batch_size","text":"dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/h5py/H5GParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5GParser/#exampless","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5GParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = None ) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 2, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Output data.webtools: All required datasets are available. data.h5py: Epoch 1, Batch 0 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 1 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 2 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 3 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 4 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 5 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 6 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 7 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 1, Batch 8 torch.Size([1, 20]) torch.Size([1, 10]) data.h5py: Epoch 2, Batch 0 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 1 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 2 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 3 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 4 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 5 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 6 torch.Size([1, 20]) torch.Size([1, 10]) data.h5py: Epoch 2, Batch 7 torch.Size([3, 20]) torch.Size([3, 10]) data.h5py: Epoch 2, Batch 8 torch.Size([3, 20]) torch.Size([3, 10]) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5GParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = mdnc . data . preprocs . ProcScaler ()) with dset . start_test () as p : for i , ( d_one , d_two ) in enumerate ( p ): d_one , d_two = d_one . cpu () . numpy (), d_two . cpu () . numpy () std_one , std_two = np . std ( d_one ), np . std ( d_two ) d_one , d_two = p . preproc . postprocess ( d_one , d_two ) std_one_ , std_two_ = np . std ( d_one ), np . std ( d_two ) print ( 'Before: {0} , {1} ; After: {2} , {3} .' . format ( std_one , std_two , std_one_ , std_two_ )) Output data.webtools: All required datasets are available. Before: 0.4213927686214447, 0.5810447931289673; After: 3.4976863861083984, 4.269893169403076. Before: 0.47204485535621643, 0.5270004868507385; After: 3.2560627460479736, 5.232884407043457. Before: 0.380888432264328, 0.5548458099365234; After: 2.69606876373291, 4.5017008781433105. Before: 0.555243968963623, 0.5082056522369385; After: 3.231991767883301, 5.085717678070068. Before: 0.39406657218933105, 0.5630286931991577; After: 2.8078441619873047, 5.10365629196167. Before: 0.49584802985191345, 0.5255910754203796; After: 2.706739664077759, 5.646749019622803. Before: 0.4346843361854553, 0.5725106000900269; After: 2.7871317863464355, 4.466533660888672. Before: 0.5043540000915527, 0.5292088389396667; After: 2.373351573944092, 4.446733474731445. Before: 0.46324262022972107, 0.6497944593429565; After: 2.350776433944702, 5.593009948730469.","title":"Exampless"},{"location":"apis/data/h5py/H5RParser/","text":"data.h5py.H5RParser \u00b6 Class \u00b7 Context \u00b7 Source dset = mdnc . data . h5py . H5RParser ( file_name , keywords , preprocfunc , batch_num = 100 , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file handle. Using the user defined keywords to get a group of datasets. Check the dataset size, and register the dataset list. For each batch, the data is randomly picked from the whole set. The h5py.Dataset variable would be transparent in the preprocfunc , i.e. the method how to pick up the random samples need to be implemented by users. Certainly, you could use this parser to load a single dataset. Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. This function is required because the random sampling needs to be implemented here. batch_num int Number of mini-batches in each epoch. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The preprocfunc is required in this case. The provided pre-processors in data.preprocs should not be used directly, because users need to implment their own random sampling pre-processor first. For example, Example Without data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] mdnc . data . h5py . H5RParser ( ... , preprocfunc = ProcCustom (), keywords = [ 'x_1' , 'x_2' ]) Use data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x mdnc . data . h5py . H5RParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . choice ( len ( ds_x1 ), replace = False , size = self . batch_size )) ind_x2 = np . sort ( self . random_rng . choice ( len ( ds_x2 ), replace = False , size = self . batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5RParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Output data.webtools: All required datasets are available. data.h5py: Epoch 1, Batch 0 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 1 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 2 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 3 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 4 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 5 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 6 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 7 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 8 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 9 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 10 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 11 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 12 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 13 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 14 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 15 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 16 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 17 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 18 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 19 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 20 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 21 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 22 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 23 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 24 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 25 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 26 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 27 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 28 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 29 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 30 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 31 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 32 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 33 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 34 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 35 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 36 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 37 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 38 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 39 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 40 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 41 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 42 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 43 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 44 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 45 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 46 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 47 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 48 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 49 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 50 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 51 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 52 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 53 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 54 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 55 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 56 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 57 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 58 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 59 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 60 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 61 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 62 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 63 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 64 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 65 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 66 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 67 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 68 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 69 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 70 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 71 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 72 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 73 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 74 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 75 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 76 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 77 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 78 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 79 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 80 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 81 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 82 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 83 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 84 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 85 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 86 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 87 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 88 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 89 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 90 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 91 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 92 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 93 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 94 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 95 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 96 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 97 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 98 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 99 torch.Size([16, 20]) torch.Size([16, 10]) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . choice ( len ( ds ), replace = False , size = self . batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5RParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Output data.webtools: All required datasets are available. data.h5py: Epoch 1, Batch 0 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 1 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 2 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 3 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 4 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 5 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 6 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 7 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 8 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 9 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 10 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 11 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 12 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 13 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 14 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 15 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 16 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 17 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 18 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 19 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 20 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 21 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 22 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 23 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 24 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 25 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 26 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 27 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 28 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 29 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 30 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 31 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 32 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 33 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 34 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 35 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 36 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 37 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 38 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 39 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 40 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 41 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 42 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 43 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 44 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 45 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 46 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 47 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 48 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 49 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 50 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 51 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 52 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 53 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 54 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 55 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 56 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 57 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 58 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 59 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 60 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 61 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 62 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 63 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 64 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 65 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 66 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 67 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 68 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 69 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 70 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 71 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 72 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 73 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 74 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 75 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 76 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 77 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 78 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 79 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 80 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 81 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 82 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 83 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 84 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 85 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 86 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 87 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 88 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 89 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 90 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 91 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 92 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 93 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 94 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 95 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 96 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 97 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 98 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 99 torch.Size([16, 20]) torch.Size([16, 10])","title":"<span class='magic-codeicon-class'>H5RParser</span>"},{"location":"apis/data/h5py/H5RParser/#datah5pyh5rparser","text":"Class \u00b7 Context \u00b7 Source dset = mdnc . data . h5py . H5RParser ( file_name , keywords , preprocfunc , batch_num = 100 , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file handle. Using the user defined keywords to get a group of datasets. Check the dataset size, and register the dataset list. For each batch, the data is randomly picked from the whole set. The h5py.Dataset variable would be transparent in the preprocfunc , i.e. the method how to pick up the random samples need to be implemented by users. Certainly, you could use this parser to load a single dataset.","title":"data.h5py.H5RParser"},{"location":"apis/data/h5py/H5RParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. This function is required because the random sampling needs to be implemented here. batch_num int Number of mini-batches in each epoch. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The preprocfunc is required in this case. The provided pre-processors in data.preprocs should not be used directly, because users need to implment their own random sampling pre-processor first. For example, Example Without data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] mdnc . data . h5py . H5RParser ( ... , preprocfunc = ProcCustom (), keywords = [ 'x_1' , 'x_2' ]) Use data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x mdnc . data . h5py . H5RParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5RParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5RParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5RParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5RParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5RParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5RParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5RParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5RParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5RParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5RParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5RParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5RParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5RParser/#examples","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . choice ( len ( ds_x1 ), replace = False , size = self . batch_size )) ind_x2 = np . sort ( self . random_rng . choice ( len ( ds_x2 ), replace = False , size = self . batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5RParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Output data.webtools: All required datasets are available. data.h5py: Epoch 1, Batch 0 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 1 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 2 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 3 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 4 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 5 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 6 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 7 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 8 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 9 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 10 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 11 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 12 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 13 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 14 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 15 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 16 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 17 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 18 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 19 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 20 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 21 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 22 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 23 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 24 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 25 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 26 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 27 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 28 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 29 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 30 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 31 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 32 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 33 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 34 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 35 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 36 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 37 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 38 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 39 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 40 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 41 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 42 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 43 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 44 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 45 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 46 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 47 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 48 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 49 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 50 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 51 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 52 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 53 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 54 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 55 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 56 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 57 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 58 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 59 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 60 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 61 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 62 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 63 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 64 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 65 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 66 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 67 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 68 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 69 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 70 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 71 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 72 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 73 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 74 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 75 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 76 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 77 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 78 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 79 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 80 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 81 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 82 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 83 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 84 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 85 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 86 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 87 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 88 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 89 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 90 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 91 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 92 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 93 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 94 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 95 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 96 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 97 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 98 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 99 torch.Size([16, 20]) torch.Size([16, 10]) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . choice ( len ( ds ), replace = False , size = self . batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5gparser.h5' ) dc . query () # Perform test. dset = mdnc . data . h5py . H5RParser ( os . path . join ( root_folder , 'test_data_h5gparser' ), keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Output data.webtools: All required datasets are available. data.h5py: Epoch 1, Batch 0 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 1 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 2 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 3 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 4 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 5 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 6 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 7 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 8 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 9 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 10 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 11 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 12 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 13 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 14 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 15 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 16 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 17 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 18 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 19 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 20 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 21 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 22 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 23 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 24 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 25 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 26 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 27 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 28 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 29 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 30 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 31 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 32 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 33 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 34 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 35 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 36 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 37 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 38 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 39 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 40 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 41 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 42 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 43 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 44 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 45 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 46 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 47 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 48 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 49 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 50 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 51 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 52 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 53 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 54 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 55 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 56 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 57 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 58 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 59 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 60 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 61 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 62 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 63 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 64 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 65 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 66 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 67 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 68 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 69 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 70 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 71 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 72 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 73 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 74 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 75 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 76 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 77 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 78 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 79 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 80 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 81 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 82 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 83 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 84 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 85 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 86 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 87 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 88 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 89 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 90 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 91 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 92 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 93 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 94 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 95 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 96 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 97 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 98 torch.Size([16, 20]) torch.Size([16, 10]) data.h5py: Epoch 1, Batch 99 torch.Size([16, 20]) torch.Size([16, 10])","title":"Examples"},{"location":"apis/data/h5py/H5SeqConverter/","text":"data.h5py.H5SeqConverter \u00b6 Class \u00b7 Context \u00b7 Source converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group . Arguments \u00b6 Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . Methods \u00b6 config \u00b6 converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. convert \u00b6 converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. copy \u00b6 converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. open \u00b6 converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . close \u00b6 converter . close () Close the converter. Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5seqconverter1.h5' ) dc . query () # Perform test. with mdnc . data . h5py . H5SeqConverter ( os . path . join ( root_folder , 'test_data_h5seqconverter1' )) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Output data.webtools: All required datasets are available. data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Convert data_to_sequence into the output file. The original data shape is (1000,), splitted into 64 parts. data.h5py: Copy data_only_copied into the output file. The data shape is (1000,). Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ([ 'test_data_h5seqconverter1.h5' , 'test_data_h5seqconverter2.h5' ]) dc . query () # Perform test. converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( os . path . join ( root_folder , 'test_data_h5seqconverter1' )) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( os . path . join ( root_folder , 'test_data_h5seqconverter2' )) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Output data.webtools: All required datasets are available. data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Open a new read file: alpha-test\\test_data_h5seqconverter1.h5 data.h5py: Open a new output file: alpha-test\\test_data_h5seqconverter1_seq.h5 data.h5py: Convert data_to_sequence into the output file. The original data shape is (1000,), splitted into 64 parts. data.h5py: Copy data_only_copied into the output file. The data shape is (1000,). data.h5py: Open a new read file: alpha-test\\test_data_h5seqconverter2.h5 data.h5py: Open a new output file: alpha-test\\test_data_h5seqconverter2_seq.h5 data.h5py: Convert data_to_sequence into the output file. The original data shape is (1000,), splitted into 64 parts. data.h5py: Copy data_only_copied into the output file. The data shape is (1000,).","title":"<span class='magic-codeicon-class'>H5SeqConverter</span>"},{"location":"apis/data/h5py/H5SeqConverter/#datah5pyh5seqconverter","text":"Class \u00b7 Context \u00b7 Source converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group .","title":"data.h5py.H5SeqConverter"},{"location":"apis/data/h5py/H5SeqConverter/#arguments","text":"Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":"Arguments"},{"location":"apis/data/h5py/H5SeqConverter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SeqConverter/#config","text":"converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":" config"},{"location":"apis/data/h5py/H5SeqConverter/#convert","text":"converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" convert"},{"location":"apis/data/h5py/H5SeqConverter/#copy","text":"converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" copy"},{"location":"apis/data/h5py/H5SeqConverter/#open","text":"converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":" open"},{"location":"apis/data/h5py/H5SeqConverter/#close","text":"converter . close () Close the converter.","title":" close"},{"location":"apis/data/h5py/H5SeqConverter/#examples","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ( 'test_data_h5seqconverter1.h5' ) dc . query () # Perform test. with mdnc . data . h5py . H5SeqConverter ( os . path . join ( root_folder , 'test_data_h5seqconverter1' )) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Output data.webtools: All required datasets are available. data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Convert data_to_sequence into the output file. The original data shape is (1000,), splitted into 64 parts. data.h5py: Copy data_only_copied into the output file. The data shape is (1000,). Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import os import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Prepare the datasets. set_list_file = os . path . join ( root_folder , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = root_folder , set_list_file = set_list_file , token = '' , verbose = False ) dc . add_query_file ([ 'test_data_h5seqconverter1.h5' , 'test_data_h5seqconverter2.h5' ]) dc . query () # Perform test. converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( os . path . join ( root_folder , 'test_data_h5seqconverter1' )) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( os . path . join ( root_folder , 'test_data_h5seqconverter2' )) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Output data.webtools: All required datasets are available. data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Open a new read file: alpha-test\\test_data_h5seqconverter1.h5 data.h5py: Open a new output file: alpha-test\\test_data_h5seqconverter1_seq.h5 data.h5py: Convert data_to_sequence into the output file. The original data shape is (1000,), splitted into 64 parts. data.h5py: Copy data_only_copied into the output file. The data shape is (1000,). data.h5py: Open a new read file: alpha-test\\test_data_h5seqconverter2.h5 data.h5py: Open a new output file: alpha-test\\test_data_h5seqconverter2_seq.h5 data.h5py: Convert data_to_sequence into the output file. The original data shape is (1000,), splitted into 64 parts. data.h5py: Copy data_only_copied into the output file. The data shape is (1000,).","title":"Examples"},{"location":"apis/data/h5py/H5SupSaver/","text":"data.h5py.H5SupSaver \u00b6 Class \u00b7 Context \u00b7 Source saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of h5py . Group and h5py . Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py . Softlink , h5py . Attributes and h5py . VirtualDataSet . Add context nesting supports for h5py . Group . This would makes the codes more elegant. Arguments \u00b6 Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. Methods \u00b6 config \u00b6 saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. get_config \u00b6 cfg = saver . get_config ( name = None ) Get the current configuration value by the given name . Requries Argument Type Description name str The name of the required config value. Returns Argument Description cfg The required config value. open \u00b6 saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value. close \u00b6 saver . close () Close the saver. dump \u00b6 saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. set_link \u00b6 saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset . set_attrs \u00b6 saver . set_attrs ( keyword , attrs = None , ** kwargs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () . set_virtual_set \u00b6 saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset. Properties \u00b6 attrs \u00b6 attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Perform test. with mdnc . data . h5py . H5SupSaver ( os . path . join ( root_folder , 'test_h5supsaver' ), enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Output data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Dump one into the file. The data shape is (25, 20). data.h5py: Dump zero into the file. The data shape is (25, 10). Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Perform test. saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( os . path . join ( root_folder , 'test_h5supsaver' )) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open )) Output data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Open a new file: alpha-test\\test_h5supsaver.h5 data.h5py: Dump test1 into the file. The data shape is (100, 20). data.h5py: Dump test2 into the file. The data shape is (100, 20). data.h5py: Dump 100 data samples into the existed dataset /group1/group2/test2. The data shape is (200, 20) now. data.h5py: Create a soft link \"test3\", pointting to \"/test1\". data.h5py: Check open: s[\"group1\"]=True, s[\"group1/group2\"]=False","title":"<span class='magic-codeicon-class'>H5SupSaver</span>"},{"location":"apis/data/h5py/H5SupSaver/#datah5pyh5supsaver","text":"Class \u00b7 Context \u00b7 Source saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of h5py . Group and h5py . Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py . Softlink , h5py . Attributes and h5py . VirtualDataSet . Add context nesting supports for h5py . Group . This would makes the codes more elegant.","title":"data.h5py.H5SupSaver"},{"location":"apis/data/h5py/H5SupSaver/#arguments","text":"Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file.","title":"Arguments"},{"location":"apis/data/h5py/H5SupSaver/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SupSaver/#config","text":"saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":" config"},{"location":"apis/data/h5py/H5SupSaver/#get_config","text":"cfg = saver . get_config ( name = None ) Get the current configuration value by the given name . Requries Argument Type Description name str The name of the required config value. Returns Argument Description cfg The required config value.","title":" get_config"},{"location":"apis/data/h5py/H5SupSaver/#open","text":"saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value.","title":" open"},{"location":"apis/data/h5py/H5SupSaver/#close","text":"saver . close () Close the saver.","title":" close"},{"location":"apis/data/h5py/H5SupSaver/#dump","text":"saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" dump"},{"location":"apis/data/h5py/H5SupSaver/#set_link","text":"saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset .","title":" set_link"},{"location":"apis/data/h5py/H5SupSaver/#set_attrs","text":"saver . set_attrs ( keyword , attrs = None , ** kwargs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () .","title":" set_attrs"},{"location":"apis/data/h5py/H5SupSaver/#set_virtual_set","text":"saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset.","title":" set_virtual_set"},{"location":"apis/data/h5py/H5SupSaver/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5SupSaver/#attrs","text":"attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager .","title":" attrs"},{"location":"apis/data/h5py/H5SupSaver/#examples","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Perform test. with mdnc . data . h5py . H5SupSaver ( os . path . join ( root_folder , 'test_h5supsaver' ), enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Output data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Dump one into the file. The data shape is (25, 20). data.h5py: Dump zero into the file. The data shape is (25, 10). Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import os import numpy as np import mdnc root_folder = 'alpha-test' os . makedirs ( root_folder , exist_ok = True ) if __name__ == '__main__' : # Perform test. saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( os . path . join ( root_folder , 'test_h5supsaver' )) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open )) Output data.h5py: Current configuration is: {'dtype': <class 'numpy.float32'>, 'shuffle': True, 'fletcher32': True, 'compression': 'gzip'} data.h5py: Open a new file: alpha-test\\test_h5supsaver.h5 data.h5py: Dump test1 into the file. The data shape is (100, 20). data.h5py: Dump test2 into the file. The data shape is (100, 20). data.h5py: Dump 100 data samples into the existed dataset /group1/group2/test2. The data shape is (200, 20) now. data.h5py: Create a soft link \"test3\", pointting to \"/test1\". data.h5py: Check open: s[\"group1\"]=True, s[\"group1/group2\"]=False","title":"Examples"},{"location":"apis/data/preprocs/ProcAbstract/","text":"data.preprocs.ProcAbstract \u00b6 Abstract Class \u00b7 Source proc = mdnc . data . preprocs . ProcAbstract ( inds = None , parent = None , _disable_inds = False ) The basic processor class supporting cascading and variable-level broadcasting: Cascading: It means the derived class of this abstract class will support using such a method ProcDerived ( parent = ProcDerived ( ... )) to create composition of processors. Variable level broadcasting: It means when _disable_inds = False the user-implemented methods, for example, def preprocess ( x ) , would be broadcasted to arbitrary number of input variables, like proc.preprocess(x1, x2, ...) . Info This is an abstract class, which means you could not create an instance of this class by codes like this proc = ProcAbstract ( ... ) The correct way to use this class it to implement a derived class from this class. The intertage has 2 requirements: The __init__ method of this class need to be called inside the __init__ method of the derived class. The preprocess() and postprocess() methods need to be implemented. We recommend to expose the argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation. Arguments \u00b6 Requries Argument Type Description inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . _disable_inds bool A flag used by developers. If set True , the broadcasting would not be used. It means that the user-implemented arguments would be exactly the arguments to be used. Warning The argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation. Abstract Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1: with inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . a = a def preprocess ( self , x ): # The input is an np.ndarray return self . a * x def postprocess ( self , x ): # The inverse operator return x / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) proc2 = ProcDerived ( a = 2.0 , inds = [ 1 , 2 ]) x_ , y_ , z_ = proc2 . preprocess ( x , y , z ) xr , yr , zr = proc2 . postprocess ( x_ , y_ , z_ ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 2) (3, 2) (4, 3) Processed error: 0.0 0.0 0.0 Inverse error: 0.0 0.0 0.0 Processed error: 0.0 0.0 0.0 Inverse error: 0.0 0.0 0.0 Example 2: without inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * x , self . a * y , self . a * z def postprocess ( self , x , y , z ): # The inverse operator return x / self . a , y / self . a , z / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 2) (3, 2) (4, 3) Processed error: 0.0 0.0 0.0 Inverse error: 0.0 0.0 0.0 In the above two examples, the processor would multiply the inputs by 2.0 . The first implementation allows users to use the argument inds to determine which variables require to be processed. The user-implemented methods in the second example would fully control the input and output arguments. Actually, the second implementation allows user to change the number of output arguments, for example: Example 3: out args changed Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * np . mean (( x , y , z ), axis = 0 ) def postprocess ( self , x_m ): # The inverse operator x = x_m / self . a return x_m , x_m , x_m proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 5 , 2 ]), np . zeros ([ 5 , 2 ]) xm = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( xm ) print ( 'Processed shape:' , xm . shape ) print ( 'Processed error:' , np . amax ( np . abs ( xm - 2 * np . mean ([ x , y , z ], axis = 0 )))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 2) Processed error: 0.0 Inverse error: 0.33333333333333326 0.33333333333333326 1.3333333333333333 This operation is not invertible. We could find that the inverse error would be greater than 0 . All derived classes of this class could be cascaded with each other. See the tutorial for checking more examples.","title":"<span class='magic-codeicon-class'>ProcAbstract</span>"},{"location":"apis/data/preprocs/ProcAbstract/#datapreprocsprocabstract","text":"Abstract Class \u00b7 Source proc = mdnc . data . preprocs . ProcAbstract ( inds = None , parent = None , _disable_inds = False ) The basic processor class supporting cascading and variable-level broadcasting: Cascading: It means the derived class of this abstract class will support using such a method ProcDerived ( parent = ProcDerived ( ... )) to create composition of processors. Variable level broadcasting: It means when _disable_inds = False the user-implemented methods, for example, def preprocess ( x ) , would be broadcasted to arbitrary number of input variables, like proc.preprocess(x1, x2, ...) . Info This is an abstract class, which means you could not create an instance of this class by codes like this proc = ProcAbstract ( ... ) The correct way to use this class it to implement a derived class from this class. The intertage has 2 requirements: The __init__ method of this class need to be called inside the __init__ method of the derived class. The preprocess() and postprocess() methods need to be implemented. We recommend to expose the argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation.","title":"data.preprocs.ProcAbstract"},{"location":"apis/data/preprocs/ProcAbstract/#arguments","text":"Requries Argument Type Description inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . _disable_inds bool A flag used by developers. If set True , the broadcasting would not be used. It means that the user-implemented arguments would be exactly the arguments to be used. Warning The argument inds and parent in the derived class. But _disable_inds should not be accessed by users. See Examples to view how to make the derivation.","title":"Arguments"},{"location":"apis/data/preprocs/ProcAbstract/#abstract-methods","text":"","title":"Abstract Methods"},{"location":"apis/data/preprocs/ProcAbstract/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcAbstract/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcAbstract/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcAbstract/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcAbstract/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcAbstract/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1: with inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . a = a def preprocess ( self , x ): # The input is an np.ndarray return self . a * x def postprocess ( self , x ): # The inverse operator return x / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) proc2 = ProcDerived ( a = 2.0 , inds = [ 1 , 2 ]) x_ , y_ , z_ = proc2 . preprocess ( x , y , z ) xr , yr , zr = proc2 . postprocess ( x_ , y_ , z_ ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 2) (3, 2) (4, 3) Processed error: 0.0 0.0 0.0 Inverse error: 0.0 0.0 0.0 Processed error: 0.0 0.0 0.0 Inverse error: 0.0 0.0 0.0 Example 2: without inds Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * x , self . a * y , self . a * z def postprocess ( self , x , y , z ): # The inverse operator return x / self . a , y / self . a , z / self . a proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 3 , 2 ]), np . ones ([ 4 , 3 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - 2 * x )), np . amax ( np . abs ( y_ - 2 * y )), np . amax ( np . abs ( z_ - 2 * z ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 2) (3, 2) (4, 3) Processed error: 0.0 0.0 0.0 Inverse error: 0.0 0.0 0.0 In the above two examples, the processor would multiply the inputs by 2.0 . The first implementation allows users to use the argument inds to determine which variables require to be processed. The user-implemented methods in the second example would fully control the input and output arguments. Actually, the second implementation allows user to change the number of output arguments, for example: Example 3: out args changed Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y , z ): # All inputs are arrays return self . a * np . mean (( x , y , z ), axis = 0 ) def postprocess ( self , x_m ): # The inverse operator x = x_m / self . a return x_m , x_m , x_m proc = ProcDerived ( a = 2.0 ) x , y , z = np . ones ([ 5 , 2 ]), np . ones ([ 5 , 2 ]), np . zeros ([ 5 , 2 ]) xm = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( xm ) print ( 'Processed shape:' , xm . shape ) print ( 'Processed error:' , np . amax ( np . abs ( xm - 2 * np . mean ([ x , y , z ], axis = 0 )))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 2) Processed error: 0.0 Inverse error: 0.33333333333333326 0.33333333333333326 1.3333333333333333 This operation is not invertible. We could find that the inverse error would be greater than 0 . All derived classes of this class could be cascaded with each other. See the tutorial for checking more examples.","title":"Examples"},{"location":"apis/data/preprocs/ProcFilter1d/","text":"data.preprocs.ProcFilter1d \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D IIR band-pass filters (also supports low-pass or high-pass filter). The IIR filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcFilter1d ( axis = 1 , parent = ProcFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing. Arguments \u00b6 Requries Argument Type Description axis int The axis where we apply the 1D filter. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40 Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , filter_type = 'butter' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output","title":"<span class='magic-codeicon-class'>ProcFilter1d</span>"},{"location":"apis/data/preprocs/ProcFilter1d/#datapreprocsprocfilter1d","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D IIR band-pass filters (also supports low-pass or high-pass filter). The IIR filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcFilter1d ( axis = 1 , parent = ProcFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing.","title":"data.preprocs.ProcFilter1d"},{"location":"apis/data/preprocs/ProcFilter1d/#arguments","text":"Requries Argument Type Description axis int The axis where we apply the 1D filter. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40","title":"Arguments"},{"location":"apis/data/preprocs/ProcFilter1d/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcFilter1d/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcFilter1d/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcFilter1d/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcFilter1d/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcFilter1d/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcFilter1d/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcFilter1d ( axis =- 1 , filter_type = 'butter' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output","title":"Examples"},{"location":"apis/data/preprocs/ProcLifter/","text":"data.preprocs.ProcLifter \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcLifter ( a , inds = None , parent = None ) This is a homogeneous processor. It use the parameter a to perform such an invertible transform: \\mathbf{y}_n = \\mathrm{sign} (\\mathbf{x}_n) * \\log (1 + a * |\\mathbf{x}_n|) This transform could strengthen the low-amplitude parts of the signal, because the data is transformed into the log domain. Arguments \u00b6 Requries Argument Type Description a float The parameter used for log-lifting the data. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the log-lifting. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the lifting. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 a \u00b6 proc . a The lifting parameter \\(a\\) . parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( - 2 * np . pi , 2 * np . pi , 200 ) proc = mdnc . data . preprocs . ProcLifter ( a = 10.0 ) x = np . cos ( np . pi * t ) + 0.5 * np . cos ( 1.5 * np . pi * t + 0.1 ) + 0.2 * np . cos ( 2.5 * np . pi * t + 0.3 ) + 0.1 * np . cos ( 3.5 * np . pi * t + 0.7 ) x_ = proc . preprocess ( x ) xr = proc . postprocess ( x_ ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t , x_ ) axs [ 1 ] . plot ( t , xr ) axs [ 2 ] . plot ( t , x ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output","title":"<span class='magic-codeicon-class'>ProcLifter</span>"},{"location":"apis/data/preprocs/ProcLifter/#datapreprocsproclifter","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcLifter ( a , inds = None , parent = None ) This is a homogeneous processor. It use the parameter a to perform such an invertible transform: \\mathbf{y}_n = \\mathrm{sign} (\\mathbf{x}_n) * \\log (1 + a * |\\mathbf{x}_n|) This transform could strengthen the low-amplitude parts of the signal, because the data is transformed into the log domain.","title":"data.preprocs.ProcLifter"},{"location":"apis/data/preprocs/ProcLifter/#arguments","text":"Requries Argument Type Description a float The parameter used for log-lifting the data. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcLifter/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcLifter/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the log-lifting. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcLifter/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the lifting. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcLifter/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcLifter/#a","text":"proc . a The lifting parameter \\(a\\) .","title":" a"},{"location":"apis/data/preprocs/ProcLifter/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcLifter/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcLifter/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( - 2 * np . pi , 2 * np . pi , 200 ) proc = mdnc . data . preprocs . ProcLifter ( a = 10.0 ) x = np . cos ( np . pi * t ) + 0.5 * np . cos ( 1.5 * np . pi * t + 0.1 ) + 0.2 * np . cos ( 2.5 * np . pi * t + 0.3 ) + 0.1 * np . cos ( 3.5 * np . pi * t + 0.7 ) x_ = proc . preprocess ( x ) xr = proc . postprocess ( x_ ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t , x_ ) axs [ 1 ] . plot ( t , xr ) axs [ 2 ] . plot ( t , x ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output","title":"Examples"},{"location":"apis/data/preprocs/ProcMerge/","text":"data.preprocs.ProcMerge \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcMerge ( procs = None , num_procs = None , parent = None ) Merge manager. This processor is inhomogeneous, and designed for merging different processors by a more efficient way. For example, p = ProcMerge ([ Proc1 ( ... ), Proc2 ( ... )]) Would apply Proc1 to the first argument, and Proc2 to the second argument. It is equivalent to p = Proc1 ( ... , inds = 0 , parent = Proc2 ( ... , inds = 1 )) This class should not be used if any sub-processor does not return the results with the same number of the input variables (out-arg changed). One exception is, the parent of this class could be an out-arg changed processor. This API is more intuitive for users to concatenate serveral processors together. It will make your codes more readable and reduce the stack level of the processors. Arguments \u00b6 Requries Argument Type Description procs ( ProcAbstract , ) A sequence of processors. Each processor is derived from mdnc.data.preprocs.ProcAbstract . Could be used for initializing this merge processor. num_procs object The number of input arguments of this processor. If not set, would infer the number from the length of the argument procs . At least one of procs or num_procs needs to be specified. The two arguments could be specified together. parent ProcAbstract An instance derived from mdnc.data.preprocs.ProcAbstract . This instance would be used as the parent of the current instance. Warning The argument num_procs should be greater than procs , if both num_procs and procs are specified. Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Operators \u00b6 __getitem__ \u00b6 proc_i = proc [ idx ] Get the i th sub-processor. Warning If one sub-processor is managing multiple indicies, the returned sub-processor would always be same for those indicies. For example, Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) proc_1 = proc [ 1 ] proc_2 = proc [ 2 ] print ( proc_m is proc_1 , proc_m is proc_2 ) This behavior is important if your proc_m is an inhomogeneous processor. It means although you get proc_2 by proc [ 2 ] , you still need to place your argument as the 2 nd input when using proc_2 . Requries Argument Type Description idx int The index of the sub-processor. Returns Argument Description proc_i An instance derived from ProcAbstract , the i th sub-processor. __setitem__ \u00b6 proc [ idx ] = proc_i Info This method supports multiple assignment, for example: Codes proc = ProcMerge ( num_procs = 3 ) proc [:] = Proc1 ( ... ) proc [ 1 : 2 ] = Proc2 ( ... ) This would be equivalent to Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) Requries Argument Type Description idx int or slice or tuple The indicies that would be overwritten by the argument proc_i . proc_i ProcAbstract An instance derived from ProcAbstract , this sub-processor would be used for overriding one or more indicies. Properties \u00b6 num_procs \u00b6 proc . num_procs The number of sub-processors for this class. If one sub-processor is used for managing multiple indicies, it will be count for mutiple times. parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured. In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Certianly, it will always be proc . has_ind = True for this class. Examples \u00b6 There are many kinds of method for using this class. For example, Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcMerge ([ mdnc . data . preprocs . ProcScaler (), mdnc . data . preprocs . ProcNSTScaler ( dim = 1 )]) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) Output Processed shape: (5, 3) (4, 2) Processed mean: 4.440892098500626e-16 2.7755575615628914e-17 Processed range: 1.0 1.0 Inverse error: 0.0 0.0 Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import numpy as np import mdnc proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , inds = 0 , parent = mdnc . data . preprocs . ProcScaler ( inds = 1 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng () x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 3) (4, 2) (4, 2) Processed mean: -1.7763568394002506e-16 -1.8041124150158794e-16 -1.314226505400029e-14 Processed range: 1.0 1.0 1.0 Inverse error: 0.0 1.1102230246251565e-16 0.0 This class could be also used for merge customized processor. But the customized processor should ensure the input and output numbers are the same, for example, Example 3 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y ): return self . a * x , ( 2 * self . a ) * y def postprocess ( self , x , y ): return x / self . a , y / ( 2 * self . a ) proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , parent = ProcDerived ( a = 2.0 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng () x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 3) (4, 2) (4, 2) Processed mean: -1.7763568394002506e-16 0.0 -5.273559366969494e-16 Processed range: 1.0 1.0 1.0 Inverse error: 0.0 2.220446049250313e-16 0.0","title":"<span class='magic-codeicon-class'>ProcMerge</span>"},{"location":"apis/data/preprocs/ProcMerge/#datapreprocsprocmerge","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcMerge ( procs = None , num_procs = None , parent = None ) Merge manager. This processor is inhomogeneous, and designed for merging different processors by a more efficient way. For example, p = ProcMerge ([ Proc1 ( ... ), Proc2 ( ... )]) Would apply Proc1 to the first argument, and Proc2 to the second argument. It is equivalent to p = Proc1 ( ... , inds = 0 , parent = Proc2 ( ... , inds = 1 )) This class should not be used if any sub-processor does not return the results with the same number of the input variables (out-arg changed). One exception is, the parent of this class could be an out-arg changed processor. This API is more intuitive for users to concatenate serveral processors together. It will make your codes more readable and reduce the stack level of the processors.","title":"data.preprocs.ProcMerge"},{"location":"apis/data/preprocs/ProcMerge/#arguments","text":"Requries Argument Type Description procs ( ProcAbstract , ) A sequence of processors. Each processor is derived from mdnc.data.preprocs.ProcAbstract . Could be used for initializing this merge processor. num_procs object The number of input arguments of this processor. If not set, would infer the number from the length of the argument procs . At least one of procs or num_procs needs to be specified. The two arguments could be specified together. parent ProcAbstract An instance derived from mdnc.data.preprocs.ProcAbstract . This instance would be used as the parent of the current instance. Warning The argument num_procs should be greater than procs , if both num_procs and procs are specified.","title":"Arguments"},{"location":"apis/data/preprocs/ProcMerge/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcMerge/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcMerge/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The n th variable would be sent to the n th processor configured for proc . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcMerge/#operators","text":"","title":"Operators"},{"location":"apis/data/preprocs/ProcMerge/#__getitem__","text":"proc_i = proc [ idx ] Get the i th sub-processor. Warning If one sub-processor is managing multiple indicies, the returned sub-processor would always be same for those indicies. For example, Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) proc_1 = proc [ 1 ] proc_2 = proc [ 2 ] print ( proc_m is proc_1 , proc_m is proc_2 ) This behavior is important if your proc_m is an inhomogeneous processor. It means although you get proc_2 by proc [ 2 ] , you still need to place your argument as the 2 nd input when using proc_2 . Requries Argument Type Description idx int The index of the sub-processor. Returns Argument Description proc_i An instance derived from ProcAbstract , the i th sub-processor.","title":" __getitem__"},{"location":"apis/data/preprocs/ProcMerge/#__setitem__","text":"proc [ idx ] = proc_i Info This method supports multiple assignment, for example: Codes proc = ProcMerge ( num_procs = 3 ) proc [:] = Proc1 ( ... ) proc [ 1 : 2 ] = Proc2 ( ... ) This would be equivalent to Codes proc_m = Proc2 ( ... ) proc = ProcMerge ([ Proc1 ( ... ), proc_m , proc_m ]) Requries Argument Type Description idx int or slice or tuple The indicies that would be overwritten by the argument proc_i . proc_i ProcAbstract An instance derived from ProcAbstract , this sub-processor would be used for overriding one or more indicies.","title":" __setitem__"},{"location":"apis/data/preprocs/ProcMerge/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcMerge/#num_procs","text":"proc . num_procs The number of sub-processors for this class. If one sub-processor is used for managing multiple indicies, it will be count for mutiple times.","title":" num_procs"},{"location":"apis/data/preprocs/ProcMerge/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcMerge/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured. In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Certianly, it will always be proc . has_ind = True for this class.","title":" has_ind"},{"location":"apis/data/preprocs/ProcMerge/#examples","text":"There are many kinds of method for using this class. For example, Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import mdnc proc = mdnc . data . preprocs . ProcMerge ([ mdnc . data . preprocs . ProcScaler (), mdnc . data . preprocs . ProcNSTScaler ( dim = 1 )]) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) Output Processed shape: (5, 3) (4, 2) Processed mean: 4.440892098500626e-16 2.7755575615628914e-17 Processed range: 1.0 1.0 Inverse error: 0.0 0.0 Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import numpy as np import mdnc proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , inds = 0 , parent = mdnc . data . preprocs . ProcScaler ( inds = 1 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng () x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 3) (4, 2) (4, 2) Processed mean: -1.7763568394002506e-16 -1.8041124150158794e-16 -1.314226505400029e-14 Processed range: 1.0 1.0 1.0 Inverse error: 0.0 1.1102230246251565e-16 0.0 This class could be also used for merge customized processor. But the customized processor should ensure the input and output numbers are the same, for example, Example 3 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import numpy as np import mdnc class ProcDerived ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , a , parent = None ): super () . __init__ ( parent = parent , _disable_inds = True ) self . a = a def preprocess ( self , x , y ): return self . a * x , ( 2 * self . a ) * y def postprocess ( self , x , y ): return x / self . a , y / ( 2 * self . a ) proc1 = mdnc . data . preprocs . ProcScaler () proc2 = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , parent = ProcDerived ( a = 2.0 )) proc = mdnc . data . preprocs . ProcMerge ( num_procs = 3 ) proc [ 0 ] = proc1 proc [ 1 :] = proc2 random_rng = np . random . default_rng () x , y , z = random_rng . normal ( loc =- 1.0 , scale = 0.1 , size = [ 5 , 3 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]), random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 4 , 2 ]) x_ , y_ , z_ = proc . preprocess ( x , y , z ) xr , yr , zr = proc . postprocess ( x_ , y_ , z_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape , z_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ ), np . mean ( z_ )) print ( 'Processed range:' , np . amax ( np . abs ( x_ )), np . amax ( np . abs ( y_ )), np . amax ( np . abs ( z_ ))) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr )), np . amax ( np . abs ( z - zr ))) Output Processed shape: (5, 3) (4, 2) (4, 2) Processed mean: -1.7763568394002506e-16 0.0 -5.273559366969494e-16 Processed range: 1.0 1.0 1.0 Inverse error: 0.0 2.220446049250313e-16 0.0","title":"Examples"},{"location":"apis/data/preprocs/ProcNSTFilter1d/","text":"data.preprocs.ProcNSTFilter1d \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , patch_length = 128 , patch_step = 64 , kaiser_coef = 1.0 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D non-stationary IIR (or FIR) band-pass filters (also supports low-pass or high-pass filter). The non-stationary filter is implemented by a sliding window. The overlapped windows would slide along the chosen axis, and the filter would be performed inside each window. After all windows filtered, they are stitched together. The filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcNSTFilter1d ( axis = 1 , parent = ProcNSTFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing. Arguments \u00b6 Requries Argument Type Description axis int The axis where we apply the 1D filter. length int The length of the to be processed data (along the chosen axis). patch_length int The length of the sliding windows. patch_step int The step between two sliding windows. This values should be smaller than patch_length , to make the windows overlapped. kaiser_coef float The coefficent of the Kaiser window taping function for each window. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description fft Use fft and sharp cut-off frequencies to perform the filter. This is an FIR filter. The filter_args would not be applied with this filter type. butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40 Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , filter_type = 'fft' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output","title":"<span class='magic-codeicon-class'>ProcNSTFilter1d</span>"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#datapreprocsprocnstfilter1d","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , patch_length = 128 , patch_step = 64 , kaiser_coef = 1.0 , band_low = 3.0 , band_high = 15.0 , nyquist = 500.0 , filter_type = 'butter' , out_type = 'sosfilt2' , filter_args = None , inds = None , parent = None ) This is a homogeneous processor. It is an implementation of the 1D non-stationary IIR (or FIR) band-pass filters (also supports low-pass or high-pass filter). The non-stationary filter is implemented by a sliding window. The overlapped windows would slide along the chosen axis, and the filter would be performed inside each window. After all windows filtered, they are stitched together. The filer would be only performed on the chosen axis. If users want to filter the data along multiple dimensions, using a stack of this instance may be needed, for example: proc = ProcNSTFilter1d ( axis = 1 , parent = ProcNSTFilter1d ( axis = 2 , ... )) Warning Plese pay attention to the results. This operation is not invertible, and the postprocess() would do nothing.","title":"data.preprocs.ProcNSTFilter1d"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#arguments","text":"Requries Argument Type Description axis int The axis where we apply the 1D filter. length int The length of the to be processed data (along the chosen axis). patch_length int The length of the sliding windows. patch_step int The step between two sliding windows. This values should be smaller than patch_length , to make the windows overlapped. kaiser_coef float The coefficent of the Kaiser window taping function for each window. band_low float The lower cut-off frequency. If only set this value, the filter would become high-pass. band_high float The higher cut-off frequency. If only set this value, the filter become high-pass. nyquist float The nyquist frequency of the data. filter_type str The IIR filter type, could be: 'butter' , 'cheby1' , 'cheby2' , 'ellip' , or 'bessel' . See the filter type list to check the details. out_type str The output filter paramter type, could be 'sosfilt2' , 'filt2' , 'sos' , 'ba' . See the out type list to check the details. filter_args str A dictionary including other filter arguments, not all arguments are required for each filter, could contain 'order' , 'ripple' , 'attenuation' . See the filter argument list to check the details. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Filter types Argument Description fft Use fft and sharp cut-off frequencies to perform the filter. This is an FIR filter. The filter_args would not be applied with this filter type. butter Butterworth IIR filter, see scipy.signal.butter . cheby1 Chebyshev type I IIR filter, see scipy.signal.cheby1 . cheby2 Chebyshev type II IIR filter, see scipy.signal.cheby2 . ellip Elliptic (Cauer) IIR filter, see scipy.signal.ellip . bessel Bessel/Thomson IIR filter, see scipy.signal.bessel . Out types Argument Description sosfilt2 Forward-backward second-order filter coefficients, see scipy.signal.sosfiltfilt . filt2 Forward-backward first-order filter coefficients, see scipy.signal.filtfilt . sos Second-order filter coefficients, see scipy.signal.sosfilt . ba First-order filter coefficients, see scipy.signal.lfilter . Filter arguments The arguments in the following table are the default values of the filter_args . If one value is marked as , it means the argument is not available for this filter. Argument butter cheby1 cheby2 ellip bessel order 10 4 10 4 10 ripple 5 5 attenuation 40 40","title":"Arguments"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the filterd results for each argument. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. Nothing would be done during the post-processing stage of this processor, i.e. x = proc . postprocess ( x ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcNSTFilter1d/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcNSTFilter1d ( axis =- 1 , length = 1024 , filter_type = 'fft' , band_low = 3.0 , band_high = 15.0 , nyquist = 100 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 1 - 0.01 , high = 1 + 0.01 , size = [ 1 , 1024 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output","title":"Examples"},{"location":"apis/data/preprocs/ProcNSTScaler/","text":"data.preprocs.ProcNSTScaler \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTScaler ( dim , kernel_length = 9 , epsilon = 1e-6 , inds = None , parent = None ) This is a homogeneous processor. It would remove the lower-frequency part of the data (smoothed data), and use this part for normalization. The normalizer could be formulated as: \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{y}_n &= \\frac{\\mathbf{x}_n - \\boldsymbol{\\mu}_n}{\\min(\\boldsymbol{\\sigma}_n, \\varepsilon)}, \\\\ \\boldsymbol{\\mu}_n &= \\mathrm{Avg. pool}(\\mathbf{x}_n,~L), \\\\ \\boldsymbol{\\sigma}_n &= \\mathrm{Max. pool}(\\mathbf{x}_n - \\boldsymbol{\\mu}_n,~L). \\end{aligned} \\right. \\end{equation} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. The value \\(L\\) is the smoothing window length, i.e. kernel_length . The value \\(\\varepsilon\\) determines the lower bound of the divisor during the scaling. It is recommended to make kernel_length large enough especially when the data is very noisy. Arguments \u00b6 Requries Argument Type Description dim int The dimension of the input data, this value would also determine the dimension of the sliding window. Could be 1 , 2 , or 3 . kernel_length int or ( int , ) The length of the sliding window. Could provide a window shape by using a sequence. epsilon The lower bound of the divisor used for scaling. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the non-stationary re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the non-stationary scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 dim \u00b6 proc . dim The dimension of the input data. kernel_length \u00b6 proc . kernel_length The length of the sliding window when calculating the low-frequnecy shifting value and scaling value. epsilon \u00b6 proc . epsilon A value used as the lower bound of the divisor. This value is set small enough in most cases. parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , kernel_length = 9 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 100 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 200 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed max:' , np . amax ( x_ ), np . amax ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( x_ [ 0 ]) axs [ 1 ] . plot ( xr [ 0 ]) axs [ 2 ] . plot ( x [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Processed shape: (5, 100) (7, 200) Processed mean: -0.003896614253474869 -0.00024727896038973684 Processed max: 1.0 1.0 Inverse error: 8.881784197001252e-16 1.7763568394002505e-15","title":"<span class='magic-codeicon-class'>ProcNSTScaler</span>"},{"location":"apis/data/preprocs/ProcNSTScaler/#datapreprocsprocnstscaler","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcNSTScaler ( dim , kernel_length = 9 , epsilon = 1e-6 , inds = None , parent = None ) This is a homogeneous processor. It would remove the lower-frequency part of the data (smoothed data), and use this part for normalization. The normalizer could be formulated as: \\begin{equation} \\left\\{ \\begin{aligned} \\mathbf{y}_n &= \\frac{\\mathbf{x}_n - \\boldsymbol{\\mu}_n}{\\min(\\boldsymbol{\\sigma}_n, \\varepsilon)}, \\\\ \\boldsymbol{\\mu}_n &= \\mathrm{Avg. pool}(\\mathbf{x}_n,~L), \\\\ \\boldsymbol{\\sigma}_n &= \\mathrm{Max. pool}(\\mathbf{x}_n - \\boldsymbol{\\mu}_n,~L). \\end{aligned} \\right. \\end{equation} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. The value \\(L\\) is the smoothing window length, i.e. kernel_length . The value \\(\\varepsilon\\) determines the lower bound of the divisor during the scaling. It is recommended to make kernel_length large enough especially when the data is very noisy.","title":"data.preprocs.ProcNSTScaler"},{"location":"apis/data/preprocs/ProcNSTScaler/#arguments","text":"Requries Argument Type Description dim int The dimension of the input data, this value would also determine the dimension of the sliding window. Could be 1 , 2 , or 3 . kernel_length int or ( int , ) The length of the sliding window. Could provide a window shape by using a sequence. epsilon The lower bound of the divisor used for scaling. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcNSTScaler/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcNSTScaler/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the non-stationary re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcNSTScaler/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the non-stationary scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcNSTScaler/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcNSTScaler/#dim","text":"proc . dim The dimension of the input data.","title":" dim"},{"location":"apis/data/preprocs/ProcNSTScaler/#kernel_length","text":"proc . kernel_length The length of the sliding window when calculating the low-frequnecy shifting value and scaling value.","title":" kernel_length"},{"location":"apis/data/preprocs/ProcNSTScaler/#epsilon","text":"proc . epsilon A value used as the lower bound of the divisor. This value is set small enough in most cases.","title":" epsilon"},{"location":"apis/data/preprocs/ProcNSTScaler/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcNSTScaler/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcNSTScaler/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcNSTScaler ( dim = 1 , kernel_length = 9 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 100 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 200 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed max:' , np . amax ( x_ ), np . amax ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( x_ [ 0 ]) axs [ 1 ] . plot ( xr [ 0 ]) axs [ 2 ] . plot ( x [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Processed shape: (5, 100) (7, 200) Processed mean: -0.003896614253474869 -0.00024727896038973684 Processed max: 1.0 1.0 Inverse error: 8.881784197001252e-16 1.7763568394002505e-15","title":"Examples"},{"location":"apis/data/preprocs/ProcPad/","text":"data.preprocs.ProcPad \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcPad ( pad_width , inds = None , parent = None , ** kwargs ) This is a homogeneous processor. Use np.pad to pad the data. This processor supports all np.pad options. Besides, this processor also support cropping. If any element in the argument pad_width is negative, would perform cropping on that axis. For example: p = ProcPad ( pad_width = (( 5 , - 5 ),)) y = p ( x ) # x.shape=(20,), y.shape=(20,) In this case, the data is padded by 5 samples at the beginning, but cropped 5 samples at the end. This operator is not invertible when cropping is applied. The postprocess would try to revert the padding / cropping configurations to match the input data. Warning If cropping is used, this processor would be not invertible (unless we have the argument mode = 'wrap' ). The postprocess() method would try to pad the cropped part with the processed data. Arguments \u00b6 Requries Argument Type Description pad_width int or ( int , int ) (( int , int ), ... ) The padding_width argument of the np.pad function. If any element is negative, it means this elment is a cropping size. This argument only supports 3 cases: width : use the same padding / cropping width along all axes. ( begin , end ) : use the same padding / cropping length for both edges along all axes. (( begin , end ), ... ) : use different padding / cropping lengths for both edges along each axis. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the padding / cropping. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator is not invertible if cropping is used in preprocess() . In this case, the cropped part would be padded by processed data (y, ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 pad_width \u00b6 proc . pad_width The padding width of the processor. parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 )), mode = 'wrap' ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 ), ( - 10 , 10 )), mode = 'constant' , constant_values = 0.0 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 1 , ncols = 3 , sharex = True , sharey = True , figsize = ( 12 , 4 )) im1 = axs [ 0 ] . imshow ( t [ 2 ]) im2 = axs [ 1 ] . imshow ( t_b [ 0 ]) im3 = axs [ 2 ] . imshow ( data [ 0 ]) fig . colorbar ( im1 , ax = axs [ 0 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im2 , ax = axs [ 1 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im3 , ax = axs [ 2 ], pad = 0.1 , orientation = 'horizontal' ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show () Output","title":"<span class='magic-codeicon-class'>ProcPad</span>"},{"location":"apis/data/preprocs/ProcPad/#datapreprocsprocpad","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcPad ( pad_width , inds = None , parent = None , ** kwargs ) This is a homogeneous processor. Use np.pad to pad the data. This processor supports all np.pad options. Besides, this processor also support cropping. If any element in the argument pad_width is negative, would perform cropping on that axis. For example: p = ProcPad ( pad_width = (( 5 , - 5 ),)) y = p ( x ) # x.shape=(20,), y.shape=(20,) In this case, the data is padded by 5 samples at the beginning, but cropped 5 samples at the end. This operator is not invertible when cropping is applied. The postprocess would try to revert the padding / cropping configurations to match the input data. Warning If cropping is used, this processor would be not invertible (unless we have the argument mode = 'wrap' ). The postprocess() method would try to pad the cropped part with the processed data.","title":"data.preprocs.ProcPad"},{"location":"apis/data/preprocs/ProcPad/#arguments","text":"Requries Argument Type Description pad_width int or ( int , int ) (( int , int ), ... ) The padding_width argument of the np.pad function. If any element is negative, it means this elment is a cropping size. This argument only supports 3 cases: width : use the same padding / cropping width along all axes. ( begin , end ) : use the same padding / cropping length for both edges along all axes. (( begin , end ), ... ) : use different padding / cropping lengths for both edges along each axis. inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcPad/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcPad/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Perform the padding / cropping. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcPad/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator is not invertible if cropping is used in preprocess() . In this case, the cropped part would be padded by processed data (y, ) . If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcPad/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcPad/#pad_width","text":"proc . pad_width The padding width of the processor.","title":" pad_width"},{"location":"apis/data/preprocs/ProcPad/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcPad/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcPad/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 )), mode = 'wrap' ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( t [ 0 ]) axs [ 1 ] . plot ( t_b [ 0 ]) axs [ 2 ] . plot ( data [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcPad ( pad_width = (( 0 , 0 ), ( 10 , - 10 ), ( - 10 , 10 )), mode = 'constant' , constant_values = 0.0 ) random_rng = np . random . default_rng () data = random_rng . uniform ( low = 0.0 , high = 1.0 , size = [ 10 , 30 , 30 ]) t = proc . preprocess ( data ) t_b = proc . postprocess ( t ) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 1 , ncols = 3 , sharex = True , sharey = True , figsize = ( 12 , 4 )) im1 = axs [ 0 ] . imshow ( t [ 2 ]) im2 = axs [ 1 ] . imshow ( t_b [ 0 ]) im3 = axs [ 2 ] . imshow ( data [ 0 ]) fig . colorbar ( im1 , ax = axs [ 0 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im2 , ax = axs [ 1 ], pad = 0.1 , orientation = 'horizontal' ) fig . colorbar ( im3 , ax = axs [ 2 ], pad = 0.1 , orientation = 'horizontal' ) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw data' ) plt . tight_layout () plt . show () Output","title":"Examples"},{"location":"apis/data/preprocs/ProcScaler/","text":"data.preprocs.ProcScaler \u00b6 Class \u00b7 Source proc = mdnc . data . preprocs . ProcScaler ( shift = None , scale = None , axis =- 1 , inds = None , parent = None ) This is a homogeneous processor. It accepts two variables shift ( \\(\\mu\\) ), scale ( \\(\\sigma\\) ) to perform the following normalization: \\begin{align} \\mathbf{y}_n = \\frac{1}{\\sigma} ( \\mathbf{x}_n - \\mu ), \\end{align} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. If not setting \\(\\mu\\) , would use the mean value of the input mini-batch to shift the argument, i.e. \\(\\mu_n = \\overline{\\mathbf{x}}_n\\) ; If not setting \\(\\sigma\\) , would use the max-abs value of the input mini-batch to scale the argument, i.e. \\(\\sigma_n = \\max |\\mathbf{x}_n - \\mu_n|\\) . The above two caulation is estimated on mini-batches. This configuration may cause unstable issues when the input mini-batches are not i.i.d.. Therefore, we recommend users to always set shift and scale manually. Arguments \u00b6 Requries Argument Type Description shift int or np . ndarray The \\(\\mu\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would shift the mean value of each mini-batch to 0. scale int or np . ndarray The \\(\\sigma\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would scale the max-abs value of each mini-batch to 1. axis int or ( int , ) The axis used for calculating the normalization parameters. If given a sequence, would calculate the paramters among higher-dimensional data. Only used when shift or scale is not None . inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () . Methods \u00b6 preprocess \u00b6 y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data. postprocess \u00b6 x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data. Properties \u00b6 shift \u00b6 proc . shift The shifting value \\(\\mu\\) . scale \u00b6 proc . scale The scaling value \\(\\sigma\\) . parent \u00b6 proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None . has_ind \u00b6 proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\". Examples \u00b6 The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcScaler ( shift = 1.0 , scale = 3.0 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 4 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 5 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed std:' , np . std ( x_ ), np . std ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( x_ [ 0 ]) axs [ 1 ] . plot ( xr [ 0 ]) axs [ 2 ] . plot ( x [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Processed shape: (5, 4) (7, 5) Processed mean: 0.3419675618982352 -0.6442542139897679 Processed std: 1.1930992341525517 2.3083982027807157 Inverse error: 0.0 0.0 Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing import mdnc random_rng = np . random . default_rng () x = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 100 , 5 ]) rsc = preprocessing . RobustScaler () rsc . fit ( x ) proc = mdnc . data . preprocs . ProcScaler ( shift = np . expand_dims ( rsc . center_ , axis = 0 ), scale = np . expand_dims ( rsc . scale_ , axis = 0 )) x_ = proc . preprocess ( x ) x_sl = rsc . transform ( x ) x_r = proc . postprocess ( x_ ) x_r_sl = rsc . inverse_transform ( x_sl ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x_sl ))) print ( 'Inverse error:' , np . amax ( np . abs ( x_r - x_r_sl ))) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( x_ [ 0 ], label = 'ProcScaler' ) axs [ 0 ] . plot ( x_sl [ 0 ], label = 'RobustScaler' ) axs [ 0 ] . legend ( loc = 'lower right' ) axs [ 1 ] . plot ( x_r [ 0 ], label = 'ProcScaler' ) axs [ 1 ] . plot ( x_r_sl [ 0 ], label = 'RobustScaler' ) axs [ 1 ] . legend ( loc = 'lower right' ) axs [ 2 ] . plot ( x [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Processed error: 0.0 Inverse error: 0.0","title":"<span class='magic-codeicon-class'>ProcScaler</span>"},{"location":"apis/data/preprocs/ProcScaler/#datapreprocsprocscaler","text":"Class \u00b7 Source proc = mdnc . data . preprocs . ProcScaler ( shift = None , scale = None , axis =- 1 , inds = None , parent = None ) This is a homogeneous processor. It accepts two variables shift ( \\(\\mu\\) ), scale ( \\(\\sigma\\) ) to perform the following normalization: \\begin{align} \\mathbf{y}_n = \\frac{1}{\\sigma} ( \\mathbf{x}_n - \\mu ), \\end{align} where \\(\\mathbf{x}_n\\) and \\(\\mathbf{y}_n\\) are the i th input argument and the corresponding output argument respectively. If not setting \\(\\mu\\) , would use the mean value of the input mini-batch to shift the argument, i.e. \\(\\mu_n = \\overline{\\mathbf{x}}_n\\) ; If not setting \\(\\sigma\\) , would use the max-abs value of the input mini-batch to scale the argument, i.e. \\(\\sigma_n = \\max |\\mathbf{x}_n - \\mu_n|\\) . The above two caulation is estimated on mini-batches. This configuration may cause unstable issues when the input mini-batches are not i.i.d.. Therefore, we recommend users to always set shift and scale manually.","title":"data.preprocs.ProcScaler"},{"location":"apis/data/preprocs/ProcScaler/#arguments","text":"Requries Argument Type Description shift int or np . ndarray The \\(\\mu\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would shift the mean value of each mini-batch to 0. scale int or np . ndarray The \\(\\sigma\\) variable used for shifting the mean value of mini-batches. This value could be an np . ndarray supporting broadcasting. If set None , would scale the max-abs value of each mini-batch to 1. axis int or ( int , ) The axis used for calculating the normalization parameters. If given a sequence, would calculate the paramters among higher-dimensional data. Only used when shift or scale is not None . inds int or ( int , ) Index or indicies of variables where the user implemented methods would be broadcasted. The variables not listed in this argument would be passed to the output without any processing. If set None , methods would be broadcasted to all variables. parent ProcAbstract Another instance derived from mdnc.data.preprocs.ProcAbstract . The output of parent . preprocess () would be used as the input of self . preprocess () . The input of self . postprocess () would be used as the input of parent . preprocess () .","title":"Arguments"},{"location":"apis/data/preprocs/ProcScaler/#methods","text":"","title":"Methods"},{"location":"apis/data/preprocs/ProcScaler/#preprocess","text":"y_1 , y_2 , ... = proc . preprocess ( x_1 , x_2 , ... ) The preprocess function. Calculate the re-scaled values from the input variables. If parent exists, the input of this function comes from the output of parent . preprocess () . Otherwise, the input would comes from the input varibable directly. Requries Argument Type Description (x, ) np . ndarray A sequence of variables. Each variable comes from the parent's outputs (if parent exists). The output of this method would be passed as the input of the next processor (if this processor is used as parent). Returns Argument Description (y, ) A sequence of np . ndarray , the final preprocessed data.","title":" preprocess"},{"location":"apis/data/preprocs/ProcScaler/#postprocess","text":"x_1 , x_2 , ... = proc . postprocess ( y_1 , y_2 , ... ) The postprocess function. The inverse operator of the scaling. If parent exists, the output of this function would be passed as the input of parent . postprocess () . Otherwise, the output would be returned to users directly. Requries Argument Type Description (y, ) np . ndarray A sequence of variables. Each variable comes from the next processors's outputs (if parent exists). The output of this method would be passed as the input of the parent's method. Returns Argument Description (x, ) A sequence of np . ndarray , the final postprocessed data.","title":" postprocess"},{"location":"apis/data/preprocs/ProcScaler/#properties","text":"","title":"Properties"},{"location":"apis/data/preprocs/ProcScaler/#shift","text":"proc . shift The shifting value \\(\\mu\\) .","title":" shift"},{"location":"apis/data/preprocs/ProcScaler/#scale","text":"proc . scale The scaling value \\(\\sigma\\) .","title":" scale"},{"location":"apis/data/preprocs/ProcScaler/#parent","text":"proc . parent The parent processor of this instance. The processor is also a derived class of ProcAbstract . If the parent does not exist, would return None .","title":" parent"},{"location":"apis/data/preprocs/ProcScaler/#has_ind","text":"proc . has_ind A bool flag, showing whether this processor and its all parent processors have inds configured or initialized with _disable_inds . In this case, the arguments of preprocess() and postprocess() would not share the same operation. We call such kind of processors \"Inhomogeneous processors\".","title":" has_ind"},{"location":"apis/data/preprocs/ProcScaler/#examples","text":"The processor need to be derived. We have two ways to implement the derivation, see the following examples. Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import numpy as np import matplotlib.pyplot as plt import mdnc proc = mdnc . data . preprocs . ProcScaler ( shift = 1.0 , scale = 3.0 ) random_rng = np . random . default_rng () x , y = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 5 , 4 ]), random_rng . normal ( loc = 1.0 , scale = 6.0 , size = [ 7 , 5 ]) x_ , y_ = proc . preprocess ( x , y ) xr , yr = proc . postprocess ( x_ , y_ ) print ( 'Processed shape:' , x_ . shape , y_ . shape ) print ( 'Processed mean:' , np . mean ( x_ ), np . mean ( y_ )) print ( 'Processed std:' , np . std ( x_ ), np . std ( y_ )) print ( 'Inverse error:' , np . amax ( np . abs ( x - xr )), np . amax ( np . abs ( y - yr ))) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( x_ [ 0 ]) axs [ 1 ] . plot ( xr [ 0 ]) axs [ 2 ] . plot ( x [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Processed shape: (5, 4) (7, 5) Processed mean: 0.3419675618982352 -0.6442542139897679 Processed std: 1.1930992341525517 2.3083982027807157 Inverse error: 0.0 0.0 Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing import mdnc random_rng = np . random . default_rng () x = random_rng . normal ( loc = 1.0 , scale = 3.0 , size = [ 100 , 5 ]) rsc = preprocessing . RobustScaler () rsc . fit ( x ) proc = mdnc . data . preprocs . ProcScaler ( shift = np . expand_dims ( rsc . center_ , axis = 0 ), scale = np . expand_dims ( rsc . scale_ , axis = 0 )) x_ = proc . preprocess ( x ) x_sl = rsc . transform ( x ) x_r = proc . postprocess ( x_ ) x_r_sl = rsc . inverse_transform ( x_sl ) print ( 'Processed error:' , np . amax ( np . abs ( x_ - x_sl ))) print ( 'Inverse error:' , np . amax ( np . abs ( x_r - x_r_sl ))) with mdnc . utils . draw . setFigure ( font_size = 12 ): fig , axs = plt . subplots ( nrows = 3 , ncols = 1 , sharex = True , figsize = ( 12 , 5 )) axs [ 0 ] . plot ( x_ [ 0 ], label = 'ProcScaler' ) axs [ 0 ] . plot ( x_sl [ 0 ], label = 'RobustScaler' ) axs [ 0 ] . legend ( loc = 'lower right' ) axs [ 1 ] . plot ( x_r [ 0 ], label = 'ProcScaler' ) axs [ 1 ] . plot ( x_r_sl [ 0 ], label = 'RobustScaler' ) axs [ 1 ] . legend ( loc = 'lower right' ) axs [ 2 ] . plot ( x [ 0 ]) axs [ 0 ] . set_ylabel ( 'Preprocessing' ) axs [ 1 ] . set_ylabel ( 'Inversed \\n preprocessing' ) axs [ 2 ] . set_ylabel ( 'Raw \\n data' ) plt . tight_layout () plt . show () Output Processed error: 0.0 Inverse error: 0.0","title":"Examples"},{"location":"apis/data/sequence/MPSequence/","text":"data.sequence.MPSequence \u00b6 Class \u00b7 Context \u00b7 Source manager = mdnc . data . sequence . MPSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: multiprocessing module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MPSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MPSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MPSequence would store the indexer during the initialization. When the start() method is invoked, two process pools would be created. The first pool maintains several processes, each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MPSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes or threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Examples \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start () as mng : for i in mng : print ( i ) Output tensor([0.], device='cuda:0') tensor([1.], device='cuda:0') tensor([2.], device='cuda:0') tensor([3.], device='cuda:0') tensor([4.], device='cuda:0') tensor([5.], device='cuda:0') tensor([6.], device='cuda:0') tensor([7.], device='cuda:0') tensor([8.], device='cuda:0') tensor([9.], device='cuda:0') tensor([10.], device='cuda:0') tensor([11.], device='cuda:0') tensor([12.], device='cuda:0') tensor([13.], device='cuda:0') tensor([14.], device='cuda:0') tensor([15.], device='cuda:0') tensor([16.], device='cuda:0') tensor([17.], device='cuda:0') tensor([18.], device='cuda:0') tensor([19.], device='cuda:0') tensor([20.], device='cuda:0') tensor([21.], device='cuda:0') tensor([22.], device='cuda:0') tensor([23.], device='cuda:0') tensor([24.], device='cuda:0') tensor([25.], device='cuda:0') tensor([26.], device='cuda:0') tensor([27.], device='cuda:0') tensor([28.], device='cuda:0') tensor([29.], device='cuda:0') tensor([30.], device='cuda:0') tensor([31.], device='cuda:0') tensor([32.], device='cuda:0') tensor([33.], device='cuda:0') tensor([34.], device='cuda:0') tensor([35.], device='cuda:0') tensor([36.], device='cuda:0') tensor([37.], device='cuda:0') tensor([38.], device='cuda:0') tensor([39.], device='cuda:0') tensor([40.], device='cuda:0') tensor([41.], device='cuda:0') tensor([42.], device='cuda:0') tensor([43.], device='cuda:0') tensor([44.], device='cuda:0') tensor([45.], device='cuda:0') tensor([46.], device='cuda:0') tensor([47.], device='cuda:0') tensor([48.], device='cuda:0') tensor([49.], device='cuda:0') tensor([50.], device='cuda:0') tensor([51.], device='cuda:0') tensor([52.], device='cuda:0') tensor([53.], device='cuda:0') tensor([54.], device='cuda:0') tensor([55.], device='cuda:0') tensor([56.], device='cuda:0') tensor([57.], device='cuda:0') tensor([58.], device='cuda:0') tensor([59.], device='cuda:0') tensor([60.], device='cuda:0') tensor([61.], device='cuda:0') tensor([62.], device='cuda:0') tensor([63.], device='cuda:0') tensor([64.], device='cuda:0') tensor([65.], device='cuda:0') tensor([66.], device='cuda:0') tensor([67.], device='cuda:0') tensor([68.], device='cuda:0') tensor([69.], device='cuda:0') tensor([70.], device='cuda:0') tensor([71.], device='cuda:0') tensor([72.], device='cuda:0') tensor([73.], device='cuda:0') tensor([74.], device='cuda:0') tensor([75.], device='cuda:0') tensor([76.], device='cuda:0') tensor([77.], device='cuda:0') tensor([78.], device='cuda:0') tensor([79.], device='cuda:0') tensor([80.], device='cuda:0') tensor([81.], device='cuda:0') tensor([82.], device='cuda:0') tensor([83.], device='cuda:0') tensor([84.], device='cuda:0') tensor([85.], device='cuda:0') tensor([86.], device='cuda:0') tensor([87.], device='cuda:0') tensor([88.], device='cuda:0') tensor([89.], device='cuda:0') tensor([90.], device='cuda:0') tensor([91.], device='cuda:0') tensor([92.], device='cuda:0') tensor([93.], device='cuda:0') tensor([94.], device='cuda:0') tensor([95.], device='cuda:0') tensor([96.], device='cuda:0') tensor([97.], device='cuda:0') tensor([98.], device='cuda:0') tensor([99.], device='cuda:0') tensor([100.], device='cuda:0') tensor([101.], device='cuda:0') tensor([102.], device='cuda:0') tensor([103.], device='cuda:0') tensor([104.], device='cuda:0') tensor([105.], device='cuda:0') tensor([106.], device='cuda:0') tensor([107.], device='cuda:0') tensor([108.], device='cuda:0') tensor([109.], device='cuda:0') tensor([110.], device='cuda:0') tensor([111.], device='cuda:0') tensor([112.], device='cuda:0') tensor([113.], device='cuda:0') tensor([114.], device='cuda:0') tensor([115.], device='cuda:0') tensor([116.], device='cuda:0') tensor([117.], device='cuda:0') tensor([118.], device='cuda:0') tensor([119.], device='cuda:0') tensor([120.], device='cuda:0') tensor([121.], device='cuda:0') tensor([122.], device='cuda:0') tensor([123.], device='cuda:0') tensor([124.], device='cuda:0') tensor([125.], device='cuda:0') tensor([126.], device='cuda:0') tensor([127.], device='cuda:0') tensor([128.], device='cuda:0') tensor([129.], device='cuda:0') tensor([130.], device='cuda:0') tensor([131.], device='cuda:0') tensor([132.], device='cuda:0') tensor([133.], device='cuda:0') tensor([134.], device='cuda:0') tensor([135.], device='cuda:0') tensor([136.], device='cuda:0') tensor([137.], device='cuda:0') tensor([138.], device='cuda:0') tensor([139.], device='cuda:0') tensor([140.], device='cuda:0') tensor([141.], device='cuda:0') tensor([142.], device='cuda:0') tensor([143.], device='cuda:0') tensor([144.], device='cuda:0') tensor([145.], device='cuda:0') tensor([146.], device='cuda:0') tensor([147.], device='cuda:0') tensor([148.], device='cuda:0') tensor([149.], device='cuda:0') tensor([150.], device='cuda:0') tensor([151.], device='cuda:0') tensor([152.], device='cuda:0') tensor([153.], device='cuda:0') tensor([154.], device='cuda:0') tensor([155.], device='cuda:0') tensor([156.], device='cuda:0') tensor([157.], device='cuda:0') tensor([158.], device='cuda:0') tensor([159.], device='cuda:0') tensor([160.], device='cuda:0') tensor([161.], device='cuda:0') tensor([162.], device='cuda:0') tensor([163.], device='cuda:0') tensor([164.], device='cuda:0') tensor([165.], device='cuda:0') tensor([166.], device='cuda:0') tensor([167.], device='cuda:0') tensor([168.], device='cuda:0') tensor([169.], device='cuda:0') tensor([170.], device='cuda:0') tensor([171.], device='cuda:0') tensor([172.], device='cuda:0') tensor([173.], device='cuda:0') tensor([174.], device='cuda:0') tensor([175.], device='cuda:0') tensor([176.], device='cuda:0') tensor([177.], device='cuda:0') tensor([178.], device='cuda:0') tensor([179.], device='cuda:0') tensor([180.], device='cuda:0') tensor([181.], device='cuda:0') tensor([182.], device='cuda:0') tensor([183.], device='cuda:0') tensor([184.], device='cuda:0') tensor([185.], device='cuda:0') tensor([186.], device='cuda:0') tensor([187.], device='cuda:0') tensor([188.], device='cuda:0') tensor([189.], device='cuda:0') tensor([190.], device='cuda:0') tensor([191.], device='cuda:0') tensor([192.], device='cuda:0') tensor([193.], device='cuda:0') tensor([194.], device='cuda:0') tensor([195.], device='cuda:0') tensor([196.], device='cuda:0') tensor([197.], device='cuda:0') tensor([198.], device='cuda:0') tensor([199.], device='cuda:0') tensor([200.], device='cuda:0') tensor([201.], device='cuda:0') tensor([202.], device='cuda:0') tensor([203.], device='cuda:0') tensor([204.], device='cuda:0') tensor([205.], device='cuda:0') tensor([206.], device='cuda:0') tensor([207.], device='cuda:0') tensor([208.], device='cuda:0') tensor([209.], device='cuda:0') tensor([210.], device='cuda:0') tensor([211.], device='cuda:0') tensor([212.], device='cuda:0') tensor([213.], device='cuda:0') tensor([214.], device='cuda:0') tensor([215.], device='cuda:0') tensor([216.], device='cuda:0') tensor([217.], device='cuda:0') tensor([218.], device='cuda:0') tensor([219.], device='cuda:0') tensor([220.], device='cuda:0') tensor([221.], device='cuda:0') tensor([222.], device='cuda:0') tensor([223.], device='cuda:0') tensor([224.], device='cuda:0') tensor([225.], device='cuda:0') tensor([226.], device='cuda:0') tensor([227.], device='cuda:0') tensor([228.], device='cuda:0') tensor([229.], device='cuda:0') tensor([230.], device='cuda:0') tensor([231.], device='cuda:0') tensor([232.], device='cuda:0') tensor([233.], device='cuda:0') tensor([234.], device='cuda:0') tensor([235.], device='cuda:0') tensor([236.], device='cuda:0') tensor([237.], device='cuda:0') tensor([238.], device='cuda:0') tensor([239.], device='cuda:0') tensor([240.], device='cuda:0') tensor([241.], device='cuda:0') tensor([242.], device='cuda:0') tensor([243.], device='cuda:0') tensor([244.], device='cuda:0') tensor([245.], device='cuda:0') tensor([246.], device='cuda:0') tensor([247.], device='cuda:0') tensor([248.], device='cuda:0') tensor([249.], device='cuda:0') tensor([250.], device='cuda:0') tensor([251.], device='cuda:0') tensor([252.], device='cuda:0') tensor([253.], device='cuda:0') tensor([254.], device='cuda:0') tensor([255.], device='cuda:0') tensor([256.], device='cuda:0') tensor([257.], device='cuda:0') tensor([258.], device='cuda:0') tensor([259.], device='cuda:0') tensor([260.], device='cuda:0') tensor([261.], device='cuda:0') tensor([262.], device='cuda:0') tensor([263.], device='cuda:0') tensor([264.], device='cuda:0') tensor([265.], device='cuda:0') tensor([266.], device='cuda:0') tensor([267.], device='cuda:0') tensor([268.], device='cuda:0') tensor([269.], device='cuda:0') tensor([270.], device='cuda:0') tensor([271.], device='cuda:0') tensor([272.], device='cuda:0') tensor([273.], device='cuda:0') tensor([274.], device='cuda:0') tensor([275.], device='cuda:0') tensor([276.], device='cuda:0') tensor([277.], device='cuda:0') tensor([278.], device='cuda:0') tensor([279.], device='cuda:0') tensor([280.], device='cuda:0') tensor([281.], device='cuda:0') tensor([282.], device='cuda:0') tensor([283.], device='cuda:0') tensor([284.], device='cuda:0') tensor([285.], device='cuda:0') tensor([286.], device='cuda:0') tensor([287.], device='cuda:0') tensor([288.], device='cuda:0') tensor([289.], device='cuda:0') tensor([290.], device='cuda:0') tensor([291.], device='cuda:0') tensor([292.], device='cuda:0') tensor([293.], device='cuda:0') tensor([294.], device='cuda:0') tensor([295.], device='cuda:0') tensor([296.], device='cuda:0') tensor([297.], device='cuda:0') tensor([298.], device='cuda:0') tensor([299.], device='cuda:0') tensor([300.], device='cuda:0') tensor([301.], device='cuda:0') tensor([302.], device='cuda:0') tensor([303.], device='cuda:0') tensor([304.], device='cuda:0') tensor([305.], device='cuda:0') tensor([306.], device='cuda:0') tensor([307.], device='cuda:0') tensor([308.], device='cuda:0') tensor([309.], device='cuda:0') tensor([310.], device='cuda:0') tensor([311.], device='cuda:0') tensor([312.], device='cuda:0') tensor([313.], device='cuda:0') tensor([314.], device='cuda:0') tensor([315.], device='cuda:0') tensor([316.], device='cuda:0') tensor([317.], device='cuda:0') tensor([318.], device='cuda:0') tensor([319.], device='cuda:0') tensor([320.], device='cuda:0') tensor([321.], device='cuda:0') tensor([322.], device='cuda:0') tensor([323.], device='cuda:0') tensor([324.], device='cuda:0') tensor([325.], device='cuda:0') tensor([326.], device='cuda:0') tensor([327.], device='cuda:0') tensor([328.], device='cuda:0') tensor([329.], device='cuda:0') tensor([330.], device='cuda:0') tensor([331.], device='cuda:0') tensor([332.], device='cuda:0') tensor([333.], device='cuda:0') tensor([334.], device='cuda:0') tensor([335.], device='cuda:0') tensor([336.], device='cuda:0') tensor([337.], device='cuda:0') tensor([338.], device='cuda:0') tensor([339.], device='cuda:0') tensor([340.], device='cuda:0') tensor([341.], device='cuda:0') tensor([342.], device='cuda:0') tensor([343.], device='cuda:0') tensor([344.], device='cuda:0') tensor([345.], device='cuda:0') tensor([346.], device='cuda:0') tensor([347.], device='cuda:0') tensor([348.], device='cuda:0') tensor([349.], device='cuda:0') tensor([350.], device='cuda:0') tensor([351.], device='cuda:0') tensor([352.], device='cuda:0') tensor([353.], device='cuda:0') tensor([354.], device='cuda:0') tensor([355.], device='cuda:0') tensor([356.], device='cuda:0') tensor([357.], device='cuda:0') tensor([358.], device='cuda:0') tensor([359.], device='cuda:0') tensor([360.], device='cuda:0') tensor([361.], device='cuda:0') tensor([362.], device='cuda:0') tensor([363.], device='cuda:0') tensor([364.], device='cuda:0') tensor([365.], device='cuda:0') tensor([366.], device='cuda:0') tensor([367.], device='cuda:0') tensor([368.], device='cuda:0') tensor([369.], device='cuda:0') tensor([370.], device='cuda:0') tensor([371.], device='cuda:0') tensor([372.], device='cuda:0') tensor([373.], device='cuda:0') tensor([374.], device='cuda:0') tensor([375.], device='cuda:0') tensor([376.], device='cuda:0') tensor([377.], device='cuda:0') tensor([378.], device='cuda:0') tensor([379.], device='cuda:0') tensor([380.], device='cuda:0') tensor([381.], device='cuda:0') tensor([382.], device='cuda:0') tensor([383.], device='cuda:0') tensor([384.], device='cuda:0') tensor([385.], device='cuda:0') tensor([386.], device='cuda:0') tensor([387.], device='cuda:0') tensor([388.], device='cuda:0') tensor([389.], device='cuda:0') tensor([390.], device='cuda:0') tensor([391.], device='cuda:0') tensor([392.], device='cuda:0') tensor([393.], device='cuda:0') tensor([394.], device='cuda:0') tensor([395.], device='cuda:0') tensor([396.], device='cuda:0') tensor([397.], device='cuda:0') tensor([398.], device='cuda:0') tensor([399.], device='cuda:0') tensor([400.], device='cuda:0') tensor([401.], device='cuda:0') tensor([402.], device='cuda:0') tensor([403.], device='cuda:0') tensor([404.], device='cuda:0') tensor([405.], device='cuda:0') tensor([406.], device='cuda:0') tensor([407.], device='cuda:0') tensor([408.], device='cuda:0') tensor([409.], device='cuda:0') tensor([410.], device='cuda:0') tensor([411.], device='cuda:0') tensor([412.], device='cuda:0') tensor([413.], device='cuda:0') tensor([414.], device='cuda:0') tensor([415.], device='cuda:0') tensor([416.], device='cuda:0') tensor([417.], device='cuda:0') tensor([418.], device='cuda:0') tensor([419.], device='cuda:0') tensor([420.], device='cuda:0') tensor([421.], device='cuda:0') tensor([422.], device='cuda:0') tensor([423.], device='cuda:0') tensor([424.], device='cuda:0') tensor([425.], device='cuda:0') tensor([426.], device='cuda:0') tensor([427.], device='cuda:0') tensor([428.], device='cuda:0') tensor([429.], device='cuda:0') tensor([430.], device='cuda:0') tensor([431.], device='cuda:0') tensor([432.], device='cuda:0') tensor([433.], device='cuda:0') tensor([434.], device='cuda:0') tensor([435.], device='cuda:0') tensor([436.], device='cuda:0') tensor([437.], device='cuda:0') tensor([438.], device='cuda:0') tensor([439.], device='cuda:0') tensor([440.], device='cuda:0') tensor([441.], device='cuda:0') tensor([442.], device='cuda:0') tensor([443.], device='cuda:0') tensor([444.], device='cuda:0') tensor([445.], device='cuda:0') tensor([446.], device='cuda:0') tensor([447.], device='cuda:0') tensor([448.], device='cuda:0') tensor([449.], device='cuda:0') tensor([450.], device='cuda:0') tensor([451.], device='cuda:0') tensor([452.], device='cuda:0') tensor([453.], device='cuda:0') tensor([454.], device='cuda:0') tensor([455.], device='cuda:0') tensor([456.], device='cuda:0') tensor([457.], device='cuda:0') tensor([458.], device='cuda:0') tensor([459.], device='cuda:0') tensor([460.], device='cuda:0') tensor([461.], device='cuda:0') tensor([462.], device='cuda:0') tensor([463.], device='cuda:0') tensor([464.], device='cuda:0') tensor([465.], device='cuda:0') tensor([466.], device='cuda:0') tensor([467.], device='cuda:0') tensor([468.], device='cuda:0') tensor([469.], device='cuda:0') tensor([470.], device='cuda:0') tensor([471.], device='cuda:0') tensor([472.], device='cuda:0') tensor([473.], device='cuda:0') tensor([474.], device='cuda:0') tensor([475.], device='cuda:0') tensor([476.], device='cuda:0') tensor([477.], device='cuda:0') tensor([478.], device='cuda:0') tensor([479.], device='cuda:0') tensor([480.], device='cuda:0') tensor([481.], device='cuda:0') tensor([482.], device='cuda:0') tensor([483.], device='cuda:0') tensor([484.], device='cuda:0') tensor([485.], device='cuda:0') tensor([486.], device='cuda:0') tensor([487.], device='cuda:0') tensor([488.], device='cuda:0') tensor([489.], device='cuda:0') tensor([490.], device='cuda:0') tensor([491.], device='cuda:0') tensor([492.], device='cuda:0') tensor([493.], device='cuda:0') tensor([494.], device='cuda:0') tensor([495.], device='cuda:0') tensor([496.], device='cuda:0') tensor([497.], device='cuda:0') tensor([498.], device='cuda:0') tensor([499.], device='cuda:0') tensor([500.], device='cuda:0') tensor([501.], device='cuda:0') tensor([502.], device='cuda:0') tensor([503.], device='cuda:0') tensor([504.], device='cuda:0') tensor([505.], device='cuda:0') tensor([506.], device='cuda:0') tensor([507.], device='cuda:0') tensor([508.], device='cuda:0') tensor([509.], device='cuda:0') tensor([510.], device='cuda:0') tensor([511.], device='cuda:0') Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i ) Output [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113] [114] [115] [116] [117] [118] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149] [150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191] [192] [193] [194] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240] [241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] [262] [263] [264] [265] [266] [267] [268] [269] [270] [271] [272] [273] [274] [275] [276] [277] [278] [279] [280] [281] [282] [283] [284] [285] [286] [287] [288] [289] [290] [291] [292] [293] [294] [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] [305] [306] [307] [308] [309] [310] [311] [312] [313] [314] [315] [316] [317] [318] [319] [320] [321] [322] [323] [324] [325] [326] [327] [328] [329] [330] [331] [332] [333] [334] [335] [336] [337] [338] [339] [340] [341] [342] [343] [344] [345] [346] [347] [348] [349] [350] [351] [352] [353] [354] [355] [356] [357] [358] [359] [360] [361] [362] [363] [364] [365] [366] [367] [368] [369] [370] [371] [372] [373] [374] [375] [376] [377] [378] [379] [380] [381] [382] [383] [384] [385] [386] [387] [388] [389] [390] [391] [392] [393] [394] [395] [396] [397] [398] [399] [400] [401] [402] [403] [404] [405] [406] [407] [408] [409] [410] [411] [412] [413] [414] [415] [416] [417] [418] [419] [420] [421] [422] [423] [424] [425] [426] [427] [428] [429] [430] [431] [432] [433] [434] [435] [436] [437] [438] [439] [440] [441] [442] [443] [444] [445] [446] [447] [448] [449] [450] [451] [452] [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] [469] [470] [471] [472] [473] [474] [475] [476] [477] [478] [479] [480] [481] [482] [483] [484] [485] [486] [487] [488] [489] [490] [491] [492] [493] [494] [495] [496] [497] [498] [499] [500] [501] [502] [503] [504] [505] [506] [507] [508] [509] [510] [511]","title":"<span class='magic-codeicon-class'>MPSequence</span>"},{"location":"apis/data/sequence/MPSequence/#datasequencempsequence","text":"Class \u00b7 Context \u00b7 Source manager = mdnc . data . sequence . MPSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: multiprocessing module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MPSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MPSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MPSequence would store the indexer during the initialization. When the start() method is invoked, two process pools would be created. The first pool maintains several processes, each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MPSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed).","title":"data.sequence.MPSequence"},{"location":"apis/data/sequence/MPSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes or threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used.","title":"Arguments"},{"location":"apis/data/sequence/MPSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MPSequence/#start","text":"manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MPSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MPSequence/#finish","text":"manager . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MPSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MPSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MPSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MPSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MPSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MPSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MPSequence/#examples","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start () as mng : for i in mng : print ( i ) Output tensor([0.], device='cuda:0') tensor([1.], device='cuda:0') tensor([2.], device='cuda:0') tensor([3.], device='cuda:0') tensor([4.], device='cuda:0') tensor([5.], device='cuda:0') tensor([6.], device='cuda:0') tensor([7.], device='cuda:0') tensor([8.], device='cuda:0') tensor([9.], device='cuda:0') tensor([10.], device='cuda:0') tensor([11.], device='cuda:0') tensor([12.], device='cuda:0') tensor([13.], device='cuda:0') tensor([14.], device='cuda:0') tensor([15.], device='cuda:0') tensor([16.], device='cuda:0') tensor([17.], device='cuda:0') tensor([18.], device='cuda:0') tensor([19.], device='cuda:0') tensor([20.], device='cuda:0') tensor([21.], device='cuda:0') tensor([22.], device='cuda:0') tensor([23.], device='cuda:0') tensor([24.], device='cuda:0') tensor([25.], device='cuda:0') tensor([26.], device='cuda:0') tensor([27.], device='cuda:0') tensor([28.], device='cuda:0') tensor([29.], device='cuda:0') tensor([30.], device='cuda:0') tensor([31.], device='cuda:0') tensor([32.], device='cuda:0') tensor([33.], device='cuda:0') tensor([34.], device='cuda:0') tensor([35.], device='cuda:0') tensor([36.], device='cuda:0') tensor([37.], device='cuda:0') tensor([38.], device='cuda:0') tensor([39.], device='cuda:0') tensor([40.], device='cuda:0') tensor([41.], device='cuda:0') tensor([42.], device='cuda:0') tensor([43.], device='cuda:0') tensor([44.], device='cuda:0') tensor([45.], device='cuda:0') tensor([46.], device='cuda:0') tensor([47.], device='cuda:0') tensor([48.], device='cuda:0') tensor([49.], device='cuda:0') tensor([50.], device='cuda:0') tensor([51.], device='cuda:0') tensor([52.], device='cuda:0') tensor([53.], device='cuda:0') tensor([54.], device='cuda:0') tensor([55.], device='cuda:0') tensor([56.], device='cuda:0') tensor([57.], device='cuda:0') tensor([58.], device='cuda:0') tensor([59.], device='cuda:0') tensor([60.], device='cuda:0') tensor([61.], device='cuda:0') tensor([62.], device='cuda:0') tensor([63.], device='cuda:0') tensor([64.], device='cuda:0') tensor([65.], device='cuda:0') tensor([66.], device='cuda:0') tensor([67.], device='cuda:0') tensor([68.], device='cuda:0') tensor([69.], device='cuda:0') tensor([70.], device='cuda:0') tensor([71.], device='cuda:0') tensor([72.], device='cuda:0') tensor([73.], device='cuda:0') tensor([74.], device='cuda:0') tensor([75.], device='cuda:0') tensor([76.], device='cuda:0') tensor([77.], device='cuda:0') tensor([78.], device='cuda:0') tensor([79.], device='cuda:0') tensor([80.], device='cuda:0') tensor([81.], device='cuda:0') tensor([82.], device='cuda:0') tensor([83.], device='cuda:0') tensor([84.], device='cuda:0') tensor([85.], device='cuda:0') tensor([86.], device='cuda:0') tensor([87.], device='cuda:0') tensor([88.], device='cuda:0') tensor([89.], device='cuda:0') tensor([90.], device='cuda:0') tensor([91.], device='cuda:0') tensor([92.], device='cuda:0') tensor([93.], device='cuda:0') tensor([94.], device='cuda:0') tensor([95.], device='cuda:0') tensor([96.], device='cuda:0') tensor([97.], device='cuda:0') tensor([98.], device='cuda:0') tensor([99.], device='cuda:0') tensor([100.], device='cuda:0') tensor([101.], device='cuda:0') tensor([102.], device='cuda:0') tensor([103.], device='cuda:0') tensor([104.], device='cuda:0') tensor([105.], device='cuda:0') tensor([106.], device='cuda:0') tensor([107.], device='cuda:0') tensor([108.], device='cuda:0') tensor([109.], device='cuda:0') tensor([110.], device='cuda:0') tensor([111.], device='cuda:0') tensor([112.], device='cuda:0') tensor([113.], device='cuda:0') tensor([114.], device='cuda:0') tensor([115.], device='cuda:0') tensor([116.], device='cuda:0') tensor([117.], device='cuda:0') tensor([118.], device='cuda:0') tensor([119.], device='cuda:0') tensor([120.], device='cuda:0') tensor([121.], device='cuda:0') tensor([122.], device='cuda:0') tensor([123.], device='cuda:0') tensor([124.], device='cuda:0') tensor([125.], device='cuda:0') tensor([126.], device='cuda:0') tensor([127.], device='cuda:0') tensor([128.], device='cuda:0') tensor([129.], device='cuda:0') tensor([130.], device='cuda:0') tensor([131.], device='cuda:0') tensor([132.], device='cuda:0') tensor([133.], device='cuda:0') tensor([134.], device='cuda:0') tensor([135.], device='cuda:0') tensor([136.], device='cuda:0') tensor([137.], device='cuda:0') tensor([138.], device='cuda:0') tensor([139.], device='cuda:0') tensor([140.], device='cuda:0') tensor([141.], device='cuda:0') tensor([142.], device='cuda:0') tensor([143.], device='cuda:0') tensor([144.], device='cuda:0') tensor([145.], device='cuda:0') tensor([146.], device='cuda:0') tensor([147.], device='cuda:0') tensor([148.], device='cuda:0') tensor([149.], device='cuda:0') tensor([150.], device='cuda:0') tensor([151.], device='cuda:0') tensor([152.], device='cuda:0') tensor([153.], device='cuda:0') tensor([154.], device='cuda:0') tensor([155.], device='cuda:0') tensor([156.], device='cuda:0') tensor([157.], device='cuda:0') tensor([158.], device='cuda:0') tensor([159.], device='cuda:0') tensor([160.], device='cuda:0') tensor([161.], device='cuda:0') tensor([162.], device='cuda:0') tensor([163.], device='cuda:0') tensor([164.], device='cuda:0') tensor([165.], device='cuda:0') tensor([166.], device='cuda:0') tensor([167.], device='cuda:0') tensor([168.], device='cuda:0') tensor([169.], device='cuda:0') tensor([170.], device='cuda:0') tensor([171.], device='cuda:0') tensor([172.], device='cuda:0') tensor([173.], device='cuda:0') tensor([174.], device='cuda:0') tensor([175.], device='cuda:0') tensor([176.], device='cuda:0') tensor([177.], device='cuda:0') tensor([178.], device='cuda:0') tensor([179.], device='cuda:0') tensor([180.], device='cuda:0') tensor([181.], device='cuda:0') tensor([182.], device='cuda:0') tensor([183.], device='cuda:0') tensor([184.], device='cuda:0') tensor([185.], device='cuda:0') tensor([186.], device='cuda:0') tensor([187.], device='cuda:0') tensor([188.], device='cuda:0') tensor([189.], device='cuda:0') tensor([190.], device='cuda:0') tensor([191.], device='cuda:0') tensor([192.], device='cuda:0') tensor([193.], device='cuda:0') tensor([194.], device='cuda:0') tensor([195.], device='cuda:0') tensor([196.], device='cuda:0') tensor([197.], device='cuda:0') tensor([198.], device='cuda:0') tensor([199.], device='cuda:0') tensor([200.], device='cuda:0') tensor([201.], device='cuda:0') tensor([202.], device='cuda:0') tensor([203.], device='cuda:0') tensor([204.], device='cuda:0') tensor([205.], device='cuda:0') tensor([206.], device='cuda:0') tensor([207.], device='cuda:0') tensor([208.], device='cuda:0') tensor([209.], device='cuda:0') tensor([210.], device='cuda:0') tensor([211.], device='cuda:0') tensor([212.], device='cuda:0') tensor([213.], device='cuda:0') tensor([214.], device='cuda:0') tensor([215.], device='cuda:0') tensor([216.], device='cuda:0') tensor([217.], device='cuda:0') tensor([218.], device='cuda:0') tensor([219.], device='cuda:0') tensor([220.], device='cuda:0') tensor([221.], device='cuda:0') tensor([222.], device='cuda:0') tensor([223.], device='cuda:0') tensor([224.], device='cuda:0') tensor([225.], device='cuda:0') tensor([226.], device='cuda:0') tensor([227.], device='cuda:0') tensor([228.], device='cuda:0') tensor([229.], device='cuda:0') tensor([230.], device='cuda:0') tensor([231.], device='cuda:0') tensor([232.], device='cuda:0') tensor([233.], device='cuda:0') tensor([234.], device='cuda:0') tensor([235.], device='cuda:0') tensor([236.], device='cuda:0') tensor([237.], device='cuda:0') tensor([238.], device='cuda:0') tensor([239.], device='cuda:0') tensor([240.], device='cuda:0') tensor([241.], device='cuda:0') tensor([242.], device='cuda:0') tensor([243.], device='cuda:0') tensor([244.], device='cuda:0') tensor([245.], device='cuda:0') tensor([246.], device='cuda:0') tensor([247.], device='cuda:0') tensor([248.], device='cuda:0') tensor([249.], device='cuda:0') tensor([250.], device='cuda:0') tensor([251.], device='cuda:0') tensor([252.], device='cuda:0') tensor([253.], device='cuda:0') tensor([254.], device='cuda:0') tensor([255.], device='cuda:0') tensor([256.], device='cuda:0') tensor([257.], device='cuda:0') tensor([258.], device='cuda:0') tensor([259.], device='cuda:0') tensor([260.], device='cuda:0') tensor([261.], device='cuda:0') tensor([262.], device='cuda:0') tensor([263.], device='cuda:0') tensor([264.], device='cuda:0') tensor([265.], device='cuda:0') tensor([266.], device='cuda:0') tensor([267.], device='cuda:0') tensor([268.], device='cuda:0') tensor([269.], device='cuda:0') tensor([270.], device='cuda:0') tensor([271.], device='cuda:0') tensor([272.], device='cuda:0') tensor([273.], device='cuda:0') tensor([274.], device='cuda:0') tensor([275.], device='cuda:0') tensor([276.], device='cuda:0') tensor([277.], device='cuda:0') tensor([278.], device='cuda:0') tensor([279.], device='cuda:0') tensor([280.], device='cuda:0') tensor([281.], device='cuda:0') tensor([282.], device='cuda:0') tensor([283.], device='cuda:0') tensor([284.], device='cuda:0') tensor([285.], device='cuda:0') tensor([286.], device='cuda:0') tensor([287.], device='cuda:0') tensor([288.], device='cuda:0') tensor([289.], device='cuda:0') tensor([290.], device='cuda:0') tensor([291.], device='cuda:0') tensor([292.], device='cuda:0') tensor([293.], device='cuda:0') tensor([294.], device='cuda:0') tensor([295.], device='cuda:0') tensor([296.], device='cuda:0') tensor([297.], device='cuda:0') tensor([298.], device='cuda:0') tensor([299.], device='cuda:0') tensor([300.], device='cuda:0') tensor([301.], device='cuda:0') tensor([302.], device='cuda:0') tensor([303.], device='cuda:0') tensor([304.], device='cuda:0') tensor([305.], device='cuda:0') tensor([306.], device='cuda:0') tensor([307.], device='cuda:0') tensor([308.], device='cuda:0') tensor([309.], device='cuda:0') tensor([310.], device='cuda:0') tensor([311.], device='cuda:0') tensor([312.], device='cuda:0') tensor([313.], device='cuda:0') tensor([314.], device='cuda:0') tensor([315.], device='cuda:0') tensor([316.], device='cuda:0') tensor([317.], device='cuda:0') tensor([318.], device='cuda:0') tensor([319.], device='cuda:0') tensor([320.], device='cuda:0') tensor([321.], device='cuda:0') tensor([322.], device='cuda:0') tensor([323.], device='cuda:0') tensor([324.], device='cuda:0') tensor([325.], device='cuda:0') tensor([326.], device='cuda:0') tensor([327.], device='cuda:0') tensor([328.], device='cuda:0') tensor([329.], device='cuda:0') tensor([330.], device='cuda:0') tensor([331.], device='cuda:0') tensor([332.], device='cuda:0') tensor([333.], device='cuda:0') tensor([334.], device='cuda:0') tensor([335.], device='cuda:0') tensor([336.], device='cuda:0') tensor([337.], device='cuda:0') tensor([338.], device='cuda:0') tensor([339.], device='cuda:0') tensor([340.], device='cuda:0') tensor([341.], device='cuda:0') tensor([342.], device='cuda:0') tensor([343.], device='cuda:0') tensor([344.], device='cuda:0') tensor([345.], device='cuda:0') tensor([346.], device='cuda:0') tensor([347.], device='cuda:0') tensor([348.], device='cuda:0') tensor([349.], device='cuda:0') tensor([350.], device='cuda:0') tensor([351.], device='cuda:0') tensor([352.], device='cuda:0') tensor([353.], device='cuda:0') tensor([354.], device='cuda:0') tensor([355.], device='cuda:0') tensor([356.], device='cuda:0') tensor([357.], device='cuda:0') tensor([358.], device='cuda:0') tensor([359.], device='cuda:0') tensor([360.], device='cuda:0') tensor([361.], device='cuda:0') tensor([362.], device='cuda:0') tensor([363.], device='cuda:0') tensor([364.], device='cuda:0') tensor([365.], device='cuda:0') tensor([366.], device='cuda:0') tensor([367.], device='cuda:0') tensor([368.], device='cuda:0') tensor([369.], device='cuda:0') tensor([370.], device='cuda:0') tensor([371.], device='cuda:0') tensor([372.], device='cuda:0') tensor([373.], device='cuda:0') tensor([374.], device='cuda:0') tensor([375.], device='cuda:0') tensor([376.], device='cuda:0') tensor([377.], device='cuda:0') tensor([378.], device='cuda:0') tensor([379.], device='cuda:0') tensor([380.], device='cuda:0') tensor([381.], device='cuda:0') tensor([382.], device='cuda:0') tensor([383.], device='cuda:0') tensor([384.], device='cuda:0') tensor([385.], device='cuda:0') tensor([386.], device='cuda:0') tensor([387.], device='cuda:0') tensor([388.], device='cuda:0') tensor([389.], device='cuda:0') tensor([390.], device='cuda:0') tensor([391.], device='cuda:0') tensor([392.], device='cuda:0') tensor([393.], device='cuda:0') tensor([394.], device='cuda:0') tensor([395.], device='cuda:0') tensor([396.], device='cuda:0') tensor([397.], device='cuda:0') tensor([398.], device='cuda:0') tensor([399.], device='cuda:0') tensor([400.], device='cuda:0') tensor([401.], device='cuda:0') tensor([402.], device='cuda:0') tensor([403.], device='cuda:0') tensor([404.], device='cuda:0') tensor([405.], device='cuda:0') tensor([406.], device='cuda:0') tensor([407.], device='cuda:0') tensor([408.], device='cuda:0') tensor([409.], device='cuda:0') tensor([410.], device='cuda:0') tensor([411.], device='cuda:0') tensor([412.], device='cuda:0') tensor([413.], device='cuda:0') tensor([414.], device='cuda:0') tensor([415.], device='cuda:0') tensor([416.], device='cuda:0') tensor([417.], device='cuda:0') tensor([418.], device='cuda:0') tensor([419.], device='cuda:0') tensor([420.], device='cuda:0') tensor([421.], device='cuda:0') tensor([422.], device='cuda:0') tensor([423.], device='cuda:0') tensor([424.], device='cuda:0') tensor([425.], device='cuda:0') tensor([426.], device='cuda:0') tensor([427.], device='cuda:0') tensor([428.], device='cuda:0') tensor([429.], device='cuda:0') tensor([430.], device='cuda:0') tensor([431.], device='cuda:0') tensor([432.], device='cuda:0') tensor([433.], device='cuda:0') tensor([434.], device='cuda:0') tensor([435.], device='cuda:0') tensor([436.], device='cuda:0') tensor([437.], device='cuda:0') tensor([438.], device='cuda:0') tensor([439.], device='cuda:0') tensor([440.], device='cuda:0') tensor([441.], device='cuda:0') tensor([442.], device='cuda:0') tensor([443.], device='cuda:0') tensor([444.], device='cuda:0') tensor([445.], device='cuda:0') tensor([446.], device='cuda:0') tensor([447.], device='cuda:0') tensor([448.], device='cuda:0') tensor([449.], device='cuda:0') tensor([450.], device='cuda:0') tensor([451.], device='cuda:0') tensor([452.], device='cuda:0') tensor([453.], device='cuda:0') tensor([454.], device='cuda:0') tensor([455.], device='cuda:0') tensor([456.], device='cuda:0') tensor([457.], device='cuda:0') tensor([458.], device='cuda:0') tensor([459.], device='cuda:0') tensor([460.], device='cuda:0') tensor([461.], device='cuda:0') tensor([462.], device='cuda:0') tensor([463.], device='cuda:0') tensor([464.], device='cuda:0') tensor([465.], device='cuda:0') tensor([466.], device='cuda:0') tensor([467.], device='cuda:0') tensor([468.], device='cuda:0') tensor([469.], device='cuda:0') tensor([470.], device='cuda:0') tensor([471.], device='cuda:0') tensor([472.], device='cuda:0') tensor([473.], device='cuda:0') tensor([474.], device='cuda:0') tensor([475.], device='cuda:0') tensor([476.], device='cuda:0') tensor([477.], device='cuda:0') tensor([478.], device='cuda:0') tensor([479.], device='cuda:0') tensor([480.], device='cuda:0') tensor([481.], device='cuda:0') tensor([482.], device='cuda:0') tensor([483.], device='cuda:0') tensor([484.], device='cuda:0') tensor([485.], device='cuda:0') tensor([486.], device='cuda:0') tensor([487.], device='cuda:0') tensor([488.], device='cuda:0') tensor([489.], device='cuda:0') tensor([490.], device='cuda:0') tensor([491.], device='cuda:0') tensor([492.], device='cuda:0') tensor([493.], device='cuda:0') tensor([494.], device='cuda:0') tensor([495.], device='cuda:0') tensor([496.], device='cuda:0') tensor([497.], device='cuda:0') tensor([498.], device='cuda:0') tensor([499.], device='cuda:0') tensor([500.], device='cuda:0') tensor([501.], device='cuda:0') tensor([502.], device='cuda:0') tensor([503.], device='cuda:0') tensor([504.], device='cuda:0') tensor([505.], device='cuda:0') tensor([506.], device='cuda:0') tensor([507.], device='cuda:0') tensor([508.], device='cuda:0') tensor([509.], device='cuda:0') tensor([510.], device='cuda:0') tensor([511.], device='cuda:0') Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i ) Output [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113] [114] [115] [116] [117] [118] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149] [150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191] [192] [193] [194] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240] [241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] [262] [263] [264] [265] [266] [267] [268] [269] [270] [271] [272] [273] [274] [275] [276] [277] [278] [279] [280] [281] [282] [283] [284] [285] [286] [287] [288] [289] [290] [291] [292] [293] [294] [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] [305] [306] [307] [308] [309] [310] [311] [312] [313] [314] [315] [316] [317] [318] [319] [320] [321] [322] [323] [324] [325] [326] [327] [328] [329] [330] [331] [332] [333] [334] [335] [336] [337] [338] [339] [340] [341] [342] [343] [344] [345] [346] [347] [348] [349] [350] [351] [352] [353] [354] [355] [356] [357] [358] [359] [360] [361] [362] [363] [364] [365] [366] [367] [368] [369] [370] [371] [372] [373] [374] [375] [376] [377] [378] [379] [380] [381] [382] [383] [384] [385] [386] [387] [388] [389] [390] [391] [392] [393] [394] [395] [396] [397] [398] [399] [400] [401] [402] [403] [404] [405] [406] [407] [408] [409] [410] [411] [412] [413] [414] [415] [416] [417] [418] [419] [420] [421] [422] [423] [424] [425] [426] [427] [428] [429] [430] [431] [432] [433] [434] [435] [436] [437] [438] [439] [440] [441] [442] [443] [444] [445] [446] [447] [448] [449] [450] [451] [452] [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] [469] [470] [471] [472] [473] [474] [475] [476] [477] [478] [479] [480] [481] [482] [483] [484] [485] [486] [487] [488] [489] [490] [491] [492] [493] [494] [495] [496] [497] [498] [499] [500] [501] [502] [503] [504] [505] [506] [507] [508] [509] [510] [511]","title":"Examples"},{"location":"apis/data/sequence/MSequence/","text":"data.sequence.MSequence \u00b6 Class \u00b7 Context \u00b7 Source manager = mdnc . data . sequence . MSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , thread_type = 'proc' , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading or multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading and multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: threading and :fontawesome-solid-external-link-alt: multiprocessing modules respectively. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MSequence would store the indexer during the initialization. When the start() method is invoked, two process (or threading) pools would be created. The first pool maintains several processes (or threads), each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Warning We do not recommend to use mdnc.data.sequence.MSequence , because it is a base class. Instead, please use mdnc.data.sequence.MTSequence or mdnc.data.sequence.MPSequence according to your preference. The only case where you use this class is, you want to make the multi-threading or multi-processing options exposed to users. Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes or threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. thread_type str The backend of the MSequence , could be 'proc' or 'thread' . out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process (or threading) pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the process (or threading) pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Examples \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start () as mng : for i in mng : print ( i ) Output tensor([0.], device='cuda:0') tensor([1.], device='cuda:0') tensor([2.], device='cuda:0') tensor([3.], device='cuda:0') tensor([4.], device='cuda:0') tensor([5.], device='cuda:0') tensor([6.], device='cuda:0') tensor([7.], device='cuda:0') tensor([8.], device='cuda:0') tensor([9.], device='cuda:0') tensor([10.], device='cuda:0') tensor([11.], device='cuda:0') tensor([12.], device='cuda:0') tensor([13.], device='cuda:0') tensor([14.], device='cuda:0') tensor([15.], device='cuda:0') tensor([16.], device='cuda:0') tensor([17.], device='cuda:0') tensor([18.], device='cuda:0') tensor([19.], device='cuda:0') tensor([20.], device='cuda:0') tensor([21.], device='cuda:0') tensor([22.], device='cuda:0') tensor([23.], device='cuda:0') tensor([24.], device='cuda:0') tensor([25.], device='cuda:0') tensor([26.], device='cuda:0') tensor([27.], device='cuda:0') tensor([28.], device='cuda:0') tensor([29.], device='cuda:0') tensor([30.], device='cuda:0') tensor([31.], device='cuda:0') tensor([32.], device='cuda:0') tensor([33.], device='cuda:0') tensor([34.], device='cuda:0') tensor([35.], device='cuda:0') tensor([36.], device='cuda:0') tensor([37.], device='cuda:0') tensor([38.], device='cuda:0') tensor([39.], device='cuda:0') tensor([40.], device='cuda:0') tensor([41.], device='cuda:0') tensor([42.], device='cuda:0') tensor([43.], device='cuda:0') tensor([44.], device='cuda:0') tensor([45.], device='cuda:0') tensor([46.], device='cuda:0') tensor([47.], device='cuda:0') tensor([48.], device='cuda:0') tensor([49.], device='cuda:0') tensor([50.], device='cuda:0') tensor([51.], device='cuda:0') tensor([52.], device='cuda:0') tensor([53.], device='cuda:0') tensor([54.], device='cuda:0') tensor([55.], device='cuda:0') tensor([56.], device='cuda:0') tensor([57.], device='cuda:0') tensor([58.], device='cuda:0') tensor([59.], device='cuda:0') tensor([60.], device='cuda:0') tensor([61.], device='cuda:0') tensor([62.], device='cuda:0') tensor([63.], device='cuda:0') tensor([64.], device='cuda:0') tensor([65.], device='cuda:0') tensor([66.], device='cuda:0') tensor([67.], device='cuda:0') tensor([68.], device='cuda:0') tensor([69.], device='cuda:0') tensor([70.], device='cuda:0') tensor([71.], device='cuda:0') tensor([72.], device='cuda:0') tensor([73.], device='cuda:0') tensor([74.], device='cuda:0') tensor([75.], device='cuda:0') tensor([76.], device='cuda:0') tensor([77.], device='cuda:0') tensor([78.], device='cuda:0') tensor([79.], device='cuda:0') tensor([80.], device='cuda:0') tensor([81.], device='cuda:0') tensor([82.], device='cuda:0') tensor([83.], device='cuda:0') tensor([84.], device='cuda:0') tensor([85.], device='cuda:0') tensor([86.], device='cuda:0') tensor([87.], device='cuda:0') tensor([88.], device='cuda:0') tensor([89.], device='cuda:0') tensor([90.], device='cuda:0') tensor([91.], device='cuda:0') tensor([92.], device='cuda:0') tensor([93.], device='cuda:0') tensor([94.], device='cuda:0') tensor([95.], device='cuda:0') tensor([96.], device='cuda:0') tensor([97.], device='cuda:0') tensor([98.], device='cuda:0') tensor([99.], device='cuda:0') tensor([100.], device='cuda:0') tensor([101.], device='cuda:0') tensor([102.], device='cuda:0') tensor([103.], device='cuda:0') tensor([104.], device='cuda:0') tensor([105.], device='cuda:0') tensor([106.], device='cuda:0') tensor([107.], device='cuda:0') tensor([108.], device='cuda:0') tensor([109.], device='cuda:0') tensor([110.], device='cuda:0') tensor([111.], device='cuda:0') tensor([112.], device='cuda:0') tensor([113.], device='cuda:0') tensor([114.], device='cuda:0') tensor([115.], device='cuda:0') tensor([116.], device='cuda:0') tensor([117.], device='cuda:0') tensor([118.], device='cuda:0') tensor([119.], device='cuda:0') tensor([120.], device='cuda:0') tensor([121.], device='cuda:0') tensor([122.], device='cuda:0') tensor([123.], device='cuda:0') tensor([124.], device='cuda:0') tensor([125.], device='cuda:0') tensor([126.], device='cuda:0') tensor([127.], device='cuda:0') tensor([128.], device='cuda:0') tensor([129.], device='cuda:0') tensor([130.], device='cuda:0') tensor([131.], device='cuda:0') tensor([132.], device='cuda:0') tensor([133.], device='cuda:0') tensor([134.], device='cuda:0') tensor([135.], device='cuda:0') tensor([136.], device='cuda:0') tensor([137.], device='cuda:0') tensor([138.], device='cuda:0') tensor([139.], device='cuda:0') tensor([140.], device='cuda:0') tensor([141.], device='cuda:0') tensor([142.], device='cuda:0') tensor([143.], device='cuda:0') tensor([144.], device='cuda:0') tensor([145.], device='cuda:0') tensor([146.], device='cuda:0') tensor([147.], device='cuda:0') tensor([148.], device='cuda:0') tensor([149.], device='cuda:0') tensor([150.], device='cuda:0') tensor([151.], device='cuda:0') tensor([152.], device='cuda:0') tensor([153.], device='cuda:0') tensor([154.], device='cuda:0') tensor([155.], device='cuda:0') tensor([156.], device='cuda:0') tensor([157.], device='cuda:0') tensor([158.], device='cuda:0') tensor([159.], device='cuda:0') tensor([160.], device='cuda:0') tensor([161.], device='cuda:0') tensor([162.], device='cuda:0') tensor([163.], device='cuda:0') tensor([164.], device='cuda:0') tensor([165.], device='cuda:0') tensor([166.], device='cuda:0') tensor([167.], device='cuda:0') tensor([168.], device='cuda:0') tensor([169.], device='cuda:0') tensor([170.], device='cuda:0') tensor([171.], device='cuda:0') tensor([172.], device='cuda:0') tensor([173.], device='cuda:0') tensor([174.], device='cuda:0') tensor([175.], device='cuda:0') tensor([176.], device='cuda:0') tensor([177.], device='cuda:0') tensor([178.], device='cuda:0') tensor([179.], device='cuda:0') tensor([180.], device='cuda:0') tensor([181.], device='cuda:0') tensor([182.], device='cuda:0') tensor([183.], device='cuda:0') tensor([184.], device='cuda:0') tensor([185.], device='cuda:0') tensor([186.], device='cuda:0') tensor([187.], device='cuda:0') tensor([188.], device='cuda:0') tensor([189.], device='cuda:0') tensor([190.], device='cuda:0') tensor([191.], device='cuda:0') tensor([192.], device='cuda:0') tensor([193.], device='cuda:0') tensor([194.], device='cuda:0') tensor([195.], device='cuda:0') tensor([196.], device='cuda:0') tensor([197.], device='cuda:0') tensor([198.], device='cuda:0') tensor([199.], device='cuda:0') tensor([200.], device='cuda:0') tensor([201.], device='cuda:0') tensor([202.], device='cuda:0') tensor([203.], device='cuda:0') tensor([204.], device='cuda:0') tensor([205.], device='cuda:0') tensor([206.], device='cuda:0') tensor([207.], device='cuda:0') tensor([208.], device='cuda:0') tensor([209.], device='cuda:0') tensor([210.], device='cuda:0') tensor([211.], device='cuda:0') tensor([212.], device='cuda:0') tensor([213.], device='cuda:0') tensor([214.], device='cuda:0') tensor([215.], device='cuda:0') tensor([216.], device='cuda:0') tensor([217.], device='cuda:0') tensor([218.], device='cuda:0') tensor([219.], device='cuda:0') tensor([220.], device='cuda:0') tensor([221.], device='cuda:0') tensor([222.], device='cuda:0') tensor([223.], device='cuda:0') tensor([224.], device='cuda:0') tensor([225.], device='cuda:0') tensor([226.], device='cuda:0') tensor([227.], device='cuda:0') tensor([228.], device='cuda:0') tensor([229.], device='cuda:0') tensor([230.], device='cuda:0') tensor([231.], device='cuda:0') tensor([232.], device='cuda:0') tensor([233.], device='cuda:0') tensor([234.], device='cuda:0') tensor([235.], device='cuda:0') tensor([236.], device='cuda:0') tensor([237.], device='cuda:0') tensor([238.], device='cuda:0') tensor([239.], device='cuda:0') tensor([240.], device='cuda:0') tensor([241.], device='cuda:0') tensor([242.], device='cuda:0') tensor([243.], device='cuda:0') tensor([244.], device='cuda:0') tensor([245.], device='cuda:0') tensor([246.], device='cuda:0') tensor([247.], device='cuda:0') tensor([248.], device='cuda:0') tensor([249.], device='cuda:0') tensor([250.], device='cuda:0') tensor([251.], device='cuda:0') tensor([252.], device='cuda:0') tensor([253.], device='cuda:0') tensor([254.], device='cuda:0') tensor([255.], device='cuda:0') tensor([256.], device='cuda:0') tensor([257.], device='cuda:0') tensor([258.], device='cuda:0') tensor([259.], device='cuda:0') tensor([260.], device='cuda:0') tensor([261.], device='cuda:0') tensor([262.], device='cuda:0') tensor([263.], device='cuda:0') tensor([264.], device='cuda:0') tensor([265.], device='cuda:0') tensor([266.], device='cuda:0') tensor([267.], device='cuda:0') tensor([268.], device='cuda:0') tensor([269.], device='cuda:0') tensor([270.], device='cuda:0') tensor([271.], device='cuda:0') tensor([272.], device='cuda:0') tensor([273.], device='cuda:0') tensor([274.], device='cuda:0') tensor([275.], device='cuda:0') tensor([276.], device='cuda:0') tensor([277.], device='cuda:0') tensor([278.], device='cuda:0') tensor([279.], device='cuda:0') tensor([280.], device='cuda:0') tensor([281.], device='cuda:0') tensor([282.], device='cuda:0') tensor([283.], device='cuda:0') tensor([284.], device='cuda:0') tensor([285.], device='cuda:0') tensor([286.], device='cuda:0') tensor([287.], device='cuda:0') tensor([288.], device='cuda:0') tensor([289.], device='cuda:0') tensor([290.], device='cuda:0') tensor([291.], device='cuda:0') tensor([292.], device='cuda:0') tensor([293.], device='cuda:0') tensor([294.], device='cuda:0') tensor([295.], device='cuda:0') tensor([296.], device='cuda:0') tensor([297.], device='cuda:0') tensor([298.], device='cuda:0') tensor([299.], device='cuda:0') tensor([300.], device='cuda:0') tensor([301.], device='cuda:0') tensor([302.], device='cuda:0') tensor([303.], device='cuda:0') tensor([304.], device='cuda:0') tensor([305.], device='cuda:0') tensor([306.], device='cuda:0') tensor([307.], device='cuda:0') tensor([308.], device='cuda:0') tensor([309.], device='cuda:0') tensor([310.], device='cuda:0') tensor([311.], device='cuda:0') tensor([312.], device='cuda:0') tensor([313.], device='cuda:0') tensor([314.], device='cuda:0') tensor([315.], device='cuda:0') tensor([316.], device='cuda:0') tensor([317.], device='cuda:0') tensor([318.], device='cuda:0') tensor([319.], device='cuda:0') tensor([320.], device='cuda:0') tensor([321.], device='cuda:0') tensor([322.], device='cuda:0') tensor([323.], device='cuda:0') tensor([324.], device='cuda:0') tensor([325.], device='cuda:0') tensor([326.], device='cuda:0') tensor([327.], device='cuda:0') tensor([328.], device='cuda:0') tensor([329.], device='cuda:0') tensor([330.], device='cuda:0') tensor([331.], device='cuda:0') tensor([332.], device='cuda:0') tensor([333.], device='cuda:0') tensor([334.], device='cuda:0') tensor([335.], device='cuda:0') tensor([336.], device='cuda:0') tensor([337.], device='cuda:0') tensor([338.], device='cuda:0') tensor([339.], device='cuda:0') tensor([340.], device='cuda:0') tensor([341.], device='cuda:0') tensor([342.], device='cuda:0') tensor([343.], device='cuda:0') tensor([344.], device='cuda:0') tensor([345.], device='cuda:0') tensor([346.], device='cuda:0') tensor([347.], device='cuda:0') tensor([348.], device='cuda:0') tensor([349.], device='cuda:0') tensor([350.], device='cuda:0') tensor([351.], device='cuda:0') tensor([352.], device='cuda:0') tensor([353.], device='cuda:0') tensor([354.], device='cuda:0') tensor([355.], device='cuda:0') tensor([356.], device='cuda:0') tensor([357.], device='cuda:0') tensor([358.], device='cuda:0') tensor([359.], device='cuda:0') tensor([360.], device='cuda:0') tensor([361.], device='cuda:0') tensor([362.], device='cuda:0') tensor([363.], device='cuda:0') tensor([364.], device='cuda:0') tensor([365.], device='cuda:0') tensor([366.], device='cuda:0') tensor([367.], device='cuda:0') tensor([368.], device='cuda:0') tensor([369.], device='cuda:0') tensor([370.], device='cuda:0') tensor([371.], device='cuda:0') tensor([372.], device='cuda:0') tensor([373.], device='cuda:0') tensor([374.], device='cuda:0') tensor([375.], device='cuda:0') tensor([376.], device='cuda:0') tensor([377.], device='cuda:0') tensor([378.], device='cuda:0') tensor([379.], device='cuda:0') tensor([380.], device='cuda:0') tensor([381.], device='cuda:0') tensor([382.], device='cuda:0') tensor([383.], device='cuda:0') tensor([384.], device='cuda:0') tensor([385.], device='cuda:0') tensor([386.], device='cuda:0') tensor([387.], device='cuda:0') tensor([388.], device='cuda:0') tensor([389.], device='cuda:0') tensor([390.], device='cuda:0') tensor([391.], device='cuda:0') tensor([392.], device='cuda:0') tensor([393.], device='cuda:0') tensor([394.], device='cuda:0') tensor([395.], device='cuda:0') tensor([396.], device='cuda:0') tensor([397.], device='cuda:0') tensor([398.], device='cuda:0') tensor([399.], device='cuda:0') tensor([400.], device='cuda:0') tensor([401.], device='cuda:0') tensor([402.], device='cuda:0') tensor([403.], device='cuda:0') tensor([404.], device='cuda:0') tensor([405.], device='cuda:0') tensor([406.], device='cuda:0') tensor([407.], device='cuda:0') tensor([408.], device='cuda:0') tensor([409.], device='cuda:0') tensor([410.], device='cuda:0') tensor([411.], device='cuda:0') tensor([412.], device='cuda:0') tensor([413.], device='cuda:0') tensor([414.], device='cuda:0') tensor([415.], device='cuda:0') tensor([416.], device='cuda:0') tensor([417.], device='cuda:0') tensor([418.], device='cuda:0') tensor([419.], device='cuda:0') tensor([420.], device='cuda:0') tensor([421.], device='cuda:0') tensor([422.], device='cuda:0') tensor([423.], device='cuda:0') tensor([424.], device='cuda:0') tensor([425.], device='cuda:0') tensor([426.], device='cuda:0') tensor([427.], device='cuda:0') tensor([428.], device='cuda:0') tensor([429.], device='cuda:0') tensor([430.], device='cuda:0') tensor([431.], device='cuda:0') tensor([432.], device='cuda:0') tensor([433.], device='cuda:0') tensor([434.], device='cuda:0') tensor([435.], device='cuda:0') tensor([436.], device='cuda:0') tensor([437.], device='cuda:0') tensor([438.], device='cuda:0') tensor([439.], device='cuda:0') tensor([440.], device='cuda:0') tensor([441.], device='cuda:0') tensor([442.], device='cuda:0') tensor([443.], device='cuda:0') tensor([444.], device='cuda:0') tensor([445.], device='cuda:0') tensor([446.], device='cuda:0') tensor([447.], device='cuda:0') tensor([448.], device='cuda:0') tensor([449.], device='cuda:0') tensor([450.], device='cuda:0') tensor([451.], device='cuda:0') tensor([452.], device='cuda:0') tensor([453.], device='cuda:0') tensor([454.], device='cuda:0') tensor([455.], device='cuda:0') tensor([456.], device='cuda:0') tensor([457.], device='cuda:0') tensor([458.], device='cuda:0') tensor([459.], device='cuda:0') tensor([460.], device='cuda:0') tensor([461.], device='cuda:0') tensor([462.], device='cuda:0') tensor([463.], device='cuda:0') tensor([464.], device='cuda:0') tensor([465.], device='cuda:0') tensor([466.], device='cuda:0') tensor([467.], device='cuda:0') tensor([468.], device='cuda:0') tensor([469.], device='cuda:0') tensor([470.], device='cuda:0') tensor([471.], device='cuda:0') tensor([472.], device='cuda:0') tensor([473.], device='cuda:0') tensor([474.], device='cuda:0') tensor([475.], device='cuda:0') tensor([476.], device='cuda:0') tensor([477.], device='cuda:0') tensor([478.], device='cuda:0') tensor([479.], device='cuda:0') tensor([480.], device='cuda:0') tensor([481.], device='cuda:0') tensor([482.], device='cuda:0') tensor([483.], device='cuda:0') tensor([484.], device='cuda:0') tensor([485.], device='cuda:0') tensor([486.], device='cuda:0') tensor([487.], device='cuda:0') tensor([488.], device='cuda:0') tensor([489.], device='cuda:0') tensor([490.], device='cuda:0') tensor([491.], device='cuda:0') tensor([492.], device='cuda:0') tensor([493.], device='cuda:0') tensor([494.], device='cuda:0') tensor([495.], device='cuda:0') tensor([496.], device='cuda:0') tensor([497.], device='cuda:0') tensor([498.], device='cuda:0') tensor([499.], device='cuda:0') tensor([500.], device='cuda:0') tensor([501.], device='cuda:0') tensor([502.], device='cuda:0') tensor([503.], device='cuda:0') tensor([504.], device='cuda:0') tensor([505.], device='cuda:0') tensor([506.], device='cuda:0') tensor([507.], device='cuda:0') tensor([508.], device='cuda:0') tensor([509.], device='cuda:0') tensor([510.], device='cuda:0') tensor([511.], device='cuda:0') Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i ) Output [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113] [114] [115] [116] [117] [118] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149] [150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191] [192] [193] [194] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240] [241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] [262] [263] [264] [265] [266] [267] [268] [269] [270] [271] [272] [273] [274] [275] [276] [277] [278] [279] [280] [281] [282] [283] [284] [285] [286] [287] [288] [289] [290] [291] [292] [293] [294] [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] [305] [306] [307] [308] [309] [310] [311] [312] [313] [314] [315] [316] [317] [318] [319] [320] [321] [322] [323] [324] [325] [326] [327] [328] [329] [330] [331] [332] [333] [334] [335] [336] [337] [338] [339] [340] [341] [342] [343] [344] [345] [346] [347] [348] [349] [350] [351] [352] [353] [354] [355] [356] [357] [358] [359] [360] [361] [362] [363] [364] [365] [366] [367] [368] [369] [370] [371] [372] [373] [374] [375] [376] [377] [378] [379] [380] [381] [382] [383] [384] [385] [386] [387] [388] [389] [390] [391] [392] [393] [394] [395] [396] [397] [398] [399] [400] [401] [402] [403] [404] [405] [406] [407] [408] [409] [410] [411] [412] [413] [414] [415] [416] [417] [418] [419] [420] [421] [422] [423] [424] [425] [426] [427] [428] [429] [430] [431] [432] [433] [434] [435] [436] [437] [438] [439] [440] [441] [442] [443] [444] [445] [446] [447] [448] [449] [450] [451] [452] [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] [469] [470] [471] [472] [473] [474] [475] [476] [477] [478] [479] [480] [481] [482] [483] [484] [485] [486] [487] [488] [489] [490] [491] [492] [493] [494] [495] [496] [497] [498] [499] [500] [501] [502] [503] [504] [505] [506] [507] [508] [509] [510] [511]","title":"<span class='magic-codeicon-class'>MSequence</span>"},{"location":"apis/data/sequence/MSequence/#datasequencemsequence","text":"Class \u00b7 Context \u00b7 Source manager = mdnc . data . sequence . MSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , thread_type = 'proc' , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading or multi-processing. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading and multi-processing codes are built on top of the :fontawesome-solid-external-link-alt: threading and :fontawesome-solid-external-link-alt: multiprocessing modules respectively. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by :fontawesome-solid-external-link-alt: torch.multiprocessing . The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MSequence would store the indexer during the initialization. When the start() method is invoked, two process (or threading) pools would be created. The first pool maintains several processes (or threads), each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Warning We do not recommend to use mdnc.data.sequence.MSequence , because it is a base class. Instead, please use mdnc.data.sequence.MTSequence or mdnc.data.sequence.MPSequence according to your preference. The only case where you use this class is, you want to make the multi-threading or multi-processing options exposed to users.","title":"data.sequence.MSequence"},{"location":"apis/data/sequence/MSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes or threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. thread_type str The backend of the MSequence , could be 'proc' or 'thread' . out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used.","title":"Arguments"},{"location":"apis/data/sequence/MSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MSequence/#start","text":"manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process (or threading) pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MSequence/#finish","text":"manager . finish () Finish the process (or threading) pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MSequence/#examples","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start () as mng : for i in mng : print ( i ) Output tensor([0.], device='cuda:0') tensor([1.], device='cuda:0') tensor([2.], device='cuda:0') tensor([3.], device='cuda:0') tensor([4.], device='cuda:0') tensor([5.], device='cuda:0') tensor([6.], device='cuda:0') tensor([7.], device='cuda:0') tensor([8.], device='cuda:0') tensor([9.], device='cuda:0') tensor([10.], device='cuda:0') tensor([11.], device='cuda:0') tensor([12.], device='cuda:0') tensor([13.], device='cuda:0') tensor([14.], device='cuda:0') tensor([15.], device='cuda:0') tensor([16.], device='cuda:0') tensor([17.], device='cuda:0') tensor([18.], device='cuda:0') tensor([19.], device='cuda:0') tensor([20.], device='cuda:0') tensor([21.], device='cuda:0') tensor([22.], device='cuda:0') tensor([23.], device='cuda:0') tensor([24.], device='cuda:0') tensor([25.], device='cuda:0') tensor([26.], device='cuda:0') tensor([27.], device='cuda:0') tensor([28.], device='cuda:0') tensor([29.], device='cuda:0') tensor([30.], device='cuda:0') tensor([31.], device='cuda:0') tensor([32.], device='cuda:0') tensor([33.], device='cuda:0') tensor([34.], device='cuda:0') tensor([35.], device='cuda:0') tensor([36.], device='cuda:0') tensor([37.], device='cuda:0') tensor([38.], device='cuda:0') tensor([39.], device='cuda:0') tensor([40.], device='cuda:0') tensor([41.], device='cuda:0') tensor([42.], device='cuda:0') tensor([43.], device='cuda:0') tensor([44.], device='cuda:0') tensor([45.], device='cuda:0') tensor([46.], device='cuda:0') tensor([47.], device='cuda:0') tensor([48.], device='cuda:0') tensor([49.], device='cuda:0') tensor([50.], device='cuda:0') tensor([51.], device='cuda:0') tensor([52.], device='cuda:0') tensor([53.], device='cuda:0') tensor([54.], device='cuda:0') tensor([55.], device='cuda:0') tensor([56.], device='cuda:0') tensor([57.], device='cuda:0') tensor([58.], device='cuda:0') tensor([59.], device='cuda:0') tensor([60.], device='cuda:0') tensor([61.], device='cuda:0') tensor([62.], device='cuda:0') tensor([63.], device='cuda:0') tensor([64.], device='cuda:0') tensor([65.], device='cuda:0') tensor([66.], device='cuda:0') tensor([67.], device='cuda:0') tensor([68.], device='cuda:0') tensor([69.], device='cuda:0') tensor([70.], device='cuda:0') tensor([71.], device='cuda:0') tensor([72.], device='cuda:0') tensor([73.], device='cuda:0') tensor([74.], device='cuda:0') tensor([75.], device='cuda:0') tensor([76.], device='cuda:0') tensor([77.], device='cuda:0') tensor([78.], device='cuda:0') tensor([79.], device='cuda:0') tensor([80.], device='cuda:0') tensor([81.], device='cuda:0') tensor([82.], device='cuda:0') tensor([83.], device='cuda:0') tensor([84.], device='cuda:0') tensor([85.], device='cuda:0') tensor([86.], device='cuda:0') tensor([87.], device='cuda:0') tensor([88.], device='cuda:0') tensor([89.], device='cuda:0') tensor([90.], device='cuda:0') tensor([91.], device='cuda:0') tensor([92.], device='cuda:0') tensor([93.], device='cuda:0') tensor([94.], device='cuda:0') tensor([95.], device='cuda:0') tensor([96.], device='cuda:0') tensor([97.], device='cuda:0') tensor([98.], device='cuda:0') tensor([99.], device='cuda:0') tensor([100.], device='cuda:0') tensor([101.], device='cuda:0') tensor([102.], device='cuda:0') tensor([103.], device='cuda:0') tensor([104.], device='cuda:0') tensor([105.], device='cuda:0') tensor([106.], device='cuda:0') tensor([107.], device='cuda:0') tensor([108.], device='cuda:0') tensor([109.], device='cuda:0') tensor([110.], device='cuda:0') tensor([111.], device='cuda:0') tensor([112.], device='cuda:0') tensor([113.], device='cuda:0') tensor([114.], device='cuda:0') tensor([115.], device='cuda:0') tensor([116.], device='cuda:0') tensor([117.], device='cuda:0') tensor([118.], device='cuda:0') tensor([119.], device='cuda:0') tensor([120.], device='cuda:0') tensor([121.], device='cuda:0') tensor([122.], device='cuda:0') tensor([123.], device='cuda:0') tensor([124.], device='cuda:0') tensor([125.], device='cuda:0') tensor([126.], device='cuda:0') tensor([127.], device='cuda:0') tensor([128.], device='cuda:0') tensor([129.], device='cuda:0') tensor([130.], device='cuda:0') tensor([131.], device='cuda:0') tensor([132.], device='cuda:0') tensor([133.], device='cuda:0') tensor([134.], device='cuda:0') tensor([135.], device='cuda:0') tensor([136.], device='cuda:0') tensor([137.], device='cuda:0') tensor([138.], device='cuda:0') tensor([139.], device='cuda:0') tensor([140.], device='cuda:0') tensor([141.], device='cuda:0') tensor([142.], device='cuda:0') tensor([143.], device='cuda:0') tensor([144.], device='cuda:0') tensor([145.], device='cuda:0') tensor([146.], device='cuda:0') tensor([147.], device='cuda:0') tensor([148.], device='cuda:0') tensor([149.], device='cuda:0') tensor([150.], device='cuda:0') tensor([151.], device='cuda:0') tensor([152.], device='cuda:0') tensor([153.], device='cuda:0') tensor([154.], device='cuda:0') tensor([155.], device='cuda:0') tensor([156.], device='cuda:0') tensor([157.], device='cuda:0') tensor([158.], device='cuda:0') tensor([159.], device='cuda:0') tensor([160.], device='cuda:0') tensor([161.], device='cuda:0') tensor([162.], device='cuda:0') tensor([163.], device='cuda:0') tensor([164.], device='cuda:0') tensor([165.], device='cuda:0') tensor([166.], device='cuda:0') tensor([167.], device='cuda:0') tensor([168.], device='cuda:0') tensor([169.], device='cuda:0') tensor([170.], device='cuda:0') tensor([171.], device='cuda:0') tensor([172.], device='cuda:0') tensor([173.], device='cuda:0') tensor([174.], device='cuda:0') tensor([175.], device='cuda:0') tensor([176.], device='cuda:0') tensor([177.], device='cuda:0') tensor([178.], device='cuda:0') tensor([179.], device='cuda:0') tensor([180.], device='cuda:0') tensor([181.], device='cuda:0') tensor([182.], device='cuda:0') tensor([183.], device='cuda:0') tensor([184.], device='cuda:0') tensor([185.], device='cuda:0') tensor([186.], device='cuda:0') tensor([187.], device='cuda:0') tensor([188.], device='cuda:0') tensor([189.], device='cuda:0') tensor([190.], device='cuda:0') tensor([191.], device='cuda:0') tensor([192.], device='cuda:0') tensor([193.], device='cuda:0') tensor([194.], device='cuda:0') tensor([195.], device='cuda:0') tensor([196.], device='cuda:0') tensor([197.], device='cuda:0') tensor([198.], device='cuda:0') tensor([199.], device='cuda:0') tensor([200.], device='cuda:0') tensor([201.], device='cuda:0') tensor([202.], device='cuda:0') tensor([203.], device='cuda:0') tensor([204.], device='cuda:0') tensor([205.], device='cuda:0') tensor([206.], device='cuda:0') tensor([207.], device='cuda:0') tensor([208.], device='cuda:0') tensor([209.], device='cuda:0') tensor([210.], device='cuda:0') tensor([211.], device='cuda:0') tensor([212.], device='cuda:0') tensor([213.], device='cuda:0') tensor([214.], device='cuda:0') tensor([215.], device='cuda:0') tensor([216.], device='cuda:0') tensor([217.], device='cuda:0') tensor([218.], device='cuda:0') tensor([219.], device='cuda:0') tensor([220.], device='cuda:0') tensor([221.], device='cuda:0') tensor([222.], device='cuda:0') tensor([223.], device='cuda:0') tensor([224.], device='cuda:0') tensor([225.], device='cuda:0') tensor([226.], device='cuda:0') tensor([227.], device='cuda:0') tensor([228.], device='cuda:0') tensor([229.], device='cuda:0') tensor([230.], device='cuda:0') tensor([231.], device='cuda:0') tensor([232.], device='cuda:0') tensor([233.], device='cuda:0') tensor([234.], device='cuda:0') tensor([235.], device='cuda:0') tensor([236.], device='cuda:0') tensor([237.], device='cuda:0') tensor([238.], device='cuda:0') tensor([239.], device='cuda:0') tensor([240.], device='cuda:0') tensor([241.], device='cuda:0') tensor([242.], device='cuda:0') tensor([243.], device='cuda:0') tensor([244.], device='cuda:0') tensor([245.], device='cuda:0') tensor([246.], device='cuda:0') tensor([247.], device='cuda:0') tensor([248.], device='cuda:0') tensor([249.], device='cuda:0') tensor([250.], device='cuda:0') tensor([251.], device='cuda:0') tensor([252.], device='cuda:0') tensor([253.], device='cuda:0') tensor([254.], device='cuda:0') tensor([255.], device='cuda:0') tensor([256.], device='cuda:0') tensor([257.], device='cuda:0') tensor([258.], device='cuda:0') tensor([259.], device='cuda:0') tensor([260.], device='cuda:0') tensor([261.], device='cuda:0') tensor([262.], device='cuda:0') tensor([263.], device='cuda:0') tensor([264.], device='cuda:0') tensor([265.], device='cuda:0') tensor([266.], device='cuda:0') tensor([267.], device='cuda:0') tensor([268.], device='cuda:0') tensor([269.], device='cuda:0') tensor([270.], device='cuda:0') tensor([271.], device='cuda:0') tensor([272.], device='cuda:0') tensor([273.], device='cuda:0') tensor([274.], device='cuda:0') tensor([275.], device='cuda:0') tensor([276.], device='cuda:0') tensor([277.], device='cuda:0') tensor([278.], device='cuda:0') tensor([279.], device='cuda:0') tensor([280.], device='cuda:0') tensor([281.], device='cuda:0') tensor([282.], device='cuda:0') tensor([283.], device='cuda:0') tensor([284.], device='cuda:0') tensor([285.], device='cuda:0') tensor([286.], device='cuda:0') tensor([287.], device='cuda:0') tensor([288.], device='cuda:0') tensor([289.], device='cuda:0') tensor([290.], device='cuda:0') tensor([291.], device='cuda:0') tensor([292.], device='cuda:0') tensor([293.], device='cuda:0') tensor([294.], device='cuda:0') tensor([295.], device='cuda:0') tensor([296.], device='cuda:0') tensor([297.], device='cuda:0') tensor([298.], device='cuda:0') tensor([299.], device='cuda:0') tensor([300.], device='cuda:0') tensor([301.], device='cuda:0') tensor([302.], device='cuda:0') tensor([303.], device='cuda:0') tensor([304.], device='cuda:0') tensor([305.], device='cuda:0') tensor([306.], device='cuda:0') tensor([307.], device='cuda:0') tensor([308.], device='cuda:0') tensor([309.], device='cuda:0') tensor([310.], device='cuda:0') tensor([311.], device='cuda:0') tensor([312.], device='cuda:0') tensor([313.], device='cuda:0') tensor([314.], device='cuda:0') tensor([315.], device='cuda:0') tensor([316.], device='cuda:0') tensor([317.], device='cuda:0') tensor([318.], device='cuda:0') tensor([319.], device='cuda:0') tensor([320.], device='cuda:0') tensor([321.], device='cuda:0') tensor([322.], device='cuda:0') tensor([323.], device='cuda:0') tensor([324.], device='cuda:0') tensor([325.], device='cuda:0') tensor([326.], device='cuda:0') tensor([327.], device='cuda:0') tensor([328.], device='cuda:0') tensor([329.], device='cuda:0') tensor([330.], device='cuda:0') tensor([331.], device='cuda:0') tensor([332.], device='cuda:0') tensor([333.], device='cuda:0') tensor([334.], device='cuda:0') tensor([335.], device='cuda:0') tensor([336.], device='cuda:0') tensor([337.], device='cuda:0') tensor([338.], device='cuda:0') tensor([339.], device='cuda:0') tensor([340.], device='cuda:0') tensor([341.], device='cuda:0') tensor([342.], device='cuda:0') tensor([343.], device='cuda:0') tensor([344.], device='cuda:0') tensor([345.], device='cuda:0') tensor([346.], device='cuda:0') tensor([347.], device='cuda:0') tensor([348.], device='cuda:0') tensor([349.], device='cuda:0') tensor([350.], device='cuda:0') tensor([351.], device='cuda:0') tensor([352.], device='cuda:0') tensor([353.], device='cuda:0') tensor([354.], device='cuda:0') tensor([355.], device='cuda:0') tensor([356.], device='cuda:0') tensor([357.], device='cuda:0') tensor([358.], device='cuda:0') tensor([359.], device='cuda:0') tensor([360.], device='cuda:0') tensor([361.], device='cuda:0') tensor([362.], device='cuda:0') tensor([363.], device='cuda:0') tensor([364.], device='cuda:0') tensor([365.], device='cuda:0') tensor([366.], device='cuda:0') tensor([367.], device='cuda:0') tensor([368.], device='cuda:0') tensor([369.], device='cuda:0') tensor([370.], device='cuda:0') tensor([371.], device='cuda:0') tensor([372.], device='cuda:0') tensor([373.], device='cuda:0') tensor([374.], device='cuda:0') tensor([375.], device='cuda:0') tensor([376.], device='cuda:0') tensor([377.], device='cuda:0') tensor([378.], device='cuda:0') tensor([379.], device='cuda:0') tensor([380.], device='cuda:0') tensor([381.], device='cuda:0') tensor([382.], device='cuda:0') tensor([383.], device='cuda:0') tensor([384.], device='cuda:0') tensor([385.], device='cuda:0') tensor([386.], device='cuda:0') tensor([387.], device='cuda:0') tensor([388.], device='cuda:0') tensor([389.], device='cuda:0') tensor([390.], device='cuda:0') tensor([391.], device='cuda:0') tensor([392.], device='cuda:0') tensor([393.], device='cuda:0') tensor([394.], device='cuda:0') tensor([395.], device='cuda:0') tensor([396.], device='cuda:0') tensor([397.], device='cuda:0') tensor([398.], device='cuda:0') tensor([399.], device='cuda:0') tensor([400.], device='cuda:0') tensor([401.], device='cuda:0') tensor([402.], device='cuda:0') tensor([403.], device='cuda:0') tensor([404.], device='cuda:0') tensor([405.], device='cuda:0') tensor([406.], device='cuda:0') tensor([407.], device='cuda:0') tensor([408.], device='cuda:0') tensor([409.], device='cuda:0') tensor([410.], device='cuda:0') tensor([411.], device='cuda:0') tensor([412.], device='cuda:0') tensor([413.], device='cuda:0') tensor([414.], device='cuda:0') tensor([415.], device='cuda:0') tensor([416.], device='cuda:0') tensor([417.], device='cuda:0') tensor([418.], device='cuda:0') tensor([419.], device='cuda:0') tensor([420.], device='cuda:0') tensor([421.], device='cuda:0') tensor([422.], device='cuda:0') tensor([423.], device='cuda:0') tensor([424.], device='cuda:0') tensor([425.], device='cuda:0') tensor([426.], device='cuda:0') tensor([427.], device='cuda:0') tensor([428.], device='cuda:0') tensor([429.], device='cuda:0') tensor([430.], device='cuda:0') tensor([431.], device='cuda:0') tensor([432.], device='cuda:0') tensor([433.], device='cuda:0') tensor([434.], device='cuda:0') tensor([435.], device='cuda:0') tensor([436.], device='cuda:0') tensor([437.], device='cuda:0') tensor([438.], device='cuda:0') tensor([439.], device='cuda:0') tensor([440.], device='cuda:0') tensor([441.], device='cuda:0') tensor([442.], device='cuda:0') tensor([443.], device='cuda:0') tensor([444.], device='cuda:0') tensor([445.], device='cuda:0') tensor([446.], device='cuda:0') tensor([447.], device='cuda:0') tensor([448.], device='cuda:0') tensor([449.], device='cuda:0') tensor([450.], device='cuda:0') tensor([451.], device='cuda:0') tensor([452.], device='cuda:0') tensor([453.], device='cuda:0') tensor([454.], device='cuda:0') tensor([455.], device='cuda:0') tensor([456.], device='cuda:0') tensor([457.], device='cuda:0') tensor([458.], device='cuda:0') tensor([459.], device='cuda:0') tensor([460.], device='cuda:0') tensor([461.], device='cuda:0') tensor([462.], device='cuda:0') tensor([463.], device='cuda:0') tensor([464.], device='cuda:0') tensor([465.], device='cuda:0') tensor([466.], device='cuda:0') tensor([467.], device='cuda:0') tensor([468.], device='cuda:0') tensor([469.], device='cuda:0') tensor([470.], device='cuda:0') tensor([471.], device='cuda:0') tensor([472.], device='cuda:0') tensor([473.], device='cuda:0') tensor([474.], device='cuda:0') tensor([475.], device='cuda:0') tensor([476.], device='cuda:0') tensor([477.], device='cuda:0') tensor([478.], device='cuda:0') tensor([479.], device='cuda:0') tensor([480.], device='cuda:0') tensor([481.], device='cuda:0') tensor([482.], device='cuda:0') tensor([483.], device='cuda:0') tensor([484.], device='cuda:0') tensor([485.], device='cuda:0') tensor([486.], device='cuda:0') tensor([487.], device='cuda:0') tensor([488.], device='cuda:0') tensor([489.], device='cuda:0') tensor([490.], device='cuda:0') tensor([491.], device='cuda:0') tensor([492.], device='cuda:0') tensor([493.], device='cuda:0') tensor([494.], device='cuda:0') tensor([495.], device='cuda:0') tensor([496.], device='cuda:0') tensor([497.], device='cuda:0') tensor([498.], device='cuda:0') tensor([499.], device='cuda:0') tensor([500.], device='cuda:0') tensor([501.], device='cuda:0') tensor([502.], device='cuda:0') tensor([503.], device='cuda:0') tensor([504.], device='cuda:0') tensor([505.], device='cuda:0') tensor([506.], device='cuda:0') tensor([507.], device='cuda:0') tensor([508.], device='cuda:0') tensor([509.], device='cuda:0') tensor([510.], device='cuda:0') tensor([511.], device='cuda:0') Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i ) Output [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113] [114] [115] [116] [117] [118] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149] [150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191] [192] [193] [194] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240] [241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] [262] [263] [264] [265] [266] [267] [268] [269] [270] [271] [272] [273] [274] [275] [276] [277] [278] [279] [280] [281] [282] [283] [284] [285] [286] [287] [288] [289] [290] [291] [292] [293] [294] [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] [305] [306] [307] [308] [309] [310] [311] [312] [313] [314] [315] [316] [317] [318] [319] [320] [321] [322] [323] [324] [325] [326] [327] [328] [329] [330] [331] [332] [333] [334] [335] [336] [337] [338] [339] [340] [341] [342] [343] [344] [345] [346] [347] [348] [349] [350] [351] [352] [353] [354] [355] [356] [357] [358] [359] [360] [361] [362] [363] [364] [365] [366] [367] [368] [369] [370] [371] [372] [373] [374] [375] [376] [377] [378] [379] [380] [381] [382] [383] [384] [385] [386] [387] [388] [389] [390] [391] [392] [393] [394] [395] [396] [397] [398] [399] [400] [401] [402] [403] [404] [405] [406] [407] [408] [409] [410] [411] [412] [413] [414] [415] [416] [417] [418] [419] [420] [421] [422] [423] [424] [425] [426] [427] [428] [429] [430] [431] [432] [433] [434] [435] [436] [437] [438] [439] [440] [441] [442] [443] [444] [445] [446] [447] [448] [449] [450] [451] [452] [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] [469] [470] [471] [472] [473] [474] [475] [476] [477] [478] [479] [480] [481] [482] [483] [484] [485] [486] [487] [488] [489] [490] [491] [492] [493] [494] [495] [496] [497] [498] [499] [500] [501] [502] [503] [504] [505] [506] [507] [508] [509] [510] [511]","title":"Examples"},{"location":"apis/data/sequence/MTSequence/","text":"data.sequence.MTSequence \u00b6 Class \u00b7 Context \u00b7 Source manager = mdnc . data . sequence . MTSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading codes are built on top of the :fontawesome-solid-external-link-alt: threading module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MTSequence] subgraph procs [Threading Pool] proc1[[Thread 1]] proc2[[Thread 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Threading Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MTSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MTSequence would store the indexer during the initialization. When the start() method is invoked, two threading pools would be created. The first pool maintains several threads, each thread would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MTSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel threads (in pool 1) by the input queue . Each thread would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the thread would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Info The argument worker does not require to be picklable in this case, because all threads are mainted in the same process. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the threading pool. When this method is invoked, the thereading pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Only reserved for compatibility for switching from MPSequence to MTSequence . This flag would not influence anything. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the threading pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the threading pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Examples \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start () as mng : for i in mng : print ( i ) Output tensor([0.], device='cuda:0') tensor([1.], device='cuda:0') tensor([2.], device='cuda:0') tensor([3.], device='cuda:0') tensor([4.], device='cuda:0') tensor([5.], device='cuda:0') tensor([6.], device='cuda:0') tensor([7.], device='cuda:0') tensor([8.], device='cuda:0') tensor([9.], device='cuda:0') tensor([10.], device='cuda:0') tensor([11.], device='cuda:0') tensor([12.], device='cuda:0') tensor([13.], device='cuda:0') tensor([14.], device='cuda:0') tensor([15.], device='cuda:0') tensor([16.], device='cuda:0') tensor([17.], device='cuda:0') tensor([18.], device='cuda:0') tensor([19.], device='cuda:0') tensor([20.], device='cuda:0') tensor([21.], device='cuda:0') tensor([22.], device='cuda:0') tensor([23.], device='cuda:0') tensor([24.], device='cuda:0') tensor([25.], device='cuda:0') tensor([26.], device='cuda:0') tensor([27.], device='cuda:0') tensor([28.], device='cuda:0') tensor([29.], device='cuda:0') tensor([30.], device='cuda:0') tensor([31.], device='cuda:0') tensor([32.], device='cuda:0') tensor([33.], device='cuda:0') tensor([34.], device='cuda:0') tensor([35.], device='cuda:0') tensor([36.], device='cuda:0') tensor([37.], device='cuda:0') tensor([38.], device='cuda:0') tensor([39.], device='cuda:0') tensor([40.], device='cuda:0') tensor([41.], device='cuda:0') tensor([42.], device='cuda:0') tensor([43.], device='cuda:0') tensor([44.], device='cuda:0') tensor([45.], device='cuda:0') tensor([46.], device='cuda:0') tensor([47.], device='cuda:0') tensor([48.], device='cuda:0') tensor([49.], device='cuda:0') tensor([50.], device='cuda:0') tensor([51.], device='cuda:0') tensor([52.], device='cuda:0') tensor([53.], device='cuda:0') tensor([54.], device='cuda:0') tensor([55.], device='cuda:0') tensor([56.], device='cuda:0') tensor([57.], device='cuda:0') tensor([58.], device='cuda:0') tensor([59.], device='cuda:0') tensor([60.], device='cuda:0') tensor([61.], device='cuda:0') tensor([62.], device='cuda:0') tensor([63.], device='cuda:0') tensor([64.], device='cuda:0') tensor([65.], device='cuda:0') tensor([66.], device='cuda:0') tensor([67.], device='cuda:0') tensor([68.], device='cuda:0') tensor([69.], device='cuda:0') tensor([70.], device='cuda:0') tensor([71.], device='cuda:0') tensor([72.], device='cuda:0') tensor([73.], device='cuda:0') tensor([74.], device='cuda:0') tensor([75.], device='cuda:0') tensor([76.], device='cuda:0') tensor([77.], device='cuda:0') tensor([78.], device='cuda:0') tensor([79.], device='cuda:0') tensor([80.], device='cuda:0') tensor([81.], device='cuda:0') tensor([82.], device='cuda:0') tensor([83.], device='cuda:0') tensor([84.], device='cuda:0') tensor([85.], device='cuda:0') tensor([86.], device='cuda:0') tensor([87.], device='cuda:0') tensor([88.], device='cuda:0') tensor([89.], device='cuda:0') tensor([90.], device='cuda:0') tensor([91.], device='cuda:0') tensor([92.], device='cuda:0') tensor([93.], device='cuda:0') tensor([94.], device='cuda:0') tensor([95.], device='cuda:0') tensor([96.], device='cuda:0') tensor([97.], device='cuda:0') tensor([98.], device='cuda:0') tensor([99.], device='cuda:0') tensor([100.], device='cuda:0') tensor([101.], device='cuda:0') tensor([102.], device='cuda:0') tensor([103.], device='cuda:0') tensor([104.], device='cuda:0') tensor([105.], device='cuda:0') tensor([106.], device='cuda:0') tensor([107.], device='cuda:0') tensor([108.], device='cuda:0') tensor([109.], device='cuda:0') tensor([110.], device='cuda:0') tensor([111.], device='cuda:0') tensor([112.], device='cuda:0') tensor([113.], device='cuda:0') tensor([114.], device='cuda:0') tensor([115.], device='cuda:0') tensor([116.], device='cuda:0') tensor([117.], device='cuda:0') tensor([118.], device='cuda:0') tensor([119.], device='cuda:0') tensor([120.], device='cuda:0') tensor([121.], device='cuda:0') tensor([122.], device='cuda:0') tensor([123.], device='cuda:0') tensor([124.], device='cuda:0') tensor([125.], device='cuda:0') tensor([126.], device='cuda:0') tensor([127.], device='cuda:0') tensor([128.], device='cuda:0') tensor([129.], device='cuda:0') tensor([130.], device='cuda:0') tensor([131.], device='cuda:0') tensor([132.], device='cuda:0') tensor([133.], device='cuda:0') tensor([134.], device='cuda:0') tensor([135.], device='cuda:0') tensor([136.], device='cuda:0') tensor([137.], device='cuda:0') tensor([138.], device='cuda:0') tensor([139.], device='cuda:0') tensor([140.], device='cuda:0') tensor([141.], device='cuda:0') tensor([142.], device='cuda:0') tensor([143.], device='cuda:0') tensor([144.], device='cuda:0') tensor([145.], device='cuda:0') tensor([146.], device='cuda:0') tensor([147.], device='cuda:0') tensor([148.], device='cuda:0') tensor([149.], device='cuda:0') tensor([150.], device='cuda:0') tensor([151.], device='cuda:0') tensor([152.], device='cuda:0') tensor([153.], device='cuda:0') tensor([154.], device='cuda:0') tensor([155.], device='cuda:0') tensor([156.], device='cuda:0') tensor([157.], device='cuda:0') tensor([158.], device='cuda:0') tensor([159.], device='cuda:0') tensor([160.], device='cuda:0') tensor([161.], device='cuda:0') tensor([162.], device='cuda:0') tensor([163.], device='cuda:0') tensor([164.], device='cuda:0') tensor([165.], device='cuda:0') tensor([166.], device='cuda:0') tensor([167.], device='cuda:0') tensor([168.], device='cuda:0') tensor([169.], device='cuda:0') tensor([170.], device='cuda:0') tensor([171.], device='cuda:0') tensor([172.], device='cuda:0') tensor([173.], device='cuda:0') tensor([174.], device='cuda:0') tensor([175.], device='cuda:0') tensor([176.], device='cuda:0') tensor([177.], device='cuda:0') tensor([178.], device='cuda:0') tensor([179.], device='cuda:0') tensor([180.], device='cuda:0') tensor([181.], device='cuda:0') tensor([182.], device='cuda:0') tensor([183.], device='cuda:0') tensor([184.], device='cuda:0') tensor([185.], device='cuda:0') tensor([186.], device='cuda:0') tensor([187.], device='cuda:0') tensor([188.], device='cuda:0') tensor([189.], device='cuda:0') tensor([190.], device='cuda:0') tensor([191.], device='cuda:0') tensor([192.], device='cuda:0') tensor([193.], device='cuda:0') tensor([194.], device='cuda:0') tensor([195.], device='cuda:0') tensor([196.], device='cuda:0') tensor([197.], device='cuda:0') tensor([198.], device='cuda:0') tensor([199.], device='cuda:0') tensor([200.], device='cuda:0') tensor([201.], device='cuda:0') tensor([202.], device='cuda:0') tensor([203.], device='cuda:0') tensor([204.], device='cuda:0') tensor([205.], device='cuda:0') tensor([206.], device='cuda:0') tensor([207.], device='cuda:0') tensor([208.], device='cuda:0') tensor([209.], device='cuda:0') tensor([210.], device='cuda:0') tensor([211.], device='cuda:0') tensor([212.], device='cuda:0') tensor([213.], device='cuda:0') tensor([214.], device='cuda:0') tensor([215.], device='cuda:0') tensor([216.], device='cuda:0') tensor([217.], device='cuda:0') tensor([218.], device='cuda:0') tensor([219.], device='cuda:0') tensor([220.], device='cuda:0') tensor([221.], device='cuda:0') tensor([222.], device='cuda:0') tensor([223.], device='cuda:0') tensor([224.], device='cuda:0') tensor([225.], device='cuda:0') tensor([226.], device='cuda:0') tensor([227.], device='cuda:0') tensor([228.], device='cuda:0') tensor([229.], device='cuda:0') tensor([230.], device='cuda:0') tensor([231.], device='cuda:0') tensor([232.], device='cuda:0') tensor([233.], device='cuda:0') tensor([234.], device='cuda:0') tensor([235.], device='cuda:0') tensor([236.], device='cuda:0') tensor([237.], device='cuda:0') tensor([238.], device='cuda:0') tensor([239.], device='cuda:0') tensor([240.], device='cuda:0') tensor([241.], device='cuda:0') tensor([242.], device='cuda:0') tensor([243.], device='cuda:0') tensor([244.], device='cuda:0') tensor([245.], device='cuda:0') tensor([246.], device='cuda:0') tensor([247.], device='cuda:0') tensor([248.], device='cuda:0') tensor([249.], device='cuda:0') tensor([250.], device='cuda:0') tensor([251.], device='cuda:0') tensor([252.], device='cuda:0') tensor([253.], device='cuda:0') tensor([254.], device='cuda:0') tensor([255.], device='cuda:0') tensor([256.], device='cuda:0') tensor([257.], device='cuda:0') tensor([258.], device='cuda:0') tensor([259.], device='cuda:0') tensor([260.], device='cuda:0') tensor([261.], device='cuda:0') tensor([262.], device='cuda:0') tensor([263.], device='cuda:0') tensor([264.], device='cuda:0') tensor([265.], device='cuda:0') tensor([266.], device='cuda:0') tensor([267.], device='cuda:0') tensor([268.], device='cuda:0') tensor([269.], device='cuda:0') tensor([270.], device='cuda:0') tensor([271.], device='cuda:0') tensor([272.], device='cuda:0') tensor([273.], device='cuda:0') tensor([274.], device='cuda:0') tensor([275.], device='cuda:0') tensor([276.], device='cuda:0') tensor([277.], device='cuda:0') tensor([278.], device='cuda:0') tensor([279.], device='cuda:0') tensor([280.], device='cuda:0') tensor([281.], device='cuda:0') tensor([282.], device='cuda:0') tensor([283.], device='cuda:0') tensor([284.], device='cuda:0') tensor([285.], device='cuda:0') tensor([286.], device='cuda:0') tensor([287.], device='cuda:0') tensor([288.], device='cuda:0') tensor([289.], device='cuda:0') tensor([290.], device='cuda:0') tensor([291.], device='cuda:0') tensor([292.], device='cuda:0') tensor([293.], device='cuda:0') tensor([294.], device='cuda:0') tensor([295.], device='cuda:0') tensor([296.], device='cuda:0') tensor([297.], device='cuda:0') tensor([298.], device='cuda:0') tensor([299.], device='cuda:0') tensor([300.], device='cuda:0') tensor([301.], device='cuda:0') tensor([302.], device='cuda:0') tensor([303.], device='cuda:0') tensor([304.], device='cuda:0') tensor([305.], device='cuda:0') tensor([306.], device='cuda:0') tensor([307.], device='cuda:0') tensor([308.], device='cuda:0') tensor([309.], device='cuda:0') tensor([310.], device='cuda:0') tensor([311.], device='cuda:0') tensor([312.], device='cuda:0') tensor([313.], device='cuda:0') tensor([314.], device='cuda:0') tensor([315.], device='cuda:0') tensor([316.], device='cuda:0') tensor([317.], device='cuda:0') tensor([318.], device='cuda:0') tensor([319.], device='cuda:0') tensor([320.], device='cuda:0') tensor([321.], device='cuda:0') tensor([322.], device='cuda:0') tensor([323.], device='cuda:0') tensor([324.], device='cuda:0') tensor([325.], device='cuda:0') tensor([326.], device='cuda:0') tensor([327.], device='cuda:0') tensor([328.], device='cuda:0') tensor([329.], device='cuda:0') tensor([330.], device='cuda:0') tensor([331.], device='cuda:0') tensor([332.], device='cuda:0') tensor([333.], device='cuda:0') tensor([334.], device='cuda:0') tensor([335.], device='cuda:0') tensor([336.], device='cuda:0') tensor([337.], device='cuda:0') tensor([338.], device='cuda:0') tensor([339.], device='cuda:0') tensor([340.], device='cuda:0') tensor([341.], device='cuda:0') tensor([342.], device='cuda:0') tensor([343.], device='cuda:0') tensor([344.], device='cuda:0') tensor([345.], device='cuda:0') tensor([346.], device='cuda:0') tensor([347.], device='cuda:0') tensor([348.], device='cuda:0') tensor([349.], device='cuda:0') tensor([350.], device='cuda:0') tensor([351.], device='cuda:0') tensor([352.], device='cuda:0') tensor([353.], device='cuda:0') tensor([354.], device='cuda:0') tensor([355.], device='cuda:0') tensor([356.], device='cuda:0') tensor([357.], device='cuda:0') tensor([358.], device='cuda:0') tensor([359.], device='cuda:0') tensor([360.], device='cuda:0') tensor([361.], device='cuda:0') tensor([362.], device='cuda:0') tensor([363.], device='cuda:0') tensor([364.], device='cuda:0') tensor([365.], device='cuda:0') tensor([366.], device='cuda:0') tensor([367.], device='cuda:0') tensor([368.], device='cuda:0') tensor([369.], device='cuda:0') tensor([370.], device='cuda:0') tensor([371.], device='cuda:0') tensor([372.], device='cuda:0') tensor([373.], device='cuda:0') tensor([374.], device='cuda:0') tensor([375.], device='cuda:0') tensor([376.], device='cuda:0') tensor([377.], device='cuda:0') tensor([378.], device='cuda:0') tensor([379.], device='cuda:0') tensor([380.], device='cuda:0') tensor([381.], device='cuda:0') tensor([382.], device='cuda:0') tensor([383.], device='cuda:0') tensor([384.], device='cuda:0') tensor([385.], device='cuda:0') tensor([386.], device='cuda:0') tensor([387.], device='cuda:0') tensor([388.], device='cuda:0') tensor([389.], device='cuda:0') tensor([390.], device='cuda:0') tensor([391.], device='cuda:0') tensor([392.], device='cuda:0') tensor([393.], device='cuda:0') tensor([394.], device='cuda:0') tensor([395.], device='cuda:0') tensor([396.], device='cuda:0') tensor([397.], device='cuda:0') tensor([398.], device='cuda:0') tensor([399.], device='cuda:0') tensor([400.], device='cuda:0') tensor([401.], device='cuda:0') tensor([402.], device='cuda:0') tensor([403.], device='cuda:0') tensor([404.], device='cuda:0') tensor([405.], device='cuda:0') tensor([406.], device='cuda:0') tensor([407.], device='cuda:0') tensor([408.], device='cuda:0') tensor([409.], device='cuda:0') tensor([410.], device='cuda:0') tensor([411.], device='cuda:0') tensor([412.], device='cuda:0') tensor([413.], device='cuda:0') tensor([414.], device='cuda:0') tensor([415.], device='cuda:0') tensor([416.], device='cuda:0') tensor([417.], device='cuda:0') tensor([418.], device='cuda:0') tensor([419.], device='cuda:0') tensor([420.], device='cuda:0') tensor([421.], device='cuda:0') tensor([422.], device='cuda:0') tensor([423.], device='cuda:0') tensor([424.], device='cuda:0') tensor([425.], device='cuda:0') tensor([426.], device='cuda:0') tensor([427.], device='cuda:0') tensor([428.], device='cuda:0') tensor([429.], device='cuda:0') tensor([430.], device='cuda:0') tensor([431.], device='cuda:0') tensor([432.], device='cuda:0') tensor([433.], device='cuda:0') tensor([434.], device='cuda:0') tensor([435.], device='cuda:0') tensor([436.], device='cuda:0') tensor([437.], device='cuda:0') tensor([438.], device='cuda:0') tensor([439.], device='cuda:0') tensor([440.], device='cuda:0') tensor([441.], device='cuda:0') tensor([442.], device='cuda:0') tensor([443.], device='cuda:0') tensor([444.], device='cuda:0') tensor([445.], device='cuda:0') tensor([446.], device='cuda:0') tensor([447.], device='cuda:0') tensor([448.], device='cuda:0') tensor([449.], device='cuda:0') tensor([450.], device='cuda:0') tensor([451.], device='cuda:0') tensor([452.], device='cuda:0') tensor([453.], device='cuda:0') tensor([454.], device='cuda:0') tensor([455.], device='cuda:0') tensor([456.], device='cuda:0') tensor([457.], device='cuda:0') tensor([458.], device='cuda:0') tensor([459.], device='cuda:0') tensor([460.], device='cuda:0') tensor([461.], device='cuda:0') tensor([462.], device='cuda:0') tensor([463.], device='cuda:0') tensor([464.], device='cuda:0') tensor([465.], device='cuda:0') tensor([466.], device='cuda:0') tensor([467.], device='cuda:0') tensor([468.], device='cuda:0') tensor([469.], device='cuda:0') tensor([470.], device='cuda:0') tensor([471.], device='cuda:0') tensor([472.], device='cuda:0') tensor([473.], device='cuda:0') tensor([474.], device='cuda:0') tensor([475.], device='cuda:0') tensor([476.], device='cuda:0') tensor([477.], device='cuda:0') tensor([478.], device='cuda:0') tensor([479.], device='cuda:0') tensor([480.], device='cuda:0') tensor([481.], device='cuda:0') tensor([482.], device='cuda:0') tensor([483.], device='cuda:0') tensor([484.], device='cuda:0') tensor([485.], device='cuda:0') tensor([486.], device='cuda:0') tensor([487.], device='cuda:0') tensor([488.], device='cuda:0') tensor([489.], device='cuda:0') tensor([490.], device='cuda:0') tensor([491.], device='cuda:0') tensor([492.], device='cuda:0') tensor([493.], device='cuda:0') tensor([494.], device='cuda:0') tensor([495.], device='cuda:0') tensor([496.], device='cuda:0') tensor([497.], device='cuda:0') tensor([498.], device='cuda:0') tensor([499.], device='cuda:0') tensor([500.], device='cuda:0') tensor([501.], device='cuda:0') tensor([502.], device='cuda:0') tensor([503.], device='cuda:0') tensor([504.], device='cuda:0') tensor([505.], device='cuda:0') tensor([506.], device='cuda:0') tensor([507.], device='cuda:0') tensor([508.], device='cuda:0') tensor([509.], device='cuda:0') tensor([510.], device='cuda:0') tensor([511.], device='cuda:0') Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i ) Output [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113] [114] [115] [116] [117] [118] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149] [150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191] [192] [193] [194] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240] [241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] [262] [263] [264] [265] [266] [267] [268] [269] [270] [271] [272] [273] [274] [275] [276] [277] [278] [279] [280] [281] [282] [283] [284] [285] [286] [287] [288] [289] [290] [291] [292] [293] [294] [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] [305] [306] [307] [308] [309] [310] [311] [312] [313] [314] [315] [316] [317] [318] [319] [320] [321] [322] [323] [324] [325] [326] [327] [328] [329] [330] [331] [332] [333] [334] [335] [336] [337] [338] [339] [340] [341] [342] [343] [344] [345] [346] [347] [348] [349] [350] [351] [352] [353] [354] [355] [356] [357] [358] [359] [360] [361] [362] [363] [364] [365] [366] [367] [368] [369] [370] [371] [372] [373] [374] [375] [376] [377] [378] [379] [380] [381] [382] [383] [384] [385] [386] [387] [388] [389] [390] [391] [392] [393] [394] [395] [396] [397] [398] [399] [400] [401] [402] [403] [404] [405] [406] [407] [408] [409] [410] [411] [412] [413] [414] [415] [416] [417] [418] [419] [420] [421] [422] [423] [424] [425] [426] [427] [428] [429] [430] [431] [432] [433] [434] [435] [436] [437] [438] [439] [440] [441] [442] [443] [444] [445] [446] [447] [448] [449] [450] [451] [452] [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] [469] [470] [471] [472] [473] [474] [475] [476] [477] [478] [479] [480] [481] [482] [483] [484] [485] [486] [487] [488] [489] [490] [491] [492] [493] [494] [495] [496] [497] [498] [499] [500] [501] [502] [503] [504] [505] [506] [507] [508] [509] [510] [511]","title":"<span class='magic-codeicon-class'>MTSequence</span>"},{"location":"apis/data/sequence/MTSequence/#datasequencemtsequence","text":"Class \u00b7 Context \u00b7 Source manager = mdnc . data . sequence . MTSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading. It is designed as an alternative :fontawesome-solid-external-link-alt: keras.utils.Sequence . The multi-threading codes are built on top of the :fontawesome-solid-external-link-alt: threading module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. The workflow of this class is described in the following figure: flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MTSequence] subgraph procs [Threading Pool] proc1[[Thread 1]] proc2[[Thread 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Threading Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MTSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MTSequence would store the indexer during the initialization. When the start() method is invoked, two threading pools would be created. The first pool maintains several threads, each thread would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MTSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel threads (in pool 1) by the input queue . Each thread would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the thread would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed).","title":"data.sequence.MTSequence"},{"location":"apis/data/sequence/MTSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Info The argument worker does not require to be picklable in this case, because all threads are mainted in the same process.","title":"Arguments"},{"location":"apis/data/sequence/MTSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MTSequence/#start","text":"manager . start ( compat = None ) Start the threading pool. When this method is invoked, the thereading pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Only reserved for compatibility for switching from MPSequence to MTSequence . This flag would not influence anything. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MTSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the threading pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MTSequence/#finish","text":"manager . finish () Finish the threading pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MTSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MTSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MTSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MTSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MTSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MTSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MTSequence/#examples","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start () as mng : for i in mng : print ( i ) Output tensor([0.], device='cuda:0') tensor([1.], device='cuda:0') tensor([2.], device='cuda:0') tensor([3.], device='cuda:0') tensor([4.], device='cuda:0') tensor([5.], device='cuda:0') tensor([6.], device='cuda:0') tensor([7.], device='cuda:0') tensor([8.], device='cuda:0') tensor([9.], device='cuda:0') tensor([10.], device='cuda:0') tensor([11.], device='cuda:0') tensor([12.], device='cuda:0') tensor([13.], device='cuda:0') tensor([14.], device='cuda:0') tensor([15.], device='cuda:0') tensor([16.], device='cuda:0') tensor([17.], device='cuda:0') tensor([18.], device='cuda:0') tensor([19.], device='cuda:0') tensor([20.], device='cuda:0') tensor([21.], device='cuda:0') tensor([22.], device='cuda:0') tensor([23.], device='cuda:0') tensor([24.], device='cuda:0') tensor([25.], device='cuda:0') tensor([26.], device='cuda:0') tensor([27.], device='cuda:0') tensor([28.], device='cuda:0') tensor([29.], device='cuda:0') tensor([30.], device='cuda:0') tensor([31.], device='cuda:0') tensor([32.], device='cuda:0') tensor([33.], device='cuda:0') tensor([34.], device='cuda:0') tensor([35.], device='cuda:0') tensor([36.], device='cuda:0') tensor([37.], device='cuda:0') tensor([38.], device='cuda:0') tensor([39.], device='cuda:0') tensor([40.], device='cuda:0') tensor([41.], device='cuda:0') tensor([42.], device='cuda:0') tensor([43.], device='cuda:0') tensor([44.], device='cuda:0') tensor([45.], device='cuda:0') tensor([46.], device='cuda:0') tensor([47.], device='cuda:0') tensor([48.], device='cuda:0') tensor([49.], device='cuda:0') tensor([50.], device='cuda:0') tensor([51.], device='cuda:0') tensor([52.], device='cuda:0') tensor([53.], device='cuda:0') tensor([54.], device='cuda:0') tensor([55.], device='cuda:0') tensor([56.], device='cuda:0') tensor([57.], device='cuda:0') tensor([58.], device='cuda:0') tensor([59.], device='cuda:0') tensor([60.], device='cuda:0') tensor([61.], device='cuda:0') tensor([62.], device='cuda:0') tensor([63.], device='cuda:0') tensor([64.], device='cuda:0') tensor([65.], device='cuda:0') tensor([66.], device='cuda:0') tensor([67.], device='cuda:0') tensor([68.], device='cuda:0') tensor([69.], device='cuda:0') tensor([70.], device='cuda:0') tensor([71.], device='cuda:0') tensor([72.], device='cuda:0') tensor([73.], device='cuda:0') tensor([74.], device='cuda:0') tensor([75.], device='cuda:0') tensor([76.], device='cuda:0') tensor([77.], device='cuda:0') tensor([78.], device='cuda:0') tensor([79.], device='cuda:0') tensor([80.], device='cuda:0') tensor([81.], device='cuda:0') tensor([82.], device='cuda:0') tensor([83.], device='cuda:0') tensor([84.], device='cuda:0') tensor([85.], device='cuda:0') tensor([86.], device='cuda:0') tensor([87.], device='cuda:0') tensor([88.], device='cuda:0') tensor([89.], device='cuda:0') tensor([90.], device='cuda:0') tensor([91.], device='cuda:0') tensor([92.], device='cuda:0') tensor([93.], device='cuda:0') tensor([94.], device='cuda:0') tensor([95.], device='cuda:0') tensor([96.], device='cuda:0') tensor([97.], device='cuda:0') tensor([98.], device='cuda:0') tensor([99.], device='cuda:0') tensor([100.], device='cuda:0') tensor([101.], device='cuda:0') tensor([102.], device='cuda:0') tensor([103.], device='cuda:0') tensor([104.], device='cuda:0') tensor([105.], device='cuda:0') tensor([106.], device='cuda:0') tensor([107.], device='cuda:0') tensor([108.], device='cuda:0') tensor([109.], device='cuda:0') tensor([110.], device='cuda:0') tensor([111.], device='cuda:0') tensor([112.], device='cuda:0') tensor([113.], device='cuda:0') tensor([114.], device='cuda:0') tensor([115.], device='cuda:0') tensor([116.], device='cuda:0') tensor([117.], device='cuda:0') tensor([118.], device='cuda:0') tensor([119.], device='cuda:0') tensor([120.], device='cuda:0') tensor([121.], device='cuda:0') tensor([122.], device='cuda:0') tensor([123.], device='cuda:0') tensor([124.], device='cuda:0') tensor([125.], device='cuda:0') tensor([126.], device='cuda:0') tensor([127.], device='cuda:0') tensor([128.], device='cuda:0') tensor([129.], device='cuda:0') tensor([130.], device='cuda:0') tensor([131.], device='cuda:0') tensor([132.], device='cuda:0') tensor([133.], device='cuda:0') tensor([134.], device='cuda:0') tensor([135.], device='cuda:0') tensor([136.], device='cuda:0') tensor([137.], device='cuda:0') tensor([138.], device='cuda:0') tensor([139.], device='cuda:0') tensor([140.], device='cuda:0') tensor([141.], device='cuda:0') tensor([142.], device='cuda:0') tensor([143.], device='cuda:0') tensor([144.], device='cuda:0') tensor([145.], device='cuda:0') tensor([146.], device='cuda:0') tensor([147.], device='cuda:0') tensor([148.], device='cuda:0') tensor([149.], device='cuda:0') tensor([150.], device='cuda:0') tensor([151.], device='cuda:0') tensor([152.], device='cuda:0') tensor([153.], device='cuda:0') tensor([154.], device='cuda:0') tensor([155.], device='cuda:0') tensor([156.], device='cuda:0') tensor([157.], device='cuda:0') tensor([158.], device='cuda:0') tensor([159.], device='cuda:0') tensor([160.], device='cuda:0') tensor([161.], device='cuda:0') tensor([162.], device='cuda:0') tensor([163.], device='cuda:0') tensor([164.], device='cuda:0') tensor([165.], device='cuda:0') tensor([166.], device='cuda:0') tensor([167.], device='cuda:0') tensor([168.], device='cuda:0') tensor([169.], device='cuda:0') tensor([170.], device='cuda:0') tensor([171.], device='cuda:0') tensor([172.], device='cuda:0') tensor([173.], device='cuda:0') tensor([174.], device='cuda:0') tensor([175.], device='cuda:0') tensor([176.], device='cuda:0') tensor([177.], device='cuda:0') tensor([178.], device='cuda:0') tensor([179.], device='cuda:0') tensor([180.], device='cuda:0') tensor([181.], device='cuda:0') tensor([182.], device='cuda:0') tensor([183.], device='cuda:0') tensor([184.], device='cuda:0') tensor([185.], device='cuda:0') tensor([186.], device='cuda:0') tensor([187.], device='cuda:0') tensor([188.], device='cuda:0') tensor([189.], device='cuda:0') tensor([190.], device='cuda:0') tensor([191.], device='cuda:0') tensor([192.], device='cuda:0') tensor([193.], device='cuda:0') tensor([194.], device='cuda:0') tensor([195.], device='cuda:0') tensor([196.], device='cuda:0') tensor([197.], device='cuda:0') tensor([198.], device='cuda:0') tensor([199.], device='cuda:0') tensor([200.], device='cuda:0') tensor([201.], device='cuda:0') tensor([202.], device='cuda:0') tensor([203.], device='cuda:0') tensor([204.], device='cuda:0') tensor([205.], device='cuda:0') tensor([206.], device='cuda:0') tensor([207.], device='cuda:0') tensor([208.], device='cuda:0') tensor([209.], device='cuda:0') tensor([210.], device='cuda:0') tensor([211.], device='cuda:0') tensor([212.], device='cuda:0') tensor([213.], device='cuda:0') tensor([214.], device='cuda:0') tensor([215.], device='cuda:0') tensor([216.], device='cuda:0') tensor([217.], device='cuda:0') tensor([218.], device='cuda:0') tensor([219.], device='cuda:0') tensor([220.], device='cuda:0') tensor([221.], device='cuda:0') tensor([222.], device='cuda:0') tensor([223.], device='cuda:0') tensor([224.], device='cuda:0') tensor([225.], device='cuda:0') tensor([226.], device='cuda:0') tensor([227.], device='cuda:0') tensor([228.], device='cuda:0') tensor([229.], device='cuda:0') tensor([230.], device='cuda:0') tensor([231.], device='cuda:0') tensor([232.], device='cuda:0') tensor([233.], device='cuda:0') tensor([234.], device='cuda:0') tensor([235.], device='cuda:0') tensor([236.], device='cuda:0') tensor([237.], device='cuda:0') tensor([238.], device='cuda:0') tensor([239.], device='cuda:0') tensor([240.], device='cuda:0') tensor([241.], device='cuda:0') tensor([242.], device='cuda:0') tensor([243.], device='cuda:0') tensor([244.], device='cuda:0') tensor([245.], device='cuda:0') tensor([246.], device='cuda:0') tensor([247.], device='cuda:0') tensor([248.], device='cuda:0') tensor([249.], device='cuda:0') tensor([250.], device='cuda:0') tensor([251.], device='cuda:0') tensor([252.], device='cuda:0') tensor([253.], device='cuda:0') tensor([254.], device='cuda:0') tensor([255.], device='cuda:0') tensor([256.], device='cuda:0') tensor([257.], device='cuda:0') tensor([258.], device='cuda:0') tensor([259.], device='cuda:0') tensor([260.], device='cuda:0') tensor([261.], device='cuda:0') tensor([262.], device='cuda:0') tensor([263.], device='cuda:0') tensor([264.], device='cuda:0') tensor([265.], device='cuda:0') tensor([266.], device='cuda:0') tensor([267.], device='cuda:0') tensor([268.], device='cuda:0') tensor([269.], device='cuda:0') tensor([270.], device='cuda:0') tensor([271.], device='cuda:0') tensor([272.], device='cuda:0') tensor([273.], device='cuda:0') tensor([274.], device='cuda:0') tensor([275.], device='cuda:0') tensor([276.], device='cuda:0') tensor([277.], device='cuda:0') tensor([278.], device='cuda:0') tensor([279.], device='cuda:0') tensor([280.], device='cuda:0') tensor([281.], device='cuda:0') tensor([282.], device='cuda:0') tensor([283.], device='cuda:0') tensor([284.], device='cuda:0') tensor([285.], device='cuda:0') tensor([286.], device='cuda:0') tensor([287.], device='cuda:0') tensor([288.], device='cuda:0') tensor([289.], device='cuda:0') tensor([290.], device='cuda:0') tensor([291.], device='cuda:0') tensor([292.], device='cuda:0') tensor([293.], device='cuda:0') tensor([294.], device='cuda:0') tensor([295.], device='cuda:0') tensor([296.], device='cuda:0') tensor([297.], device='cuda:0') tensor([298.], device='cuda:0') tensor([299.], device='cuda:0') tensor([300.], device='cuda:0') tensor([301.], device='cuda:0') tensor([302.], device='cuda:0') tensor([303.], device='cuda:0') tensor([304.], device='cuda:0') tensor([305.], device='cuda:0') tensor([306.], device='cuda:0') tensor([307.], device='cuda:0') tensor([308.], device='cuda:0') tensor([309.], device='cuda:0') tensor([310.], device='cuda:0') tensor([311.], device='cuda:0') tensor([312.], device='cuda:0') tensor([313.], device='cuda:0') tensor([314.], device='cuda:0') tensor([315.], device='cuda:0') tensor([316.], device='cuda:0') tensor([317.], device='cuda:0') tensor([318.], device='cuda:0') tensor([319.], device='cuda:0') tensor([320.], device='cuda:0') tensor([321.], device='cuda:0') tensor([322.], device='cuda:0') tensor([323.], device='cuda:0') tensor([324.], device='cuda:0') tensor([325.], device='cuda:0') tensor([326.], device='cuda:0') tensor([327.], device='cuda:0') tensor([328.], device='cuda:0') tensor([329.], device='cuda:0') tensor([330.], device='cuda:0') tensor([331.], device='cuda:0') tensor([332.], device='cuda:0') tensor([333.], device='cuda:0') tensor([334.], device='cuda:0') tensor([335.], device='cuda:0') tensor([336.], device='cuda:0') tensor([337.], device='cuda:0') tensor([338.], device='cuda:0') tensor([339.], device='cuda:0') tensor([340.], device='cuda:0') tensor([341.], device='cuda:0') tensor([342.], device='cuda:0') tensor([343.], device='cuda:0') tensor([344.], device='cuda:0') tensor([345.], device='cuda:0') tensor([346.], device='cuda:0') tensor([347.], device='cuda:0') tensor([348.], device='cuda:0') tensor([349.], device='cuda:0') tensor([350.], device='cuda:0') tensor([351.], device='cuda:0') tensor([352.], device='cuda:0') tensor([353.], device='cuda:0') tensor([354.], device='cuda:0') tensor([355.], device='cuda:0') tensor([356.], device='cuda:0') tensor([357.], device='cuda:0') tensor([358.], device='cuda:0') tensor([359.], device='cuda:0') tensor([360.], device='cuda:0') tensor([361.], device='cuda:0') tensor([362.], device='cuda:0') tensor([363.], device='cuda:0') tensor([364.], device='cuda:0') tensor([365.], device='cuda:0') tensor([366.], device='cuda:0') tensor([367.], device='cuda:0') tensor([368.], device='cuda:0') tensor([369.], device='cuda:0') tensor([370.], device='cuda:0') tensor([371.], device='cuda:0') tensor([372.], device='cuda:0') tensor([373.], device='cuda:0') tensor([374.], device='cuda:0') tensor([375.], device='cuda:0') tensor([376.], device='cuda:0') tensor([377.], device='cuda:0') tensor([378.], device='cuda:0') tensor([379.], device='cuda:0') tensor([380.], device='cuda:0') tensor([381.], device='cuda:0') tensor([382.], device='cuda:0') tensor([383.], device='cuda:0') tensor([384.], device='cuda:0') tensor([385.], device='cuda:0') tensor([386.], device='cuda:0') tensor([387.], device='cuda:0') tensor([388.], device='cuda:0') tensor([389.], device='cuda:0') tensor([390.], device='cuda:0') tensor([391.], device='cuda:0') tensor([392.], device='cuda:0') tensor([393.], device='cuda:0') tensor([394.], device='cuda:0') tensor([395.], device='cuda:0') tensor([396.], device='cuda:0') tensor([397.], device='cuda:0') tensor([398.], device='cuda:0') tensor([399.], device='cuda:0') tensor([400.], device='cuda:0') tensor([401.], device='cuda:0') tensor([402.], device='cuda:0') tensor([403.], device='cuda:0') tensor([404.], device='cuda:0') tensor([405.], device='cuda:0') tensor([406.], device='cuda:0') tensor([407.], device='cuda:0') tensor([408.], device='cuda:0') tensor([409.], device='cuda:0') tensor([410.], device='cuda:0') tensor([411.], device='cuda:0') tensor([412.], device='cuda:0') tensor([413.], device='cuda:0') tensor([414.], device='cuda:0') tensor([415.], device='cuda:0') tensor([416.], device='cuda:0') tensor([417.], device='cuda:0') tensor([418.], device='cuda:0') tensor([419.], device='cuda:0') tensor([420.], device='cuda:0') tensor([421.], device='cuda:0') tensor([422.], device='cuda:0') tensor([423.], device='cuda:0') tensor([424.], device='cuda:0') tensor([425.], device='cuda:0') tensor([426.], device='cuda:0') tensor([427.], device='cuda:0') tensor([428.], device='cuda:0') tensor([429.], device='cuda:0') tensor([430.], device='cuda:0') tensor([431.], device='cuda:0') tensor([432.], device='cuda:0') tensor([433.], device='cuda:0') tensor([434.], device='cuda:0') tensor([435.], device='cuda:0') tensor([436.], device='cuda:0') tensor([437.], device='cuda:0') tensor([438.], device='cuda:0') tensor([439.], device='cuda:0') tensor([440.], device='cuda:0') tensor([441.], device='cuda:0') tensor([442.], device='cuda:0') tensor([443.], device='cuda:0') tensor([444.], device='cuda:0') tensor([445.], device='cuda:0') tensor([446.], device='cuda:0') tensor([447.], device='cuda:0') tensor([448.], device='cuda:0') tensor([449.], device='cuda:0') tensor([450.], device='cuda:0') tensor([451.], device='cuda:0') tensor([452.], device='cuda:0') tensor([453.], device='cuda:0') tensor([454.], device='cuda:0') tensor([455.], device='cuda:0') tensor([456.], device='cuda:0') tensor([457.], device='cuda:0') tensor([458.], device='cuda:0') tensor([459.], device='cuda:0') tensor([460.], device='cuda:0') tensor([461.], device='cuda:0') tensor([462.], device='cuda:0') tensor([463.], device='cuda:0') tensor([464.], device='cuda:0') tensor([465.], device='cuda:0') tensor([466.], device='cuda:0') tensor([467.], device='cuda:0') tensor([468.], device='cuda:0') tensor([469.], device='cuda:0') tensor([470.], device='cuda:0') tensor([471.], device='cuda:0') tensor([472.], device='cuda:0') tensor([473.], device='cuda:0') tensor([474.], device='cuda:0') tensor([475.], device='cuda:0') tensor([476.], device='cuda:0') tensor([477.], device='cuda:0') tensor([478.], device='cuda:0') tensor([479.], device='cuda:0') tensor([480.], device='cuda:0') tensor([481.], device='cuda:0') tensor([482.], device='cuda:0') tensor([483.], device='cuda:0') tensor([484.], device='cuda:0') tensor([485.], device='cuda:0') tensor([486.], device='cuda:0') tensor([487.], device='cuda:0') tensor([488.], device='cuda:0') tensor([489.], device='cuda:0') tensor([490.], device='cuda:0') tensor([491.], device='cuda:0') tensor([492.], device='cuda:0') tensor([493.], device='cuda:0') tensor([494.], device='cuda:0') tensor([495.], device='cuda:0') tensor([496.], device='cuda:0') tensor([497.], device='cuda:0') tensor([498.], device='cuda:0') tensor([499.], device='cuda:0') tensor([500.], device='cuda:0') tensor([501.], device='cuda:0') tensor([502.], device='cuda:0') tensor([503.], device='cuda:0') tensor([504.], device='cuda:0') tensor([505.], device='cuda:0') tensor([506.], device='cuda:0') tensor([507.], device='cuda:0') tensor([508.], device='cuda:0') tensor([509.], device='cuda:0') tensor([510.], device='cuda:0') tensor([511.], device='cuda:0') Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) if __name__ == '__main__' : with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i ) Output [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113] [114] [115] [116] [117] [118] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149] [150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191] [192] [193] [194] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240] [241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] [262] [263] [264] [265] [266] [267] [268] [269] [270] [271] [272] [273] [274] [275] [276] [277] [278] [279] [280] [281] [282] [283] [284] [285] [286] [287] [288] [289] [290] [291] [292] [293] [294] [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] [305] [306] [307] [308] [309] [310] [311] [312] [313] [314] [315] [316] [317] [318] [319] [320] [321] [322] [323] [324] [325] [326] [327] [328] [329] [330] [331] [332] [333] [334] [335] [336] [337] [338] [339] [340] [341] [342] [343] [344] [345] [346] [347] [348] [349] [350] [351] [352] [353] [354] [355] [356] [357] [358] [359] [360] [361] [362] [363] [364] [365] [366] [367] [368] [369] [370] [371] [372] [373] [374] [375] [376] [377] [378] [379] [380] [381] [382] [383] [384] [385] [386] [387] [388] [389] [390] [391] [392] [393] [394] [395] [396] [397] [398] [399] [400] [401] [402] [403] [404] [405] [406] [407] [408] [409] [410] [411] [412] [413] [414] [415] [416] [417] [418] [419] [420] [421] [422] [423] [424] [425] [426] [427] [428] [429] [430] [431] [432] [433] [434] [435] [436] [437] [438] [439] [440] [441] [442] [443] [444] [445] [446] [447] [448] [449] [450] [451] [452] [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] [469] [470] [471] [472] [473] [474] [475] [476] [477] [478] [479] [480] [481] [482] [483] [484] [485] [486] [487] [488] [489] [490] [491] [492] [493] [494] [495] [496] [497] [498] [499] [500] [501] [502] [503] [504] [505] [506] [507] [508] [509] [510] [511]","title":"Examples"},{"location":"apis/data/webtools/DataChecker/","text":"data.webtools.DataChecker \u00b6 Class \u00b7 Source dchecker = mdnc . data . webtools . DataChecker ( root = './datasets' , set_list_file = 'web-data' , token = '' , verbose = False ) This data checker could check the local dataset folder, find the not existing datasets and fetch those required datasets from online repositories or links. The workflow is illustrated in the following figure, flowchart LR subgraph dchecker [DataChecker] init(__init__) add(add_query_file) query(query) end init -->|load| webdata[(set_list_file)] add -->|load| fnames[(file_names)] query --> start webdata --> |set_names| start fnames --> |query_list| ifblock flow:::flowstyle subgraph flow [query work flow] start([for each<br>dataset]) --> |set_names| eachset([for each<br>item]) eachset -->|set_name| ifblock{set_name<br>in<br>query_list?}:::ifstyle ifblock -->|yes| ifblock2{file<br>exists?}:::ifstyle ifblock2 --> |yes| eachset ifblock2 --> |no| download[Download<br>the dataset] download --> start end classDef ifstyle fill:#eee, stroke: #999; classDef flowstyle fill:#FEEEF0, stroke: #b54051; To use this class, users require to follow 3 steps: Initialize the DataChecker with the set_list_file argument, which is a json file. This file defines where the online datasets stored and what those datasets have. Use add_query_file to add the require data file name. Invoke query , this method would start iterate the dataset list, then find and download all online datasets which satisfies the following conditions: Has a file item that does not locally exist. Has a file item that is required by the query list. A private repository requires a token. In this case, the argument token need to be not blank. Arguments \u00b6 Requries Argument Type Description root str The root path of all maintained local datasets. set_list_file str A json file recording the online repository paths (the file name extension could be absent) of the required datasets. token int or ( int , ) The default Github OAuth token for downloading files from private repositories. If not set, the downloading from public repositories would not be influenced. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request. Methods \u00b6 init_set_list \u00b6 dchecker . init_set_list ( file_name = 'web-data' ) This method should get used by users manually. It is used for creating an initialized .json config file for the DataChecker . Requries Argument Type Description file_name str The name of the to-be-created dataset config file. clear \u00b6 dchecker . clear () Clear the query list. The query list is a list to required dataset names. This function is not necessary to be used frequently, because DataChecker may only need to be invoked for one time. add_query_file \u00b6 dchecker . add_query_file ( file_names ) Add one or more file names in the query list. Add file names into the required dataset name list. For each different application, the required datasets could be different. The query file list should be a sub-set of the whole list given by set_list_file . Requries Argument Type Description file_name str ( str , ) The could be one or a list of file name strs, including all requried dataset names for the current program. This argument could also be one or a list of file name strs, including all requried dataset names for the current program. query \u00b6 dchecker . query () Search the files in the query list, and download the datasets. Properties \u00b6 token \u00b6 dchecker . token Check or set the Github OAuth token. Examples \u00b6 Here we show an example of creating and using the config file. Example Codes 1 2 3 4 5 6 7 8 9 import os import numpy as np import mdnc set_list_file = os . path . join ( './datasets' , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = './datasets' , set_list_file = set_list_file , token = '' , verbose = True ) dc . add_query_file ( 'dataset_file_name_01.txt' ) dc . query () Output data.webtools: There are required dataset missing. Start downloading from the online repository... Get test-datasets-1.tar.xz: 216B [00:00, 108kB/s] data.webtools: Successfully download all required datasets. The config file should be formatted like the following json examples: Example of meta-data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"set_list\" : [ { \"tag\" : \"test\" , \"asset\" : \"test-datasets-1.tar.xz\" , \"items\" : [ \"dataset_file_name_01.txt\" , \"dataset_file_name_02.txt\" ] } ], \"user\" : \"cainmagi\" , \"repo\" : \"MDNC\" } where the set_list contains a list of dictionaries. Each dictionary represents an xz file. The keyword items represents a list of file names inside the xz file.","title":"<span class='magic-codeicon-class'>DataChecker</span>"},{"location":"apis/data/webtools/DataChecker/#datawebtoolsdatachecker","text":"Class \u00b7 Source dchecker = mdnc . data . webtools . DataChecker ( root = './datasets' , set_list_file = 'web-data' , token = '' , verbose = False ) This data checker could check the local dataset folder, find the not existing datasets and fetch those required datasets from online repositories or links. The workflow is illustrated in the following figure, flowchart LR subgraph dchecker [DataChecker] init(__init__) add(add_query_file) query(query) end init -->|load| webdata[(set_list_file)] add -->|load| fnames[(file_names)] query --> start webdata --> |set_names| start fnames --> |query_list| ifblock flow:::flowstyle subgraph flow [query work flow] start([for each<br>dataset]) --> |set_names| eachset([for each<br>item]) eachset -->|set_name| ifblock{set_name<br>in<br>query_list?}:::ifstyle ifblock -->|yes| ifblock2{file<br>exists?}:::ifstyle ifblock2 --> |yes| eachset ifblock2 --> |no| download[Download<br>the dataset] download --> start end classDef ifstyle fill:#eee, stroke: #999; classDef flowstyle fill:#FEEEF0, stroke: #b54051; To use this class, users require to follow 3 steps: Initialize the DataChecker with the set_list_file argument, which is a json file. This file defines where the online datasets stored and what those datasets have. Use add_query_file to add the require data file name. Invoke query , this method would start iterate the dataset list, then find and download all online datasets which satisfies the following conditions: Has a file item that does not locally exist. Has a file item that is required by the query list. A private repository requires a token. In this case, the argument token need to be not blank.","title":"data.webtools.DataChecker"},{"location":"apis/data/webtools/DataChecker/#arguments","text":"Requries Argument Type Description root str The root path of all maintained local datasets. set_list_file str A json file recording the online repository paths (the file name extension could be absent) of the required datasets. token int or ( int , ) The default Github OAuth token for downloading files from private repositories. If not set, the downloading from public repositories would not be influenced. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/DataChecker/#methods","text":"","title":"Methods"},{"location":"apis/data/webtools/DataChecker/#init_set_list","text":"dchecker . init_set_list ( file_name = 'web-data' ) This method should get used by users manually. It is used for creating an initialized .json config file for the DataChecker . Requries Argument Type Description file_name str The name of the to-be-created dataset config file.","title":" init_set_list"},{"location":"apis/data/webtools/DataChecker/#clear","text":"dchecker . clear () Clear the query list. The query list is a list to required dataset names. This function is not necessary to be used frequently, because DataChecker may only need to be invoked for one time.","title":" clear"},{"location":"apis/data/webtools/DataChecker/#add_query_file","text":"dchecker . add_query_file ( file_names ) Add one or more file names in the query list. Add file names into the required dataset name list. For each different application, the required datasets could be different. The query file list should be a sub-set of the whole list given by set_list_file . Requries Argument Type Description file_name str ( str , ) The could be one or a list of file name strs, including all requried dataset names for the current program. This argument could also be one or a list of file name strs, including all requried dataset names for the current program.","title":" add_query_file"},{"location":"apis/data/webtools/DataChecker/#query","text":"dchecker . query () Search the files in the query list, and download the datasets.","title":" query"},{"location":"apis/data/webtools/DataChecker/#properties","text":"","title":"Properties"},{"location":"apis/data/webtools/DataChecker/#token","text":"dchecker . token Check or set the Github OAuth token.","title":" token"},{"location":"apis/data/webtools/DataChecker/#examples","text":"Here we show an example of creating and using the config file. Example Codes 1 2 3 4 5 6 7 8 9 import os import numpy as np import mdnc set_list_file = os . path . join ( './datasets' , 'web-data' ) mdnc . data . webtools . DataChecker . init_set_list ( set_list_file ) dc = mdnc . data . webtools . DataChecker ( root = './datasets' , set_list_file = set_list_file , token = '' , verbose = True ) dc . add_query_file ( 'dataset_file_name_01.txt' ) dc . query () Output data.webtools: There are required dataset missing. Start downloading from the online repository... Get test-datasets-1.tar.xz: 216B [00:00, 108kB/s] data.webtools: Successfully download all required datasets. The config file should be formatted like the following json examples: Example of meta-data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"set_list\" : [ { \"tag\" : \"test\" , \"asset\" : \"test-datasets-1.tar.xz\" , \"items\" : [ \"dataset_file_name_01.txt\" , \"dataset_file_name_02.txt\" ] } ], \"user\" : \"cainmagi\" , \"repo\" : \"MDNC\" } where the set_list contains a list of dictionaries. Each dictionary represents an xz file. The keyword items represents a list of file names inside the xz file.","title":"Examples"},{"location":"apis/data/webtools/download_tarball/","text":"data.webtools.download_tarball \u00b6 Function \u00b7 Source mdnc . data . webtools . download_tarball ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically. This tool is used for downloading the assets from github repositories. It would: Try to detect the data info in public mode; If fails (the Github repository could not be accessed), switch to private downloading mode. The private mode requires a Github OAuth token for getting access to the file. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details. Tip The mechanics of this function is a little bit complicated. It is mainly inspired by the following codes: https://gist.github.com/devhero/8ae2229d9ea1a59003ced4587c9cb236 https://gist.github.com/maxim/6e15aa45ba010ab030c4 Arguments \u00b6 Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example 1 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s] Example 2 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"<span class='magic-codeicon-function'>download_tarball</span>"},{"location":"apis/data/webtools/download_tarball/#datawebtoolsdownload_tarball","text":"Function \u00b7 Source mdnc . data . webtools . download_tarball ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically. This tool is used for downloading the assets from github repositories. It would: Try to detect the data info in public mode; If fails (the Github repository could not be accessed), switch to private downloading mode. The private mode requires a Github OAuth token for getting access to the file. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details. Tip The mechanics of this function is a little bit complicated. It is mainly inspired by the following codes: https://gist.github.com/devhero/8ae2229d9ea1a59003ced4587c9cb236 https://gist.github.com/maxim/6e15aa45ba010ab030c4","title":"data.webtools.download_tarball"},{"location":"apis/data/webtools/download_tarball/#arguments","text":"Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball/#examples","text":"Example 1 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s] Example 2 Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"Examples"},{"location":"apis/data/webtools/download_tarball_link/","text":"data.webtools.download_tarball_link \u00b6 Function \u00b7 Source mdnc . data . webtools . download_tarball_link ( link , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a web link, and extract it automatically. This function is equivalent to using wget . For example, downloading a xz file: wget -O- <link>.tar.xz | tar xJ -C <path>/ || fail The tarball is directed by the link. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details. Arguments \u00b6 Requries Argument Type Description link str The whole web link, pointting to or redicted to the data file. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_link ( 'https://github.com/cainmagi/Dockerfiles/releases/download/xubuntu-v1.5-u20.04/share-pixmaps.tar.xz' , path = './downloads' , verbose = True ) Output Get share-pixmaps.tar.xz: 134kB [00:00, 1.65MB/s]","title":"<span class='magic-codeicon-function'>download_tarball_link</span>"},{"location":"apis/data/webtools/download_tarball_link/#datawebtoolsdownload_tarball_link","text":"Function \u00b7 Source mdnc . data . webtools . download_tarball_link ( link , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a web link, and extract it automatically. This function is equivalent to using wget . For example, downloading a xz file: wget -O- <link>.tar.xz | tar xJ -C <path>/ || fail The tarball is directed by the link. The tarball would be sent to pipeline and not get stored. Now supports gz , bz2 or xz format, see tarfile to view the details.","title":"data.webtools.download_tarball_link"},{"location":"apis/data/webtools/download_tarball_link/#arguments","text":"Requries Argument Type Description link str The whole web link, pointting to or redicted to the data file. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball_link/#examples","text":"Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_link ( 'https://github.com/cainmagi/Dockerfiles/releases/download/xubuntu-v1.5-u20.04/share-pixmaps.tar.xz' , path = './downloads' , verbose = True ) Output Get share-pixmaps.tar.xz: 134kB [00:00, 1.65MB/s]","title":"Examples"},{"location":"apis/data/webtools/download_tarball_private/","text":"data.webtools.download_tarball_private \u00b6 Function \u00b7 Source mdnc . data . webtools . download_tarball_private ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (private). This tool should only be used for downloading assets from private repositories. Although it could be also used for public repositories, we do not recommend to use it in those cases, because it would still require a token even the repository is public. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning Using the general interface mdnc.data.webtools.download_tarball is more recommended. Arguments \u00b6 Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_private ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"<span class='magic-codeicon-function'>download_tarball_private</span>"},{"location":"apis/data/webtools/download_tarball_private/#datawebtoolsdownload_tarball_private","text":"Function \u00b7 Source mdnc . data . webtools . download_tarball_private ( user , repo , tag , asset , path = '.' , mode = 'auto' , token = None , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (private). This tool should only be used for downloading assets from private repositories. Although it could be also used for public repositories, we do not recommend to use it in those cases, because it would still require a token even the repository is public. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning Using the general interface mdnc.data.webtools.download_tarball is more recommended.","title":"data.webtools.download_tarball_private"},{"location":"apis/data/webtools/download_tarball_private/#arguments","text":"Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. token str A given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. To learn how to set the token, please refer to mdnc.data.webtools.get_token . verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball_private/#examples","text":"Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_private ( user = 'cainmagi' , repo = 'React-builder-for-static-sites' , tag = '0.1' , asset = 'test-datasets-1.tar.xz' , path = './downloads' , token = '' , verbose = True ) Output data.webtools: A Github OAuth token is required for downloading the data in private repository. Please provide your OAuth token: Token:**************************************** data.webtools: Tips: specify the environment variable $GITTOKEN or $GITHUB_API_TOKEN could help you skip this step. Get test-datasets-1.tar.xz: 216B [00:00, 217kB/s]","title":"Examples"},{"location":"apis/data/webtools/download_tarball_public/","text":"data.webtools.download_tarball_public \u00b6 Function \u00b7 Source mdnc . data . webtools . download_tarball_public ( user , repo , tag , asset , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (public). This tool only supports public github repositories. This method could be replaced by mdnc.data.webtools.download_tarball_link , but we do not recommend to do that. Compared to that method, this function is more robust, because it fetches the meta-data before downloading the dataset. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning This function is only designed for downloading public data. If your repository is private, please use mdnc.data.webtools.download_tarball_private for instead. Certainly, using the general interface mdnc.data.webtools.download_tarball is more recommended. Arguments \u00b6 Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request. Examples \u00b6 Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_public ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s]","title":"<span class='magic-codeicon-function'>download_tarball_public</span>"},{"location":"apis/data/webtools/download_tarball_public/#datawebtoolsdownload_tarball_public","text":"Function \u00b7 Source mdnc . data . webtools . download_tarball_public ( user , repo , tag , asset , path = '.' , mode = 'auto' , verbose = False ) Download an online tarball from a Github release asset, and extract it automatically (public). This tool only supports public github repositories. This method could be replaced by mdnc.data.webtools.download_tarball_link , but we do not recommend to do that. Compared to that method, this function is more robust, because it fetches the meta-data before downloading the dataset. Now supports gz , bz2 or xz format, see tarfile to view the details. Warning This function is only designed for downloading public data. If your repository is private, please use mdnc.data.webtools.download_tarball_private for instead. Certainly, using the general interface mdnc.data.webtools.download_tarball is more recommended.","title":"data.webtools.download_tarball_public"},{"location":"apis/data/webtools/download_tarball_public/#arguments","text":"Requries Argument Type Description user str The Github owner name of the repository, could be an organization. repo str The Github repository name. tag str The Github release tag where the data is uploaded. asset str The github asset (tarball) name (including the file name postfix) to be downloaded. path str The extracted data root path. Should be a folder path. mode str The mode of extraction. Could be 'gz' , 'bz2' , 'xz' or 'auto' . When using 'auto' , the format would be guessed by the posfix of the file name in the link. verbose bool A flag, whether to show the downloaded size during the web request.","title":"Arguments"},{"location":"apis/data/webtools/download_tarball_public/#examples","text":"Example Codes 1 2 3 import mdnc mdnc . data . webtools . download_tarball_public ( user = 'cainmagi' , repo = 'Dockerfiles' , tag = 'xubuntu-v1.5-u20.04' , asset = 'xconfigs-u20-04.tar.xz' , path = './downloads' , verbose = True ) Output Get xconfigs-u20-04.tar.xz: 3.06kB [00:00, 263kB/s]","title":"Examples"},{"location":"apis/data/webtools/get_token/","text":"data.webtools.get_token \u00b6 Function \u00b7 Source token = mdnc . data . webtools . get_token ( token = '' , silent = True ) Automatically get the Github OAuth token, if the argument token is missing. This function would try to get the token in the following orders: Try to find the value of the environmental variable GITTOKEN . If not found, try to find the value of the environmental variable GITHUB_API_TOKEN . If not found, and silent is False , would ask users to input the token. When silent is True , would return '' . Tip How to get the token? Please read this page: Git automation with OAuth tokens Tip The token could be formatted like the following two forms: With user name GITTOKEN = myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Without name GITTOKEN = b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Another tip is that, you could skip entering the user name and password if you clone a private repository like this: git clone https://myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc@github.com/myusername/myreponame.git myrepo A repository cloned by this way does not require the user name and password for pull and push . Arguments \u00b6 Requries Argument Type Description token str The given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. silent bool A flag. If set True and the token could not be found anywhere, this tool would not ask for a token, but just return '' . Returns Argument Description token A str . This is the detected OAuth token. Examples \u00b6 Example Codes Run bash , 1 $GITTOKEN = xxxxxxxxxxxxxx Then, run python , 1 2 3 4 import mdnc token = mdnc . data . webtools . get_token ( token = '' ) print ( token ) Output xxxxxxxxxxxxxx","title":"<span class='magic-codeicon-function'>get_token</span>"},{"location":"apis/data/webtools/get_token/#datawebtoolsget_token","text":"Function \u00b7 Source token = mdnc . data . webtools . get_token ( token = '' , silent = True ) Automatically get the Github OAuth token, if the argument token is missing. This function would try to get the token in the following orders: Try to find the value of the environmental variable GITTOKEN . If not found, try to find the value of the environmental variable GITHUB_API_TOKEN . If not found, and silent is False , would ask users to input the token. When silent is True , would return '' . Tip How to get the token? Please read this page: Git automation with OAuth tokens Tip The token could be formatted like the following two forms: With user name GITTOKEN = myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Without name GITTOKEN = b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc Another tip is that, you could skip entering the user name and password if you clone a private repository like this: git clone https://myusername:b05bpgw2dcn5okqpeltlz858eoi6x6j3wrrjhhhc@github.com/myusername/myreponame.git myrepo A repository cloned by this way does not require the user name and password for pull and push .","title":"data.webtools.get_token"},{"location":"apis/data/webtools/get_token/#arguments","text":"Requries Argument Type Description token str The given OAuth token. Only when this argument is unset, the program will try to find a token from enviornmental variables. silent bool A flag. If set True and the token could not be found anywhere, this tool would not ask for a token, but just return '' . Returns Argument Description token A str . This is the detected OAuth token.","title":"Arguments"},{"location":"apis/data/webtools/get_token/#examples","text":"Example Codes Run bash , 1 $GITTOKEN = xxxxxxxxxxxxxx Then, run python , 1 2 3 4 import mdnc token = mdnc . data . webtools . get_token ( token = '' ) print ( token ) Output xxxxxxxxxxxxxx","title":"Examples"},{"location":"apis/modules/conv/AE1d/","text":"modules.conv.AE1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . AE1d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D convolutional auto-encoder. The network structure is almost the same as mdnc.modules.conv.UNet1d but all block-level skip connections are removed. Generally, using mdnc.modules.conv.UNet1d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . AE1d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 64] 12,288 _ConvModernNd-9 [-1, 64, 64] 0 _BlockConvStkNd-10 [-1, 64, 64] 0 InstanceNorm1d-11 [-1, 64, 64] 128 PReLU-12 [-1, 64, 64] 64 Conv1d-13 [-1, 128, 64] 24,576 _ConvModernNd-14 [-1, 128, 64] 0 InstanceNorm1d-15 [-1, 128, 64] 256 PReLU-16 [-1, 128, 64] 128 Conv1d-17 [-1, 128, 32] 49,152 _ConvModernNd-18 [-1, 128, 32] 0 _BlockConvStkNd-19 [-1, 128, 32] 0 InstanceNorm1d-20 [-1, 128, 32] 256 PReLU-21 [-1, 128, 32] 128 Conv1d-22 [-1, 256, 32] 98,304 _ConvModernNd-23 [-1, 256, 32] 0 InstanceNorm1d-24 [-1, 256, 32] 512 PReLU-25 [-1, 256, 32] 256 Conv1d-26 [-1, 256, 32] 196,608 _ConvModernNd-27 [-1, 256, 32] 0 InstanceNorm1d-28 [-1, 256, 32] 512 PReLU-29 [-1, 256, 32] 256 Conv1d-30 [-1, 256, 16] 196,608 _ConvModernNd-31 [-1, 256, 16] 0 _BlockConvStkNd-32 [-1, 256, 16] 0 InstanceNorm1d-33 [-1, 256, 16] 512 PReLU-34 [-1, 256, 16] 256 Conv1d-35 [-1, 512, 16] 393,216 _ConvModernNd-36 [-1, 512, 16] 0 InstanceNorm1d-37 [-1, 512, 16] 1,024 PReLU-38 [-1, 512, 16] 512 Conv1d-39 [-1, 512, 16] 786,432 _ConvModernNd-40 [-1, 512, 16] 0 InstanceNorm1d-41 [-1, 512, 16] 1,024 PReLU-42 [-1, 512, 16] 512 Conv1d-43 [-1, 512, 8] 786,432 _ConvModernNd-44 [-1, 512, 8] 0 _BlockConvStkNd-45 [-1, 512, 8] 0 InstanceNorm1d-46 [-1, 512, 8] 1,024 PReLU-47 [-1, 512, 8] 512 Conv1d-48 [-1, 1024, 8] 1,572,864 _ConvModernNd-49 [-1, 1024, 8] 0 InstanceNorm1d-50 [-1, 1024, 8] 2,048 PReLU-51 [-1, 1024, 8] 1,024 Conv1d-52 [-1, 1024, 8] 3,145,728 _ConvModernNd-53 [-1, 1024, 8] 0 InstanceNorm1d-54 [-1, 1024, 8] 2,048 PReLU-55 [-1, 1024, 8] 1,024 Upsample-56 [-1, 1024, 16] 0 Conv1d-57 [-1, 512, 16] 1,572,864 _ConvModernNd-58 [-1, 512, 16] 0 _BlockConvStkNd-59 [-1, 512, 16] 0 InstanceNorm1d-60 [-1, 512, 16] 1,024 PReLU-61 [-1, 512, 16] 512 Conv1d-62 [-1, 512, 16] 786,432 _ConvModernNd-63 [-1, 512, 16] 0 InstanceNorm1d-64 [-1, 512, 16] 1,024 PReLU-65 [-1, 512, 16] 512 Conv1d-66 [-1, 512, 16] 786,432 _ConvModernNd-67 [-1, 512, 16] 0 InstanceNorm1d-68 [-1, 512, 16] 1,024 PReLU-69 [-1, 512, 16] 512 Upsample-70 [-1, 512, 32] 0 Conv1d-71 [-1, 256, 32] 393,216 _ConvModernNd-72 [-1, 256, 32] 0 _BlockConvStkNd-73 [-1, 256, 32] 0 InstanceNorm1d-74 [-1, 256, 32] 512 PReLU-75 [-1, 256, 32] 256 Conv1d-76 [-1, 256, 32] 196,608 _ConvModernNd-77 [-1, 256, 32] 0 InstanceNorm1d-78 [-1, 256, 32] 512 PReLU-79 [-1, 256, 32] 256 Conv1d-80 [-1, 256, 32] 196,608 _ConvModernNd-81 [-1, 256, 32] 0 InstanceNorm1d-82 [-1, 256, 32] 512 PReLU-83 [-1, 256, 32] 256 Upsample-84 [-1, 256, 64] 0 Conv1d-85 [-1, 128, 64] 98,304 _ConvModernNd-86 [-1, 128, 64] 0 _BlockConvStkNd-87 [-1, 128, 64] 0 InstanceNorm1d-88 [-1, 128, 64] 256 PReLU-89 [-1, 128, 64] 128 Conv1d-90 [-1, 128, 64] 49,152 _ConvModernNd-91 [-1, 128, 64] 0 InstanceNorm1d-92 [-1, 128, 64] 256 PReLU-93 [-1, 128, 64] 128 Upsample-94 [-1, 128, 128] 0 Conv1d-95 [-1, 64, 128] 24,576 _ConvModernNd-96 [-1, 64, 128] 0 _BlockConvStkNd-97 [-1, 64, 128] 0 InstanceNorm1d-98 [-1, 64, 128] 128 PReLU-99 [-1, 64, 128] 64 Conv1d-100 [-1, 64, 128] 12,288 _ConvModernNd-101 [-1, 64, 128] 0 InstanceNorm1d-102 [-1, 64, 128] 128 PReLU-103 [-1, 64, 128] 64 Conv1d-104 [-1, 64, 128] 12,288 _ConvModernNd-105 [-1, 64, 128] 0 _BlockConvStkNd-106 [-1, 64, 128] 0 Conv1d-107 [-1, 3, 128] 963 AE1d-108 [-1, 3, 128] 0 ================================================================ Total params: 11,427,651 Trainable params: 11,427,651 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 6.26 Params size (MB): 43.59 Estimated Total Size (MB): 49.85 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>AE1d</span>"},{"location":"apis/modules/conv/AE1d/#modulesconvae1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . AE1d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D convolutional auto-encoder. The network structure is almost the same as mdnc.modules.conv.UNet1d but all block-level skip connections are removed. Generally, using mdnc.modules.conv.UNet1d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route.","title":"modules.conv.AE1d"},{"location":"apis/modules/conv/AE1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/AE1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/AE1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length.","title":" __call__"},{"location":"apis/modules/conv/AE1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/AE1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/conv/AE1d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . AE1d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 64] 12,288 _ConvModernNd-9 [-1, 64, 64] 0 _BlockConvStkNd-10 [-1, 64, 64] 0 InstanceNorm1d-11 [-1, 64, 64] 128 PReLU-12 [-1, 64, 64] 64 Conv1d-13 [-1, 128, 64] 24,576 _ConvModernNd-14 [-1, 128, 64] 0 InstanceNorm1d-15 [-1, 128, 64] 256 PReLU-16 [-1, 128, 64] 128 Conv1d-17 [-1, 128, 32] 49,152 _ConvModernNd-18 [-1, 128, 32] 0 _BlockConvStkNd-19 [-1, 128, 32] 0 InstanceNorm1d-20 [-1, 128, 32] 256 PReLU-21 [-1, 128, 32] 128 Conv1d-22 [-1, 256, 32] 98,304 _ConvModernNd-23 [-1, 256, 32] 0 InstanceNorm1d-24 [-1, 256, 32] 512 PReLU-25 [-1, 256, 32] 256 Conv1d-26 [-1, 256, 32] 196,608 _ConvModernNd-27 [-1, 256, 32] 0 InstanceNorm1d-28 [-1, 256, 32] 512 PReLU-29 [-1, 256, 32] 256 Conv1d-30 [-1, 256, 16] 196,608 _ConvModernNd-31 [-1, 256, 16] 0 _BlockConvStkNd-32 [-1, 256, 16] 0 InstanceNorm1d-33 [-1, 256, 16] 512 PReLU-34 [-1, 256, 16] 256 Conv1d-35 [-1, 512, 16] 393,216 _ConvModernNd-36 [-1, 512, 16] 0 InstanceNorm1d-37 [-1, 512, 16] 1,024 PReLU-38 [-1, 512, 16] 512 Conv1d-39 [-1, 512, 16] 786,432 _ConvModernNd-40 [-1, 512, 16] 0 InstanceNorm1d-41 [-1, 512, 16] 1,024 PReLU-42 [-1, 512, 16] 512 Conv1d-43 [-1, 512, 8] 786,432 _ConvModernNd-44 [-1, 512, 8] 0 _BlockConvStkNd-45 [-1, 512, 8] 0 InstanceNorm1d-46 [-1, 512, 8] 1,024 PReLU-47 [-1, 512, 8] 512 Conv1d-48 [-1, 1024, 8] 1,572,864 _ConvModernNd-49 [-1, 1024, 8] 0 InstanceNorm1d-50 [-1, 1024, 8] 2,048 PReLU-51 [-1, 1024, 8] 1,024 Conv1d-52 [-1, 1024, 8] 3,145,728 _ConvModernNd-53 [-1, 1024, 8] 0 InstanceNorm1d-54 [-1, 1024, 8] 2,048 PReLU-55 [-1, 1024, 8] 1,024 Upsample-56 [-1, 1024, 16] 0 Conv1d-57 [-1, 512, 16] 1,572,864 _ConvModernNd-58 [-1, 512, 16] 0 _BlockConvStkNd-59 [-1, 512, 16] 0 InstanceNorm1d-60 [-1, 512, 16] 1,024 PReLU-61 [-1, 512, 16] 512 Conv1d-62 [-1, 512, 16] 786,432 _ConvModernNd-63 [-1, 512, 16] 0 InstanceNorm1d-64 [-1, 512, 16] 1,024 PReLU-65 [-1, 512, 16] 512 Conv1d-66 [-1, 512, 16] 786,432 _ConvModernNd-67 [-1, 512, 16] 0 InstanceNorm1d-68 [-1, 512, 16] 1,024 PReLU-69 [-1, 512, 16] 512 Upsample-70 [-1, 512, 32] 0 Conv1d-71 [-1, 256, 32] 393,216 _ConvModernNd-72 [-1, 256, 32] 0 _BlockConvStkNd-73 [-1, 256, 32] 0 InstanceNorm1d-74 [-1, 256, 32] 512 PReLU-75 [-1, 256, 32] 256 Conv1d-76 [-1, 256, 32] 196,608 _ConvModernNd-77 [-1, 256, 32] 0 InstanceNorm1d-78 [-1, 256, 32] 512 PReLU-79 [-1, 256, 32] 256 Conv1d-80 [-1, 256, 32] 196,608 _ConvModernNd-81 [-1, 256, 32] 0 InstanceNorm1d-82 [-1, 256, 32] 512 PReLU-83 [-1, 256, 32] 256 Upsample-84 [-1, 256, 64] 0 Conv1d-85 [-1, 128, 64] 98,304 _ConvModernNd-86 [-1, 128, 64] 0 _BlockConvStkNd-87 [-1, 128, 64] 0 InstanceNorm1d-88 [-1, 128, 64] 256 PReLU-89 [-1, 128, 64] 128 Conv1d-90 [-1, 128, 64] 49,152 _ConvModernNd-91 [-1, 128, 64] 0 InstanceNorm1d-92 [-1, 128, 64] 256 PReLU-93 [-1, 128, 64] 128 Upsample-94 [-1, 128, 128] 0 Conv1d-95 [-1, 64, 128] 24,576 _ConvModernNd-96 [-1, 64, 128] 0 _BlockConvStkNd-97 [-1, 64, 128] 0 InstanceNorm1d-98 [-1, 64, 128] 128 PReLU-99 [-1, 64, 128] 64 Conv1d-100 [-1, 64, 128] 12,288 _ConvModernNd-101 [-1, 64, 128] 0 InstanceNorm1d-102 [-1, 64, 128] 128 PReLU-103 [-1, 64, 128] 64 Conv1d-104 [-1, 64, 128] 12,288 _ConvModernNd-105 [-1, 64, 128] 0 _BlockConvStkNd-106 [-1, 64, 128] 0 Conv1d-107 [-1, 3, 128] 963 AE1d-108 [-1, 3, 128] 0 ================================================================ Total params: 11,427,651 Trainable params: 11,427,651 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 6.26 Params size (MB): 43.59 Estimated Total Size (MB): 49.85 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/AE2d/","text":"modules.conv.AE2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . AE2d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D convolutional auto-encoder. The network structure is almost the same as mdnc.modules.conv.UNet2d but all block-level skip connections are removed. Generally, using mdnc.modules.conv.UNet2d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . AE2d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 32, 32] 36,864 _ConvModernNd-9 [-1, 64, 32, 32] 0 _BlockConvStkNd-10 [-1, 64, 32, 32] 0 InstanceNorm2d-11 [-1, 64, 32, 32] 128 PReLU-12 [-1, 64, 32, 32] 64 Conv2d-13 [-1, 128, 32, 32] 73,728 _ConvModernNd-14 [-1, 128, 32, 32] 0 InstanceNorm2d-15 [-1, 128, 32, 32] 256 PReLU-16 [-1, 128, 32, 32] 128 Conv2d-17 [-1, 128, 16, 16] 147,456 _ConvModernNd-18 [-1, 128, 16, 16] 0 _BlockConvStkNd-19 [-1, 128, 16, 16] 0 InstanceNorm2d-20 [-1, 128, 16, 16] 256 PReLU-21 [-1, 128, 16, 16] 128 Conv2d-22 [-1, 256, 16, 16] 294,912 _ConvModernNd-23 [-1, 256, 16, 16] 0 InstanceNorm2d-24 [-1, 256, 16, 16] 512 PReLU-25 [-1, 256, 16, 16] 256 Conv2d-26 [-1, 256, 16, 16] 589,824 _ConvModernNd-27 [-1, 256, 16, 16] 0 InstanceNorm2d-28 [-1, 256, 16, 16] 512 PReLU-29 [-1, 256, 16, 16] 256 Conv2d-30 [-1, 256, 8, 8] 589,824 _ConvModernNd-31 [-1, 256, 8, 8] 0 _BlockConvStkNd-32 [-1, 256, 8, 8] 0 InstanceNorm2d-33 [-1, 256, 8, 8] 512 PReLU-34 [-1, 256, 8, 8] 256 Conv2d-35 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-36 [-1, 512, 8, 8] 0 InstanceNorm2d-37 [-1, 512, 8, 8] 1,024 PReLU-38 [-1, 512, 8, 8] 512 Conv2d-39 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-40 [-1, 512, 8, 8] 0 InstanceNorm2d-41 [-1, 512, 8, 8] 1,024 PReLU-42 [-1, 512, 8, 8] 512 Conv2d-43 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-44 [-1, 512, 4, 4] 0 _BlockConvStkNd-45 [-1, 512, 4, 4] 0 InstanceNorm2d-46 [-1, 512, 4, 4] 1,024 PReLU-47 [-1, 512, 4, 4] 512 Conv2d-48 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-49 [-1, 1024, 4, 4] 0 InstanceNorm2d-50 [-1, 1024, 4, 4] 2,048 PReLU-51 [-1, 1024, 4, 4] 1,024 Conv2d-52 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-53 [-1, 1024, 4, 4] 0 InstanceNorm2d-54 [-1, 1024, 4, 4] 2,048 PReLU-55 [-1, 1024, 4, 4] 1,024 Upsample-56 [-1, 1024, 8, 8] 0 Conv2d-57 [-1, 512, 8, 8] 4,718,592 _ConvModernNd-58 [-1, 512, 8, 8] 0 _BlockConvStkNd-59 [-1, 512, 8, 8] 0 InstanceNorm2d-60 [-1, 512, 8, 8] 1,024 PReLU-61 [-1, 512, 8, 8] 512 Conv2d-62 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-63 [-1, 512, 8, 8] 0 InstanceNorm2d-64 [-1, 512, 8, 8] 1,024 PReLU-65 [-1, 512, 8, 8] 512 Conv2d-66 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-67 [-1, 512, 8, 8] 0 InstanceNorm2d-68 [-1, 512, 8, 8] 1,024 PReLU-69 [-1, 512, 8, 8] 512 Upsample-70 [-1, 512, 16, 16] 0 Conv2d-71 [-1, 256, 16, 16] 1,179,648 _ConvModernNd-72 [-1, 256, 16, 16] 0 _BlockConvStkNd-73 [-1, 256, 16, 16] 0 InstanceNorm2d-74 [-1, 256, 16, 16] 512 PReLU-75 [-1, 256, 16, 16] 256 Conv2d-76 [-1, 256, 16, 16] 589,824 _ConvModernNd-77 [-1, 256, 16, 16] 0 InstanceNorm2d-78 [-1, 256, 16, 16] 512 PReLU-79 [-1, 256, 16, 16] 256 Conv2d-80 [-1, 256, 16, 16] 589,824 _ConvModernNd-81 [-1, 256, 16, 16] 0 InstanceNorm2d-82 [-1, 256, 16, 16] 512 PReLU-83 [-1, 256, 16, 16] 256 Upsample-84 [-1, 256, 32, 32] 0 Conv2d-85 [-1, 128, 32, 32] 294,912 _ConvModernNd-86 [-1, 128, 32, 32] 0 _BlockConvStkNd-87 [-1, 128, 32, 32] 0 InstanceNorm2d-88 [-1, 128, 32, 32] 256 PReLU-89 [-1, 128, 32, 32] 128 Conv2d-90 [-1, 128, 32, 32] 147,456 _ConvModernNd-91 [-1, 128, 32, 32] 0 InstanceNorm2d-92 [-1, 128, 32, 32] 256 PReLU-93 [-1, 128, 32, 32] 128 Upsample-94 [-1, 128, 64, 64] 0 Conv2d-95 [-1, 64, 64, 64] 73,728 _ConvModernNd-96 [-1, 64, 64, 64] 0 _BlockConvStkNd-97 [-1, 64, 64, 64] 0 InstanceNorm2d-98 [-1, 64, 64, 63] 128 PReLU-99 [-1, 64, 64, 63] 64 Conv2d-100 [-1, 64, 64, 63] 36,864 _ConvModernNd-101 [-1, 64, 64, 63] 0 InstanceNorm2d-102 [-1, 64, 64, 63] 128 PReLU-103 [-1, 64, 64, 63] 64 Conv2d-104 [-1, 64, 64, 63] 36,864 _ConvModernNd-105 [-1, 64, 64, 63] 0 _BlockConvStkNd-106 [-1, 64, 64, 63] 0 Conv2d-107 [-1, 3, 64, 63] 4,803 AE2d-108 [-1, 3, 64, 63] 0 ================================================================ Total params: 34,241,859 Trainable params: 34,241,859 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 79.62 Params size (MB): 130.62 Estimated Total Size (MB): 210.29 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>AE2d</span>"},{"location":"apis/modules/conv/AE2d/#modulesconvae2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . AE2d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D convolutional auto-encoder. The network structure is almost the same as mdnc.modules.conv.UNet2d but all block-level skip connections are removed. Generally, using mdnc.modules.conv.UNet2d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route.","title":"modules.conv.AE2d"},{"location":"apis/modules/conv/AE2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/AE2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/AE2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size.","title":" __call__"},{"location":"apis/modules/conv/AE2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/AE2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/conv/AE2d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . AE2d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 32, 32] 36,864 _ConvModernNd-9 [-1, 64, 32, 32] 0 _BlockConvStkNd-10 [-1, 64, 32, 32] 0 InstanceNorm2d-11 [-1, 64, 32, 32] 128 PReLU-12 [-1, 64, 32, 32] 64 Conv2d-13 [-1, 128, 32, 32] 73,728 _ConvModernNd-14 [-1, 128, 32, 32] 0 InstanceNorm2d-15 [-1, 128, 32, 32] 256 PReLU-16 [-1, 128, 32, 32] 128 Conv2d-17 [-1, 128, 16, 16] 147,456 _ConvModernNd-18 [-1, 128, 16, 16] 0 _BlockConvStkNd-19 [-1, 128, 16, 16] 0 InstanceNorm2d-20 [-1, 128, 16, 16] 256 PReLU-21 [-1, 128, 16, 16] 128 Conv2d-22 [-1, 256, 16, 16] 294,912 _ConvModernNd-23 [-1, 256, 16, 16] 0 InstanceNorm2d-24 [-1, 256, 16, 16] 512 PReLU-25 [-1, 256, 16, 16] 256 Conv2d-26 [-1, 256, 16, 16] 589,824 _ConvModernNd-27 [-1, 256, 16, 16] 0 InstanceNorm2d-28 [-1, 256, 16, 16] 512 PReLU-29 [-1, 256, 16, 16] 256 Conv2d-30 [-1, 256, 8, 8] 589,824 _ConvModernNd-31 [-1, 256, 8, 8] 0 _BlockConvStkNd-32 [-1, 256, 8, 8] 0 InstanceNorm2d-33 [-1, 256, 8, 8] 512 PReLU-34 [-1, 256, 8, 8] 256 Conv2d-35 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-36 [-1, 512, 8, 8] 0 InstanceNorm2d-37 [-1, 512, 8, 8] 1,024 PReLU-38 [-1, 512, 8, 8] 512 Conv2d-39 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-40 [-1, 512, 8, 8] 0 InstanceNorm2d-41 [-1, 512, 8, 8] 1,024 PReLU-42 [-1, 512, 8, 8] 512 Conv2d-43 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-44 [-1, 512, 4, 4] 0 _BlockConvStkNd-45 [-1, 512, 4, 4] 0 InstanceNorm2d-46 [-1, 512, 4, 4] 1,024 PReLU-47 [-1, 512, 4, 4] 512 Conv2d-48 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-49 [-1, 1024, 4, 4] 0 InstanceNorm2d-50 [-1, 1024, 4, 4] 2,048 PReLU-51 [-1, 1024, 4, 4] 1,024 Conv2d-52 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-53 [-1, 1024, 4, 4] 0 InstanceNorm2d-54 [-1, 1024, 4, 4] 2,048 PReLU-55 [-1, 1024, 4, 4] 1,024 Upsample-56 [-1, 1024, 8, 8] 0 Conv2d-57 [-1, 512, 8, 8] 4,718,592 _ConvModernNd-58 [-1, 512, 8, 8] 0 _BlockConvStkNd-59 [-1, 512, 8, 8] 0 InstanceNorm2d-60 [-1, 512, 8, 8] 1,024 PReLU-61 [-1, 512, 8, 8] 512 Conv2d-62 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-63 [-1, 512, 8, 8] 0 InstanceNorm2d-64 [-1, 512, 8, 8] 1,024 PReLU-65 [-1, 512, 8, 8] 512 Conv2d-66 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-67 [-1, 512, 8, 8] 0 InstanceNorm2d-68 [-1, 512, 8, 8] 1,024 PReLU-69 [-1, 512, 8, 8] 512 Upsample-70 [-1, 512, 16, 16] 0 Conv2d-71 [-1, 256, 16, 16] 1,179,648 _ConvModernNd-72 [-1, 256, 16, 16] 0 _BlockConvStkNd-73 [-1, 256, 16, 16] 0 InstanceNorm2d-74 [-1, 256, 16, 16] 512 PReLU-75 [-1, 256, 16, 16] 256 Conv2d-76 [-1, 256, 16, 16] 589,824 _ConvModernNd-77 [-1, 256, 16, 16] 0 InstanceNorm2d-78 [-1, 256, 16, 16] 512 PReLU-79 [-1, 256, 16, 16] 256 Conv2d-80 [-1, 256, 16, 16] 589,824 _ConvModernNd-81 [-1, 256, 16, 16] 0 InstanceNorm2d-82 [-1, 256, 16, 16] 512 PReLU-83 [-1, 256, 16, 16] 256 Upsample-84 [-1, 256, 32, 32] 0 Conv2d-85 [-1, 128, 32, 32] 294,912 _ConvModernNd-86 [-1, 128, 32, 32] 0 _BlockConvStkNd-87 [-1, 128, 32, 32] 0 InstanceNorm2d-88 [-1, 128, 32, 32] 256 PReLU-89 [-1, 128, 32, 32] 128 Conv2d-90 [-1, 128, 32, 32] 147,456 _ConvModernNd-91 [-1, 128, 32, 32] 0 InstanceNorm2d-92 [-1, 128, 32, 32] 256 PReLU-93 [-1, 128, 32, 32] 128 Upsample-94 [-1, 128, 64, 64] 0 Conv2d-95 [-1, 64, 64, 64] 73,728 _ConvModernNd-96 [-1, 64, 64, 64] 0 _BlockConvStkNd-97 [-1, 64, 64, 64] 0 InstanceNorm2d-98 [-1, 64, 64, 63] 128 PReLU-99 [-1, 64, 64, 63] 64 Conv2d-100 [-1, 64, 64, 63] 36,864 _ConvModernNd-101 [-1, 64, 64, 63] 0 InstanceNorm2d-102 [-1, 64, 64, 63] 128 PReLU-103 [-1, 64, 64, 63] 64 Conv2d-104 [-1, 64, 64, 63] 36,864 _ConvModernNd-105 [-1, 64, 64, 63] 0 _BlockConvStkNd-106 [-1, 64, 64, 63] 0 Conv2d-107 [-1, 3, 64, 63] 4,803 AE2d-108 [-1, 3, 64, 63] 0 ================================================================ Total params: 34,241,859 Trainable params: 34,241,859 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 79.62 Params size (MB): 130.62 Estimated Total Size (MB): 210.29 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/AE3d/","text":"modules.conv.AE3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . AE3d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D convolutional auto-encoder. The network structure is almost the same as mdnc.modules.conv.UNet3d but all block-level skip connections are removed. Generally, using mdnc.modules.conv.UNet3d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . AE3d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-9 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-10 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-11 [-1, 64, 16, 16, 15] 128 PReLU-12 [-1, 64, 16, 16, 15] 64 Conv3d-13 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-14 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 128, 16, 16, 15] 256 PReLU-16 [-1, 128, 16, 16, 15] 128 Conv3d-17 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-18 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-19 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-20 [-1, 128, 8, 8, 8] 256 PReLU-21 [-1, 128, 8, 8, 8] 128 Conv3d-22 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-23 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-24 [-1, 256, 8, 8, 8] 512 PReLU-25 [-1, 256, 8, 8, 8] 256 Conv3d-26 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-27 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 256, 8, 8, 8] 512 PReLU-29 [-1, 256, 8, 8, 8] 256 Conv3d-30 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-31 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-32 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-33 [-1, 256, 4, 4, 4] 512 PReLU-34 [-1, 256, 4, 4, 4] 256 Conv3d-35 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-36 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-37 [-1, 512, 4, 4, 4] 1,024 PReLU-38 [-1, 512, 4, 4, 4] 512 Conv3d-39 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-40 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 512, 4, 4, 4] 1,024 PReLU-42 [-1, 512, 4, 4, 4] 512 Conv3d-43 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-44 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-45 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-46 [-1, 512, 2, 2, 2] 1,024 PReLU-47 [-1, 512, 2, 2, 2] 512 Conv3d-48 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-49 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-50 [-1, 1024, 2, 2, 2] 2,048 PReLU-51 [-1, 1024, 2, 2, 2] 1,024 Conv3d-52 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-53 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 1024, 2, 2, 2] 2,048 PReLU-55 [-1, 1024, 2, 2, 2] 1,024 Upsample-56 [-1, 1024, 4, 4, 4] 0 Conv3d-57 [-1, 512, 4, 4, 4] 14,155,776 _ConvModernNd-58 [-1, 512, 4, 4, 4] 0 _BlockConvStkNd-59 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-60 [-1, 512, 4, 4, 4] 1,024 PReLU-61 [-1, 512, 4, 4, 4] 512 Conv3d-62 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-63 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-64 [-1, 512, 4, 4, 4] 1,024 PReLU-65 [-1, 512, 4, 4, 4] 512 Conv3d-66 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-67 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-68 [-1, 512, 4, 4, 4] 1,024 PReLU-69 [-1, 512, 4, 4, 4] 512 Upsample-70 [-1, 512, 8, 8, 8] 0 Conv3d-71 [-1, 256, 8, 8, 8] 3,538,944 _ConvModernNd-72 [-1, 256, 8, 8, 8] 0 _BlockConvStkNd-73 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-74 [-1, 256, 8, 8, 8] 512 PReLU-75 [-1, 256, 8, 8, 8] 256 Conv3d-76 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-77 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-78 [-1, 256, 8, 8, 8] 512 PReLU-79 [-1, 256, 8, 8, 8] 256 Conv3d-80 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-81 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-82 [-1, 256, 8, 8, 8] 512 PReLU-83 [-1, 256, 8, 8, 8] 256 Upsample-84 [-1, 256, 16, 16, 16] 0 Conv3d-85 [-1, 128, 16, 16, 16] 884,736 _ConvModernNd-86 [-1, 128, 16, 16, 16] 0 _BlockConvStkNd-87 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-88 [-1, 128, 16, 16, 15] 256 PReLU-89 [-1, 128, 16, 16, 15] 128 Conv3d-90 [-1, 128, 16, 16, 15] 442,368 _ConvModernNd-91 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-92 [-1, 128, 16, 16, 15] 256 PReLU-93 [-1, 128, 16, 16, 15] 128 Upsample-94 [-1, 128, 32, 32, 30] 0 Conv3d-95 [-1, 64, 32, 32, 30] 221,184 _ConvModernNd-96 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-97 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-98 [-1, 64, 31, 32, 30] 128 PReLU-99 [-1, 64, 31, 32, 30] 64 Conv3d-100 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-101 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-102 [-1, 64, 31, 32, 30] 128 PReLU-103 [-1, 64, 31, 32, 30] 64 Conv3d-104 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-105 [-1, 64, 31, 32, 30] 0 _BlockConvStkNd-106 [-1, 64, 31, 32, 30] 0 Conv3d-107 [-1, 3, 31, 32, 30] 24,003 AE3d-108 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 102,699,843 Trainable params: 102,699,843 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 408.27 Params size (MB): 391.77 Estimated Total Size (MB): 800.38 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>AE3d</span>"},{"location":"apis/modules/conv/AE3d/#modulesconvae3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . AE3d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D convolutional auto-encoder. The network structure is almost the same as mdnc.modules.conv.UNet3d but all block-level skip connections are removed. Generally, using mdnc.modules.conv.UNet3d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route.","title":"modules.conv.AE3d"},{"location":"apis/modules/conv/AE3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/AE3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/AE3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size.","title":" __call__"},{"location":"apis/modules/conv/AE3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/AE3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/conv/AE3d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . AE3d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-9 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-10 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-11 [-1, 64, 16, 16, 15] 128 PReLU-12 [-1, 64, 16, 16, 15] 64 Conv3d-13 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-14 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 128, 16, 16, 15] 256 PReLU-16 [-1, 128, 16, 16, 15] 128 Conv3d-17 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-18 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-19 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-20 [-1, 128, 8, 8, 8] 256 PReLU-21 [-1, 128, 8, 8, 8] 128 Conv3d-22 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-23 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-24 [-1, 256, 8, 8, 8] 512 PReLU-25 [-1, 256, 8, 8, 8] 256 Conv3d-26 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-27 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 256, 8, 8, 8] 512 PReLU-29 [-1, 256, 8, 8, 8] 256 Conv3d-30 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-31 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-32 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-33 [-1, 256, 4, 4, 4] 512 PReLU-34 [-1, 256, 4, 4, 4] 256 Conv3d-35 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-36 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-37 [-1, 512, 4, 4, 4] 1,024 PReLU-38 [-1, 512, 4, 4, 4] 512 Conv3d-39 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-40 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 512, 4, 4, 4] 1,024 PReLU-42 [-1, 512, 4, 4, 4] 512 Conv3d-43 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-44 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-45 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-46 [-1, 512, 2, 2, 2] 1,024 PReLU-47 [-1, 512, 2, 2, 2] 512 Conv3d-48 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-49 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-50 [-1, 1024, 2, 2, 2] 2,048 PReLU-51 [-1, 1024, 2, 2, 2] 1,024 Conv3d-52 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-53 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 1024, 2, 2, 2] 2,048 PReLU-55 [-1, 1024, 2, 2, 2] 1,024 Upsample-56 [-1, 1024, 4, 4, 4] 0 Conv3d-57 [-1, 512, 4, 4, 4] 14,155,776 _ConvModernNd-58 [-1, 512, 4, 4, 4] 0 _BlockConvStkNd-59 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-60 [-1, 512, 4, 4, 4] 1,024 PReLU-61 [-1, 512, 4, 4, 4] 512 Conv3d-62 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-63 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-64 [-1, 512, 4, 4, 4] 1,024 PReLU-65 [-1, 512, 4, 4, 4] 512 Conv3d-66 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-67 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-68 [-1, 512, 4, 4, 4] 1,024 PReLU-69 [-1, 512, 4, 4, 4] 512 Upsample-70 [-1, 512, 8, 8, 8] 0 Conv3d-71 [-1, 256, 8, 8, 8] 3,538,944 _ConvModernNd-72 [-1, 256, 8, 8, 8] 0 _BlockConvStkNd-73 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-74 [-1, 256, 8, 8, 8] 512 PReLU-75 [-1, 256, 8, 8, 8] 256 Conv3d-76 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-77 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-78 [-1, 256, 8, 8, 8] 512 PReLU-79 [-1, 256, 8, 8, 8] 256 Conv3d-80 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-81 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-82 [-1, 256, 8, 8, 8] 512 PReLU-83 [-1, 256, 8, 8, 8] 256 Upsample-84 [-1, 256, 16, 16, 16] 0 Conv3d-85 [-1, 128, 16, 16, 16] 884,736 _ConvModernNd-86 [-1, 128, 16, 16, 16] 0 _BlockConvStkNd-87 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-88 [-1, 128, 16, 16, 15] 256 PReLU-89 [-1, 128, 16, 16, 15] 128 Conv3d-90 [-1, 128, 16, 16, 15] 442,368 _ConvModernNd-91 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-92 [-1, 128, 16, 16, 15] 256 PReLU-93 [-1, 128, 16, 16, 15] 128 Upsample-94 [-1, 128, 32, 32, 30] 0 Conv3d-95 [-1, 64, 32, 32, 30] 221,184 _ConvModernNd-96 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-97 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-98 [-1, 64, 31, 32, 30] 128 PReLU-99 [-1, 64, 31, 32, 30] 64 Conv3d-100 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-101 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-102 [-1, 64, 31, 32, 30] 128 PReLU-103 [-1, 64, 31, 32, 30] 64 Conv3d-104 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-105 [-1, 64, 31, 32, 30] 0 _BlockConvStkNd-106 [-1, 64, 31, 32, 30] 0 Conv3d-107 [-1, 3, 31, 32, 30] 24,003 AE3d-108 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 102,699,843 Trainable params: 102,699,843 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 408.27 Params size (MB): 391.77 Estimated Total Size (MB): 800.38 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/ConvModern1d/","text":"modules.conv.ConvModern1d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . conv . ConvModern1d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) The implementation for the 1D modern convolutional layer. It supports both down-sampling mode and up-sampling modes. The modern convolutional layer is a stack of convolution, normalization and activation. Shown in the following chart: flowchart TB conv[Convolution] --> norm[Normalization] --> actv[Activation] In the following paper, a new op composition order is proposed for building residual block. This idea is may help the performance get improved. Identity Mappings in Deep Residual Networks The basic idea of this method is shown in the following diagram: flowchart TB actv[Activation] --> norm[Normalization] --> conv[Convolution] This idea is called \"pre-activation\" in some works. We also support this implementation. By setting the argument layer_order = 'new' , the \"pre-activation\" method would be used for building the layer. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int The kernel size of this layer. stride int The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int The length of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the output data length. Examples \u00b6 In the first example, we build a modern convolutional layer with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern1d ( 16 , 32 , kernel_size = 3 , stride = 2 , padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 16, 255] 32 PReLU-2 [-1, 16, 255] 16 Conv1d-3 [-1, 32, 128] 1,536 ConvModern1d-4 [-1, 32, 128] 0 ================================================================ Total params: 1,584 Trainable params: 1,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.12 Params size (MB): 0.01 Estimated Total Size (MB): 0.15 ---------------------------------------------------------------- Note that the output length would be 128 in this example, because the same padding is used for the input. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern1d ( 32 , 16 , kernel_size = 3 , output_size = 255 , padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 32, 128] 64 PReLU-2 [-1, 32, 128] 32 Upsample-3 [-1, 32, 255] 0 Conv1d-4 [-1, 16, 255] 1,536 ConvModern1d-5 [-1, 16, 255] 0 ================================================================ Total params: 1,632 Trainable params: 1,632 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.19 Params size (MB): 0.01 Estimated Total Size (MB): 0.21 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>ConvModern1d</span>"},{"location":"apis/modules/conv/ConvModern1d/#modulesconvconvmodern1d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . conv . ConvModern1d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) The implementation for the 1D modern convolutional layer. It supports both down-sampling mode and up-sampling modes. The modern convolutional layer is a stack of convolution, normalization and activation. Shown in the following chart: flowchart TB conv[Convolution] --> norm[Normalization] --> actv[Activation] In the following paper, a new op composition order is proposed for building residual block. This idea is may help the performance get improved. Identity Mappings in Deep Residual Networks The basic idea of this method is shown in the following diagram: flowchart TB actv[Activation] --> norm[Normalization] --> conv[Convolution] This idea is called \"pre-activation\" in some works. We also support this implementation. By setting the argument layer_order = 'new' , the \"pre-activation\" method would be used for building the layer.","title":"modules.conv.ConvModern1d"},{"location":"apis/modules/conv/ConvModern1d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int The kernel size of this layer. stride int The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int The length of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/conv/ConvModern1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/ConvModern1d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the output data length.","title":" __call__"},{"location":"apis/modules/conv/ConvModern1d/#examples","text":"In the first example, we build a modern convolutional layer with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern1d ( 16 , 32 , kernel_size = 3 , stride = 2 , padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 16, 255] 32 PReLU-2 [-1, 16, 255] 16 Conv1d-3 [-1, 32, 128] 1,536 ConvModern1d-4 [-1, 32, 128] 0 ================================================================ Total params: 1,584 Trainable params: 1,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.12 Params size (MB): 0.01 Estimated Total Size (MB): 0.15 ---------------------------------------------------------------- Note that the output length would be 128 in this example, because the same padding is used for the input. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern1d ( 32 , 16 , kernel_size = 3 , output_size = 255 , padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 32, 128] 64 PReLU-2 [-1, 32, 128] 32 Upsample-3 [-1, 32, 255] 0 Conv1d-4 [-1, 16, 255] 1,536 ConvModern1d-5 [-1, 16, 255] 0 ================================================================ Total params: 1,632 Trainable params: 1,632 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.19 Params size (MB): 0.01 Estimated Total Size (MB): 0.21 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/ConvModern2d/","text":"modules.conv.ConvModern2d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . conv . ConvModern2d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) The implementation for the 2D modern convolutional layer. It supports both down-sampling mode and up-sampling modes. The modern convolutional layer is a stack of convolution, normalization and activation. Shown in the following chart: flowchart TB conv[Convolution] --> norm[Normalization] --> actv[Activation] In the following paper, a new op composition order is proposed for building residual block. This idea is may help the performance get improved. Identity Mappings in Deep Residual Networks The basic idea of this method is shown in the following diagram: flowchart TB actv[Activation] --> norm[Normalization] --> conv[Convolution] This idea is called \"pre-activation\" in some works. We also support this implementation. By setting the argument layer_order = 'new' , the \"pre-activation\" method would be used for building the layer. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int ) The kernel size of this layer. stride int or ( int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the output data size. Examples \u00b6 In the first example, we build a modern convolutional layer with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern2d ( 16 , 32 , kernel_size = 3 , stride = ( 1 , 2 ), padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 4 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 16, 4, 255] 32 PReLU-2 [-1, 16, 4, 255] 16 Conv2d-3 [-1, 32, 4, 128] 4,608 ConvModern2d-4 [-1, 32, 4, 128] 0 ================================================================ Total params: 4,656 Trainable params: 4,656 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 0.50 Params size (MB): 0.02 Estimated Total Size (MB): 0.58 ---------------------------------------------------------------- Note that the output size would be (4, 128) in this example, because the same padding is used for both two axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern2d ( 32 , 16 , kernel_size = 3 , output_size = ( 4 , 255 ), padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 4 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 32, 4, 128] 64 PReLU-2 [-1, 32, 4, 128] 32 Upsample-3 [-1, 32, 4, 255] 0 Conv2d-4 [-1, 16, 4, 255] 4,608 ConvModern2d-5 [-1, 16, 4, 255] 0 ================================================================ Total params: 4,704 Trainable params: 4,704 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 0.75 Params size (MB): 0.02 Estimated Total Size (MB): 0.83 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>ConvModern2d</span>"},{"location":"apis/modules/conv/ConvModern2d/#modulesconvconvmodern2d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . conv . ConvModern2d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) The implementation for the 2D modern convolutional layer. It supports both down-sampling mode and up-sampling modes. The modern convolutional layer is a stack of convolution, normalization and activation. Shown in the following chart: flowchart TB conv[Convolution] --> norm[Normalization] --> actv[Activation] In the following paper, a new op composition order is proposed for building residual block. This idea is may help the performance get improved. Identity Mappings in Deep Residual Networks The basic idea of this method is shown in the following diagram: flowchart TB actv[Activation] --> norm[Normalization] --> conv[Convolution] This idea is called \"pre-activation\" in some works. We also support this implementation. By setting the argument layer_order = 'new' , the \"pre-activation\" method would be used for building the layer.","title":"modules.conv.ConvModern2d"},{"location":"apis/modules/conv/ConvModern2d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int ) The kernel size of this layer. stride int or ( int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/conv/ConvModern2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/ConvModern2d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the output data size.","title":" __call__"},{"location":"apis/modules/conv/ConvModern2d/#examples","text":"In the first example, we build a modern convolutional layer with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern2d ( 16 , 32 , kernel_size = 3 , stride = ( 1 , 2 ), padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 4 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 16, 4, 255] 32 PReLU-2 [-1, 16, 4, 255] 16 Conv2d-3 [-1, 32, 4, 128] 4,608 ConvModern2d-4 [-1, 32, 4, 128] 0 ================================================================ Total params: 4,656 Trainable params: 4,656 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 0.50 Params size (MB): 0.02 Estimated Total Size (MB): 0.58 ---------------------------------------------------------------- Note that the output size would be (4, 128) in this example, because the same padding is used for both two axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern2d ( 32 , 16 , kernel_size = 3 , output_size = ( 4 , 255 ), padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 4 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 32, 4, 128] 64 PReLU-2 [-1, 32, 4, 128] 32 Upsample-3 [-1, 32, 4, 255] 0 Conv2d-4 [-1, 16, 4, 255] 4,608 ConvModern2d-5 [-1, 16, 4, 255] 0 ================================================================ Total params: 4,704 Trainable params: 4,704 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 0.75 Params size (MB): 0.02 Estimated Total Size (MB): 0.83 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/ConvModern3d/","text":"modules.conv.ConvModern3d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . conv . ConvModern3d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) The implementation for the 3D modern convolutional layer. It supports both down-sampling mode and up-sampling modes. The modern convolutional layer is a stack of convolution, normalization and activation. Shown in the following chart: flowchart TB conv[Convolution] --> norm[Normalization] --> actv[Activation] In the following paper, a new op composition order is proposed for building residual block. This idea is may help the performance get improved. Identity Mappings in Deep Residual Networks The basic idea of this method is shown in the following diagram: flowchart TB actv[Activation] --> norm[Normalization] --> conv[Convolution] This idea is called \"pre-activation\" in some works. We also support this implementation. By setting the argument layer_order = 'new' , the \"pre-activation\" method would be used for building the layer. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int , int ) The kernel size of this layer. stride int or ( int , int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the output data size. Examples \u00b6 In the first example, we build a modern convolutional layer with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern3d ( 16 , 32 , kernel_size = ( 3 , 1 , 3 ), stride = ( 2 , 1 , 2 ), padding = ( 1 , 0 , 1 ), scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 32 , 4 , 63 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 16, 32, 4, 63] 32 PReLU-2 [-1, 16, 32, 4, 63] 16 Conv3d-3 [-1, 32, 16, 4, 32] 4,608 ConvModern3d-4 [-1, 32, 16, 4, 32] 0 ================================================================ Total params: 4,656 Trainable params: 4,656 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.49 Forward/backward pass size (MB): 2.97 Params size (MB): 0.02 Estimated Total Size (MB): 3.48 ---------------------------------------------------------------- Note that the output size would be (16, 4, 32) in this example, because the same padding is used for all three axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern3d ( 32 , 16 , kernel_size = ( 3 , 1 , 3 ), output_size = ( 32 , 4 , 63 ), padding = ( 1 , 0 , 1 ), scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 16 , 4 , 32 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 32, 16, 4, 32] 64 PReLU-2 [-1, 32, 16, 4, 32] 32 Upsample-3 [-1, 32, 32, 4, 63] 0 Conv3d-4 [-1, 16, 32, 4, 63] 4,608 ConvModern3d-5 [-1, 16, 32, 4, 63] 0 ================================================================ Total params: 4,704 Trainable params: 4,704 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.25 Forward/backward pass size (MB): 4.94 Params size (MB): 0.02 Estimated Total Size (MB): 5.21 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>ConvModern3d</span>"},{"location":"apis/modules/conv/ConvModern3d/#modulesconvconvmodern3d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . conv . ConvModern3d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) The implementation for the 3D modern convolutional layer. It supports both down-sampling mode and up-sampling modes. The modern convolutional layer is a stack of convolution, normalization and activation. Shown in the following chart: flowchart TB conv[Convolution] --> norm[Normalization] --> actv[Activation] In the following paper, a new op composition order is proposed for building residual block. This idea is may help the performance get improved. Identity Mappings in Deep Residual Networks The basic idea of this method is shown in the following diagram: flowchart TB actv[Activation] --> norm[Normalization] --> conv[Convolution] This idea is called \"pre-activation\" in some works. We also support this implementation. By setting the argument layer_order = 'new' , the \"pre-activation\" method would be used for building the layer.","title":"modules.conv.ConvModern3d"},{"location":"apis/modules/conv/ConvModern3d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int , int ) The kernel size of this layer. stride int or ( int , int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/conv/ConvModern3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/ConvModern3d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the output data size.","title":" __call__"},{"location":"apis/modules/conv/ConvModern3d/#examples","text":"In the first example, we build a modern convolutional layer with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern3d ( 16 , 32 , kernel_size = ( 3 , 1 , 3 ), stride = ( 2 , 1 , 2 ), padding = ( 1 , 0 , 1 ), scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 32 , 4 , 63 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 16, 32, 4, 63] 32 PReLU-2 [-1, 16, 32, 4, 63] 16 Conv3d-3 [-1, 32, 16, 4, 32] 4,608 ConvModern3d-4 [-1, 32, 16, 4, 32] 0 ================================================================ Total params: 4,656 Trainable params: 4,656 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.49 Forward/backward pass size (MB): 2.97 Params size (MB): 0.02 Estimated Total Size (MB): 3.48 ---------------------------------------------------------------- Note that the output size would be (16, 4, 32) in this example, because the same padding is used for all three axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . conv . ConvModern3d ( 32 , 16 , kernel_size = ( 3 , 1 , 3 ), output_size = ( 32 , 4 , 63 ), padding = ( 1 , 0 , 1 ), scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 16 , 4 , 32 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 32, 16, 4, 32] 64 PReLU-2 [-1, 32, 16, 4, 32] 32 Upsample-3 [-1, 32, 32, 4, 63] 0 Conv3d-4 [-1, 16, 32, 4, 63] 4,608 ConvModern3d-5 [-1, 16, 32, 4, 63] 0 ================================================================ Total params: 4,704 Trainable params: 4,704 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.25 Forward/backward pass size (MB): 4.94 Params size (MB): 0.02 Estimated Total Size (MB): 5.21 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/DecoderNet1d/","text":"modules.conv.DecoderNet1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . DecoderNet1d ( channel , layers , out_size , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 1D convolutional decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] layers\"] u2[\"Block 2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] un[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv1d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int The length of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. kernel_size int The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 1D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the input feature maps (see input_size ) respectively. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the output data size specified by the argument out_size . Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. input_size \u00b6 net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = 32 , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 18. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 132,096 Conv1d-2 [-1, 1024, 4] 3,145,728 InstanceNorm1d-3 [-1, 1024, 4] 2,048 PReLU-4 [-1, 1024, 4] 1,024 Conv1d-5 [-1, 512, 4] 1,572,864 _ConvModernNd-6 [-1, 512, 4] 0 InstanceNorm1d-7 [-1, 512, 4] 1,024 PReLU-8 [-1, 512, 4] 512 Conv1d-9 [-1, 512, 4] 786,432 _ConvModernNd-10 [-1, 512, 4] 0 InstanceNorm1d-11 [-1, 512, 4] 1,024 PReLU-12 [-1, 512, 4] 512 Upsample-13 [-1, 512, 8] 0 Conv1d-14 [-1, 512, 8] 786,432 _ConvModernNd-15 [-1, 512, 8] 0 _BlockConvStkNd-16 [-1, 512, 8] 0 InstanceNorm1d-17 [-1, 512, 8] 1,024 PReLU-18 [-1, 512, 8] 512 Conv1d-19 [-1, 256, 8] 393,216 _ConvModernNd-20 [-1, 256, 8] 0 InstanceNorm1d-21 [-1, 256, 8] 512 PReLU-22 [-1, 256, 8] 256 Conv1d-23 [-1, 256, 8] 196,608 _ConvModernNd-24 [-1, 256, 8] 0 InstanceNorm1d-25 [-1, 256, 8] 512 PReLU-26 [-1, 256, 8] 256 Upsample-27 [-1, 256, 16] 0 Conv1d-28 [-1, 256, 16] 196,608 _ConvModernNd-29 [-1, 256, 16] 0 _BlockConvStkNd-30 [-1, 256, 16] 0 InstanceNorm1d-31 [-1, 256, 16] 512 PReLU-32 [-1, 256, 16] 256 Conv1d-33 [-1, 128, 16] 98,304 _ConvModernNd-34 [-1, 128, 16] 0 InstanceNorm1d-35 [-1, 128, 16] 256 PReLU-36 [-1, 128, 16] 128 Conv1d-37 [-1, 128, 16] 49,152 _ConvModernNd-38 [-1, 128, 16] 0 InstanceNorm1d-39 [-1, 128, 16] 256 PReLU-40 [-1, 128, 16] 128 Upsample-41 [-1, 128, 32] 0 Conv1d-42 [-1, 128, 32] 49,152 _ConvModernNd-43 [-1, 128, 32] 0 _BlockConvStkNd-44 [-1, 128, 32] 0 InstanceNorm1d-45 [-1, 128, 32] 256 PReLU-46 [-1, 128, 32] 128 Conv1d-47 [-1, 64, 32] 24,576 _ConvModernNd-48 [-1, 64, 32] 0 InstanceNorm1d-49 [-1, 64, 32] 128 PReLU-50 [-1, 64, 32] 64 Conv1d-51 [-1, 64, 32] 12,288 _ConvModernNd-52 [-1, 64, 32] 0 InstanceNorm1d-53 [-1, 64, 32] 128 PReLU-54 [-1, 64, 32] 64 Upsample-55 [-1, 64, 64] 0 Conv1d-56 [-1, 64, 64] 12,288 _ConvModernNd-57 [-1, 64, 64] 0 _BlockConvStkNd-58 [-1, 64, 64] 0 InstanceNorm1d-59 [-1, 64, 64] 128 PReLU-60 [-1, 64, 64] 64 Conv1d-61 [-1, 64, 64] 12,288 _ConvModernNd-62 [-1, 64, 64] 0 InstanceNorm1d-63 [-1, 64, 64] 128 PReLU-64 [-1, 64, 64] 64 Conv1d-65 [-1, 64, 64] 12,288 _ConvModernNd-66 [-1, 64, 64] 0 InstanceNorm1d-67 [-1, 64, 64] 128 PReLU-68 [-1, 64, 64] 64 Upsample-69 [-1, 64, 128] 0 Conv1d-70 [-1, 64, 128] 12,288 _ConvModernNd-71 [-1, 64, 128] 0 _BlockConvStkNd-72 [-1, 64, 128] 0 Conv1d-73 [-1, 3, 128] 963 DecoderNet1d-74 [-1, 3, 128] 0 ================================================================ Total params: 7,505,667 Trainable params: 7,505,667 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 1.88 Params size (MB): 28.63 Estimated Total Size (MB): 30.51 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = None , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 17. The input size is (1024, 4). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 3,145,728 InstanceNorm1d-2 [-1, 1024, 4] 2,048 PReLU-3 [-1, 1024, 4] 1,024 Conv1d-4 [-1, 512, 4] 1,572,864 _ConvModernNd-5 [-1, 512, 4] 0 InstanceNorm1d-6 [-1, 512, 4] 1,024 PReLU-7 [-1, 512, 4] 512 Conv1d-8 [-1, 512, 4] 786,432 _ConvModernNd-9 [-1, 512, 4] 0 InstanceNorm1d-10 [-1, 512, 4] 1,024 PReLU-11 [-1, 512, 4] 512 Upsample-12 [-1, 512, 8] 0 Conv1d-13 [-1, 512, 8] 786,432 _ConvModernNd-14 [-1, 512, 8] 0 _BlockConvStkNd-15 [-1, 512, 8] 0 InstanceNorm1d-16 [-1, 512, 8] 1,024 PReLU-17 [-1, 512, 8] 512 Conv1d-18 [-1, 256, 8] 393,216 _ConvModernNd-19 [-1, 256, 8] 0 InstanceNorm1d-20 [-1, 256, 8] 512 PReLU-21 [-1, 256, 8] 256 Conv1d-22 [-1, 256, 8] 196,608 _ConvModernNd-23 [-1, 256, 8] 0 InstanceNorm1d-24 [-1, 256, 8] 512 PReLU-25 [-1, 256, 8] 256 Upsample-26 [-1, 256, 16] 0 Conv1d-27 [-1, 256, 16] 196,608 _ConvModernNd-28 [-1, 256, 16] 0 _BlockConvStkNd-29 [-1, 256, 16] 0 InstanceNorm1d-30 [-1, 256, 16] 512 PReLU-31 [-1, 256, 16] 256 Conv1d-32 [-1, 128, 16] 98,304 _ConvModernNd-33 [-1, 128, 16] 0 InstanceNorm1d-34 [-1, 128, 16] 256 PReLU-35 [-1, 128, 16] 128 Conv1d-36 [-1, 128, 16] 49,152 _ConvModernNd-37 [-1, 128, 16] 0 InstanceNorm1d-38 [-1, 128, 16] 256 PReLU-39 [-1, 128, 16] 128 Upsample-40 [-1, 128, 32] 0 Conv1d-41 [-1, 128, 32] 49,152 _ConvModernNd-42 [-1, 128, 32] 0 _BlockConvStkNd-43 [-1, 128, 32] 0 InstanceNorm1d-44 [-1, 128, 32] 256 PReLU-45 [-1, 128, 32] 128 Conv1d-46 [-1, 64, 32] 24,576 _ConvModernNd-47 [-1, 64, 32] 0 InstanceNorm1d-48 [-1, 64, 32] 128 PReLU-49 [-1, 64, 32] 64 Conv1d-50 [-1, 64, 32] 12,288 _ConvModernNd-51 [-1, 64, 32] 0 InstanceNorm1d-52 [-1, 64, 32] 128 PReLU-53 [-1, 64, 32] 64 Upsample-54 [-1, 64, 64] 0 Conv1d-55 [-1, 64, 64] 12,288 _ConvModernNd-56 [-1, 64, 64] 0 _BlockConvStkNd-57 [-1, 64, 64] 0 InstanceNorm1d-58 [-1, 64, 64] 128 PReLU-59 [-1, 64, 64] 64 Conv1d-60 [-1, 64, 64] 12,288 _ConvModernNd-61 [-1, 64, 64] 0 InstanceNorm1d-62 [-1, 64, 64] 128 PReLU-63 [-1, 64, 64] 64 Conv1d-64 [-1, 64, 64] 12,288 _ConvModernNd-65 [-1, 64, 64] 0 InstanceNorm1d-66 [-1, 64, 64] 128 PReLU-67 [-1, 64, 64] 64 Upsample-68 [-1, 64, 128] 0 Conv1d-69 [-1, 64, 128] 12,288 _ConvModernNd-70 [-1, 64, 128] 0 _BlockConvStkNd-71 [-1, 64, 128] 0 Conv1d-72 [-1, 3, 128] 963 DecoderNet1d-73 [-1, 3, 128] 0 ================================================================ Total params: 7,373,571 Trainable params: 7,373,571 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 1.85 Params size (MB): 28.13 Estimated Total Size (MB): 29.99 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>DecoderNet1d</span>"},{"location":"apis/modules/conv/DecoderNet1d/#modulesconvdecodernet1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . DecoderNet1d ( channel , layers , out_size , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 1D convolutional decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] layers\"] u2[\"Block 2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] un[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv1d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps.","title":"modules.conv.DecoderNet1d"},{"location":"apis/modules/conv/DecoderNet1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int The length of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. kernel_size int The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/DecoderNet1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/DecoderNet1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 1D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the input feature maps (see input_size ) respectively. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the output data size specified by the argument out_size .","title":" __call__"},{"location":"apis/modules/conv/DecoderNet1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/DecoderNet1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/conv/DecoderNet1d/#input_size","text":"net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None .","title":" input_size"},{"location":"apis/modules/conv/DecoderNet1d/#examples","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = 32 , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 18. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 132,096 Conv1d-2 [-1, 1024, 4] 3,145,728 InstanceNorm1d-3 [-1, 1024, 4] 2,048 PReLU-4 [-1, 1024, 4] 1,024 Conv1d-5 [-1, 512, 4] 1,572,864 _ConvModernNd-6 [-1, 512, 4] 0 InstanceNorm1d-7 [-1, 512, 4] 1,024 PReLU-8 [-1, 512, 4] 512 Conv1d-9 [-1, 512, 4] 786,432 _ConvModernNd-10 [-1, 512, 4] 0 InstanceNorm1d-11 [-1, 512, 4] 1,024 PReLU-12 [-1, 512, 4] 512 Upsample-13 [-1, 512, 8] 0 Conv1d-14 [-1, 512, 8] 786,432 _ConvModernNd-15 [-1, 512, 8] 0 _BlockConvStkNd-16 [-1, 512, 8] 0 InstanceNorm1d-17 [-1, 512, 8] 1,024 PReLU-18 [-1, 512, 8] 512 Conv1d-19 [-1, 256, 8] 393,216 _ConvModernNd-20 [-1, 256, 8] 0 InstanceNorm1d-21 [-1, 256, 8] 512 PReLU-22 [-1, 256, 8] 256 Conv1d-23 [-1, 256, 8] 196,608 _ConvModernNd-24 [-1, 256, 8] 0 InstanceNorm1d-25 [-1, 256, 8] 512 PReLU-26 [-1, 256, 8] 256 Upsample-27 [-1, 256, 16] 0 Conv1d-28 [-1, 256, 16] 196,608 _ConvModernNd-29 [-1, 256, 16] 0 _BlockConvStkNd-30 [-1, 256, 16] 0 InstanceNorm1d-31 [-1, 256, 16] 512 PReLU-32 [-1, 256, 16] 256 Conv1d-33 [-1, 128, 16] 98,304 _ConvModernNd-34 [-1, 128, 16] 0 InstanceNorm1d-35 [-1, 128, 16] 256 PReLU-36 [-1, 128, 16] 128 Conv1d-37 [-1, 128, 16] 49,152 _ConvModernNd-38 [-1, 128, 16] 0 InstanceNorm1d-39 [-1, 128, 16] 256 PReLU-40 [-1, 128, 16] 128 Upsample-41 [-1, 128, 32] 0 Conv1d-42 [-1, 128, 32] 49,152 _ConvModernNd-43 [-1, 128, 32] 0 _BlockConvStkNd-44 [-1, 128, 32] 0 InstanceNorm1d-45 [-1, 128, 32] 256 PReLU-46 [-1, 128, 32] 128 Conv1d-47 [-1, 64, 32] 24,576 _ConvModernNd-48 [-1, 64, 32] 0 InstanceNorm1d-49 [-1, 64, 32] 128 PReLU-50 [-1, 64, 32] 64 Conv1d-51 [-1, 64, 32] 12,288 _ConvModernNd-52 [-1, 64, 32] 0 InstanceNorm1d-53 [-1, 64, 32] 128 PReLU-54 [-1, 64, 32] 64 Upsample-55 [-1, 64, 64] 0 Conv1d-56 [-1, 64, 64] 12,288 _ConvModernNd-57 [-1, 64, 64] 0 _BlockConvStkNd-58 [-1, 64, 64] 0 InstanceNorm1d-59 [-1, 64, 64] 128 PReLU-60 [-1, 64, 64] 64 Conv1d-61 [-1, 64, 64] 12,288 _ConvModernNd-62 [-1, 64, 64] 0 InstanceNorm1d-63 [-1, 64, 64] 128 PReLU-64 [-1, 64, 64] 64 Conv1d-65 [-1, 64, 64] 12,288 _ConvModernNd-66 [-1, 64, 64] 0 InstanceNorm1d-67 [-1, 64, 64] 128 PReLU-68 [-1, 64, 64] 64 Upsample-69 [-1, 64, 128] 0 Conv1d-70 [-1, 64, 128] 12,288 _ConvModernNd-71 [-1, 64, 128] 0 _BlockConvStkNd-72 [-1, 64, 128] 0 Conv1d-73 [-1, 3, 128] 963 DecoderNet1d-74 [-1, 3, 128] 0 ================================================================ Total params: 7,505,667 Trainable params: 7,505,667 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 1.88 Params size (MB): 28.63 Estimated Total Size (MB): 30.51 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = None , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 17. The input size is (1024, 4). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 3,145,728 InstanceNorm1d-2 [-1, 1024, 4] 2,048 PReLU-3 [-1, 1024, 4] 1,024 Conv1d-4 [-1, 512, 4] 1,572,864 _ConvModernNd-5 [-1, 512, 4] 0 InstanceNorm1d-6 [-1, 512, 4] 1,024 PReLU-7 [-1, 512, 4] 512 Conv1d-8 [-1, 512, 4] 786,432 _ConvModernNd-9 [-1, 512, 4] 0 InstanceNorm1d-10 [-1, 512, 4] 1,024 PReLU-11 [-1, 512, 4] 512 Upsample-12 [-1, 512, 8] 0 Conv1d-13 [-1, 512, 8] 786,432 _ConvModernNd-14 [-1, 512, 8] 0 _BlockConvStkNd-15 [-1, 512, 8] 0 InstanceNorm1d-16 [-1, 512, 8] 1,024 PReLU-17 [-1, 512, 8] 512 Conv1d-18 [-1, 256, 8] 393,216 _ConvModernNd-19 [-1, 256, 8] 0 InstanceNorm1d-20 [-1, 256, 8] 512 PReLU-21 [-1, 256, 8] 256 Conv1d-22 [-1, 256, 8] 196,608 _ConvModernNd-23 [-1, 256, 8] 0 InstanceNorm1d-24 [-1, 256, 8] 512 PReLU-25 [-1, 256, 8] 256 Upsample-26 [-1, 256, 16] 0 Conv1d-27 [-1, 256, 16] 196,608 _ConvModernNd-28 [-1, 256, 16] 0 _BlockConvStkNd-29 [-1, 256, 16] 0 InstanceNorm1d-30 [-1, 256, 16] 512 PReLU-31 [-1, 256, 16] 256 Conv1d-32 [-1, 128, 16] 98,304 _ConvModernNd-33 [-1, 128, 16] 0 InstanceNorm1d-34 [-1, 128, 16] 256 PReLU-35 [-1, 128, 16] 128 Conv1d-36 [-1, 128, 16] 49,152 _ConvModernNd-37 [-1, 128, 16] 0 InstanceNorm1d-38 [-1, 128, 16] 256 PReLU-39 [-1, 128, 16] 128 Upsample-40 [-1, 128, 32] 0 Conv1d-41 [-1, 128, 32] 49,152 _ConvModernNd-42 [-1, 128, 32] 0 _BlockConvStkNd-43 [-1, 128, 32] 0 InstanceNorm1d-44 [-1, 128, 32] 256 PReLU-45 [-1, 128, 32] 128 Conv1d-46 [-1, 64, 32] 24,576 _ConvModernNd-47 [-1, 64, 32] 0 InstanceNorm1d-48 [-1, 64, 32] 128 PReLU-49 [-1, 64, 32] 64 Conv1d-50 [-1, 64, 32] 12,288 _ConvModernNd-51 [-1, 64, 32] 0 InstanceNorm1d-52 [-1, 64, 32] 128 PReLU-53 [-1, 64, 32] 64 Upsample-54 [-1, 64, 64] 0 Conv1d-55 [-1, 64, 64] 12,288 _ConvModernNd-56 [-1, 64, 64] 0 _BlockConvStkNd-57 [-1, 64, 64] 0 InstanceNorm1d-58 [-1, 64, 64] 128 PReLU-59 [-1, 64, 64] 64 Conv1d-60 [-1, 64, 64] 12,288 _ConvModernNd-61 [-1, 64, 64] 0 InstanceNorm1d-62 [-1, 64, 64] 128 PReLU-63 [-1, 64, 64] 64 Conv1d-64 [-1, 64, 64] 12,288 _ConvModernNd-65 [-1, 64, 64] 0 InstanceNorm1d-66 [-1, 64, 64] 128 PReLU-67 [-1, 64, 64] 64 Upsample-68 [-1, 64, 128] 0 Conv1d-69 [-1, 64, 128] 12,288 _ConvModernNd-70 [-1, 64, 128] 0 _BlockConvStkNd-71 [-1, 64, 128] 0 Conv1d-72 [-1, 3, 128] 963 DecoderNet1d-73 [-1, 3, 128] 0 ================================================================ Total params: 7,373,571 Trainable params: 7,373,571 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 1.85 Params size (MB): 28.13 Estimated Total Size (MB): 29.99 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/DecoderNet2d/","text":"modules.conv.DecoderNet2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . DecoderNet2d ( channel , layers , out_size , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 2D convolutional decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] layers\"] u2[\"Block 2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] un[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv2d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 2D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the output data size specified by the argument out_size . Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. input_size \u00b6 net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = 32 , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 18. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 132,096 Conv2d-2 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-3 [-1, 1024, 2, 2] 2,048 PReLU-4 [-1, 1024, 2, 2] 1,024 Conv2d-5 [-1, 512, 2, 2] 4,718,592 _ConvModernNd-6 [-1, 512, 2, 2] 0 InstanceNorm2d-7 [-1, 512, 2, 2] 1,024 PReLU-8 [-1, 512, 2, 2] 512 Conv2d-9 [-1, 512, 2, 2] 2,359,296 _ConvModernNd-10 [-1, 512, 2, 2] 0 InstanceNorm2d-11 [-1, 512, 2, 2] 1,024 PReLU-12 [-1, 512, 2, 2] 512 Upsample-13 [-1, 512, 4, 4] 0 Conv2d-14 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-15 [-1, 512, 4, 4] 0 _BlockConvStkNd-16 [-1, 512, 4, 4] 0 InstanceNorm2d-17 [-1, 512, 4, 4] 1,024 PReLU-18 [-1, 512, 4, 4] 512 Conv2d-19 [-1, 256, 4, 4] 1,179,648 _ConvModernNd-20 [-1, 256, 4, 4] 0 InstanceNorm2d-21 [-1, 256, 4, 4] 512 PReLU-22 [-1, 256, 4, 4] 256 Conv2d-23 [-1, 256, 4, 4] 589,824 _ConvModernNd-24 [-1, 256, 4, 4] 0 InstanceNorm2d-25 [-1, 256, 4, 4] 512 PReLU-26 [-1, 256, 4, 4] 256 Upsample-27 [-1, 256, 8, 8] 0 Conv2d-28 [-1, 256, 8, 8] 589,824 _ConvModernNd-29 [-1, 256, 8, 8] 0 _BlockConvStkNd-30 [-1, 256, 8, 8] 0 InstanceNorm2d-31 [-1, 256, 8, 8] 512 PReLU-32 [-1, 256, 8, 8] 256 Conv2d-33 [-1, 128, 8, 8] 294,912 _ConvModernNd-34 [-1, 128, 8, 8] 0 InstanceNorm2d-35 [-1, 128, 8, 8] 256 PReLU-36 [-1, 128, 8, 8] 128 Conv2d-37 [-1, 128, 8, 8] 147,456 _ConvModernNd-38 [-1, 128, 8, 8] 0 InstanceNorm2d-39 [-1, 128, 8, 8] 256 PReLU-40 [-1, 128, 8, 8] 128 Upsample-41 [-1, 128, 16, 16] 0 Conv2d-42 [-1, 128, 16, 16] 147,456 _ConvModernNd-43 [-1, 128, 16, 16] 0 _BlockConvStkNd-44 [-1, 128, 16, 16] 0 InstanceNorm2d-45 [-1, 128, 16, 16] 256 PReLU-46 [-1, 128, 16, 16] 128 Conv2d-47 [-1, 64, 16, 16] 73,728 _ConvModernNd-48 [-1, 64, 16, 16] 0 InstanceNorm2d-49 [-1, 64, 16, 16] 128 PReLU-50 [-1, 64, 16, 16] 64 Conv2d-51 [-1, 64, 16, 16] 36,864 _ConvModernNd-52 [-1, 64, 16, 16] 0 InstanceNorm2d-53 [-1, 64, 16, 16] 128 PReLU-54 [-1, 64, 16, 16] 64 Upsample-55 [-1, 64, 32, 32] 0 Conv2d-56 [-1, 64, 32, 32] 36,864 _ConvModernNd-57 [-1, 64, 32, 32] 0 _BlockConvStkNd-58 [-1, 64, 32, 32] 0 InstanceNorm2d-59 [-1, 64, 32, 32] 128 PReLU-60 [-1, 64, 32, 32] 64 Conv2d-61 [-1, 64, 32, 32] 36,864 _ConvModernNd-62 [-1, 64, 32, 32] 0 InstanceNorm2d-63 [-1, 64, 32, 32] 128 PReLU-64 [-1, 64, 32, 32] 64 Conv2d-65 [-1, 64, 32, 32] 36,864 _ConvModernNd-66 [-1, 64, 32, 32] 0 InstanceNorm2d-67 [-1, 64, 32, 32] 128 PReLU-68 [-1, 64, 32, 32] 64 Upsample-69 [-1, 64, 64, 64] 0 Conv2d-70 [-1, 64, 64, 64] 36,864 _ConvModernNd-71 [-1, 64, 64, 64] 0 _BlockConvStkNd-72 [-1, 64, 64, 64] 0 Conv2d-73 [-1, 3, 64, 63] 4,803 DecoderNet2d-74 [-1, 3, 64, 63] 0 ================================================================ Total params: 22,230,531 Trainable params: 22,230,531 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 19.81 Params size (MB): 84.80 Estimated Total Size (MB): 104.61 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = None , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 17. The input size is (1024, 2, 2). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-2 [-1, 1024, 2, 2] 2,048 PReLU-3 [-1, 1024, 2, 2] 1,024 Conv2d-4 [-1, 512, 2, 2] 4,718,592 _ConvModernNd-5 [-1, 512, 2, 2] 0 InstanceNorm2d-6 [-1, 512, 2, 2] 1,024 PReLU-7 [-1, 512, 2, 2] 512 Conv2d-8 [-1, 512, 2, 2] 2,359,296 _ConvModernNd-9 [-1, 512, 2, 2] 0 InstanceNorm2d-10 [-1, 512, 2, 2] 1,024 PReLU-11 [-1, 512, 2, 2] 512 Upsample-12 [-1, 512, 4, 4] 0 Conv2d-13 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-14 [-1, 512, 4, 4] 0 _BlockConvStkNd-15 [-1, 512, 4, 4] 0 InstanceNorm2d-16 [-1, 512, 4, 4] 1,024 PReLU-17 [-1, 512, 4, 4] 512 Conv2d-18 [-1, 256, 4, 4] 1,179,648 _ConvModernNd-19 [-1, 256, 4, 4] 0 InstanceNorm2d-20 [-1, 256, 4, 4] 512 PReLU-21 [-1, 256, 4, 4] 256 Conv2d-22 [-1, 256, 4, 4] 589,824 _ConvModernNd-23 [-1, 256, 4, 4] 0 InstanceNorm2d-24 [-1, 256, 4, 4] 512 PReLU-25 [-1, 256, 4, 4] 256 Upsample-26 [-1, 256, 8, 8] 0 Conv2d-27 [-1, 256, 8, 8] 589,824 _ConvModernNd-28 [-1, 256, 8, 8] 0 _BlockConvStkNd-29 [-1, 256, 8, 8] 0 InstanceNorm2d-30 [-1, 256, 8, 8] 512 PReLU-31 [-1, 256, 8, 8] 256 Conv2d-32 [-1, 128, 8, 8] 294,912 _ConvModernNd-33 [-1, 128, 8, 8] 0 InstanceNorm2d-34 [-1, 128, 8, 8] 256 PReLU-35 [-1, 128, 8, 8] 128 Conv2d-36 [-1, 128, 8, 8] 147,456 _ConvModernNd-37 [-1, 128, 8, 8] 0 InstanceNorm2d-38 [-1, 128, 8, 8] 256 PReLU-39 [-1, 128, 8, 8] 128 Upsample-40 [-1, 128, 16, 16] 0 Conv2d-41 [-1, 128, 16, 16] 147,456 _ConvModernNd-42 [-1, 128, 16, 16] 0 _BlockConvStkNd-43 [-1, 128, 16, 16] 0 InstanceNorm2d-44 [-1, 128, 16, 16] 256 PReLU-45 [-1, 128, 16, 16] 128 Conv2d-46 [-1, 64, 16, 16] 73,728 _ConvModernNd-47 [-1, 64, 16, 16] 0 InstanceNorm2d-48 [-1, 64, 16, 16] 128 PReLU-49 [-1, 64, 16, 16] 64 Conv2d-50 [-1, 64, 16, 16] 36,864 _ConvModernNd-51 [-1, 64, 16, 16] 0 InstanceNorm2d-52 [-1, 64, 16, 16] 128 PReLU-53 [-1, 64, 16, 16] 64 Upsample-54 [-1, 64, 32, 32] 0 Conv2d-55 [-1, 64, 32, 32] 36,864 _ConvModernNd-56 [-1, 64, 32, 32] 0 _BlockConvStkNd-57 [-1, 64, 32, 32] 0 InstanceNorm2d-58 [-1, 64, 32, 32] 128 PReLU-59 [-1, 64, 32, 32] 64 Conv2d-60 [-1, 64, 32, 32] 36,864 _ConvModernNd-61 [-1, 64, 32, 32] 0 InstanceNorm2d-62 [-1, 64, 32, 32] 128 PReLU-63 [-1, 64, 32, 32] 64 Conv2d-64 [-1, 64, 32, 32] 36,864 _ConvModernNd-65 [-1, 64, 32, 32] 0 InstanceNorm2d-66 [-1, 64, 32, 32] 128 PReLU-67 [-1, 64, 32, 32] 64 Upsample-68 [-1, 64, 64, 64] 0 Conv2d-69 [-1, 64, 64, 64] 36,864 _ConvModernNd-70 [-1, 64, 64, 64] 0 _BlockConvStkNd-71 [-1, 64, 64, 64] 0 Conv2d-72 [-1, 3, 64, 63] 4,803 DecoderNet2d-73 [-1, 3, 64, 63] 0 ================================================================ Total params: 22,098,435 Trainable params: 22,098,435 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 19.78 Params size (MB): 84.30 Estimated Total Size (MB): 104.09 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>DecoderNet2d</span>"},{"location":"apis/modules/conv/DecoderNet2d/#modulesconvdecodernet2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . DecoderNet2d ( channel , layers , out_size , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 2D convolutional decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] layers\"] u2[\"Block 2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] un[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv2d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps.","title":"modules.conv.DecoderNet2d"},{"location":"apis/modules/conv/DecoderNet2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/DecoderNet2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/DecoderNet2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 2D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the output data size specified by the argument out_size .","title":" __call__"},{"location":"apis/modules/conv/DecoderNet2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/DecoderNet2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/conv/DecoderNet2d/#input_size","text":"net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None .","title":" input_size"},{"location":"apis/modules/conv/DecoderNet2d/#examples","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = 32 , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 18. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 132,096 Conv2d-2 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-3 [-1, 1024, 2, 2] 2,048 PReLU-4 [-1, 1024, 2, 2] 1,024 Conv2d-5 [-1, 512, 2, 2] 4,718,592 _ConvModernNd-6 [-1, 512, 2, 2] 0 InstanceNorm2d-7 [-1, 512, 2, 2] 1,024 PReLU-8 [-1, 512, 2, 2] 512 Conv2d-9 [-1, 512, 2, 2] 2,359,296 _ConvModernNd-10 [-1, 512, 2, 2] 0 InstanceNorm2d-11 [-1, 512, 2, 2] 1,024 PReLU-12 [-1, 512, 2, 2] 512 Upsample-13 [-1, 512, 4, 4] 0 Conv2d-14 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-15 [-1, 512, 4, 4] 0 _BlockConvStkNd-16 [-1, 512, 4, 4] 0 InstanceNorm2d-17 [-1, 512, 4, 4] 1,024 PReLU-18 [-1, 512, 4, 4] 512 Conv2d-19 [-1, 256, 4, 4] 1,179,648 _ConvModernNd-20 [-1, 256, 4, 4] 0 InstanceNorm2d-21 [-1, 256, 4, 4] 512 PReLU-22 [-1, 256, 4, 4] 256 Conv2d-23 [-1, 256, 4, 4] 589,824 _ConvModernNd-24 [-1, 256, 4, 4] 0 InstanceNorm2d-25 [-1, 256, 4, 4] 512 PReLU-26 [-1, 256, 4, 4] 256 Upsample-27 [-1, 256, 8, 8] 0 Conv2d-28 [-1, 256, 8, 8] 589,824 _ConvModernNd-29 [-1, 256, 8, 8] 0 _BlockConvStkNd-30 [-1, 256, 8, 8] 0 InstanceNorm2d-31 [-1, 256, 8, 8] 512 PReLU-32 [-1, 256, 8, 8] 256 Conv2d-33 [-1, 128, 8, 8] 294,912 _ConvModernNd-34 [-1, 128, 8, 8] 0 InstanceNorm2d-35 [-1, 128, 8, 8] 256 PReLU-36 [-1, 128, 8, 8] 128 Conv2d-37 [-1, 128, 8, 8] 147,456 _ConvModernNd-38 [-1, 128, 8, 8] 0 InstanceNorm2d-39 [-1, 128, 8, 8] 256 PReLU-40 [-1, 128, 8, 8] 128 Upsample-41 [-1, 128, 16, 16] 0 Conv2d-42 [-1, 128, 16, 16] 147,456 _ConvModernNd-43 [-1, 128, 16, 16] 0 _BlockConvStkNd-44 [-1, 128, 16, 16] 0 InstanceNorm2d-45 [-1, 128, 16, 16] 256 PReLU-46 [-1, 128, 16, 16] 128 Conv2d-47 [-1, 64, 16, 16] 73,728 _ConvModernNd-48 [-1, 64, 16, 16] 0 InstanceNorm2d-49 [-1, 64, 16, 16] 128 PReLU-50 [-1, 64, 16, 16] 64 Conv2d-51 [-1, 64, 16, 16] 36,864 _ConvModernNd-52 [-1, 64, 16, 16] 0 InstanceNorm2d-53 [-1, 64, 16, 16] 128 PReLU-54 [-1, 64, 16, 16] 64 Upsample-55 [-1, 64, 32, 32] 0 Conv2d-56 [-1, 64, 32, 32] 36,864 _ConvModernNd-57 [-1, 64, 32, 32] 0 _BlockConvStkNd-58 [-1, 64, 32, 32] 0 InstanceNorm2d-59 [-1, 64, 32, 32] 128 PReLU-60 [-1, 64, 32, 32] 64 Conv2d-61 [-1, 64, 32, 32] 36,864 _ConvModernNd-62 [-1, 64, 32, 32] 0 InstanceNorm2d-63 [-1, 64, 32, 32] 128 PReLU-64 [-1, 64, 32, 32] 64 Conv2d-65 [-1, 64, 32, 32] 36,864 _ConvModernNd-66 [-1, 64, 32, 32] 0 InstanceNorm2d-67 [-1, 64, 32, 32] 128 PReLU-68 [-1, 64, 32, 32] 64 Upsample-69 [-1, 64, 64, 64] 0 Conv2d-70 [-1, 64, 64, 64] 36,864 _ConvModernNd-71 [-1, 64, 64, 64] 0 _BlockConvStkNd-72 [-1, 64, 64, 64] 0 Conv2d-73 [-1, 3, 64, 63] 4,803 DecoderNet2d-74 [-1, 3, 64, 63] 0 ================================================================ Total params: 22,230,531 Trainable params: 22,230,531 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 19.81 Params size (MB): 84.80 Estimated Total Size (MB): 104.61 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = None , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 17. The input size is (1024, 2, 2). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-2 [-1, 1024, 2, 2] 2,048 PReLU-3 [-1, 1024, 2, 2] 1,024 Conv2d-4 [-1, 512, 2, 2] 4,718,592 _ConvModernNd-5 [-1, 512, 2, 2] 0 InstanceNorm2d-6 [-1, 512, 2, 2] 1,024 PReLU-7 [-1, 512, 2, 2] 512 Conv2d-8 [-1, 512, 2, 2] 2,359,296 _ConvModernNd-9 [-1, 512, 2, 2] 0 InstanceNorm2d-10 [-1, 512, 2, 2] 1,024 PReLU-11 [-1, 512, 2, 2] 512 Upsample-12 [-1, 512, 4, 4] 0 Conv2d-13 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-14 [-1, 512, 4, 4] 0 _BlockConvStkNd-15 [-1, 512, 4, 4] 0 InstanceNorm2d-16 [-1, 512, 4, 4] 1,024 PReLU-17 [-1, 512, 4, 4] 512 Conv2d-18 [-1, 256, 4, 4] 1,179,648 _ConvModernNd-19 [-1, 256, 4, 4] 0 InstanceNorm2d-20 [-1, 256, 4, 4] 512 PReLU-21 [-1, 256, 4, 4] 256 Conv2d-22 [-1, 256, 4, 4] 589,824 _ConvModernNd-23 [-1, 256, 4, 4] 0 InstanceNorm2d-24 [-1, 256, 4, 4] 512 PReLU-25 [-1, 256, 4, 4] 256 Upsample-26 [-1, 256, 8, 8] 0 Conv2d-27 [-1, 256, 8, 8] 589,824 _ConvModernNd-28 [-1, 256, 8, 8] 0 _BlockConvStkNd-29 [-1, 256, 8, 8] 0 InstanceNorm2d-30 [-1, 256, 8, 8] 512 PReLU-31 [-1, 256, 8, 8] 256 Conv2d-32 [-1, 128, 8, 8] 294,912 _ConvModernNd-33 [-1, 128, 8, 8] 0 InstanceNorm2d-34 [-1, 128, 8, 8] 256 PReLU-35 [-1, 128, 8, 8] 128 Conv2d-36 [-1, 128, 8, 8] 147,456 _ConvModernNd-37 [-1, 128, 8, 8] 0 InstanceNorm2d-38 [-1, 128, 8, 8] 256 PReLU-39 [-1, 128, 8, 8] 128 Upsample-40 [-1, 128, 16, 16] 0 Conv2d-41 [-1, 128, 16, 16] 147,456 _ConvModernNd-42 [-1, 128, 16, 16] 0 _BlockConvStkNd-43 [-1, 128, 16, 16] 0 InstanceNorm2d-44 [-1, 128, 16, 16] 256 PReLU-45 [-1, 128, 16, 16] 128 Conv2d-46 [-1, 64, 16, 16] 73,728 _ConvModernNd-47 [-1, 64, 16, 16] 0 InstanceNorm2d-48 [-1, 64, 16, 16] 128 PReLU-49 [-1, 64, 16, 16] 64 Conv2d-50 [-1, 64, 16, 16] 36,864 _ConvModernNd-51 [-1, 64, 16, 16] 0 InstanceNorm2d-52 [-1, 64, 16, 16] 128 PReLU-53 [-1, 64, 16, 16] 64 Upsample-54 [-1, 64, 32, 32] 0 Conv2d-55 [-1, 64, 32, 32] 36,864 _ConvModernNd-56 [-1, 64, 32, 32] 0 _BlockConvStkNd-57 [-1, 64, 32, 32] 0 InstanceNorm2d-58 [-1, 64, 32, 32] 128 PReLU-59 [-1, 64, 32, 32] 64 Conv2d-60 [-1, 64, 32, 32] 36,864 _ConvModernNd-61 [-1, 64, 32, 32] 0 InstanceNorm2d-62 [-1, 64, 32, 32] 128 PReLU-63 [-1, 64, 32, 32] 64 Conv2d-64 [-1, 64, 32, 32] 36,864 _ConvModernNd-65 [-1, 64, 32, 32] 0 InstanceNorm2d-66 [-1, 64, 32, 32] 128 PReLU-67 [-1, 64, 32, 32] 64 Upsample-68 [-1, 64, 64, 64] 0 Conv2d-69 [-1, 64, 64, 64] 36,864 _ConvModernNd-70 [-1, 64, 64, 64] 0 _BlockConvStkNd-71 [-1, 64, 64, 64] 0 Conv2d-72 [-1, 3, 64, 63] 4,803 DecoderNet2d-73 [-1, 3, 64, 63] 0 ================================================================ Total params: 22,098,435 Trainable params: 22,098,435 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 19.78 Params size (MB): 84.30 Estimated Total Size (MB): 104.09 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/DecoderNet3d/","text":"modules.conv.DecoderNet3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . DecoderNet3d ( channel , layers , out_size , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 3D convolutional decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] layers\"] u2[\"Block 2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] un[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv3d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 3D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the output data size specified by the argument out_size . Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. input_size \u00b6 net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = 32 , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 18. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 33,792 Conv3d-2 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-3 [-1, 1024, 1, 1, 1] 2,048 PReLU-4 [-1, 1024, 1, 1, 1] 1,024 Conv3d-5 [-1, 512, 1, 1, 1] 14,155,776 _ConvModernNd-6 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-7 [-1, 512, 1, 1, 1] 1,024 PReLU-8 [-1, 512, 1, 1, 1] 512 Conv3d-9 [-1, 512, 1, 1, 1] 7,077,888 _ConvModernNd-10 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-11 [-1, 512, 1, 1, 1] 1,024 PReLU-12 [-1, 512, 1, 1, 1] 512 Upsample-13 [-1, 512, 2, 2, 2] 0 Conv3d-14 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-15 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-16 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-17 [-1, 512, 2, 2, 2] 1,024 PReLU-18 [-1, 512, 2, 2, 2] 512 Conv3d-19 [-1, 256, 2, 2, 2] 3,538,944 _ConvModernNd-20 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-21 [-1, 256, 2, 2, 2] 512 PReLU-22 [-1, 256, 2, 2, 2] 256 Conv3d-23 [-1, 256, 2, 2, 2] 1,769,472 _ConvModernNd-24 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-25 [-1, 256, 2, 2, 2] 512 PReLU-26 [-1, 256, 2, 2, 2] 256 Upsample-27 [-1, 256, 4, 4, 4] 0 Conv3d-28 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-29 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-30 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-31 [-1, 256, 4, 4, 4] 512 PReLU-32 [-1, 256, 4, 4, 4] 256 Conv3d-33 [-1, 128, 4, 4, 4] 884,736 _ConvModernNd-34 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-35 [-1, 128, 4, 4, 4] 256 PReLU-36 [-1, 128, 4, 4, 4] 128 Conv3d-37 [-1, 128, 4, 4, 4] 442,368 _ConvModernNd-38 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-39 [-1, 128, 4, 4, 4] 256 PReLU-40 [-1, 128, 4, 4, 4] 128 Upsample-41 [-1, 128, 8, 8, 8] 0 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-43 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-44 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-45 [-1, 128, 8, 8, 8] 256 PReLU-46 [-1, 128, 8, 8, 8] 128 Conv3d-47 [-1, 64, 8, 8, 8] 221,184 _ConvModernNd-48 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-49 [-1, 64, 8, 8, 8] 128 PReLU-50 [-1, 64, 8, 8, 8] 64 Conv3d-51 [-1, 64, 8, 8, 8] 110,592 _ConvModernNd-52 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-53 [-1, 64, 8, 8, 8] 128 PReLU-54 [-1, 64, 8, 8, 8] 64 Upsample-55 [-1, 64, 16, 16, 16] 0 Conv3d-56 [-1, 64, 16, 16, 16] 110,592 _ConvModernNd-57 [-1, 64, 16, 16, 16] 0 _BlockConvStkNd-58 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-59 [-1, 64, 16, 16, 15] 128 PReLU-60 [-1, 64, 16, 16, 15] 64 Conv3d-61 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-62 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-63 [-1, 64, 16, 16, 15] 128 PReLU-64 [-1, 64, 16, 16, 15] 64 Conv3d-65 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-66 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-67 [-1, 64, 16, 16, 15] 128 PReLU-68 [-1, 64, 16, 16, 15] 64 Upsample-69 [-1, 64, 32, 32, 30] 0 Conv3d-70 [-1, 64, 32, 32, 30] 110,592 _ConvModernNd-71 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-72 [-1, 64, 32, 32, 30] 0 Conv3d-73 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-74 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 66,314,499 Trainable params: 66,314,499 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 94.74 Params size (MB): 252.97 Estimated Total Size (MB): 347.71 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = None , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 17. The input size is (1024, 1, 1, 1). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-2 [-1, 1024, 1, 1, 1] 2,048 PReLU-3 [-1, 1024, 1, 1, 1] 1,024 Conv3d-4 [-1, 512, 1, 1, 1] 14,155,776 _ConvModernNd-5 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-6 [-1, 512, 1, 1, 1] 1,024 PReLU-7 [-1, 512, 1, 1, 1] 512 Conv3d-8 [-1, 512, 1, 1, 1] 7,077,888 _ConvModernNd-9 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-10 [-1, 512, 1, 1, 1] 1,024 PReLU-11 [-1, 512, 1, 1, 1] 512 Upsample-12 [-1, 512, 2, 2, 2] 0 Conv3d-13 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-14 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-15 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-16 [-1, 512, 2, 2, 2] 1,024 PReLU-17 [-1, 512, 2, 2, 2] 512 Conv3d-18 [-1, 256, 2, 2, 2] 3,538,944 _ConvModernNd-19 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-20 [-1, 256, 2, 2, 2] 512 PReLU-21 [-1, 256, 2, 2, 2] 256 Conv3d-22 [-1, 256, 2, 2, 2] 1,769,472 _ConvModernNd-23 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-24 [-1, 256, 2, 2, 2] 512 PReLU-25 [-1, 256, 2, 2, 2] 256 Upsample-26 [-1, 256, 4, 4, 4] 0 Conv3d-27 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-28 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-29 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-30 [-1, 256, 4, 4, 4] 512 PReLU-31 [-1, 256, 4, 4, 4] 256 Conv3d-32 [-1, 128, 4, 4, 4] 884,736 _ConvModernNd-33 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-34 [-1, 128, 4, 4, 4] 256 PReLU-35 [-1, 128, 4, 4, 4] 128 Conv3d-36 [-1, 128, 4, 4, 4] 442,368 _ConvModernNd-37 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-38 [-1, 128, 4, 4, 4] 256 PReLU-39 [-1, 128, 4, 4, 4] 128 Upsample-40 [-1, 128, 8, 8, 8] 0 Conv3d-41 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-42 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-43 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-44 [-1, 128, 8, 8, 8] 256 PReLU-45 [-1, 128, 8, 8, 8] 128 Conv3d-46 [-1, 64, 8, 8, 8] 221,184 _ConvModernNd-47 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-48 [-1, 64, 8, 8, 8] 128 PReLU-49 [-1, 64, 8, 8, 8] 64 Conv3d-50 [-1, 64, 8, 8, 8] 110,592 _ConvModernNd-51 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-52 [-1, 64, 8, 8, 8] 128 PReLU-53 [-1, 64, 8, 8, 8] 64 Upsample-54 [-1, 64, 16, 16, 16] 0 Conv3d-55 [-1, 64, 16, 16, 16] 110,592 _ConvModernNd-56 [-1, 64, 16, 16, 16] 0 _BlockConvStkNd-57 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-58 [-1, 64, 16, 16, 15] 128 PReLU-59 [-1, 64, 16, 16, 15] 64 Conv3d-60 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-61 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-62 [-1, 64, 16, 16, 15] 128 PReLU-63 [-1, 64, 16, 16, 15] 64 Conv3d-64 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-65 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-66 [-1, 64, 16, 16, 15] 128 PReLU-67 [-1, 64, 16, 16, 15] 64 Upsample-68 [-1, 64, 32, 32, 30] 0 Conv3d-69 [-1, 64, 32, 32, 30] 110,592 _ConvModernNd-70 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-71 [-1, 64, 32, 32, 30] 0 Conv3d-72 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-73 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 66,280,707 Trainable params: 66,280,707 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 94.73 Params size (MB): 252.84 Estimated Total Size (MB): 347.57 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>DecoderNet3d</span>"},{"location":"apis/modules/conv/DecoderNet3d/#modulesconvdecodernet3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . DecoderNet3d ( channel , layers , out_size , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 3D convolutional decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] layers\"] u2[\"Block 2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] un[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv3d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps.","title":"modules.conv.DecoderNet3d"},{"location":"apis/modules/conv/DecoderNet3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/DecoderNet3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/DecoderNet3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 3D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the output data size specified by the argument out_size .","title":" __call__"},{"location":"apis/modules/conv/DecoderNet3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/DecoderNet3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/conv/DecoderNet3d/#input_size","text":"net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None .","title":" input_size"},{"location":"apis/modules/conv/DecoderNet3d/#examples","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = 32 , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 18. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 33,792 Conv3d-2 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-3 [-1, 1024, 1, 1, 1] 2,048 PReLU-4 [-1, 1024, 1, 1, 1] 1,024 Conv3d-5 [-1, 512, 1, 1, 1] 14,155,776 _ConvModernNd-6 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-7 [-1, 512, 1, 1, 1] 1,024 PReLU-8 [-1, 512, 1, 1, 1] 512 Conv3d-9 [-1, 512, 1, 1, 1] 7,077,888 _ConvModernNd-10 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-11 [-1, 512, 1, 1, 1] 1,024 PReLU-12 [-1, 512, 1, 1, 1] 512 Upsample-13 [-1, 512, 2, 2, 2] 0 Conv3d-14 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-15 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-16 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-17 [-1, 512, 2, 2, 2] 1,024 PReLU-18 [-1, 512, 2, 2, 2] 512 Conv3d-19 [-1, 256, 2, 2, 2] 3,538,944 _ConvModernNd-20 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-21 [-1, 256, 2, 2, 2] 512 PReLU-22 [-1, 256, 2, 2, 2] 256 Conv3d-23 [-1, 256, 2, 2, 2] 1,769,472 _ConvModernNd-24 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-25 [-1, 256, 2, 2, 2] 512 PReLU-26 [-1, 256, 2, 2, 2] 256 Upsample-27 [-1, 256, 4, 4, 4] 0 Conv3d-28 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-29 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-30 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-31 [-1, 256, 4, 4, 4] 512 PReLU-32 [-1, 256, 4, 4, 4] 256 Conv3d-33 [-1, 128, 4, 4, 4] 884,736 _ConvModernNd-34 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-35 [-1, 128, 4, 4, 4] 256 PReLU-36 [-1, 128, 4, 4, 4] 128 Conv3d-37 [-1, 128, 4, 4, 4] 442,368 _ConvModernNd-38 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-39 [-1, 128, 4, 4, 4] 256 PReLU-40 [-1, 128, 4, 4, 4] 128 Upsample-41 [-1, 128, 8, 8, 8] 0 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-43 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-44 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-45 [-1, 128, 8, 8, 8] 256 PReLU-46 [-1, 128, 8, 8, 8] 128 Conv3d-47 [-1, 64, 8, 8, 8] 221,184 _ConvModernNd-48 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-49 [-1, 64, 8, 8, 8] 128 PReLU-50 [-1, 64, 8, 8, 8] 64 Conv3d-51 [-1, 64, 8, 8, 8] 110,592 _ConvModernNd-52 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-53 [-1, 64, 8, 8, 8] 128 PReLU-54 [-1, 64, 8, 8, 8] 64 Upsample-55 [-1, 64, 16, 16, 16] 0 Conv3d-56 [-1, 64, 16, 16, 16] 110,592 _ConvModernNd-57 [-1, 64, 16, 16, 16] 0 _BlockConvStkNd-58 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-59 [-1, 64, 16, 16, 15] 128 PReLU-60 [-1, 64, 16, 16, 15] 64 Conv3d-61 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-62 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-63 [-1, 64, 16, 16, 15] 128 PReLU-64 [-1, 64, 16, 16, 15] 64 Conv3d-65 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-66 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-67 [-1, 64, 16, 16, 15] 128 PReLU-68 [-1, 64, 16, 16, 15] 64 Upsample-69 [-1, 64, 32, 32, 30] 0 Conv3d-70 [-1, 64, 32, 32, 30] 110,592 _ConvModernNd-71 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-72 [-1, 64, 32, 32, 30] 0 Conv3d-73 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-74 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 66,314,499 Trainable params: 66,314,499 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 94.74 Params size (MB): 252.97 Estimated Total Size (MB): 347.71 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . conv . DecoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_length = None , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 17. The input size is (1024, 1, 1, 1). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-2 [-1, 1024, 1, 1, 1] 2,048 PReLU-3 [-1, 1024, 1, 1, 1] 1,024 Conv3d-4 [-1, 512, 1, 1, 1] 14,155,776 _ConvModernNd-5 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-6 [-1, 512, 1, 1, 1] 1,024 PReLU-7 [-1, 512, 1, 1, 1] 512 Conv3d-8 [-1, 512, 1, 1, 1] 7,077,888 _ConvModernNd-9 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-10 [-1, 512, 1, 1, 1] 1,024 PReLU-11 [-1, 512, 1, 1, 1] 512 Upsample-12 [-1, 512, 2, 2, 2] 0 Conv3d-13 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-14 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-15 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-16 [-1, 512, 2, 2, 2] 1,024 PReLU-17 [-1, 512, 2, 2, 2] 512 Conv3d-18 [-1, 256, 2, 2, 2] 3,538,944 _ConvModernNd-19 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-20 [-1, 256, 2, 2, 2] 512 PReLU-21 [-1, 256, 2, 2, 2] 256 Conv3d-22 [-1, 256, 2, 2, 2] 1,769,472 _ConvModernNd-23 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-24 [-1, 256, 2, 2, 2] 512 PReLU-25 [-1, 256, 2, 2, 2] 256 Upsample-26 [-1, 256, 4, 4, 4] 0 Conv3d-27 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-28 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-29 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-30 [-1, 256, 4, 4, 4] 512 PReLU-31 [-1, 256, 4, 4, 4] 256 Conv3d-32 [-1, 128, 4, 4, 4] 884,736 _ConvModernNd-33 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-34 [-1, 128, 4, 4, 4] 256 PReLU-35 [-1, 128, 4, 4, 4] 128 Conv3d-36 [-1, 128, 4, 4, 4] 442,368 _ConvModernNd-37 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-38 [-1, 128, 4, 4, 4] 256 PReLU-39 [-1, 128, 4, 4, 4] 128 Upsample-40 [-1, 128, 8, 8, 8] 0 Conv3d-41 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-42 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-43 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-44 [-1, 128, 8, 8, 8] 256 PReLU-45 [-1, 128, 8, 8, 8] 128 Conv3d-46 [-1, 64, 8, 8, 8] 221,184 _ConvModernNd-47 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-48 [-1, 64, 8, 8, 8] 128 PReLU-49 [-1, 64, 8, 8, 8] 64 Conv3d-50 [-1, 64, 8, 8, 8] 110,592 _ConvModernNd-51 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-52 [-1, 64, 8, 8, 8] 128 PReLU-53 [-1, 64, 8, 8, 8] 64 Upsample-54 [-1, 64, 16, 16, 16] 0 Conv3d-55 [-1, 64, 16, 16, 16] 110,592 _ConvModernNd-56 [-1, 64, 16, 16, 16] 0 _BlockConvStkNd-57 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-58 [-1, 64, 16, 16, 15] 128 PReLU-59 [-1, 64, 16, 16, 15] 64 Conv3d-60 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-61 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-62 [-1, 64, 16, 16, 15] 128 PReLU-63 [-1, 64, 16, 16, 15] 64 Conv3d-64 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-65 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-66 [-1, 64, 16, 16, 15] 128 PReLU-67 [-1, 64, 16, 16, 15] 64 Upsample-68 [-1, 64, 32, 32, 30] 0 Conv3d-69 [-1, 64, 32, 32, 30] 110,592 _ConvModernNd-70 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-71 [-1, 64, 32, 32, 30] 0 Conv3d-72 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-73 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 66,280,707 Trainable params: 66,280,707 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 94.73 Params size (MB): 252.84 Estimated Total Size (MB): 347.57 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/EncoderNet1d/","text":"modules.conv.EncoderNet1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . EncoderNet1d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 1D convolutional encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each down-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None . Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the last output stage (block) respectively. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. Examples \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 128] 12,288 _ConvModernNd-9 [-1, 64, 128] 0 InstanceNorm1d-10 [-1, 64, 128] 128 PReLU-11 [-1, 64, 128] 64 Conv1d-12 [-1, 64, 64] 12,288 _ConvModernNd-13 [-1, 64, 64] 0 _BlockConvStkNd-14 [-1, 64, 64] 0 InstanceNorm1d-15 [-1, 64, 64] 128 PReLU-16 [-1, 64, 64] 64 Conv1d-17 [-1, 128, 64] 24,576 _ConvModernNd-18 [-1, 128, 64] 0 InstanceNorm1d-19 [-1, 128, 64] 256 PReLU-20 [-1, 128, 64] 128 Conv1d-21 [-1, 128, 64] 49,152 _ConvModernNd-22 [-1, 128, 64] 0 InstanceNorm1d-23 [-1, 128, 64] 256 PReLU-24 [-1, 128, 64] 128 Conv1d-25 [-1, 128, 32] 49,152 _ConvModernNd-26 [-1, 128, 32] 0 _BlockConvStkNd-27 [-1, 128, 32] 0 InstanceNorm1d-28 [-1, 128, 32] 256 PReLU-29 [-1, 128, 32] 128 Conv1d-30 [-1, 256, 32] 98,304 _ConvModernNd-31 [-1, 256, 32] 0 InstanceNorm1d-32 [-1, 256, 32] 512 PReLU-33 [-1, 256, 32] 256 Conv1d-34 [-1, 256, 32] 196,608 _ConvModernNd-35 [-1, 256, 32] 0 InstanceNorm1d-36 [-1, 256, 32] 512 PReLU-37 [-1, 256, 32] 256 Conv1d-38 [-1, 256, 16] 196,608 _ConvModernNd-39 [-1, 256, 16] 0 _BlockConvStkNd-40 [-1, 256, 16] 0 InstanceNorm1d-41 [-1, 256, 16] 512 PReLU-42 [-1, 256, 16] 256 Conv1d-43 [-1, 512, 16] 393,216 _ConvModernNd-44 [-1, 512, 16] 0 InstanceNorm1d-45 [-1, 512, 16] 1,024 PReLU-46 [-1, 512, 16] 512 Conv1d-47 [-1, 512, 16] 786,432 _ConvModernNd-48 [-1, 512, 16] 0 InstanceNorm1d-49 [-1, 512, 16] 1,024 PReLU-50 [-1, 512, 16] 512 Conv1d-51 [-1, 512, 8] 786,432 _ConvModernNd-52 [-1, 512, 8] 0 _BlockConvStkNd-53 [-1, 512, 8] 0 InstanceNorm1d-54 [-1, 512, 8] 1,024 PReLU-55 [-1, 512, 8] 512 Conv1d-56 [-1, 1024, 8] 1,572,864 _ConvModernNd-57 [-1, 1024, 8] 0 InstanceNorm1d-58 [-1, 1024, 8] 2,048 PReLU-59 [-1, 1024, 8] 1,024 Conv1d-60 [-1, 1024, 8] 3,145,728 _ConvModernNd-61 [-1, 1024, 8] 0 InstanceNorm1d-62 [-1, 1024, 8] 2,048 PReLU-63 [-1, 1024, 8] 1,024 Conv1d-64 [-1, 1024, 4] 3,145,728 _ConvModernNd-65 [-1, 1024, 4] 0 _BlockConvStkNd-66 [-1, 1024, 4] 0 Conv1d-67 [-1, 1024, 4] 3,146,752 AdaptiveAvgPool1d-68 [-1, 1024, 1] 0 Linear-69 [-1, 32] 32,800 EncoderNet1d-70 [-1, 32] 0 ================================================================ Total params: 13,677,152 Trainable params: 13,677,152 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 3.45 Params size (MB): 52.17 Estimated Total Size (MB): 55.62 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 128] 12,288 _ConvModernNd-9 [-1, 64, 128] 0 InstanceNorm1d-10 [-1, 64, 128] 128 PReLU-11 [-1, 64, 128] 64 Conv1d-12 [-1, 64, 64] 12,288 _ConvModernNd-13 [-1, 64, 64] 0 _BlockConvStkNd-14 [-1, 64, 64] 0 InstanceNorm1d-15 [-1, 64, 64] 128 PReLU-16 [-1, 64, 64] 64 Conv1d-17 [-1, 128, 64] 24,576 _ConvModernNd-18 [-1, 128, 64] 0 InstanceNorm1d-19 [-1, 128, 64] 256 PReLU-20 [-1, 128, 64] 128 Conv1d-21 [-1, 128, 64] 49,152 _ConvModernNd-22 [-1, 128, 64] 0 InstanceNorm1d-23 [-1, 128, 64] 256 PReLU-24 [-1, 128, 64] 128 Conv1d-25 [-1, 128, 32] 49,152 _ConvModernNd-26 [-1, 128, 32] 0 _BlockConvStkNd-27 [-1, 128, 32] 0 InstanceNorm1d-28 [-1, 128, 32] 256 PReLU-29 [-1, 128, 32] 128 Conv1d-30 [-1, 256, 32] 98,304 _ConvModernNd-31 [-1, 256, 32] 0 InstanceNorm1d-32 [-1, 256, 32] 512 PReLU-33 [-1, 256, 32] 256 Conv1d-34 [-1, 256, 32] 196,608 _ConvModernNd-35 [-1, 256, 32] 0 InstanceNorm1d-36 [-1, 256, 32] 512 PReLU-37 [-1, 256, 32] 256 Conv1d-38 [-1, 256, 16] 196,608 _ConvModernNd-39 [-1, 256, 16] 0 _BlockConvStkNd-40 [-1, 256, 16] 0 InstanceNorm1d-41 [-1, 256, 16] 512 PReLU-42 [-1, 256, 16] 256 Conv1d-43 [-1, 512, 16] 393,216 _ConvModernNd-44 [-1, 512, 16] 0 InstanceNorm1d-45 [-1, 512, 16] 1,024 PReLU-46 [-1, 512, 16] 512 Conv1d-47 [-1, 512, 16] 786,432 _ConvModernNd-48 [-1, 512, 16] 0 InstanceNorm1d-49 [-1, 512, 16] 1,024 PReLU-50 [-1, 512, 16] 512 Conv1d-51 [-1, 512, 8] 786,432 _ConvModernNd-52 [-1, 512, 8] 0 _BlockConvStkNd-53 [-1, 512, 8] 0 InstanceNorm1d-54 [-1, 512, 8] 1,024 PReLU-55 [-1, 512, 8] 512 Conv1d-56 [-1, 1024, 8] 1,572,864 _ConvModernNd-57 [-1, 1024, 8] 0 InstanceNorm1d-58 [-1, 1024, 8] 2,048 PReLU-59 [-1, 1024, 8] 1,024 Conv1d-60 [-1, 1024, 8] 3,145,728 _ConvModernNd-61 [-1, 1024, 8] 0 InstanceNorm1d-62 [-1, 1024, 8] 2,048 PReLU-63 [-1, 1024, 8] 1,024 Conv1d-64 [-1, 1024, 4] 3,145,728 _ConvModernNd-65 [-1, 1024, 4] 0 _BlockConvStkNd-66 [-1, 1024, 4] 0 Conv1d-67 [-1, 1024, 4] 3,146,752 EncoderNet1d-68 [-1, 1024, 4] 0 ================================================================ Total params: 13,644,352 Trainable params: 13,644,352 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 3.47 Params size (MB): 52.05 Estimated Total Size (MB): 55.52 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>EncoderNet1d</span>"},{"location":"apis/modules/conv/EncoderNet1d/#modulesconvencodernet1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . EncoderNet1d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 1D convolutional encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each down-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None .","title":"modules.conv.EncoderNet1d"},{"location":"apis/modules/conv/EncoderNet1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/conv/EncoderNet1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/EncoderNet1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the last output stage (block) respectively.","title":" __call__"},{"location":"apis/modules/conv/EncoderNet1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/EncoderNet1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/conv/EncoderNet1d/#examples","text":"Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 128] 12,288 _ConvModernNd-9 [-1, 64, 128] 0 InstanceNorm1d-10 [-1, 64, 128] 128 PReLU-11 [-1, 64, 128] 64 Conv1d-12 [-1, 64, 64] 12,288 _ConvModernNd-13 [-1, 64, 64] 0 _BlockConvStkNd-14 [-1, 64, 64] 0 InstanceNorm1d-15 [-1, 64, 64] 128 PReLU-16 [-1, 64, 64] 64 Conv1d-17 [-1, 128, 64] 24,576 _ConvModernNd-18 [-1, 128, 64] 0 InstanceNorm1d-19 [-1, 128, 64] 256 PReLU-20 [-1, 128, 64] 128 Conv1d-21 [-1, 128, 64] 49,152 _ConvModernNd-22 [-1, 128, 64] 0 InstanceNorm1d-23 [-1, 128, 64] 256 PReLU-24 [-1, 128, 64] 128 Conv1d-25 [-1, 128, 32] 49,152 _ConvModernNd-26 [-1, 128, 32] 0 _BlockConvStkNd-27 [-1, 128, 32] 0 InstanceNorm1d-28 [-1, 128, 32] 256 PReLU-29 [-1, 128, 32] 128 Conv1d-30 [-1, 256, 32] 98,304 _ConvModernNd-31 [-1, 256, 32] 0 InstanceNorm1d-32 [-1, 256, 32] 512 PReLU-33 [-1, 256, 32] 256 Conv1d-34 [-1, 256, 32] 196,608 _ConvModernNd-35 [-1, 256, 32] 0 InstanceNorm1d-36 [-1, 256, 32] 512 PReLU-37 [-1, 256, 32] 256 Conv1d-38 [-1, 256, 16] 196,608 _ConvModernNd-39 [-1, 256, 16] 0 _BlockConvStkNd-40 [-1, 256, 16] 0 InstanceNorm1d-41 [-1, 256, 16] 512 PReLU-42 [-1, 256, 16] 256 Conv1d-43 [-1, 512, 16] 393,216 _ConvModernNd-44 [-1, 512, 16] 0 InstanceNorm1d-45 [-1, 512, 16] 1,024 PReLU-46 [-1, 512, 16] 512 Conv1d-47 [-1, 512, 16] 786,432 _ConvModernNd-48 [-1, 512, 16] 0 InstanceNorm1d-49 [-1, 512, 16] 1,024 PReLU-50 [-1, 512, 16] 512 Conv1d-51 [-1, 512, 8] 786,432 _ConvModernNd-52 [-1, 512, 8] 0 _BlockConvStkNd-53 [-1, 512, 8] 0 InstanceNorm1d-54 [-1, 512, 8] 1,024 PReLU-55 [-1, 512, 8] 512 Conv1d-56 [-1, 1024, 8] 1,572,864 _ConvModernNd-57 [-1, 1024, 8] 0 InstanceNorm1d-58 [-1, 1024, 8] 2,048 PReLU-59 [-1, 1024, 8] 1,024 Conv1d-60 [-1, 1024, 8] 3,145,728 _ConvModernNd-61 [-1, 1024, 8] 0 InstanceNorm1d-62 [-1, 1024, 8] 2,048 PReLU-63 [-1, 1024, 8] 1,024 Conv1d-64 [-1, 1024, 4] 3,145,728 _ConvModernNd-65 [-1, 1024, 4] 0 _BlockConvStkNd-66 [-1, 1024, 4] 0 Conv1d-67 [-1, 1024, 4] 3,146,752 AdaptiveAvgPool1d-68 [-1, 1024, 1] 0 Linear-69 [-1, 32] 32,800 EncoderNet1d-70 [-1, 32] 0 ================================================================ Total params: 13,677,152 Trainable params: 13,677,152 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 3.45 Params size (MB): 52.17 Estimated Total Size (MB): 55.62 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet1d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 128] 12,288 _ConvModernNd-9 [-1, 64, 128] 0 InstanceNorm1d-10 [-1, 64, 128] 128 PReLU-11 [-1, 64, 128] 64 Conv1d-12 [-1, 64, 64] 12,288 _ConvModernNd-13 [-1, 64, 64] 0 _BlockConvStkNd-14 [-1, 64, 64] 0 InstanceNorm1d-15 [-1, 64, 64] 128 PReLU-16 [-1, 64, 64] 64 Conv1d-17 [-1, 128, 64] 24,576 _ConvModernNd-18 [-1, 128, 64] 0 InstanceNorm1d-19 [-1, 128, 64] 256 PReLU-20 [-1, 128, 64] 128 Conv1d-21 [-1, 128, 64] 49,152 _ConvModernNd-22 [-1, 128, 64] 0 InstanceNorm1d-23 [-1, 128, 64] 256 PReLU-24 [-1, 128, 64] 128 Conv1d-25 [-1, 128, 32] 49,152 _ConvModernNd-26 [-1, 128, 32] 0 _BlockConvStkNd-27 [-1, 128, 32] 0 InstanceNorm1d-28 [-1, 128, 32] 256 PReLU-29 [-1, 128, 32] 128 Conv1d-30 [-1, 256, 32] 98,304 _ConvModernNd-31 [-1, 256, 32] 0 InstanceNorm1d-32 [-1, 256, 32] 512 PReLU-33 [-1, 256, 32] 256 Conv1d-34 [-1, 256, 32] 196,608 _ConvModernNd-35 [-1, 256, 32] 0 InstanceNorm1d-36 [-1, 256, 32] 512 PReLU-37 [-1, 256, 32] 256 Conv1d-38 [-1, 256, 16] 196,608 _ConvModernNd-39 [-1, 256, 16] 0 _BlockConvStkNd-40 [-1, 256, 16] 0 InstanceNorm1d-41 [-1, 256, 16] 512 PReLU-42 [-1, 256, 16] 256 Conv1d-43 [-1, 512, 16] 393,216 _ConvModernNd-44 [-1, 512, 16] 0 InstanceNorm1d-45 [-1, 512, 16] 1,024 PReLU-46 [-1, 512, 16] 512 Conv1d-47 [-1, 512, 16] 786,432 _ConvModernNd-48 [-1, 512, 16] 0 InstanceNorm1d-49 [-1, 512, 16] 1,024 PReLU-50 [-1, 512, 16] 512 Conv1d-51 [-1, 512, 8] 786,432 _ConvModernNd-52 [-1, 512, 8] 0 _BlockConvStkNd-53 [-1, 512, 8] 0 InstanceNorm1d-54 [-1, 512, 8] 1,024 PReLU-55 [-1, 512, 8] 512 Conv1d-56 [-1, 1024, 8] 1,572,864 _ConvModernNd-57 [-1, 1024, 8] 0 InstanceNorm1d-58 [-1, 1024, 8] 2,048 PReLU-59 [-1, 1024, 8] 1,024 Conv1d-60 [-1, 1024, 8] 3,145,728 _ConvModernNd-61 [-1, 1024, 8] 0 InstanceNorm1d-62 [-1, 1024, 8] 2,048 PReLU-63 [-1, 1024, 8] 1,024 Conv1d-64 [-1, 1024, 4] 3,145,728 _ConvModernNd-65 [-1, 1024, 4] 0 _BlockConvStkNd-66 [-1, 1024, 4] 0 Conv1d-67 [-1, 1024, 4] 3,146,752 EncoderNet1d-68 [-1, 1024, 4] 0 ================================================================ Total params: 13,644,352 Trainable params: 13,644,352 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 3.47 Params size (MB): 52.05 Estimated Total Size (MB): 55.52 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/EncoderNet2d/","text":"modules.conv.EncoderNet2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . EncoderNet2d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 2D convolutional encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each down-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None . Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the last output stage (block) respectively. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. Examples \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 64, 63] 36,864 _ConvModernNd-9 [-1, 64, 64, 63] 0 InstanceNorm2d-10 [-1, 64, 64, 63] 128 PReLU-11 [-1, 64, 64, 63] 64 Conv2d-12 [-1, 64, 32, 32] 36,864 _ConvModernNd-13 [-1, 64, 32, 32] 0 _BlockConvStkNd-14 [-1, 64, 32, 32] 0 InstanceNorm2d-15 [-1, 64, 32, 32] 128 PReLU-16 [-1, 64, 32, 32] 64 Conv2d-17 [-1, 128, 32, 32] 73,728 _ConvModernNd-18 [-1, 128, 32, 32] 0 InstanceNorm2d-19 [-1, 128, 32, 32] 256 PReLU-20 [-1, 128, 32, 32] 128 Conv2d-21 [-1, 128, 32, 32] 147,456 _ConvModernNd-22 [-1, 128, 32, 32] 0 InstanceNorm2d-23 [-1, 128, 32, 32] 256 PReLU-24 [-1, 128, 32, 32] 128 Conv2d-25 [-1, 128, 16, 16] 147,456 _ConvModernNd-26 [-1, 128, 16, 16] 0 _BlockConvStkNd-27 [-1, 128, 16, 16] 0 InstanceNorm2d-28 [-1, 128, 16, 16] 256 PReLU-29 [-1, 128, 16, 16] 128 Conv2d-30 [-1, 256, 16, 16] 294,912 _ConvModernNd-31 [-1, 256, 16, 16] 0 InstanceNorm2d-32 [-1, 256, 16, 16] 512 PReLU-33 [-1, 256, 16, 16] 256 Conv2d-34 [-1, 256, 16, 16] 589,824 _ConvModernNd-35 [-1, 256, 16, 16] 0 InstanceNorm2d-36 [-1, 256, 16, 16] 512 PReLU-37 [-1, 256, 16, 16] 256 Conv2d-38 [-1, 256, 8, 8] 589,824 _ConvModernNd-39 [-1, 256, 8, 8] 0 _BlockConvStkNd-40 [-1, 256, 8, 8] 0 InstanceNorm2d-41 [-1, 256, 8, 8] 512 PReLU-42 [-1, 256, 8, 8] 256 Conv2d-43 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-44 [-1, 512, 8, 8] 0 InstanceNorm2d-45 [-1, 512, 8, 8] 1,024 PReLU-46 [-1, 512, 8, 8] 512 Conv2d-47 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-48 [-1, 512, 8, 8] 0 InstanceNorm2d-49 [-1, 512, 8, 8] 1,024 PReLU-50 [-1, 512, 8, 8] 512 Conv2d-51 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-52 [-1, 512, 4, 4] 0 _BlockConvStkNd-53 [-1, 512, 4, 4] 0 InstanceNorm2d-54 [-1, 512, 4, 4] 1,024 PReLU-55 [-1, 512, 4, 4] 512 Conv2d-56 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-57 [-1, 1024, 4, 4] 0 InstanceNorm2d-58 [-1, 1024, 4, 4] 2,048 PReLU-59 [-1, 1024, 4, 4] 1,024 Conv2d-60 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-61 [-1, 1024, 4, 4] 0 InstanceNorm2d-62 [-1, 1024, 4, 4] 2,048 PReLU-63 [-1, 1024, 4, 4] 1,024 Conv2d-64 [-1, 1024, 2, 2] 9,437,184 _ConvModernNd-65 [-1, 1024, 2, 2] 0 _BlockConvStkNd-66 [-1, 1024, 2, 2] 0 Conv2d-67 [-1, 1024, 2, 2] 9,438,208 AdaptiveAvgPool2d-68 [-1, 1024, 1, 1] 0 Linear-69 [-1, 32] 32,800 EncoderNet2d-70 [-1, 32] 0 ================================================================ Total params: 40,935,776 Trainable params: 40,935,776 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 41.48 Params size (MB): 156.16 Estimated Total Size (MB): 197.68 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 64, 63] 36,864 _ConvModernNd-9 [-1, 64, 64, 63] 0 InstanceNorm2d-10 [-1, 64, 64, 63] 128 PReLU-11 [-1, 64, 64, 63] 64 Conv2d-12 [-1, 64, 32, 32] 36,864 _ConvModernNd-13 [-1, 64, 32, 32] 0 _BlockConvStkNd-14 [-1, 64, 32, 32] 0 InstanceNorm2d-15 [-1, 64, 32, 32] 128 PReLU-16 [-1, 64, 32, 32] 64 Conv2d-17 [-1, 128, 32, 32] 73,728 _ConvModernNd-18 [-1, 128, 32, 32] 0 InstanceNorm2d-19 [-1, 128, 32, 32] 256 PReLU-20 [-1, 128, 32, 32] 128 Conv2d-21 [-1, 128, 32, 32] 147,456 _ConvModernNd-22 [-1, 128, 32, 32] 0 InstanceNorm2d-23 [-1, 128, 32, 32] 256 PReLU-24 [-1, 128, 32, 32] 128 Conv2d-25 [-1, 128, 16, 16] 147,456 _ConvModernNd-26 [-1, 128, 16, 16] 0 _BlockConvStkNd-27 [-1, 128, 16, 16] 0 InstanceNorm2d-28 [-1, 128, 16, 16] 256 PReLU-29 [-1, 128, 16, 16] 128 Conv2d-30 [-1, 256, 16, 16] 294,912 _ConvModernNd-31 [-1, 256, 16, 16] 0 InstanceNorm2d-32 [-1, 256, 16, 16] 512 PReLU-33 [-1, 256, 16, 16] 256 Conv2d-34 [-1, 256, 16, 16] 589,824 _ConvModernNd-35 [-1, 256, 16, 16] 0 InstanceNorm2d-36 [-1, 256, 16, 16] 512 PReLU-37 [-1, 256, 16, 16] 256 Conv2d-38 [-1, 256, 8, 8] 589,824 _ConvModernNd-39 [-1, 256, 8, 8] 0 _BlockConvStkNd-40 [-1, 256, 8, 8] 0 InstanceNorm2d-41 [-1, 256, 8, 8] 512 PReLU-42 [-1, 256, 8, 8] 256 Conv2d-43 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-44 [-1, 512, 8, 8] 0 InstanceNorm2d-45 [-1, 512, 8, 8] 1,024 PReLU-46 [-1, 512, 8, 8] 512 Conv2d-47 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-48 [-1, 512, 8, 8] 0 InstanceNorm2d-49 [-1, 512, 8, 8] 1,024 PReLU-50 [-1, 512, 8, 8] 512 Conv2d-51 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-52 [-1, 512, 4, 4] 0 _BlockConvStkNd-53 [-1, 512, 4, 4] 0 InstanceNorm2d-54 [-1, 512, 4, 4] 1,024 PReLU-55 [-1, 512, 4, 4] 512 Conv2d-56 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-57 [-1, 1024, 4, 4] 0 InstanceNorm2d-58 [-1, 1024, 4, 4] 2,048 PReLU-59 [-1, 1024, 4, 4] 1,024 Conv2d-60 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-61 [-1, 1024, 4, 4] 0 InstanceNorm2d-62 [-1, 1024, 4, 4] 2,048 PReLU-63 [-1, 1024, 4, 4] 1,024 Conv2d-64 [-1, 1024, 2, 2] 9,437,184 _ConvModernNd-65 [-1, 1024, 2, 2] 0 _BlockConvStkNd-66 [-1, 1024, 2, 2] 0 Conv2d-67 [-1, 1024, 2, 2] 9,438,208 EncoderNet2d-68 [-1, 1024, 2, 2] 0 ================================================================ Total params: 40,902,976 Trainable params: 40,902,976 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 41.50 Params size (MB): 156.03 Estimated Total Size (MB): 197.58 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>EncoderNet2d</span>"},{"location":"apis/modules/conv/EncoderNet2d/#modulesconvencodernet2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . EncoderNet2d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 2D convolutional encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each down-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None .","title":"modules.conv.EncoderNet2d"},{"location":"apis/modules/conv/EncoderNet2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/conv/EncoderNet2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/EncoderNet2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the last output stage (block) respectively.","title":" __call__"},{"location":"apis/modules/conv/EncoderNet2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/EncoderNet2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/conv/EncoderNet2d/#examples","text":"Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 64, 63] 36,864 _ConvModernNd-9 [-1, 64, 64, 63] 0 InstanceNorm2d-10 [-1, 64, 64, 63] 128 PReLU-11 [-1, 64, 64, 63] 64 Conv2d-12 [-1, 64, 32, 32] 36,864 _ConvModernNd-13 [-1, 64, 32, 32] 0 _BlockConvStkNd-14 [-1, 64, 32, 32] 0 InstanceNorm2d-15 [-1, 64, 32, 32] 128 PReLU-16 [-1, 64, 32, 32] 64 Conv2d-17 [-1, 128, 32, 32] 73,728 _ConvModernNd-18 [-1, 128, 32, 32] 0 InstanceNorm2d-19 [-1, 128, 32, 32] 256 PReLU-20 [-1, 128, 32, 32] 128 Conv2d-21 [-1, 128, 32, 32] 147,456 _ConvModernNd-22 [-1, 128, 32, 32] 0 InstanceNorm2d-23 [-1, 128, 32, 32] 256 PReLU-24 [-1, 128, 32, 32] 128 Conv2d-25 [-1, 128, 16, 16] 147,456 _ConvModernNd-26 [-1, 128, 16, 16] 0 _BlockConvStkNd-27 [-1, 128, 16, 16] 0 InstanceNorm2d-28 [-1, 128, 16, 16] 256 PReLU-29 [-1, 128, 16, 16] 128 Conv2d-30 [-1, 256, 16, 16] 294,912 _ConvModernNd-31 [-1, 256, 16, 16] 0 InstanceNorm2d-32 [-1, 256, 16, 16] 512 PReLU-33 [-1, 256, 16, 16] 256 Conv2d-34 [-1, 256, 16, 16] 589,824 _ConvModernNd-35 [-1, 256, 16, 16] 0 InstanceNorm2d-36 [-1, 256, 16, 16] 512 PReLU-37 [-1, 256, 16, 16] 256 Conv2d-38 [-1, 256, 8, 8] 589,824 _ConvModernNd-39 [-1, 256, 8, 8] 0 _BlockConvStkNd-40 [-1, 256, 8, 8] 0 InstanceNorm2d-41 [-1, 256, 8, 8] 512 PReLU-42 [-1, 256, 8, 8] 256 Conv2d-43 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-44 [-1, 512, 8, 8] 0 InstanceNorm2d-45 [-1, 512, 8, 8] 1,024 PReLU-46 [-1, 512, 8, 8] 512 Conv2d-47 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-48 [-1, 512, 8, 8] 0 InstanceNorm2d-49 [-1, 512, 8, 8] 1,024 PReLU-50 [-1, 512, 8, 8] 512 Conv2d-51 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-52 [-1, 512, 4, 4] 0 _BlockConvStkNd-53 [-1, 512, 4, 4] 0 InstanceNorm2d-54 [-1, 512, 4, 4] 1,024 PReLU-55 [-1, 512, 4, 4] 512 Conv2d-56 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-57 [-1, 1024, 4, 4] 0 InstanceNorm2d-58 [-1, 1024, 4, 4] 2,048 PReLU-59 [-1, 1024, 4, 4] 1,024 Conv2d-60 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-61 [-1, 1024, 4, 4] 0 InstanceNorm2d-62 [-1, 1024, 4, 4] 2,048 PReLU-63 [-1, 1024, 4, 4] 1,024 Conv2d-64 [-1, 1024, 2, 2] 9,437,184 _ConvModernNd-65 [-1, 1024, 2, 2] 0 _BlockConvStkNd-66 [-1, 1024, 2, 2] 0 Conv2d-67 [-1, 1024, 2, 2] 9,438,208 AdaptiveAvgPool2d-68 [-1, 1024, 1, 1] 0 Linear-69 [-1, 32] 32,800 EncoderNet2d-70 [-1, 32] 0 ================================================================ Total params: 40,935,776 Trainable params: 40,935,776 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 41.48 Params size (MB): 156.16 Estimated Total Size (MB): 197.68 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet2d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 64, 63] 36,864 _ConvModernNd-9 [-1, 64, 64, 63] 0 InstanceNorm2d-10 [-1, 64, 64, 63] 128 PReLU-11 [-1, 64, 64, 63] 64 Conv2d-12 [-1, 64, 32, 32] 36,864 _ConvModernNd-13 [-1, 64, 32, 32] 0 _BlockConvStkNd-14 [-1, 64, 32, 32] 0 InstanceNorm2d-15 [-1, 64, 32, 32] 128 PReLU-16 [-1, 64, 32, 32] 64 Conv2d-17 [-1, 128, 32, 32] 73,728 _ConvModernNd-18 [-1, 128, 32, 32] 0 InstanceNorm2d-19 [-1, 128, 32, 32] 256 PReLU-20 [-1, 128, 32, 32] 128 Conv2d-21 [-1, 128, 32, 32] 147,456 _ConvModernNd-22 [-1, 128, 32, 32] 0 InstanceNorm2d-23 [-1, 128, 32, 32] 256 PReLU-24 [-1, 128, 32, 32] 128 Conv2d-25 [-1, 128, 16, 16] 147,456 _ConvModernNd-26 [-1, 128, 16, 16] 0 _BlockConvStkNd-27 [-1, 128, 16, 16] 0 InstanceNorm2d-28 [-1, 128, 16, 16] 256 PReLU-29 [-1, 128, 16, 16] 128 Conv2d-30 [-1, 256, 16, 16] 294,912 _ConvModernNd-31 [-1, 256, 16, 16] 0 InstanceNorm2d-32 [-1, 256, 16, 16] 512 PReLU-33 [-1, 256, 16, 16] 256 Conv2d-34 [-1, 256, 16, 16] 589,824 _ConvModernNd-35 [-1, 256, 16, 16] 0 InstanceNorm2d-36 [-1, 256, 16, 16] 512 PReLU-37 [-1, 256, 16, 16] 256 Conv2d-38 [-1, 256, 8, 8] 589,824 _ConvModernNd-39 [-1, 256, 8, 8] 0 _BlockConvStkNd-40 [-1, 256, 8, 8] 0 InstanceNorm2d-41 [-1, 256, 8, 8] 512 PReLU-42 [-1, 256, 8, 8] 256 Conv2d-43 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-44 [-1, 512, 8, 8] 0 InstanceNorm2d-45 [-1, 512, 8, 8] 1,024 PReLU-46 [-1, 512, 8, 8] 512 Conv2d-47 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-48 [-1, 512, 8, 8] 0 InstanceNorm2d-49 [-1, 512, 8, 8] 1,024 PReLU-50 [-1, 512, 8, 8] 512 Conv2d-51 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-52 [-1, 512, 4, 4] 0 _BlockConvStkNd-53 [-1, 512, 4, 4] 0 InstanceNorm2d-54 [-1, 512, 4, 4] 1,024 PReLU-55 [-1, 512, 4, 4] 512 Conv2d-56 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-57 [-1, 1024, 4, 4] 0 InstanceNorm2d-58 [-1, 1024, 4, 4] 2,048 PReLU-59 [-1, 1024, 4, 4] 1,024 Conv2d-60 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-61 [-1, 1024, 4, 4] 0 InstanceNorm2d-62 [-1, 1024, 4, 4] 2,048 PReLU-63 [-1, 1024, 4, 4] 1,024 Conv2d-64 [-1, 1024, 2, 2] 9,437,184 _ConvModernNd-65 [-1, 1024, 2, 2] 0 _BlockConvStkNd-66 [-1, 1024, 2, 2] 0 Conv2d-67 [-1, 1024, 2, 2] 9,438,208 EncoderNet2d-68 [-1, 1024, 2, 2] 0 ================================================================ Total params: 40,902,976 Trainable params: 40,902,976 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 41.50 Params size (MB): 156.03 Estimated Total Size (MB): 197.58 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/EncoderNet3d/","text":"modules.conv.EncoderNet3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . EncoderNet3d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 3D convolutional encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each down-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None . Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the last output stage (block) respectively. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. Examples \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-9 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-10 [-1, 64, 31, 32, 30] 128 PReLU-11 [-1, 64, 31, 32, 30] 64 Conv3d-12 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-13 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-14 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 64, 16, 16, 15] 128 PReLU-16 [-1, 64, 16, 16, 15] 64 Conv3d-17 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-18 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-19 [-1, 128, 16, 16, 15] 256 PReLU-20 [-1, 128, 16, 16, 15] 128 Conv3d-21 [-1, 128, 16, 16, 15] 442,368 _ConvModernNd-22 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-23 [-1, 128, 16, 16, 15] 256 PReLU-24 [-1, 128, 16, 16, 15] 128 Conv3d-25 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-26 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-27 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 128, 8, 8, 8] 256 PReLU-29 [-1, 128, 8, 8, 8] 128 Conv3d-30 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-31 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-32 [-1, 256, 8, 8, 8] 512 PReLU-33 [-1, 256, 8, 8, 8] 256 Conv3d-34 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-35 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-36 [-1, 256, 8, 8, 8] 512 PReLU-37 [-1, 256, 8, 8, 8] 256 Conv3d-38 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-39 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-40 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 256, 4, 4, 4] 512 PReLU-42 [-1, 256, 4, 4, 4] 256 Conv3d-43 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-44 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-45 [-1, 512, 4, 4, 4] 1,024 PReLU-46 [-1, 512, 4, 4, 4] 512 Conv3d-47 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-48 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-49 [-1, 512, 4, 4, 4] 1,024 PReLU-50 [-1, 512, 4, 4, 4] 512 Conv3d-51 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-52 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-53 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 512, 2, 2, 2] 1,024 PReLU-55 [-1, 512, 2, 2, 2] 512 Conv3d-56 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-57 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-58 [-1, 1024, 2, 2, 2] 2,048 PReLU-59 [-1, 1024, 2, 2, 2] 1,024 Conv3d-60 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-61 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-62 [-1, 1024, 2, 2, 2] 2,048 PReLU-63 [-1, 1024, 2, 2, 2] 1,024 Conv3d-64 [-1, 1024, 1, 1, 1] 28,311,552 _ConvModernNd-65 [-1, 1024, 1, 1, 1] 0 _BlockConvStkNd-66 [-1, 1024, 1, 1, 1] 0 Conv3d-67 [-1, 1024, 1, 1, 1] 28,312,576 AdaptiveAvgPool3d-68 [-1, 1024, 1, 1, 1] 0 Linear-69 [-1, 32] 32,800 EncoderNet3d-70 [-1, 32] 0 ================================================================ Total params: 122,719,328 Trainable params: 122,719,328 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 213.04 Params size (MB): 468.14 Estimated Total Size (MB): 681.52 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-9 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-10 [-1, 64, 31, 32, 30] 128 PReLU-11 [-1, 64, 31, 32, 30] 64 Conv3d-12 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-13 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-14 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 64, 16, 16, 15] 128 PReLU-16 [-1, 64, 16, 16, 15] 64 Conv3d-17 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-18 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-19 [-1, 128, 16, 16, 15] 256 PReLU-20 [-1, 128, 16, 16, 15] 128 Conv3d-21 [-1, 128, 16, 16, 15] 442,368 _ConvModernNd-22 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-23 [-1, 128, 16, 16, 15] 256 PReLU-24 [-1, 128, 16, 16, 15] 128 Conv3d-25 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-26 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-27 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 128, 8, 8, 8] 256 PReLU-29 [-1, 128, 8, 8, 8] 128 Conv3d-30 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-31 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-32 [-1, 256, 8, 8, 8] 512 PReLU-33 [-1, 256, 8, 8, 8] 256 Conv3d-34 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-35 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-36 [-1, 256, 8, 8, 8] 512 PReLU-37 [-1, 256, 8, 8, 8] 256 Conv3d-38 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-39 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-40 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 256, 4, 4, 4] 512 PReLU-42 [-1, 256, 4, 4, 4] 256 Conv3d-43 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-44 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-45 [-1, 512, 4, 4, 4] 1,024 PReLU-46 [-1, 512, 4, 4, 4] 512 Conv3d-47 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-48 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-49 [-1, 512, 4, 4, 4] 1,024 PReLU-50 [-1, 512, 4, 4, 4] 512 Conv3d-51 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-52 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-53 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 512, 2, 2, 2] 1,024 PReLU-55 [-1, 512, 2, 2, 2] 512 Conv3d-56 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-57 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-58 [-1, 1024, 2, 2, 2] 2,048 PReLU-59 [-1, 1024, 2, 2, 2] 1,024 Conv3d-60 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-61 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-62 [-1, 1024, 2, 2, 2] 2,048 PReLU-63 [-1, 1024, 2, 2, 2] 1,024 Conv3d-64 [-1, 1024, 1, 1, 1] 28,311,552 _ConvModernNd-65 [-1, 1024, 1, 1, 1] 0 _BlockConvStkNd-66 [-1, 1024, 1, 1, 1] 0 Conv3d-67 [-1, 1024, 1, 1, 1] 28,312,576 EncoderNet3d-68 [-1, 1024, 1, 1, 1] 0 ================================================================ Total params: 122,686,528 Trainable params: 122,686,528 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 213.04 Params size (MB): 468.01 Estimated Total Size (MB): 681.39 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>EncoderNet3d</span>"},{"location":"apis/modules/conv/EncoderNet3d/#modulesconvencodernet3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . EncoderNet3d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 3D convolutional encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each down-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None .","title":"modules.conv.EncoderNet3d"},{"location":"apis/modules/conv/EncoderNet3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/conv/EncoderNet3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/EncoderNet3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the last output stage (block) respectively.","title":" __call__"},{"location":"apis/modules/conv/EncoderNet3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/EncoderNet3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/conv/EncoderNet3d/#examples","text":"Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-9 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-10 [-1, 64, 31, 32, 30] 128 PReLU-11 [-1, 64, 31, 32, 30] 64 Conv3d-12 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-13 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-14 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 64, 16, 16, 15] 128 PReLU-16 [-1, 64, 16, 16, 15] 64 Conv3d-17 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-18 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-19 [-1, 128, 16, 16, 15] 256 PReLU-20 [-1, 128, 16, 16, 15] 128 Conv3d-21 [-1, 128, 16, 16, 15] 442,368 _ConvModernNd-22 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-23 [-1, 128, 16, 16, 15] 256 PReLU-24 [-1, 128, 16, 16, 15] 128 Conv3d-25 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-26 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-27 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 128, 8, 8, 8] 256 PReLU-29 [-1, 128, 8, 8, 8] 128 Conv3d-30 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-31 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-32 [-1, 256, 8, 8, 8] 512 PReLU-33 [-1, 256, 8, 8, 8] 256 Conv3d-34 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-35 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-36 [-1, 256, 8, 8, 8] 512 PReLU-37 [-1, 256, 8, 8, 8] 256 Conv3d-38 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-39 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-40 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 256, 4, 4, 4] 512 PReLU-42 [-1, 256, 4, 4, 4] 256 Conv3d-43 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-44 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-45 [-1, 512, 4, 4, 4] 1,024 PReLU-46 [-1, 512, 4, 4, 4] 512 Conv3d-47 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-48 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-49 [-1, 512, 4, 4, 4] 1,024 PReLU-50 [-1, 512, 4, 4, 4] 512 Conv3d-51 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-52 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-53 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 512, 2, 2, 2] 1,024 PReLU-55 [-1, 512, 2, 2, 2] 512 Conv3d-56 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-57 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-58 [-1, 1024, 2, 2, 2] 2,048 PReLU-59 [-1, 1024, 2, 2, 2] 1,024 Conv3d-60 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-61 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-62 [-1, 1024, 2, 2, 2] 2,048 PReLU-63 [-1, 1024, 2, 2, 2] 1,024 Conv3d-64 [-1, 1024, 1, 1, 1] 28,311,552 _ConvModernNd-65 [-1, 1024, 1, 1, 1] 0 _BlockConvStkNd-66 [-1, 1024, 1, 1, 1] 0 Conv3d-67 [-1, 1024, 1, 1, 1] 28,312,576 AdaptiveAvgPool3d-68 [-1, 1024, 1, 1, 1] 0 Linear-69 [-1, 32] 32,800 EncoderNet3d-70 [-1, 32] 0 ================================================================ Total params: 122,719,328 Trainable params: 122,719,328 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 213.04 Params size (MB): 468.14 Estimated Total Size (MB): 681.52 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . EncoderNet3d ( 64 , [ 3 , 3 , 3 , 3 , 3 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 17. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-9 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-10 [-1, 64, 31, 32, 30] 128 PReLU-11 [-1, 64, 31, 32, 30] 64 Conv3d-12 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-13 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-14 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 64, 16, 16, 15] 128 PReLU-16 [-1, 64, 16, 16, 15] 64 Conv3d-17 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-18 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-19 [-1, 128, 16, 16, 15] 256 PReLU-20 [-1, 128, 16, 16, 15] 128 Conv3d-21 [-1, 128, 16, 16, 15] 442,368 _ConvModernNd-22 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-23 [-1, 128, 16, 16, 15] 256 PReLU-24 [-1, 128, 16, 16, 15] 128 Conv3d-25 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-26 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-27 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 128, 8, 8, 8] 256 PReLU-29 [-1, 128, 8, 8, 8] 128 Conv3d-30 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-31 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-32 [-1, 256, 8, 8, 8] 512 PReLU-33 [-1, 256, 8, 8, 8] 256 Conv3d-34 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-35 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-36 [-1, 256, 8, 8, 8] 512 PReLU-37 [-1, 256, 8, 8, 8] 256 Conv3d-38 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-39 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-40 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 256, 4, 4, 4] 512 PReLU-42 [-1, 256, 4, 4, 4] 256 Conv3d-43 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-44 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-45 [-1, 512, 4, 4, 4] 1,024 PReLU-46 [-1, 512, 4, 4, 4] 512 Conv3d-47 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-48 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-49 [-1, 512, 4, 4, 4] 1,024 PReLU-50 [-1, 512, 4, 4, 4] 512 Conv3d-51 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-52 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-53 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 512, 2, 2, 2] 1,024 PReLU-55 [-1, 512, 2, 2, 2] 512 Conv3d-56 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-57 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-58 [-1, 1024, 2, 2, 2] 2,048 PReLU-59 [-1, 1024, 2, 2, 2] 1,024 Conv3d-60 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-61 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-62 [-1, 1024, 2, 2, 2] 2,048 PReLU-63 [-1, 1024, 2, 2, 2] 1,024 Conv3d-64 [-1, 1024, 1, 1, 1] 28,311,552 _ConvModernNd-65 [-1, 1024, 1, 1, 1] 0 _BlockConvStkNd-66 [-1, 1024, 1, 1, 1] 0 Conv3d-67 [-1, 1024, 1, 1, 1] 28,312,576 EncoderNet3d-68 [-1, 1024, 1, 1, 1] 0 ================================================================ Total params: 122,686,528 Trainable params: 122,686,528 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 213.04 Params size (MB): 468.01 Estimated Total Size (MB): 681.39 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/UNet1d/","text":"modules.conv.UNet1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . UNet1d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D convolutional U-Net. The network is inspired by: milesial/Pytorch-UNet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . UNet1d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 64] 12,288 _ConvModernNd-9 [-1, 64, 64] 0 _BlockConvStkNd-10 [-1, 64, 64] 0 [-1, 64, 128] InstanceNorm1d-11 [-1, 64, 64] 128 PReLU-12 [-1, 64, 64] 64 Conv1d-13 [-1, 128, 64] 24,576 _ConvModernNd-14 [-1, 128, 64] 0 InstanceNorm1d-15 [-1, 128, 64] 256 PReLU-16 [-1, 128, 64] 128 Conv1d-17 [-1, 128, 32] 49,152 _ConvModernNd-18 [-1, 128, 32] 0 _BlockConvStkNd-19 [-1, 128, 32] 0 [-1, 128, 64] InstanceNorm1d-20 [-1, 128, 32] 256 PReLU-21 [-1, 128, 32] 128 Conv1d-22 [-1, 256, 32] 98,304 _ConvModernNd-23 [-1, 256, 32] 0 InstanceNorm1d-24 [-1, 256, 32] 512 PReLU-25 [-1, 256, 32] 256 Conv1d-26 [-1, 256, 32] 196,608 _ConvModernNd-27 [-1, 256, 32] 0 InstanceNorm1d-28 [-1, 256, 32] 512 PReLU-29 [-1, 256, 32] 256 Conv1d-30 [-1, 256, 16] 196,608 _ConvModernNd-31 [-1, 256, 16] 0 _BlockConvStkNd-32 [-1, 256, 16] 0 [-1, 256, 32] InstanceNorm1d-33 [-1, 256, 16] 512 PReLU-34 [-1, 256, 16] 256 Conv1d-35 [-1, 512, 16] 393,216 _ConvModernNd-36 [-1, 512, 16] 0 InstanceNorm1d-37 [-1, 512, 16] 1,024 PReLU-38 [-1, 512, 16] 512 Conv1d-39 [-1, 512, 16] 786,432 _ConvModernNd-40 [-1, 512, 16] 0 InstanceNorm1d-41 [-1, 512, 16] 1,024 PReLU-42 [-1, 512, 16] 512 Conv1d-43 [-1, 512, 8] 786,432 _ConvModernNd-44 [-1, 512, 8] 0 _BlockConvStkNd-45 [-1, 512, 8] 0 [-1, 512, 16] InstanceNorm1d-46 [-1, 512, 8] 1,024 PReLU-47 [-1, 512, 8] 512 Conv1d-48 [-1, 1024, 8] 1,572,864 _ConvModernNd-49 [-1, 1024, 8] 0 InstanceNorm1d-50 [-1, 1024, 8] 2,048 PReLU-51 [-1, 1024, 8] 1,024 Conv1d-52 [-1, 1024, 8] 3,145,728 _ConvModernNd-53 [-1, 1024, 8] 0 InstanceNorm1d-54 [-1, 1024, 8] 2,048 PReLU-55 [-1, 1024, 8] 1,024 Upsample-56 [-1, 1024, 16] 0 Conv1d-57 [-1, 512, 16] 1,572,864 _ConvModernNd-58 [-1, 512, 16] 0 _BlockConvStkNd-59 [-1, 512, 16] 0 InstanceNorm1d-60 [-1, 1024, 16] 2,048 PReLU-61 [-1, 1024, 16] 1,024 Conv1d-62 [-1, 512, 16] 1,572,864 _ConvModernNd-63 [-1, 512, 16] 0 InstanceNorm1d-64 [-1, 512, 16] 1,024 PReLU-65 [-1, 512, 16] 512 Conv1d-66 [-1, 512, 16] 786,432 _ConvModernNd-67 [-1, 512, 16] 0 InstanceNorm1d-68 [-1, 512, 16] 1,024 PReLU-69 [-1, 512, 16] 512 Upsample-70 [-1, 512, 32] 0 Conv1d-71 [-1, 256, 32] 393,216 _ConvModernNd-72 [-1, 256, 32] 0 _BlockConvStkNd-73 [-1, 256, 32] 0 InstanceNorm1d-74 [-1, 512, 32] 1,024 PReLU-75 [-1, 512, 32] 512 Conv1d-76 [-1, 256, 32] 393,216 _ConvModernNd-77 [-1, 256, 32] 0 InstanceNorm1d-78 [-1, 256, 32] 512 PReLU-79 [-1, 256, 32] 256 Conv1d-80 [-1, 256, 32] 196,608 _ConvModernNd-81 [-1, 256, 32] 0 InstanceNorm1d-82 [-1, 256, 32] 512 PReLU-83 [-1, 256, 32] 256 Upsample-84 [-1, 256, 64] 0 Conv1d-85 [-1, 128, 64] 98,304 _ConvModernNd-86 [-1, 128, 64] 0 _BlockConvStkNd-87 [-1, 128, 64] 0 InstanceNorm1d-88 [-1, 256, 64] 512 PReLU-89 [-1, 256, 64] 256 Conv1d-90 [-1, 128, 64] 98,304 _ConvModernNd-91 [-1, 128, 64] 0 InstanceNorm1d-92 [-1, 128, 64] 256 PReLU-93 [-1, 128, 64] 128 Upsample-94 [-1, 128, 128] 0 Conv1d-95 [-1, 64, 128] 24,576 _ConvModernNd-96 [-1, 64, 128] 0 _BlockConvStkNd-97 [-1, 64, 128] 0 InstanceNorm1d-98 [-1, 128, 128] 256 PReLU-99 [-1, 128, 128] 128 Conv1d-100 [-1, 64, 128] 24,576 _ConvModernNd-101 [-1, 64, 128] 0 InstanceNorm1d-102 [-1, 64, 128] 128 PReLU-103 [-1, 64, 128] 64 Conv1d-104 [-1, 64, 128] 12,288 _ConvModernNd-105 [-1, 64, 128] 0 _BlockConvStkNd-106 [-1, 64, 128] 0 Conv1d-107 [-1, 1, 128] 321 UNet1d-108 [-1, 1, 128] 0 ================================================================ Total params: 12,474,369 Trainable params: 12,474,369 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 7.00 Params size (MB): 47.59 Estimated Total Size (MB): 54.59 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>UNet1d</span>"},{"location":"apis/modules/conv/UNet1d/#modulesconvunet1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . UNet1d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D convolutional U-Net. The network is inspired by: milesial/Pytorch-UNet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation.","title":"modules.conv.UNet1d"},{"location":"apis/modules/conv/UNet1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/UNet1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/UNet1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length.","title":" __call__"},{"location":"apis/modules/conv/UNet1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/UNet1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/conv/UNet1d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . UNet1d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 12,288 _ConvModernNd-5 [-1, 64, 128] 0 InstanceNorm1d-6 [-1, 64, 128] 128 PReLU-7 [-1, 64, 128] 64 Conv1d-8 [-1, 64, 64] 12,288 _ConvModernNd-9 [-1, 64, 64] 0 _BlockConvStkNd-10 [-1, 64, 64] 0 [-1, 64, 128] InstanceNorm1d-11 [-1, 64, 64] 128 PReLU-12 [-1, 64, 64] 64 Conv1d-13 [-1, 128, 64] 24,576 _ConvModernNd-14 [-1, 128, 64] 0 InstanceNorm1d-15 [-1, 128, 64] 256 PReLU-16 [-1, 128, 64] 128 Conv1d-17 [-1, 128, 32] 49,152 _ConvModernNd-18 [-1, 128, 32] 0 _BlockConvStkNd-19 [-1, 128, 32] 0 [-1, 128, 64] InstanceNorm1d-20 [-1, 128, 32] 256 PReLU-21 [-1, 128, 32] 128 Conv1d-22 [-1, 256, 32] 98,304 _ConvModernNd-23 [-1, 256, 32] 0 InstanceNorm1d-24 [-1, 256, 32] 512 PReLU-25 [-1, 256, 32] 256 Conv1d-26 [-1, 256, 32] 196,608 _ConvModernNd-27 [-1, 256, 32] 0 InstanceNorm1d-28 [-1, 256, 32] 512 PReLU-29 [-1, 256, 32] 256 Conv1d-30 [-1, 256, 16] 196,608 _ConvModernNd-31 [-1, 256, 16] 0 _BlockConvStkNd-32 [-1, 256, 16] 0 [-1, 256, 32] InstanceNorm1d-33 [-1, 256, 16] 512 PReLU-34 [-1, 256, 16] 256 Conv1d-35 [-1, 512, 16] 393,216 _ConvModernNd-36 [-1, 512, 16] 0 InstanceNorm1d-37 [-1, 512, 16] 1,024 PReLU-38 [-1, 512, 16] 512 Conv1d-39 [-1, 512, 16] 786,432 _ConvModernNd-40 [-1, 512, 16] 0 InstanceNorm1d-41 [-1, 512, 16] 1,024 PReLU-42 [-1, 512, 16] 512 Conv1d-43 [-1, 512, 8] 786,432 _ConvModernNd-44 [-1, 512, 8] 0 _BlockConvStkNd-45 [-1, 512, 8] 0 [-1, 512, 16] InstanceNorm1d-46 [-1, 512, 8] 1,024 PReLU-47 [-1, 512, 8] 512 Conv1d-48 [-1, 1024, 8] 1,572,864 _ConvModernNd-49 [-1, 1024, 8] 0 InstanceNorm1d-50 [-1, 1024, 8] 2,048 PReLU-51 [-1, 1024, 8] 1,024 Conv1d-52 [-1, 1024, 8] 3,145,728 _ConvModernNd-53 [-1, 1024, 8] 0 InstanceNorm1d-54 [-1, 1024, 8] 2,048 PReLU-55 [-1, 1024, 8] 1,024 Upsample-56 [-1, 1024, 16] 0 Conv1d-57 [-1, 512, 16] 1,572,864 _ConvModernNd-58 [-1, 512, 16] 0 _BlockConvStkNd-59 [-1, 512, 16] 0 InstanceNorm1d-60 [-1, 1024, 16] 2,048 PReLU-61 [-1, 1024, 16] 1,024 Conv1d-62 [-1, 512, 16] 1,572,864 _ConvModernNd-63 [-1, 512, 16] 0 InstanceNorm1d-64 [-1, 512, 16] 1,024 PReLU-65 [-1, 512, 16] 512 Conv1d-66 [-1, 512, 16] 786,432 _ConvModernNd-67 [-1, 512, 16] 0 InstanceNorm1d-68 [-1, 512, 16] 1,024 PReLU-69 [-1, 512, 16] 512 Upsample-70 [-1, 512, 32] 0 Conv1d-71 [-1, 256, 32] 393,216 _ConvModernNd-72 [-1, 256, 32] 0 _BlockConvStkNd-73 [-1, 256, 32] 0 InstanceNorm1d-74 [-1, 512, 32] 1,024 PReLU-75 [-1, 512, 32] 512 Conv1d-76 [-1, 256, 32] 393,216 _ConvModernNd-77 [-1, 256, 32] 0 InstanceNorm1d-78 [-1, 256, 32] 512 PReLU-79 [-1, 256, 32] 256 Conv1d-80 [-1, 256, 32] 196,608 _ConvModernNd-81 [-1, 256, 32] 0 InstanceNorm1d-82 [-1, 256, 32] 512 PReLU-83 [-1, 256, 32] 256 Upsample-84 [-1, 256, 64] 0 Conv1d-85 [-1, 128, 64] 98,304 _ConvModernNd-86 [-1, 128, 64] 0 _BlockConvStkNd-87 [-1, 128, 64] 0 InstanceNorm1d-88 [-1, 256, 64] 512 PReLU-89 [-1, 256, 64] 256 Conv1d-90 [-1, 128, 64] 98,304 _ConvModernNd-91 [-1, 128, 64] 0 InstanceNorm1d-92 [-1, 128, 64] 256 PReLU-93 [-1, 128, 64] 128 Upsample-94 [-1, 128, 128] 0 Conv1d-95 [-1, 64, 128] 24,576 _ConvModernNd-96 [-1, 64, 128] 0 _BlockConvStkNd-97 [-1, 64, 128] 0 InstanceNorm1d-98 [-1, 128, 128] 256 PReLU-99 [-1, 128, 128] 128 Conv1d-100 [-1, 64, 128] 24,576 _ConvModernNd-101 [-1, 64, 128] 0 InstanceNorm1d-102 [-1, 64, 128] 128 PReLU-103 [-1, 64, 128] 64 Conv1d-104 [-1, 64, 128] 12,288 _ConvModernNd-105 [-1, 64, 128] 0 _BlockConvStkNd-106 [-1, 64, 128] 0 Conv1d-107 [-1, 1, 128] 321 UNet1d-108 [-1, 1, 128] 0 ================================================================ Total params: 12,474,369 Trainable params: 12,474,369 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 7.00 Params size (MB): 47.59 Estimated Total Size (MB): 54.59 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/UNet2d/","text":"modules.conv.UNet2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . UNet2d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D convolutional U-Net. The network is inspired by: milesial/Pytorch-UNet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . UNet2d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 32, 32] 36,864 _ConvModernNd-9 [-1, 64, 32, 32] 0 _BlockConvStkNd-10 [-1, 64, 32, 32] 0 [-1, 64, 64, 63] InstanceNorm2d-11 [-1, 64, 32, 32] 128 PReLU-12 [-1, 64, 32, 32] 64 Conv2d-13 [-1, 128, 32, 32] 73,728 _ConvModernNd-14 [-1, 128, 32, 32] 0 InstanceNorm2d-15 [-1, 128, 32, 32] 256 PReLU-16 [-1, 128, 32, 32] 128 Conv2d-17 [-1, 128, 16, 16] 147,456 _ConvModernNd-18 [-1, 128, 16, 16] 0 _BlockConvStkNd-19 [-1, 128, 16, 16] 0 [-1, 128, 32, 32] InstanceNorm2d-20 [-1, 128, 16, 16] 256 PReLU-21 [-1, 128, 16, 16] 128 Conv2d-22 [-1, 256, 16, 16] 294,912 _ConvModernNd-23 [-1, 256, 16, 16] 0 InstanceNorm2d-24 [-1, 256, 16, 16] 512 PReLU-25 [-1, 256, 16, 16] 256 Conv2d-26 [-1, 256, 16, 16] 589,824 _ConvModernNd-27 [-1, 256, 16, 16] 0 InstanceNorm2d-28 [-1, 256, 16, 16] 512 PReLU-29 [-1, 256, 16, 16] 256 Conv2d-30 [-1, 256, 8, 8] 589,824 _ConvModernNd-31 [-1, 256, 8, 8] 0 _BlockConvStkNd-32 [-1, 256, 8, 8] 0 [-1, 256, 16, 16] InstanceNorm2d-33 [-1, 256, 8, 8] 512 PReLU-34 [-1, 256, 8, 8] 256 Conv2d-35 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-36 [-1, 512, 8, 8] 0 InstanceNorm2d-37 [-1, 512, 8, 8] 1,024 PReLU-38 [-1, 512, 8, 8] 512 Conv2d-39 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-40 [-1, 512, 8, 8] 0 InstanceNorm2d-41 [-1, 512, 8, 8] 1,024 PReLU-42 [-1, 512, 8, 8] 512 Conv2d-43 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-44 [-1, 512, 4, 4] 0 _BlockConvStkNd-45 [-1, 512, 4, 4] 0 [-1, 512, 8, 8] InstanceNorm2d-46 [-1, 512, 4, 4] 1,024 PReLU-47 [-1, 512, 4, 4] 512 Conv2d-48 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-49 [-1, 1024, 4, 4] 0 InstanceNorm2d-50 [-1, 1024, 4, 4] 2,048 PReLU-51 [-1, 1024, 4, 4] 1,024 Conv2d-52 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-53 [-1, 1024, 4, 4] 0 InstanceNorm2d-54 [-1, 1024, 4, 4] 2,048 PReLU-55 [-1, 1024, 4, 4] 1,024 Upsample-56 [-1, 1024, 8, 8] 0 Conv2d-57 [-1, 512, 8, 8] 4,718,592 _ConvModernNd-58 [-1, 512, 8, 8] 0 _BlockConvStkNd-59 [-1, 512, 8, 8] 0 InstanceNorm2d-60 [-1, 1024, 8, 8] 2,048 PReLU-61 [-1, 1024, 8, 8] 1,024 Conv2d-62 [-1, 512, 8, 8] 4,718,592 _ConvModernNd-63 [-1, 512, 8, 8] 0 InstanceNorm2d-64 [-1, 512, 8, 8] 1,024 PReLU-65 [-1, 512, 8, 8] 512 Conv2d-66 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-67 [-1, 512, 8, 8] 0 InstanceNorm2d-68 [-1, 512, 8, 8] 1,024 PReLU-69 [-1, 512, 8, 8] 512 Upsample-70 [-1, 512, 16, 16] 0 Conv2d-71 [-1, 256, 16, 16] 1,179,648 _ConvModernNd-72 [-1, 256, 16, 16] 0 _BlockConvStkNd-73 [-1, 256, 16, 16] 0 InstanceNorm2d-74 [-1, 512, 16, 16] 1,024 PReLU-75 [-1, 512, 16, 16] 512 Conv2d-76 [-1, 256, 16, 16] 1,179,648 _ConvModernNd-77 [-1, 256, 16, 16] 0 InstanceNorm2d-78 [-1, 256, 16, 16] 512 PReLU-79 [-1, 256, 16, 16] 256 Conv2d-80 [-1, 256, 16, 16] 589,824 _ConvModernNd-81 [-1, 256, 16, 16] 0 InstanceNorm2d-82 [-1, 256, 16, 16] 512 PReLU-83 [-1, 256, 16, 16] 256 Upsample-84 [-1, 256, 32, 32] 0 Conv2d-85 [-1, 128, 32, 32] 294,912 _ConvModernNd-86 [-1, 128, 32, 32] 0 _BlockConvStkNd-87 [-1, 128, 32, 32] 0 InstanceNorm2d-88 [-1, 256, 32, 32] 512 PReLU-89 [-1, 256, 32, 32] 256 Conv2d-90 [-1, 128, 32, 32] 294,912 _ConvModernNd-91 [-1, 128, 32, 32] 0 InstanceNorm2d-92 [-1, 128, 32, 32] 256 PReLU-93 [-1, 128, 32, 32] 128 Upsample-94 [-1, 128, 64, 64] 0 Conv2d-95 [-1, 64, 64, 64] 73,728 _ConvModernNd-96 [-1, 64, 64, 64] 0 _BlockConvStkNd-97 [-1, 64, 64, 64] 0 InstanceNorm2d-98 [-1, 128, 64, 63] 256 PReLU-99 [-1, 128, 64, 63] 128 Conv2d-100 [-1, 64, 64, 63] 73,728 _ConvModernNd-101 [-1, 64, 64, 63] 0 InstanceNorm2d-102 [-1, 64, 64, 63] 128 PReLU-103 [-1, 64, 64, 63] 64 Conv2d-104 [-1, 64, 64, 63] 36,864 _ConvModernNd-105 [-1, 64, 64, 63] 0 _BlockConvStkNd-106 [-1, 64, 64, 63] 0 Conv2d-107 [-1, 1, 64, 63] 1,601 UNet2d-108 [-1, 1, 64, 63] 0 ================================================================ Total params: 37,374,977 Trainable params: 37,374,977 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 90.66 Params size (MB): 142.57 Estimated Total Size (MB): 233.28 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>UNet2d</span>"},{"location":"apis/modules/conv/UNet2d/#modulesconvunet2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . UNet2d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D convolutional U-Net. The network is inspired by: milesial/Pytorch-UNet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation.","title":"modules.conv.UNet2d"},{"location":"apis/modules/conv/UNet2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/UNet2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/UNet2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size.","title":" __call__"},{"location":"apis/modules/conv/UNet2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/UNet2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/conv/UNet2d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . UNet2d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 36,864 _ConvModernNd-5 [-1, 64, 64, 63] 0 InstanceNorm2d-6 [-1, 64, 64, 63] 128 PReLU-7 [-1, 64, 64, 63] 64 Conv2d-8 [-1, 64, 32, 32] 36,864 _ConvModernNd-9 [-1, 64, 32, 32] 0 _BlockConvStkNd-10 [-1, 64, 32, 32] 0 [-1, 64, 64, 63] InstanceNorm2d-11 [-1, 64, 32, 32] 128 PReLU-12 [-1, 64, 32, 32] 64 Conv2d-13 [-1, 128, 32, 32] 73,728 _ConvModernNd-14 [-1, 128, 32, 32] 0 InstanceNorm2d-15 [-1, 128, 32, 32] 256 PReLU-16 [-1, 128, 32, 32] 128 Conv2d-17 [-1, 128, 16, 16] 147,456 _ConvModernNd-18 [-1, 128, 16, 16] 0 _BlockConvStkNd-19 [-1, 128, 16, 16] 0 [-1, 128, 32, 32] InstanceNorm2d-20 [-1, 128, 16, 16] 256 PReLU-21 [-1, 128, 16, 16] 128 Conv2d-22 [-1, 256, 16, 16] 294,912 _ConvModernNd-23 [-1, 256, 16, 16] 0 InstanceNorm2d-24 [-1, 256, 16, 16] 512 PReLU-25 [-1, 256, 16, 16] 256 Conv2d-26 [-1, 256, 16, 16] 589,824 _ConvModernNd-27 [-1, 256, 16, 16] 0 InstanceNorm2d-28 [-1, 256, 16, 16] 512 PReLU-29 [-1, 256, 16, 16] 256 Conv2d-30 [-1, 256, 8, 8] 589,824 _ConvModernNd-31 [-1, 256, 8, 8] 0 _BlockConvStkNd-32 [-1, 256, 8, 8] 0 [-1, 256, 16, 16] InstanceNorm2d-33 [-1, 256, 8, 8] 512 PReLU-34 [-1, 256, 8, 8] 256 Conv2d-35 [-1, 512, 8, 8] 1,179,648 _ConvModernNd-36 [-1, 512, 8, 8] 0 InstanceNorm2d-37 [-1, 512, 8, 8] 1,024 PReLU-38 [-1, 512, 8, 8] 512 Conv2d-39 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-40 [-1, 512, 8, 8] 0 InstanceNorm2d-41 [-1, 512, 8, 8] 1,024 PReLU-42 [-1, 512, 8, 8] 512 Conv2d-43 [-1, 512, 4, 4] 2,359,296 _ConvModernNd-44 [-1, 512, 4, 4] 0 _BlockConvStkNd-45 [-1, 512, 4, 4] 0 [-1, 512, 8, 8] InstanceNorm2d-46 [-1, 512, 4, 4] 1,024 PReLU-47 [-1, 512, 4, 4] 512 Conv2d-48 [-1, 1024, 4, 4] 4,718,592 _ConvModernNd-49 [-1, 1024, 4, 4] 0 InstanceNorm2d-50 [-1, 1024, 4, 4] 2,048 PReLU-51 [-1, 1024, 4, 4] 1,024 Conv2d-52 [-1, 1024, 4, 4] 9,437,184 _ConvModernNd-53 [-1, 1024, 4, 4] 0 InstanceNorm2d-54 [-1, 1024, 4, 4] 2,048 PReLU-55 [-1, 1024, 4, 4] 1,024 Upsample-56 [-1, 1024, 8, 8] 0 Conv2d-57 [-1, 512, 8, 8] 4,718,592 _ConvModernNd-58 [-1, 512, 8, 8] 0 _BlockConvStkNd-59 [-1, 512, 8, 8] 0 InstanceNorm2d-60 [-1, 1024, 8, 8] 2,048 PReLU-61 [-1, 1024, 8, 8] 1,024 Conv2d-62 [-1, 512, 8, 8] 4,718,592 _ConvModernNd-63 [-1, 512, 8, 8] 0 InstanceNorm2d-64 [-1, 512, 8, 8] 1,024 PReLU-65 [-1, 512, 8, 8] 512 Conv2d-66 [-1, 512, 8, 8] 2,359,296 _ConvModernNd-67 [-1, 512, 8, 8] 0 InstanceNorm2d-68 [-1, 512, 8, 8] 1,024 PReLU-69 [-1, 512, 8, 8] 512 Upsample-70 [-1, 512, 16, 16] 0 Conv2d-71 [-1, 256, 16, 16] 1,179,648 _ConvModernNd-72 [-1, 256, 16, 16] 0 _BlockConvStkNd-73 [-1, 256, 16, 16] 0 InstanceNorm2d-74 [-1, 512, 16, 16] 1,024 PReLU-75 [-1, 512, 16, 16] 512 Conv2d-76 [-1, 256, 16, 16] 1,179,648 _ConvModernNd-77 [-1, 256, 16, 16] 0 InstanceNorm2d-78 [-1, 256, 16, 16] 512 PReLU-79 [-1, 256, 16, 16] 256 Conv2d-80 [-1, 256, 16, 16] 589,824 _ConvModernNd-81 [-1, 256, 16, 16] 0 InstanceNorm2d-82 [-1, 256, 16, 16] 512 PReLU-83 [-1, 256, 16, 16] 256 Upsample-84 [-1, 256, 32, 32] 0 Conv2d-85 [-1, 128, 32, 32] 294,912 _ConvModernNd-86 [-1, 128, 32, 32] 0 _BlockConvStkNd-87 [-1, 128, 32, 32] 0 InstanceNorm2d-88 [-1, 256, 32, 32] 512 PReLU-89 [-1, 256, 32, 32] 256 Conv2d-90 [-1, 128, 32, 32] 294,912 _ConvModernNd-91 [-1, 128, 32, 32] 0 InstanceNorm2d-92 [-1, 128, 32, 32] 256 PReLU-93 [-1, 128, 32, 32] 128 Upsample-94 [-1, 128, 64, 64] 0 Conv2d-95 [-1, 64, 64, 64] 73,728 _ConvModernNd-96 [-1, 64, 64, 64] 0 _BlockConvStkNd-97 [-1, 64, 64, 64] 0 InstanceNorm2d-98 [-1, 128, 64, 63] 256 PReLU-99 [-1, 128, 64, 63] 128 Conv2d-100 [-1, 64, 64, 63] 73,728 _ConvModernNd-101 [-1, 64, 64, 63] 0 InstanceNorm2d-102 [-1, 64, 64, 63] 128 PReLU-103 [-1, 64, 64, 63] 64 Conv2d-104 [-1, 64, 64, 63] 36,864 _ConvModernNd-105 [-1, 64, 64, 63] 0 _BlockConvStkNd-106 [-1, 64, 64, 63] 0 Conv2d-107 [-1, 1, 64, 63] 1,601 UNet2d-108 [-1, 1, 64, 63] 0 ================================================================ Total params: 37,374,977 Trainable params: 37,374,977 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 90.66 Params size (MB): 142.57 Estimated Total Size (MB): 233.28 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/UNet3d/","text":"modules.conv.UNet3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . UNet3d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D convolutional U-Net. The network is inspired by: milesial/Pytorch-UNet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . UNet3d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-9 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-10 [-1, 64, 16, 16, 15] 0 [-1, 64, 31, 32, 30] InstanceNorm3d-11 [-1, 64, 16, 16, 15] 128 PReLU-12 [-1, 64, 16, 16, 15] 64 Conv3d-13 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-14 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 128, 16, 16, 15] 256 PReLU-16 [-1, 128, 16, 16, 15] 128 Conv3d-17 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-18 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-19 [-1, 128, 8, 8, 8] 0 [-1, 128, 16, 16, 15] InstanceNorm3d-20 [-1, 128, 8, 8, 8] 256 PReLU-21 [-1, 128, 8, 8, 8] 128 Conv3d-22 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-23 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-24 [-1, 256, 8, 8, 8] 512 PReLU-25 [-1, 256, 8, 8, 8] 256 Conv3d-26 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-27 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 256, 8, 8, 8] 512 PReLU-29 [-1, 256, 8, 8, 8] 256 Conv3d-30 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-31 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-32 [-1, 256, 4, 4, 4] 0 [-1, 256, 8, 8, 8] InstanceNorm3d-33 [-1, 256, 4, 4, 4] 512 PReLU-34 [-1, 256, 4, 4, 4] 256 Conv3d-35 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-36 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-37 [-1, 512, 4, 4, 4] 1,024 PReLU-38 [-1, 512, 4, 4, 4] 512 Conv3d-39 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-40 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 512, 4, 4, 4] 1,024 PReLU-42 [-1, 512, 4, 4, 4] 512 Conv3d-43 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-44 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-45 [-1, 512, 2, 2, 2] 0 [-1, 512, 4, 4, 4] InstanceNorm3d-46 [-1, 512, 2, 2, 2] 1,024 PReLU-47 [-1, 512, 2, 2, 2] 512 Conv3d-48 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-49 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-50 [-1, 1024, 2, 2, 2] 2,048 PReLU-51 [-1, 1024, 2, 2, 2] 1,024 Conv3d-52 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-53 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 1024, 2, 2, 2] 2,048 PReLU-55 [-1, 1024, 2, 2, 2] 1,024 Upsample-56 [-1, 1024, 4, 4, 4] 0 Conv3d-57 [-1, 512, 4, 4, 4] 14,155,776 _ConvModernNd-58 [-1, 512, 4, 4, 4] 0 _BlockConvStkNd-59 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-60 [-1, 1024, 4, 4, 4] 2,048 PReLU-61 [-1, 1024, 4, 4, 4] 1,024 Conv3d-62 [-1, 512, 4, 4, 4] 14,155,776 _ConvModernNd-63 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-64 [-1, 512, 4, 4, 4] 1,024 PReLU-65 [-1, 512, 4, 4, 4] 512 Conv3d-66 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-67 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-68 [-1, 512, 4, 4, 4] 1,024 PReLU-69 [-1, 512, 4, 4, 4] 512 Upsample-70 [-1, 512, 8, 8, 8] 0 Conv3d-71 [-1, 256, 8, 8, 8] 3,538,944 _ConvModernNd-72 [-1, 256, 8, 8, 8] 0 _BlockConvStkNd-73 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-74 [-1, 512, 8, 8, 8] 1,024 PReLU-75 [-1, 512, 8, 8, 8] 512 Conv3d-76 [-1, 256, 8, 8, 8] 3,538,944 _ConvModernNd-77 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-78 [-1, 256, 8, 8, 8] 512 PReLU-79 [-1, 256, 8, 8, 8] 256 Conv3d-80 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-81 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-82 [-1, 256, 8, 8, 8] 512 PReLU-83 [-1, 256, 8, 8, 8] 256 Upsample-84 [-1, 256, 16, 16, 16] 0 Conv3d-85 [-1, 128, 16, 16, 16] 884,736 _ConvModernNd-86 [-1, 128, 16, 16, 16] 0 _BlockConvStkNd-87 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-88 [-1, 256, 16, 16, 15] 512 PReLU-89 [-1, 256, 16, 16, 15] 256 Conv3d-90 [-1, 128, 16, 16, 15] 884,736 _ConvModernNd-91 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-92 [-1, 128, 16, 16, 15] 256 PReLU-93 [-1, 128, 16, 16, 15] 128 Upsample-94 [-1, 128, 32, 32, 30] 0 Conv3d-95 [-1, 64, 32, 32, 30] 221,184 _ConvModernNd-96 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-97 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-98 [-1, 128, 31, 32, 30] 256 PReLU-99 [-1, 128, 31, 32, 30] 128 Conv3d-100 [-1, 64, 31, 32, 30] 221,184 _ConvModernNd-101 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-102 [-1, 64, 31, 32, 30] 128 PReLU-103 [-1, 64, 31, 32, 30] 64 Conv3d-104 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-105 [-1, 64, 31, 32, 30] 0 _BlockConvStkNd-106 [-1, 64, 31, 32, 30] 0 Conv3d-107 [-1, 1, 31, 32, 30] 8,001 UNet3d-108 [-1, 1, 31, 32, 30] 0 ================================================================ Total params: 112,087,041 Trainable params: 112,087,041 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 465.95 Params size (MB): 427.58 Estimated Total Size (MB): 893.87 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>UNet3d</span>"},{"location":"apis/modules/conv/UNet3d/#modulesconvunet3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . conv . UNet3d ( channel , layers , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D convolutional U-Net. The network is inspired by: milesial/Pytorch-UNet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] layers\"] b2[\"Block 2<br>Stack of layers[1] layers\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] layers\"] u1[\"Block 2n-1<br>Stack of layers[0] layers\"] u2[\"Block 2n-2<br>Stack of layers[1] layers\"] ui[\"Block ...<br>Stack of ... layers\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated modern convolutional layers (see mdnc.modules.conv.ConvModern3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation.","title":"modules.conv.UNet3d"},{"location":"apis/modules/conv/UNet3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of convolutional layers of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. kernel_size int or ( int , int , int ) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/UNet3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/conv/UNet3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size.","title":" __call__"},{"location":"apis/modules/conv/UNet3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/conv/UNet3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/conv/UNet3d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . conv . UNet3d ( 64 , [ 2 , 2 , 3 , 3 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-5 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-6 [-1, 64, 31, 32, 30] 128 PReLU-7 [-1, 64, 31, 32, 30] 64 Conv3d-8 [-1, 64, 16, 16, 15] 110,592 _ConvModernNd-9 [-1, 64, 16, 16, 15] 0 _BlockConvStkNd-10 [-1, 64, 16, 16, 15] 0 [-1, 64, 31, 32, 30] InstanceNorm3d-11 [-1, 64, 16, 16, 15] 128 PReLU-12 [-1, 64, 16, 16, 15] 64 Conv3d-13 [-1, 128, 16, 16, 15] 221,184 _ConvModernNd-14 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-15 [-1, 128, 16, 16, 15] 256 PReLU-16 [-1, 128, 16, 16, 15] 128 Conv3d-17 [-1, 128, 8, 8, 8] 442,368 _ConvModernNd-18 [-1, 128, 8, 8, 8] 0 _BlockConvStkNd-19 [-1, 128, 8, 8, 8] 0 [-1, 128, 16, 16, 15] InstanceNorm3d-20 [-1, 128, 8, 8, 8] 256 PReLU-21 [-1, 128, 8, 8, 8] 128 Conv3d-22 [-1, 256, 8, 8, 8] 884,736 _ConvModernNd-23 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-24 [-1, 256, 8, 8, 8] 512 PReLU-25 [-1, 256, 8, 8, 8] 256 Conv3d-26 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-27 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-28 [-1, 256, 8, 8, 8] 512 PReLU-29 [-1, 256, 8, 8, 8] 256 Conv3d-30 [-1, 256, 4, 4, 4] 1,769,472 _ConvModernNd-31 [-1, 256, 4, 4, 4] 0 _BlockConvStkNd-32 [-1, 256, 4, 4, 4] 0 [-1, 256, 8, 8, 8] InstanceNorm3d-33 [-1, 256, 4, 4, 4] 512 PReLU-34 [-1, 256, 4, 4, 4] 256 Conv3d-35 [-1, 512, 4, 4, 4] 3,538,944 _ConvModernNd-36 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-37 [-1, 512, 4, 4, 4] 1,024 PReLU-38 [-1, 512, 4, 4, 4] 512 Conv3d-39 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-40 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-41 [-1, 512, 4, 4, 4] 1,024 PReLU-42 [-1, 512, 4, 4, 4] 512 Conv3d-43 [-1, 512, 2, 2, 2] 7,077,888 _ConvModernNd-44 [-1, 512, 2, 2, 2] 0 _BlockConvStkNd-45 [-1, 512, 2, 2, 2] 0 [-1, 512, 4, 4, 4] InstanceNorm3d-46 [-1, 512, 2, 2, 2] 1,024 PReLU-47 [-1, 512, 2, 2, 2] 512 Conv3d-48 [-1, 1024, 2, 2, 2] 14,155,776 _ConvModernNd-49 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-50 [-1, 1024, 2, 2, 2] 2,048 PReLU-51 [-1, 1024, 2, 2, 2] 1,024 Conv3d-52 [-1, 1024, 2, 2, 2] 28,311,552 _ConvModernNd-53 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-54 [-1, 1024, 2, 2, 2] 2,048 PReLU-55 [-1, 1024, 2, 2, 2] 1,024 Upsample-56 [-1, 1024, 4, 4, 4] 0 Conv3d-57 [-1, 512, 4, 4, 4] 14,155,776 _ConvModernNd-58 [-1, 512, 4, 4, 4] 0 _BlockConvStkNd-59 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-60 [-1, 1024, 4, 4, 4] 2,048 PReLU-61 [-1, 1024, 4, 4, 4] 1,024 Conv3d-62 [-1, 512, 4, 4, 4] 14,155,776 _ConvModernNd-63 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-64 [-1, 512, 4, 4, 4] 1,024 PReLU-65 [-1, 512, 4, 4, 4] 512 Conv3d-66 [-1, 512, 4, 4, 4] 7,077,888 _ConvModernNd-67 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-68 [-1, 512, 4, 4, 4] 1,024 PReLU-69 [-1, 512, 4, 4, 4] 512 Upsample-70 [-1, 512, 8, 8, 8] 0 Conv3d-71 [-1, 256, 8, 8, 8] 3,538,944 _ConvModernNd-72 [-1, 256, 8, 8, 8] 0 _BlockConvStkNd-73 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-74 [-1, 512, 8, 8, 8] 1,024 PReLU-75 [-1, 512, 8, 8, 8] 512 Conv3d-76 [-1, 256, 8, 8, 8] 3,538,944 _ConvModernNd-77 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-78 [-1, 256, 8, 8, 8] 512 PReLU-79 [-1, 256, 8, 8, 8] 256 Conv3d-80 [-1, 256, 8, 8, 8] 1,769,472 _ConvModernNd-81 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-82 [-1, 256, 8, 8, 8] 512 PReLU-83 [-1, 256, 8, 8, 8] 256 Upsample-84 [-1, 256, 16, 16, 16] 0 Conv3d-85 [-1, 128, 16, 16, 16] 884,736 _ConvModernNd-86 [-1, 128, 16, 16, 16] 0 _BlockConvStkNd-87 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-88 [-1, 256, 16, 16, 15] 512 PReLU-89 [-1, 256, 16, 16, 15] 256 Conv3d-90 [-1, 128, 16, 16, 15] 884,736 _ConvModernNd-91 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-92 [-1, 128, 16, 16, 15] 256 PReLU-93 [-1, 128, 16, 16, 15] 128 Upsample-94 [-1, 128, 32, 32, 30] 0 Conv3d-95 [-1, 64, 32, 32, 30] 221,184 _ConvModernNd-96 [-1, 64, 32, 32, 30] 0 _BlockConvStkNd-97 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-98 [-1, 128, 31, 32, 30] 256 PReLU-99 [-1, 128, 31, 32, 30] 128 Conv3d-100 [-1, 64, 31, 32, 30] 221,184 _ConvModernNd-101 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-102 [-1, 64, 31, 32, 30] 128 PReLU-103 [-1, 64, 31, 32, 30] 64 Conv3d-104 [-1, 64, 31, 32, 30] 110,592 _ConvModernNd-105 [-1, 64, 31, 32, 30] 0 _BlockConvStkNd-106 [-1, 64, 31, 32, 30] 0 Conv3d-107 [-1, 1, 31, 32, 30] 8,001 UNet3d-108 [-1, 1, 31, 32, 30] 0 ================================================================ Total params: 112,087,041 Trainable params: 112,087,041 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 465.95 Params size (MB): 427.58 Estimated Total Size (MB): 893.87 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/conv/aestar/","text":"modules.conv.ae* \u00b6 Function \u00b7 nn.Module net = ae * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.conv.AE*d . Arguments \u00b6 Requries Argument Type Description order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. APIs \u00b6 API Net depth Channel Layer configs Source ae12 3 64 [ 2 , 2 , 2 ] ae16 4 64 [ 2 , 2 , 2 , 2 ] ae17 3 64 [ 3 , 3 , 3 ] ae23 4 64 [ 3 , 3 , 3 , 3 ] ae29 5 64 [ 3 , 3 , 3 , 3 , 3 ]","title":"<span class='magic-codeicon-function'>ae*</span>"},{"location":"apis/modules/conv/aestar/#modulesconvae","text":"Function \u00b7 nn.Module net = ae * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.conv.AE*d .","title":"modules.conv.ae*"},{"location":"apis/modules/conv/aestar/#arguments","text":"Requries Argument Type Description order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/aestar/#apis","text":"API Net depth Channel Layer configs Source ae12 3 64 [ 2 , 2 , 2 ] ae16 4 64 [ 2 , 2 , 2 , 2 ] ae17 3 64 [ 3 , 3 , 3 ] ae23 4 64 [ 3 , 3 , 3 , 3 ] ae29 5 64 [ 3 , 3 , 3 , 3 , 3 ]","title":"APIs"},{"location":"apis/modules/conv/decnetstar/","text":"modules.conv.decnet* \u00b6 Function \u00b7 nn.Module net = decnet * ( out_size , order = 2 , kernel_size = 3 , in_length = 2 , out_planes = 1 ) Instant presents of mdnc.module.conv.DecoderNet*d . Arguments \u00b6 Requries Argument Type Description out_size int or ( int ,) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. APIs \u00b6 API Net depth Channel Layer configs Source decnet13 5 64 [ 2 , 2 , 2 , 2 , 2 ] decnet16 5 64 [ 3 , 3 , 2 , 2 , 2 ] decnet18 5 64 [ 3 , 3 , 3 , 3 , 3 ] decnet23 5 64 [ 4 , 4 , 4 , 4 , 4 ]","title":"<span class='magic-codeicon-function'>decnet*</span>"},{"location":"apis/modules/conv/decnetstar/#modulesconvdecnet","text":"Function \u00b7 nn.Module net = decnet * ( out_size , order = 2 , kernel_size = 3 , in_length = 2 , out_planes = 1 ) Instant presents of mdnc.module.conv.DecoderNet*d .","title":"modules.conv.decnet*"},{"location":"apis/modules/conv/decnetstar/#arguments","text":"Requries Argument Type Description out_size int or ( int ,) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/decnetstar/#apis","text":"API Net depth Channel Layer configs Source decnet13 5 64 [ 2 , 2 , 2 , 2 , 2 ] decnet16 5 64 [ 3 , 3 , 2 , 2 , 2 ] decnet18 5 64 [ 3 , 3 , 3 , 3 , 3 ] decnet23 5 64 [ 4 , 4 , 4 , 4 , 4 ]","title":"APIs"},{"location":"apis/modules/conv/encnetstar/","text":"modules.conv.encnet* \u00b6 Function \u00b7 nn.Module net = encnet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_length = 2 ) Instant presents of mdnc.module.conv.EncoderNet*d . Arguments \u00b6 Requries Argument Type Description order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. APIs \u00b6 API Net depth Channel Layer configs Source encnet12 5 64 [ 2 , 2 , 2 , 2 , 2 ] encnet15 5 64 [ 2 , 2 , 3 , 3 , 3 ] encnet17 5 64 [ 3 , 3 , 3 , 3 , 3 ] encnet22 5 64 [ 4 , 4 , 4 , 4 , 4 ]","title":"<span class='magic-codeicon-function'>encnet*</span>"},{"location":"apis/modules/conv/encnetstar/#modulesconvencnet","text":"Function \u00b7 nn.Module net = encnet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_length = 2 ) Instant presents of mdnc.module.conv.EncoderNet*d .","title":"modules.conv.encnet*"},{"location":"apis/modules/conv/encnetstar/#arguments","text":"Requries Argument Type Description order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/conv/encnetstar/#apis","text":"API Net depth Channel Layer configs Source encnet12 5 64 [ 2 , 2 , 2 , 2 , 2 ] encnet15 5 64 [ 2 , 2 , 3 , 3 , 3 ] encnet17 5 64 [ 3 , 3 , 3 , 3 , 3 ] encnet22 5 64 [ 4 , 4 , 4 , 4 , 4 ]","title":"APIs"},{"location":"apis/modules/conv/unetstar/","text":"modules.conv.unet* \u00b6 Function \u00b7 nn.Module net = unet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.conv.UNet*d . Arguments \u00b6 Requries Argument Type Description order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data. APIs \u00b6 API Net depth Channel Layer configs Source unet12 3 64 [ 2 , 2 , 2 ] unet16 4 64 [ 2 , 2 , 2 , 2 ] unet17 3 64 [ 3 , 3 , 3 ] unet23 4 64 [ 3 , 3 , 3 , 3 ] unet29 5 64 [ 3 , 3 , 3 , 3 , 3 ] where unet29 is a nearly replicated work of milesial/Pytorch-UNet . The only difference of unet29 is two extra convolutional layers for the input and output mapping.","title":"<span class='magic-codeicon-function'>unet*</span>"},{"location":"apis/modules/conv/unetstar/#modulesconvunet","text":"Function \u00b7 nn.Module net = unet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.conv.UNet*d .","title":"modules.conv.unet*"},{"location":"apis/modules/conv/unetstar/#arguments","text":"Requries Argument Type Description order int The order of the convolutional layers, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each convolutional layer. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/conv/unetstar/#apis","text":"API Net depth Channel Layer configs Source unet12 3 64 [ 2 , 2 , 2 ] unet16 4 64 [ 2 , 2 , 2 , 2 ] unet17 3 64 [ 3 , 3 , 3 ] unet23 4 64 [ 3 , 3 , 3 , 3 ] unet29 5 64 [ 3 , 3 , 3 , 3 , 3 ] where unet29 is a nearly replicated work of milesial/Pytorch-UNet . The only difference of unet29 is two extra convolutional layers for the input and output mapping.","title":"APIs"},{"location":"apis/modules/resnet/AE1d/","text":"modules.resnet.AE1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . AE1d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D residual auto-encoder. The network structure is almost the same as mdnc.modules.resnet.UNet1d but all block-level skip connections are removed. Generally, using mdnc.modules.resnet.UNet1d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . AE1d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 8] 3,145,728 InstanceNorm1d-118 [-1, 1024, 8] 2,048 PReLU-119 [-1, 1024, 8] 1,024 Conv1d-120 [-1, 1024, 8] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 8] 0 InstanceNorm1d-122 [-1, 1024, 8] 2,048 PReLU-123 [-1, 1024, 8] 1,024 Conv1d-124 [-1, 1024, 8] 1,048,576 InstanceNorm1d-125 [-1, 1024, 8] 2,048 PReLU-126 [-1, 1024, 8] 1,024 Upsample-127 [-1, 1024, 16] 0 Conv1d-128 [-1, 1024, 16] 3,145,728 InstanceNorm1d-129 [-1, 1024, 16] 2,048 PReLU-130 [-1, 1024, 16] 1,024 Conv1d-131 [-1, 512, 16] 524,288 Upsample-132 [-1, 1024, 16] 0 Conv1d-133 [-1, 512, 16] 524,288 InstanceNorm1d-134 [-1, 512, 16] 1,024 _BlockBo...eckNd-135 [-1, 512, 16] 0 _BlockResStkNd-136 [-1, 512, 16] 0 InstanceNorm1d-137 [-1, 512, 16] 1,024 PReLU-138 [-1, 512, 16] 512 Conv1d-139 [-1, 512, 16] 262,144 InstanceNorm1d-140 [-1, 512, 16] 1,024 PReLU-141 [-1, 512, 16] 512 Conv1d-142 [-1, 512, 16] 786,432 InstanceNorm1d-143 [-1, 512, 16] 1,024 PReLU-144 [-1, 512, 16] 512 Conv1d-145 [-1, 512, 16] 262,144 _BlockBo...eckNd-146 [-1, 512, 16] 0 InstanceNorm1d-147 [-1, 512, 16] 1,024 PReLU-148 [-1, 512, 16] 512 Conv1d-149 [-1, 512, 16] 262,144 InstanceNorm1d-150 [-1, 512, 16] 1,024 PReLU-151 [-1, 512, 16] 512 Upsample-152 [-1, 512, 32] 0 Conv1d-153 [-1, 512, 32] 786,432 InstanceNorm1d-154 [-1, 512, 32] 1,024 PReLU-155 [-1, 512, 32] 512 Conv1d-156 [-1, 256, 32] 131,072 Upsample-157 [-1, 512, 32] 0 Conv1d-158 [-1, 256, 32] 131,072 InstanceNorm1d-159 [-1, 256, 32] 512 _BlockBo...eckNd-160 [-1, 256, 32] 0 _BlockResStkNd-161 [-1, 256, 32] 0 InstanceNorm1d-162 [-1, 256, 32] 512 PReLU-163 [-1, 256, 32] 256 Conv1d-164 [-1, 256, 32] 65,536 InstanceNorm1d-165 [-1, 256, 32] 512 PReLU-166 [-1, 256, 32] 256 Conv1d-167 [-1, 256, 32] 196,608 InstanceNorm1d-168 [-1, 256, 32] 512 PReLU-169 [-1, 256, 32] 256 Conv1d-170 [-1, 256, 32] 65,536 _BlockBo...eckNd-171 [-1, 256, 32] 0 InstanceNorm1d-172 [-1, 256, 32] 512 PReLU-173 [-1, 256, 32] 256 Conv1d-174 [-1, 256, 32] 65,536 InstanceNorm1d-175 [-1, 256, 32] 512 PReLU-176 [-1, 256, 32] 256 Upsample-177 [-1, 256, 64] 0 Conv1d-178 [-1, 256, 64] 196,608 InstanceNorm1d-179 [-1, 256, 64] 512 PReLU-180 [-1, 256, 64] 256 Conv1d-181 [-1, 128, 64] 32,768 Upsample-182 [-1, 256, 64] 0 Conv1d-183 [-1, 128, 64] 32,768 InstanceNorm1d-184 [-1, 128, 64] 256 _BlockBo...eckNd-185 [-1, 128, 64] 0 _BlockResStkNd-186 [-1, 128, 64] 0 InstanceNorm1d-187 [-1, 128, 64] 256 PReLU-188 [-1, 128, 64] 128 Conv1d-189 [-1, 128, 64] 16,384 InstanceNorm1d-190 [-1, 128, 64] 256 PReLU-191 [-1, 128, 64] 128 Conv1d-192 [-1, 128, 64] 49,152 InstanceNorm1d-193 [-1, 128, 64] 256 PReLU-194 [-1, 128, 64] 128 Conv1d-195 [-1, 128, 64] 16,384 _BlockBo...eckNd-196 [-1, 128, 64] 0 InstanceNorm1d-197 [-1, 128, 64] 256 PReLU-198 [-1, 128, 64] 128 Conv1d-199 [-1, 128, 64] 16,384 InstanceNorm1d-200 [-1, 128, 64] 256 PReLU-201 [-1, 128, 64] 128 Upsample-202 [-1, 128, 128] 0 Conv1d-203 [-1, 128, 128] 49,152 InstanceNorm1d-204 [-1, 128, 128] 256 PReLU-205 [-1, 128, 128] 128 Conv1d-206 [-1, 64, 128] 8,192 Upsample-207 [-1, 128, 128] 0 Conv1d-208 [-1, 64, 128] 8,192 InstanceNorm1d-209 [-1, 64, 128] 128 _BlockBo...eckNd-210 [-1, 64, 128] 0 _BlockResStkNd-211 [-1, 64, 128] 0 InstanceNorm1d-212 [-1, 64, 128] 128 PReLU-213 [-1, 64, 128] 64 Conv1d-214 [-1, 64, 128] 4,096 InstanceNorm1d-215 [-1, 64, 128] 128 PReLU-216 [-1, 64, 128] 64 Conv1d-217 [-1, 64, 128] 12,288 InstanceNorm1d-218 [-1, 64, 128] 128 PReLU-219 [-1, 64, 128] 64 Conv1d-220 [-1, 64, 128] 4,096 _BlockBo...eckNd-221 [-1, 64, 128] 0 InstanceNorm1d-222 [-1, 64, 128] 128 PReLU-223 [-1, 64, 128] 64 Conv1d-224 [-1, 64, 128] 4,096 InstanceNorm1d-225 [-1, 64, 128] 128 PReLU-226 [-1, 64, 128] 64 Conv1d-227 [-1, 64, 128] 12,288 InstanceNorm1d-228 [-1, 64, 128] 128 PReLU-229 [-1, 64, 128] 64 Conv1d-230 [-1, 64, 128] 4,096 _BlockBo...eckNd-231 [-1, 64, 128] 0 _BlockResStkNd-232 [-1, 64, 128] 0 Conv1d-233 [-1, 1, 128] 321 AE1d-234 [-1, 1, 128] 0 ================================================================ Total params: 18,924,609 Trainable params: 18,924,609 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 13.75 Params size (MB): 72.19 Estimated Total Size (MB): 85.95 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>AE1d</span>"},{"location":"apis/modules/resnet/AE1d/#modulesresnetae1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . AE1d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D residual auto-encoder. The network structure is almost the same as mdnc.modules.resnet.UNet1d but all block-level skip connections are removed. Generally, using mdnc.modules.resnet.UNet1d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route.","title":"modules.resnet.AE1d"},{"location":"apis/modules/resnet/AE1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/AE1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/AE1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length.","title":" __call__"},{"location":"apis/modules/resnet/AE1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/AE1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/resnet/AE1d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . AE1d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 8] 3,145,728 InstanceNorm1d-118 [-1, 1024, 8] 2,048 PReLU-119 [-1, 1024, 8] 1,024 Conv1d-120 [-1, 1024, 8] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 8] 0 InstanceNorm1d-122 [-1, 1024, 8] 2,048 PReLU-123 [-1, 1024, 8] 1,024 Conv1d-124 [-1, 1024, 8] 1,048,576 InstanceNorm1d-125 [-1, 1024, 8] 2,048 PReLU-126 [-1, 1024, 8] 1,024 Upsample-127 [-1, 1024, 16] 0 Conv1d-128 [-1, 1024, 16] 3,145,728 InstanceNorm1d-129 [-1, 1024, 16] 2,048 PReLU-130 [-1, 1024, 16] 1,024 Conv1d-131 [-1, 512, 16] 524,288 Upsample-132 [-1, 1024, 16] 0 Conv1d-133 [-1, 512, 16] 524,288 InstanceNorm1d-134 [-1, 512, 16] 1,024 _BlockBo...eckNd-135 [-1, 512, 16] 0 _BlockResStkNd-136 [-1, 512, 16] 0 InstanceNorm1d-137 [-1, 512, 16] 1,024 PReLU-138 [-1, 512, 16] 512 Conv1d-139 [-1, 512, 16] 262,144 InstanceNorm1d-140 [-1, 512, 16] 1,024 PReLU-141 [-1, 512, 16] 512 Conv1d-142 [-1, 512, 16] 786,432 InstanceNorm1d-143 [-1, 512, 16] 1,024 PReLU-144 [-1, 512, 16] 512 Conv1d-145 [-1, 512, 16] 262,144 _BlockBo...eckNd-146 [-1, 512, 16] 0 InstanceNorm1d-147 [-1, 512, 16] 1,024 PReLU-148 [-1, 512, 16] 512 Conv1d-149 [-1, 512, 16] 262,144 InstanceNorm1d-150 [-1, 512, 16] 1,024 PReLU-151 [-1, 512, 16] 512 Upsample-152 [-1, 512, 32] 0 Conv1d-153 [-1, 512, 32] 786,432 InstanceNorm1d-154 [-1, 512, 32] 1,024 PReLU-155 [-1, 512, 32] 512 Conv1d-156 [-1, 256, 32] 131,072 Upsample-157 [-1, 512, 32] 0 Conv1d-158 [-1, 256, 32] 131,072 InstanceNorm1d-159 [-1, 256, 32] 512 _BlockBo...eckNd-160 [-1, 256, 32] 0 _BlockResStkNd-161 [-1, 256, 32] 0 InstanceNorm1d-162 [-1, 256, 32] 512 PReLU-163 [-1, 256, 32] 256 Conv1d-164 [-1, 256, 32] 65,536 InstanceNorm1d-165 [-1, 256, 32] 512 PReLU-166 [-1, 256, 32] 256 Conv1d-167 [-1, 256, 32] 196,608 InstanceNorm1d-168 [-1, 256, 32] 512 PReLU-169 [-1, 256, 32] 256 Conv1d-170 [-1, 256, 32] 65,536 _BlockBo...eckNd-171 [-1, 256, 32] 0 InstanceNorm1d-172 [-1, 256, 32] 512 PReLU-173 [-1, 256, 32] 256 Conv1d-174 [-1, 256, 32] 65,536 InstanceNorm1d-175 [-1, 256, 32] 512 PReLU-176 [-1, 256, 32] 256 Upsample-177 [-1, 256, 64] 0 Conv1d-178 [-1, 256, 64] 196,608 InstanceNorm1d-179 [-1, 256, 64] 512 PReLU-180 [-1, 256, 64] 256 Conv1d-181 [-1, 128, 64] 32,768 Upsample-182 [-1, 256, 64] 0 Conv1d-183 [-1, 128, 64] 32,768 InstanceNorm1d-184 [-1, 128, 64] 256 _BlockBo...eckNd-185 [-1, 128, 64] 0 _BlockResStkNd-186 [-1, 128, 64] 0 InstanceNorm1d-187 [-1, 128, 64] 256 PReLU-188 [-1, 128, 64] 128 Conv1d-189 [-1, 128, 64] 16,384 InstanceNorm1d-190 [-1, 128, 64] 256 PReLU-191 [-1, 128, 64] 128 Conv1d-192 [-1, 128, 64] 49,152 InstanceNorm1d-193 [-1, 128, 64] 256 PReLU-194 [-1, 128, 64] 128 Conv1d-195 [-1, 128, 64] 16,384 _BlockBo...eckNd-196 [-1, 128, 64] 0 InstanceNorm1d-197 [-1, 128, 64] 256 PReLU-198 [-1, 128, 64] 128 Conv1d-199 [-1, 128, 64] 16,384 InstanceNorm1d-200 [-1, 128, 64] 256 PReLU-201 [-1, 128, 64] 128 Upsample-202 [-1, 128, 128] 0 Conv1d-203 [-1, 128, 128] 49,152 InstanceNorm1d-204 [-1, 128, 128] 256 PReLU-205 [-1, 128, 128] 128 Conv1d-206 [-1, 64, 128] 8,192 Upsample-207 [-1, 128, 128] 0 Conv1d-208 [-1, 64, 128] 8,192 InstanceNorm1d-209 [-1, 64, 128] 128 _BlockBo...eckNd-210 [-1, 64, 128] 0 _BlockResStkNd-211 [-1, 64, 128] 0 InstanceNorm1d-212 [-1, 64, 128] 128 PReLU-213 [-1, 64, 128] 64 Conv1d-214 [-1, 64, 128] 4,096 InstanceNorm1d-215 [-1, 64, 128] 128 PReLU-216 [-1, 64, 128] 64 Conv1d-217 [-1, 64, 128] 12,288 InstanceNorm1d-218 [-1, 64, 128] 128 PReLU-219 [-1, 64, 128] 64 Conv1d-220 [-1, 64, 128] 4,096 _BlockBo...eckNd-221 [-1, 64, 128] 0 InstanceNorm1d-222 [-1, 64, 128] 128 PReLU-223 [-1, 64, 128] 64 Conv1d-224 [-1, 64, 128] 4,096 InstanceNorm1d-225 [-1, 64, 128] 128 PReLU-226 [-1, 64, 128] 64 Conv1d-227 [-1, 64, 128] 12,288 InstanceNorm1d-228 [-1, 64, 128] 128 PReLU-229 [-1, 64, 128] 64 Conv1d-230 [-1, 64, 128] 4,096 _BlockBo...eckNd-231 [-1, 64, 128] 0 _BlockResStkNd-232 [-1, 64, 128] 0 Conv1d-233 [-1, 1, 128] 321 AE1d-234 [-1, 1, 128] 0 ================================================================ Total params: 18,924,609 Trainable params: 18,924,609 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 13.75 Params size (MB): 72.19 Estimated Total Size (MB): 85.95 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/AE2d/","text":"modules.resnet.AE2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . AE2d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D residual auto-encoder. The network structure is almost the same as mdnc.modules.resnet.UNet2d but all block-level skip connections are removed. Generally, using mdnc.modules.resnet.UNet2d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . AE2d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 4, 4] 9,437,184 InstanceNorm2d-118 [-1, 1024, 4, 4] 2,048 PReLU-119 [-1, 1024, 4, 4] 1,024 Conv2d-120 [-1, 1024, 4, 4] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 4, 4] 0 InstanceNorm2d-122 [-1, 1024, 4, 4] 2,048 PReLU-123 [-1, 1024, 4, 4] 1,024 Conv2d-124 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-125 [-1, 1024, 4, 4] 2,048 PReLU-126 [-1, 1024, 4, 4] 1,024 Upsample-127 [-1, 1024, 8, 8] 0 Conv2d-128 [-1, 1024, 8, 8] 9,437,184 InstanceNorm2d-129 [-1, 1024, 8, 8] 2,048 PReLU-130 [-1, 1024, 8, 8] 1,024 Conv2d-131 [-1, 512, 8, 8] 524,288 Upsample-132 [-1, 1024, 8, 8] 0 Conv2d-133 [-1, 512, 8, 8] 524,288 InstanceNorm2d-134 [-1, 512, 8, 8] 1,024 _BlockBo...eckNd-135 [-1, 512, 8, 8] 0 _BlockResStkNd-136 [-1, 512, 8, 8] 0 InstanceNorm2d-137 [-1, 512, 8, 8] 1,024 PReLU-138 [-1, 512, 8, 8] 512 Conv2d-139 [-1, 512, 8, 8] 262,144 InstanceNorm2d-140 [-1, 512, 8, 8] 1,024 PReLU-141 [-1, 512, 8, 8] 512 Conv2d-142 [-1, 512, 8, 8] 2,359,296 InstanceNorm2d-143 [-1, 512, 8, 8] 1,024 PReLU-144 [-1, 512, 8, 8] 512 Conv2d-145 [-1, 512, 8, 8] 262,144 _BlockBo...eckNd-146 [-1, 512, 8, 8] 0 InstanceNorm2d-147 [-1, 512, 8, 8] 1,024 PReLU-148 [-1, 512, 8, 8] 512 Conv2d-149 [-1, 512, 8, 8] 262,144 InstanceNorm2d-150 [-1, 512, 8, 8] 1,024 PReLU-151 [-1, 512, 8, 8] 512 Upsample-152 [-1, 512, 16, 16] 0 Conv2d-153 [-1, 512, 16, 16] 2,359,296 InstanceNorm2d-154 [-1, 512, 16, 16] 1,024 PReLU-155 [-1, 512, 16, 16] 512 Conv2d-156 [-1, 256, 16, 16] 131,072 Upsample-157 [-1, 512, 16, 16] 0 Conv2d-158 [-1, 256, 16, 16] 131,072 InstanceNorm2d-159 [-1, 256, 16, 16] 512 _BlockBo...eckNd-160 [-1, 256, 16, 16] 0 _BlockResStkNd-161 [-1, 256, 16, 16] 0 InstanceNorm2d-162 [-1, 256, 16, 16] 512 PReLU-163 [-1, 256, 16, 16] 256 Conv2d-164 [-1, 256, 16, 16] 65,536 InstanceNorm2d-165 [-1, 256, 16, 16] 512 PReLU-166 [-1, 256, 16, 16] 256 Conv2d-167 [-1, 256, 16, 16] 589,824 InstanceNorm2d-168 [-1, 256, 16, 16] 512 PReLU-169 [-1, 256, 16, 16] 256 Conv2d-170 [-1, 256, 16, 16] 65,536 _BlockBo...eckNd-171 [-1, 256, 16, 16] 0 InstanceNorm2d-172 [-1, 256, 16, 16] 512 PReLU-173 [-1, 256, 16, 16] 256 Conv2d-174 [-1, 256, 16, 16] 65,536 InstanceNorm2d-175 [-1, 256, 16, 16] 512 PReLU-176 [-1, 256, 16, 16] 256 Upsample-177 [-1, 256, 32, 32] 0 Conv2d-178 [-1, 256, 32, 32] 589,824 InstanceNorm2d-179 [-1, 256, 32, 32] 512 PReLU-180 [-1, 256, 32, 32] 256 Conv2d-181 [-1, 128, 32, 32] 32,768 Upsample-182 [-1, 256, 32, 32] 0 Conv2d-183 [-1, 128, 32, 32] 32,768 InstanceNorm2d-184 [-1, 128, 32, 32] 256 _BlockBo...eckNd-185 [-1, 128, 32, 32] 0 _BlockResStkNd-186 [-1, 128, 32, 32] 0 InstanceNorm2d-187 [-1, 128, 32, 32] 256 PReLU-188 [-1, 128, 32, 32] 128 Conv2d-189 [-1, 128, 32, 32] 16,384 InstanceNorm2d-190 [-1, 128, 32, 32] 256 PReLU-191 [-1, 128, 32, 32] 128 Conv2d-192 [-1, 128, 32, 32] 147,456 InstanceNorm2d-193 [-1, 128, 32, 32] 256 PReLU-194 [-1, 128, 32, 32] 128 Conv2d-195 [-1, 128, 32, 32] 16,384 _BlockBo...eckNd-196 [-1, 128, 32, 32] 0 InstanceNorm2d-197 [-1, 128, 32, 32] 256 PReLU-198 [-1, 128, 32, 32] 128 Conv2d-199 [-1, 128, 32, 32] 16,384 InstanceNorm2d-200 [-1, 128, 32, 32] 256 PReLU-201 [-1, 128, 32, 32] 128 Upsample-202 [-1, 128, 64, 64] 0 Conv2d-203 [-1, 128, 64, 64] 147,456 InstanceNorm2d-204 [-1, 128, 64, 64] 256 PReLU-205 [-1, 128, 64, 64] 128 Conv2d-206 [-1, 64, 64, 64] 8,192 Upsample-207 [-1, 128, 64, 64] 0 Conv2d-208 [-1, 64, 64, 64] 8,192 InstanceNorm2d-209 [-1, 64, 64, 64] 128 _BlockBo...eckNd-210 [-1, 64, 64, 64] 0 _BlockResStkNd-211 [-1, 64, 64, 64] 0 InstanceNorm2d-212 [-1, 64, 64, 63] 128 PReLU-213 [-1, 64, 64, 63] 64 Conv2d-214 [-1, 64, 64, 63] 4,096 InstanceNorm2d-215 [-1, 64, 64, 63] 128 PReLU-216 [-1, 64, 64, 63] 64 Conv2d-217 [-1, 64, 64, 63] 36,864 InstanceNorm2d-218 [-1, 64, 64, 63] 128 PReLU-219 [-1, 64, 64, 63] 64 Conv2d-220 [-1, 64, 64, 63] 4,096 _BlockBo...eckNd-221 [-1, 64, 64, 63] 0 InstanceNorm2d-222 [-1, 64, 64, 63] 128 PReLU-223 [-1, 64, 64, 63] 64 Conv2d-224 [-1, 64, 64, 63] 4,096 InstanceNorm2d-225 [-1, 64, 64, 63] 128 PReLU-226 [-1, 64, 64, 63] 64 Conv2d-227 [-1, 64, 64, 63] 36,864 InstanceNorm2d-228 [-1, 64, 64, 63] 128 PReLU-229 [-1, 64, 64, 63] 64 Conv2d-230 [-1, 64, 64, 63] 4,096 _BlockBo...eckNd-231 [-1, 64, 64, 63] 0 _BlockResStkNd-232 [-1, 64, 64, 63] 0 Conv2d-233 [-1, 1, 64, 63] 1,601 AE2d-234 [-1, 1, 64, 63] 0 ================================================================ Total params: 39,893,057 Trainable params: 39,893,057 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 188.53 Params size (MB): 152.18 Estimated Total Size (MB): 340.76 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>AE2d</span>"},{"location":"apis/modules/resnet/AE2d/#modulesresnetae2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . AE2d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D residual auto-encoder. The network structure is almost the same as mdnc.modules.resnet.UNet2d but all block-level skip connections are removed. Generally, using mdnc.modules.resnet.UNet2d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route.","title":"modules.resnet.AE2d"},{"location":"apis/modules/resnet/AE2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/AE2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/AE2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size.","title":" __call__"},{"location":"apis/modules/resnet/AE2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/AE2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/resnet/AE2d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . AE2d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 4, 4] 9,437,184 InstanceNorm2d-118 [-1, 1024, 4, 4] 2,048 PReLU-119 [-1, 1024, 4, 4] 1,024 Conv2d-120 [-1, 1024, 4, 4] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 4, 4] 0 InstanceNorm2d-122 [-1, 1024, 4, 4] 2,048 PReLU-123 [-1, 1024, 4, 4] 1,024 Conv2d-124 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-125 [-1, 1024, 4, 4] 2,048 PReLU-126 [-1, 1024, 4, 4] 1,024 Upsample-127 [-1, 1024, 8, 8] 0 Conv2d-128 [-1, 1024, 8, 8] 9,437,184 InstanceNorm2d-129 [-1, 1024, 8, 8] 2,048 PReLU-130 [-1, 1024, 8, 8] 1,024 Conv2d-131 [-1, 512, 8, 8] 524,288 Upsample-132 [-1, 1024, 8, 8] 0 Conv2d-133 [-1, 512, 8, 8] 524,288 InstanceNorm2d-134 [-1, 512, 8, 8] 1,024 _BlockBo...eckNd-135 [-1, 512, 8, 8] 0 _BlockResStkNd-136 [-1, 512, 8, 8] 0 InstanceNorm2d-137 [-1, 512, 8, 8] 1,024 PReLU-138 [-1, 512, 8, 8] 512 Conv2d-139 [-1, 512, 8, 8] 262,144 InstanceNorm2d-140 [-1, 512, 8, 8] 1,024 PReLU-141 [-1, 512, 8, 8] 512 Conv2d-142 [-1, 512, 8, 8] 2,359,296 InstanceNorm2d-143 [-1, 512, 8, 8] 1,024 PReLU-144 [-1, 512, 8, 8] 512 Conv2d-145 [-1, 512, 8, 8] 262,144 _BlockBo...eckNd-146 [-1, 512, 8, 8] 0 InstanceNorm2d-147 [-1, 512, 8, 8] 1,024 PReLU-148 [-1, 512, 8, 8] 512 Conv2d-149 [-1, 512, 8, 8] 262,144 InstanceNorm2d-150 [-1, 512, 8, 8] 1,024 PReLU-151 [-1, 512, 8, 8] 512 Upsample-152 [-1, 512, 16, 16] 0 Conv2d-153 [-1, 512, 16, 16] 2,359,296 InstanceNorm2d-154 [-1, 512, 16, 16] 1,024 PReLU-155 [-1, 512, 16, 16] 512 Conv2d-156 [-1, 256, 16, 16] 131,072 Upsample-157 [-1, 512, 16, 16] 0 Conv2d-158 [-1, 256, 16, 16] 131,072 InstanceNorm2d-159 [-1, 256, 16, 16] 512 _BlockBo...eckNd-160 [-1, 256, 16, 16] 0 _BlockResStkNd-161 [-1, 256, 16, 16] 0 InstanceNorm2d-162 [-1, 256, 16, 16] 512 PReLU-163 [-1, 256, 16, 16] 256 Conv2d-164 [-1, 256, 16, 16] 65,536 InstanceNorm2d-165 [-1, 256, 16, 16] 512 PReLU-166 [-1, 256, 16, 16] 256 Conv2d-167 [-1, 256, 16, 16] 589,824 InstanceNorm2d-168 [-1, 256, 16, 16] 512 PReLU-169 [-1, 256, 16, 16] 256 Conv2d-170 [-1, 256, 16, 16] 65,536 _BlockBo...eckNd-171 [-1, 256, 16, 16] 0 InstanceNorm2d-172 [-1, 256, 16, 16] 512 PReLU-173 [-1, 256, 16, 16] 256 Conv2d-174 [-1, 256, 16, 16] 65,536 InstanceNorm2d-175 [-1, 256, 16, 16] 512 PReLU-176 [-1, 256, 16, 16] 256 Upsample-177 [-1, 256, 32, 32] 0 Conv2d-178 [-1, 256, 32, 32] 589,824 InstanceNorm2d-179 [-1, 256, 32, 32] 512 PReLU-180 [-1, 256, 32, 32] 256 Conv2d-181 [-1, 128, 32, 32] 32,768 Upsample-182 [-1, 256, 32, 32] 0 Conv2d-183 [-1, 128, 32, 32] 32,768 InstanceNorm2d-184 [-1, 128, 32, 32] 256 _BlockBo...eckNd-185 [-1, 128, 32, 32] 0 _BlockResStkNd-186 [-1, 128, 32, 32] 0 InstanceNorm2d-187 [-1, 128, 32, 32] 256 PReLU-188 [-1, 128, 32, 32] 128 Conv2d-189 [-1, 128, 32, 32] 16,384 InstanceNorm2d-190 [-1, 128, 32, 32] 256 PReLU-191 [-1, 128, 32, 32] 128 Conv2d-192 [-1, 128, 32, 32] 147,456 InstanceNorm2d-193 [-1, 128, 32, 32] 256 PReLU-194 [-1, 128, 32, 32] 128 Conv2d-195 [-1, 128, 32, 32] 16,384 _BlockBo...eckNd-196 [-1, 128, 32, 32] 0 InstanceNorm2d-197 [-1, 128, 32, 32] 256 PReLU-198 [-1, 128, 32, 32] 128 Conv2d-199 [-1, 128, 32, 32] 16,384 InstanceNorm2d-200 [-1, 128, 32, 32] 256 PReLU-201 [-1, 128, 32, 32] 128 Upsample-202 [-1, 128, 64, 64] 0 Conv2d-203 [-1, 128, 64, 64] 147,456 InstanceNorm2d-204 [-1, 128, 64, 64] 256 PReLU-205 [-1, 128, 64, 64] 128 Conv2d-206 [-1, 64, 64, 64] 8,192 Upsample-207 [-1, 128, 64, 64] 0 Conv2d-208 [-1, 64, 64, 64] 8,192 InstanceNorm2d-209 [-1, 64, 64, 64] 128 _BlockBo...eckNd-210 [-1, 64, 64, 64] 0 _BlockResStkNd-211 [-1, 64, 64, 64] 0 InstanceNorm2d-212 [-1, 64, 64, 63] 128 PReLU-213 [-1, 64, 64, 63] 64 Conv2d-214 [-1, 64, 64, 63] 4,096 InstanceNorm2d-215 [-1, 64, 64, 63] 128 PReLU-216 [-1, 64, 64, 63] 64 Conv2d-217 [-1, 64, 64, 63] 36,864 InstanceNorm2d-218 [-1, 64, 64, 63] 128 PReLU-219 [-1, 64, 64, 63] 64 Conv2d-220 [-1, 64, 64, 63] 4,096 _BlockBo...eckNd-221 [-1, 64, 64, 63] 0 InstanceNorm2d-222 [-1, 64, 64, 63] 128 PReLU-223 [-1, 64, 64, 63] 64 Conv2d-224 [-1, 64, 64, 63] 4,096 InstanceNorm2d-225 [-1, 64, 64, 63] 128 PReLU-226 [-1, 64, 64, 63] 64 Conv2d-227 [-1, 64, 64, 63] 36,864 InstanceNorm2d-228 [-1, 64, 64, 63] 128 PReLU-229 [-1, 64, 64, 63] 64 Conv2d-230 [-1, 64, 64, 63] 4,096 _BlockBo...eckNd-231 [-1, 64, 64, 63] 0 _BlockResStkNd-232 [-1, 64, 64, 63] 0 Conv2d-233 [-1, 1, 64, 63] 1,601 AE2d-234 [-1, 1, 64, 63] 0 ================================================================ Total params: 39,893,057 Trainable params: 39,893,057 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 188.53 Params size (MB): 152.18 Estimated Total Size (MB): 340.76 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/AE3d/","text":"modules.resnet.AE3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . AE3d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D residual auto-encoder. The network structure is almost the same as mdnc.modules.resnet.UNet3d but all block-level skip connections are removed. Generally, using mdnc.modules.resnet.UNet3d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . AE3d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 2, 2, 2] 28,311,552 InstanceNorm3d-118 [-1, 1024, 2, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2, 2] 1,024 Conv3d-120 [-1, 1024, 2, 2, 2] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-122 [-1, 1024, 2, 2, 2] 2,048 PReLU-123 [-1, 1024, 2, 2, 2] 1,024 Conv3d-124 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-125 [-1, 1024, 2, 2, 2] 2,048 PReLU-126 [-1, 1024, 2, 2, 2] 1,024 Upsample-127 [-1, 1024, 4, 4, 4] 0 Conv3d-128 [-1, 1024, 4, 4, 4] 28,311,552 InstanceNorm3d-129 [-1, 1024, 4, 4, 4] 2,048 PReLU-130 [-1, 1024, 4, 4, 4] 1,024 Conv3d-131 [-1, 512, 4, 4, 4] 524,288 Upsample-132 [-1, 1024, 4, 4, 4] 0 Conv3d-133 [-1, 512, 4, 4, 4] 524,288 InstanceNorm3d-134 [-1, 512, 4, 4, 4] 1,024 _BlockBo...eckNd-135 [-1, 512, 4, 4, 4] 0 _BlockResStkNd-136 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-137 [-1, 512, 4, 4, 4] 1,024 PReLU-138 [-1, 512, 4, 4, 4] 512 Conv3d-139 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-140 [-1, 512, 4, 4, 4] 1,024 PReLU-141 [-1, 512, 4, 4, 4] 512 Conv3d-142 [-1, 512, 4, 4, 4] 7,077,888 InstanceNorm3d-143 [-1, 512, 4, 4, 4] 1,024 PReLU-144 [-1, 512, 4, 4, 4] 512 Conv3d-145 [-1, 512, 4, 4, 4] 262,144 _BlockBo...eckNd-146 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-147 [-1, 512, 4, 4, 4] 1,024 PReLU-148 [-1, 512, 4, 4, 4] 512 Conv3d-149 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-150 [-1, 512, 4, 4, 4] 1,024 PReLU-151 [-1, 512, 4, 4, 4] 512 Upsample-152 [-1, 512, 8, 8, 8] 0 Conv3d-153 [-1, 512, 8, 8, 8] 7,077,888 InstanceNorm3d-154 [-1, 512, 8, 8, 8] 1,024 PReLU-155 [-1, 512, 8, 8, 8] 512 Conv3d-156 [-1, 256, 8, 8, 8] 131,072 Upsample-157 [-1, 512, 8, 8, 8] 0 Conv3d-158 [-1, 256, 8, 8, 8] 131,072 InstanceNorm3d-159 [-1, 256, 8, 8, 8] 512 _BlockBo...eckNd-160 [-1, 256, 8, 8, 8] 0 _BlockResStkNd-161 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-162 [-1, 256, 8, 8, 8] 512 PReLU-163 [-1, 256, 8, 8, 8] 256 Conv3d-164 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-165 [-1, 256, 8, 8, 8] 512 PReLU-166 [-1, 256, 8, 8, 8] 256 Conv3d-167 [-1, 256, 8, 8, 8] 1,769,472 InstanceNorm3d-168 [-1, 256, 8, 8, 8] 512 PReLU-169 [-1, 256, 8, 8, 8] 256 Conv3d-170 [-1, 256, 8, 8, 8] 65,536 _BlockBo...eckNd-171 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-172 [-1, 256, 8, 8, 8] 512 PReLU-173 [-1, 256, 8, 8, 8] 256 Conv3d-174 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-175 [-1, 256, 8, 8, 8] 512 PReLU-176 [-1, 256, 8, 8, 8] 256 Upsample-177 [-1, 256, 16, 16, 16] 0 Conv3d-178 [-1, 256, 16, 16, 16] 1,769,472 InstanceNorm3d-179 [-1, 256, 16, 16, 16] 512 PReLU-180 [-1, 256, 16, 16, 16] 256 Conv3d-181 [-1, 128, 16, 16, 16] 32,768 Upsample-182 [-1, 256, 16, 16, 16] 0 Conv3d-183 [-1, 128, 16, 16, 16] 32,768 InstanceNorm3d-184 [-1, 128, 16, 16, 16] 256 _BlockBo...eckNd-185 [-1, 128, 16, 16, 16] 0 _BlockResStkNd-186 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-187 [-1, 128, 16, 16, 15] 256 PReLU-188 [-1, 128, 16, 16, 15] 128 Conv3d-189 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-190 [-1, 128, 16, 16, 15] 256 PReLU-191 [-1, 128, 16, 16, 15] 128 Conv3d-192 [-1, 128, 16, 16, 15] 442,368 InstanceNorm3d-193 [-1, 128, 16, 16, 15] 256 PReLU-194 [-1, 128, 16, 16, 15] 128 Conv3d-195 [-1, 128, 16, 16, 15] 16,384 _BlockBo...eckNd-196 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-197 [-1, 128, 16, 16, 15] 256 PReLU-198 [-1, 128, 16, 16, 15] 128 Conv3d-199 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-200 [-1, 128, 16, 16, 15] 256 PReLU-201 [-1, 128, 16, 16, 15] 128 Upsample-202 [-1, 128, 32, 32, 30] 0 Conv3d-203 [-1, 128, 32, 32, 30] 442,368 InstanceNorm3d-204 [-1, 128, 32, 32, 30] 256 PReLU-205 [-1, 128, 32, 32, 30] 128 Conv3d-206 [-1, 64, 32, 32, 30] 8,192 Upsample-207 [-1, 128, 32, 32, 30] 0 Conv3d-208 [-1, 64, 32, 32, 30] 8,192 InstanceNorm3d-209 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-210 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-211 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-212 [-1, 64, 31, 32, 30] 128 PReLU-213 [-1, 64, 31, 32, 30] 64 Conv3d-214 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-215 [-1, 64, 31, 32, 30] 128 PReLU-216 [-1, 64, 31, 32, 30] 64 Conv3d-217 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-218 [-1, 64, 31, 32, 30] 128 PReLU-219 [-1, 64, 31, 32, 30] 64 Conv3d-220 [-1, 64, 31, 32, 30] 4,096 _BlockBo...eckNd-221 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-222 [-1, 64, 31, 32, 30] 128 PReLU-223 [-1, 64, 31, 32, 30] 64 Conv3d-224 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-225 [-1, 64, 31, 32, 30] 128 PReLU-226 [-1, 64, 31, 32, 30] 64 Conv3d-227 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-228 [-1, 64, 31, 32, 30] 128 PReLU-229 [-1, 64, 31, 32, 30] 64 Conv3d-230 [-1, 64, 31, 32, 30] 4,096 _BlockBo...eckNd-231 [-1, 64, 31, 32, 30] 0 _BlockResStkNd-232 [-1, 64, 31, 32, 30] 0 Conv3d-233 [-1, 1, 31, 32, 30] 8,001 AE3d-234 [-1, 1, 31, 32, 30] 0 ================================================================ Total params: 102,808,641 Trainable params: 102,808,641 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 1003.55 Params size (MB): 392.18 Estimated Total Size (MB): 1396.07 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>AE3d</span>"},{"location":"apis/modules/resnet/AE3d/#modulesresnetae3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . AE3d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D residual auto-encoder. The network structure is almost the same as mdnc.modules.resnet.UNet3d but all block-level skip connections are removed. Generally, using mdnc.modules.resnet.UNet3d should be a better choice. The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route.","title":"modules.resnet.AE3d"},{"location":"apis/modules/resnet/AE3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/AE3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/AE3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size.","title":" __call__"},{"location":"apis/modules/resnet/AE3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/AE3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/resnet/AE3d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . AE3d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 25. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 2, 2, 2] 28,311,552 InstanceNorm3d-118 [-1, 1024, 2, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2, 2] 1,024 Conv3d-120 [-1, 1024, 2, 2, 2] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-122 [-1, 1024, 2, 2, 2] 2,048 PReLU-123 [-1, 1024, 2, 2, 2] 1,024 Conv3d-124 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-125 [-1, 1024, 2, 2, 2] 2,048 PReLU-126 [-1, 1024, 2, 2, 2] 1,024 Upsample-127 [-1, 1024, 4, 4, 4] 0 Conv3d-128 [-1, 1024, 4, 4, 4] 28,311,552 InstanceNorm3d-129 [-1, 1024, 4, 4, 4] 2,048 PReLU-130 [-1, 1024, 4, 4, 4] 1,024 Conv3d-131 [-1, 512, 4, 4, 4] 524,288 Upsample-132 [-1, 1024, 4, 4, 4] 0 Conv3d-133 [-1, 512, 4, 4, 4] 524,288 InstanceNorm3d-134 [-1, 512, 4, 4, 4] 1,024 _BlockBo...eckNd-135 [-1, 512, 4, 4, 4] 0 _BlockResStkNd-136 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-137 [-1, 512, 4, 4, 4] 1,024 PReLU-138 [-1, 512, 4, 4, 4] 512 Conv3d-139 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-140 [-1, 512, 4, 4, 4] 1,024 PReLU-141 [-1, 512, 4, 4, 4] 512 Conv3d-142 [-1, 512, 4, 4, 4] 7,077,888 InstanceNorm3d-143 [-1, 512, 4, 4, 4] 1,024 PReLU-144 [-1, 512, 4, 4, 4] 512 Conv3d-145 [-1, 512, 4, 4, 4] 262,144 _BlockBo...eckNd-146 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-147 [-1, 512, 4, 4, 4] 1,024 PReLU-148 [-1, 512, 4, 4, 4] 512 Conv3d-149 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-150 [-1, 512, 4, 4, 4] 1,024 PReLU-151 [-1, 512, 4, 4, 4] 512 Upsample-152 [-1, 512, 8, 8, 8] 0 Conv3d-153 [-1, 512, 8, 8, 8] 7,077,888 InstanceNorm3d-154 [-1, 512, 8, 8, 8] 1,024 PReLU-155 [-1, 512, 8, 8, 8] 512 Conv3d-156 [-1, 256, 8, 8, 8] 131,072 Upsample-157 [-1, 512, 8, 8, 8] 0 Conv3d-158 [-1, 256, 8, 8, 8] 131,072 InstanceNorm3d-159 [-1, 256, 8, 8, 8] 512 _BlockBo...eckNd-160 [-1, 256, 8, 8, 8] 0 _BlockResStkNd-161 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-162 [-1, 256, 8, 8, 8] 512 PReLU-163 [-1, 256, 8, 8, 8] 256 Conv3d-164 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-165 [-1, 256, 8, 8, 8] 512 PReLU-166 [-1, 256, 8, 8, 8] 256 Conv3d-167 [-1, 256, 8, 8, 8] 1,769,472 InstanceNorm3d-168 [-1, 256, 8, 8, 8] 512 PReLU-169 [-1, 256, 8, 8, 8] 256 Conv3d-170 [-1, 256, 8, 8, 8] 65,536 _BlockBo...eckNd-171 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-172 [-1, 256, 8, 8, 8] 512 PReLU-173 [-1, 256, 8, 8, 8] 256 Conv3d-174 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-175 [-1, 256, 8, 8, 8] 512 PReLU-176 [-1, 256, 8, 8, 8] 256 Upsample-177 [-1, 256, 16, 16, 16] 0 Conv3d-178 [-1, 256, 16, 16, 16] 1,769,472 InstanceNorm3d-179 [-1, 256, 16, 16, 16] 512 PReLU-180 [-1, 256, 16, 16, 16] 256 Conv3d-181 [-1, 128, 16, 16, 16] 32,768 Upsample-182 [-1, 256, 16, 16, 16] 0 Conv3d-183 [-1, 128, 16, 16, 16] 32,768 InstanceNorm3d-184 [-1, 128, 16, 16, 16] 256 _BlockBo...eckNd-185 [-1, 128, 16, 16, 16] 0 _BlockResStkNd-186 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-187 [-1, 128, 16, 16, 15] 256 PReLU-188 [-1, 128, 16, 16, 15] 128 Conv3d-189 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-190 [-1, 128, 16, 16, 15] 256 PReLU-191 [-1, 128, 16, 16, 15] 128 Conv3d-192 [-1, 128, 16, 16, 15] 442,368 InstanceNorm3d-193 [-1, 128, 16, 16, 15] 256 PReLU-194 [-1, 128, 16, 16, 15] 128 Conv3d-195 [-1, 128, 16, 16, 15] 16,384 _BlockBo...eckNd-196 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-197 [-1, 128, 16, 16, 15] 256 PReLU-198 [-1, 128, 16, 16, 15] 128 Conv3d-199 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-200 [-1, 128, 16, 16, 15] 256 PReLU-201 [-1, 128, 16, 16, 15] 128 Upsample-202 [-1, 128, 32, 32, 30] 0 Conv3d-203 [-1, 128, 32, 32, 30] 442,368 InstanceNorm3d-204 [-1, 128, 32, 32, 30] 256 PReLU-205 [-1, 128, 32, 32, 30] 128 Conv3d-206 [-1, 64, 32, 32, 30] 8,192 Upsample-207 [-1, 128, 32, 32, 30] 0 Conv3d-208 [-1, 64, 32, 32, 30] 8,192 InstanceNorm3d-209 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-210 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-211 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-212 [-1, 64, 31, 32, 30] 128 PReLU-213 [-1, 64, 31, 32, 30] 64 Conv3d-214 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-215 [-1, 64, 31, 32, 30] 128 PReLU-216 [-1, 64, 31, 32, 30] 64 Conv3d-217 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-218 [-1, 64, 31, 32, 30] 128 PReLU-219 [-1, 64, 31, 32, 30] 64 Conv3d-220 [-1, 64, 31, 32, 30] 4,096 _BlockBo...eckNd-221 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-222 [-1, 64, 31, 32, 30] 128 PReLU-223 [-1, 64, 31, 32, 30] 64 Conv3d-224 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-225 [-1, 64, 31, 32, 30] 128 PReLU-226 [-1, 64, 31, 32, 30] 64 Conv3d-227 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-228 [-1, 64, 31, 32, 30] 128 PReLU-229 [-1, 64, 31, 32, 30] 64 Conv3d-230 [-1, 64, 31, 32, 30] 4,096 _BlockBo...eckNd-231 [-1, 64, 31, 32, 30] 0 _BlockResStkNd-232 [-1, 64, 31, 32, 30] 0 Conv3d-233 [-1, 1, 31, 32, 30] 8,001 AE3d-234 [-1, 1, 31, 32, 30] 0 ================================================================ Total params: 102,808,641 Trainable params: 102,808,641 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 1003.55 Params size (MB): 392.18 Estimated Total Size (MB): 1396.07 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/BlockBottleneck1d/","text":"modules.resnet.BlockBottleneck1d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockBottleneck1d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the bottleneck (second-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; where the projection convolutional layer is implemented by a convolution with kernel_size = 1 . Compared to the plain block, this implementation requires fewer paramters, but provides a deeper stack. If the channel of the output changes, or the size of the output changes, a projection layer is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern1d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int The kernel size of this layer. stride int The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int The length of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the output data length. Examples \u00b6 In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck1d ( 16 , 32 , kernel_size = 3 , stride = 2 , padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 16, 255] 32 PReLU-2 [-1, 16, 255] 16 Conv1d-3 [-1, 16, 255] 256 InstanceNorm1d-4 [-1, 16, 255] 32 PReLU-5 [-1, 16, 255] 16 Conv1d-6 [-1, 16, 128] 768 InstanceNorm1d-7 [-1, 16, 128] 32 PReLU-8 [-1, 16, 128] 16 Conv1d-9 [-1, 32, 128] 512 Conv1d-10 [-1, 32, 128] 512 InstanceNorm1d-11 [-1, 32, 128] 64 BlockBottleneck1d-12 [-1, 32, 128] 0 ================================================================ Total params: 2,256 Trainable params: 2,256 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.33 Params size (MB): 0.01 Estimated Total Size (MB): 0.35 ---------------------------------------------------------------- Note that the output length would be 128 in this example, because the same padding is used for the input. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck1d ( 32 , 16 , kernel_size = 3 , output_size = 255 , padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 32, 128] 64 PReLU-2 [-1, 32, 128] 32 Conv1d-3 [-1, 32, 128] 1,024 InstanceNorm1d-4 [-1, 32, 128] 64 PReLU-5 [-1, 32, 128] 32 Upsample-6 [-1, 32, 255] 0 Conv1d-7 [-1, 32, 255] 3,072 InstanceNorm1d-8 [-1, 32, 255] 64 PReLU-9 [-1, 32, 255] 32 Conv1d-10 [-1, 16, 255] 512 Upsample-11 [-1, 32, 255] 0 Conv1d-12 [-1, 16, 255] 512 InstanceNorm1d-13 [-1, 16, 255] 32 BlockBottleneck1d-14 [-1, 16, 255] 0 ================================================================ Total params: 5,440 Trainable params: 5,440 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.59 Params size (MB): 0.02 Estimated Total Size (MB): 0.63 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>BlockBottleneck1d</span>"},{"location":"apis/modules/resnet/BlockBottleneck1d/#modulesresnetblockbottleneck1d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockBottleneck1d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the bottleneck (second-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; where the projection convolutional layer is implemented by a convolution with kernel_size = 1 . Compared to the plain block, this implementation requires fewer paramters, but provides a deeper stack. If the channel of the output changes, or the size of the output changes, a projection layer is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern1d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it.","title":"modules.resnet.BlockBottleneck1d"},{"location":"apis/modules/resnet/BlockBottleneck1d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int The kernel size of this layer. stride int The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int The length of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/resnet/BlockBottleneck1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/BlockBottleneck1d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the output data length.","title":" __call__"},{"location":"apis/modules/resnet/BlockBottleneck1d/#examples","text":"In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck1d ( 16 , 32 , kernel_size = 3 , stride = 2 , padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 16, 255] 32 PReLU-2 [-1, 16, 255] 16 Conv1d-3 [-1, 16, 255] 256 InstanceNorm1d-4 [-1, 16, 255] 32 PReLU-5 [-1, 16, 255] 16 Conv1d-6 [-1, 16, 128] 768 InstanceNorm1d-7 [-1, 16, 128] 32 PReLU-8 [-1, 16, 128] 16 Conv1d-9 [-1, 32, 128] 512 Conv1d-10 [-1, 32, 128] 512 InstanceNorm1d-11 [-1, 32, 128] 64 BlockBottleneck1d-12 [-1, 32, 128] 0 ================================================================ Total params: 2,256 Trainable params: 2,256 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.33 Params size (MB): 0.01 Estimated Total Size (MB): 0.35 ---------------------------------------------------------------- Note that the output length would be 128 in this example, because the same padding is used for the input. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck1d ( 32 , 16 , kernel_size = 3 , output_size = 255 , padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 32, 128] 64 PReLU-2 [-1, 32, 128] 32 Conv1d-3 [-1, 32, 128] 1,024 InstanceNorm1d-4 [-1, 32, 128] 64 PReLU-5 [-1, 32, 128] 32 Upsample-6 [-1, 32, 255] 0 Conv1d-7 [-1, 32, 255] 3,072 InstanceNorm1d-8 [-1, 32, 255] 64 PReLU-9 [-1, 32, 255] 32 Conv1d-10 [-1, 16, 255] 512 Upsample-11 [-1, 32, 255] 0 Conv1d-12 [-1, 16, 255] 512 InstanceNorm1d-13 [-1, 16, 255] 32 BlockBottleneck1d-14 [-1, 16, 255] 0 ================================================================ Total params: 5,440 Trainable params: 5,440 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.59 Params size (MB): 0.02 Estimated Total Size (MB): 0.63 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/BlockBottleneck2d/","text":"modules.resnet.BlockBottleneck2d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockBottleneck2d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the bottleneck (second-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; where the projection convolutional layer is implemented by a convolution with kernel_size = 1 . Compared to the plain block, this implementation requires fewer paramters, but provides a deeper stack. If the channel of the output changes, or the size of the output changes, a projection layer is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern2d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int ) The kernel size of this layer. stride int or ( int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the output data size. Examples \u00b6 In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck2d ( 16 , 32 , kernel_size = 3 , stride = ( 1 , 2 ), padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 4 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 16, 4, 255] 32 PReLU-2 [-1, 16, 4, 255] 16 Conv2d-3 [-1, 16, 4, 255] 256 InstanceNorm2d-4 [-1, 16, 4, 255] 32 PReLU-5 [-1, 16, 4, 255] 16 Conv2d-6 [-1, 16, 4, 128] 2,304 InstanceNorm2d-7 [-1, 16, 4, 128] 32 PReLU-8 [-1, 16, 4, 128] 16 Conv2d-9 [-1, 32, 4, 128] 512 Conv2d-10 [-1, 32, 4, 128] 512 InstanceNorm2d-11 [-1, 32, 4, 128] 64 BlockBottleneck2d-12 [-1, 32, 4, 128] 0 ================================================================ Total params: 3,792 Trainable params: 3,792 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 1.31 Params size (MB): 0.01 Estimated Total Size (MB): 1.39 ---------------------------------------------------------------- Note that the output size would be (4, 128) in this example, because the same padding is used for both two axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck2d ( 32 , 16 , kernel_size = 3 , output_size = ( 4 , 255 ), padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 4 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 32, 4, 128] 64 PReLU-2 [-1, 32, 4, 128] 32 Conv2d-3 [-1, 32, 4, 128] 1,024 InstanceNorm2d-4 [-1, 32, 4, 128] 64 PReLU-5 [-1, 32, 4, 128] 32 Upsample-6 [-1, 32, 4, 255] 0 Conv2d-7 [-1, 32, 4, 255] 9,216 InstanceNorm2d-8 [-1, 32, 4, 255] 64 PReLU-9 [-1, 32, 4, 255] 32 Conv2d-10 [-1, 16, 4, 255] 512 Upsample-11 [-1, 32, 4, 255] 0 Conv2d-12 [-1, 16, 4, 255] 512 InstanceNorm2d-13 [-1, 16, 4, 255] 32 BlockBottleneck2d-14 [-1, 16, 4, 255] 0 ================================================================ Total params: 11,584 Trainable params: 11,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 2.37 Params size (MB): 0.04 Estimated Total Size (MB): 2.47 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>BlockBottleneck2d</span>"},{"location":"apis/modules/resnet/BlockBottleneck2d/#modulesresnetblockbottleneck2d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockBottleneck2d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the bottleneck (second-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; where the projection convolutional layer is implemented by a convolution with kernel_size = 1 . Compared to the plain block, this implementation requires fewer paramters, but provides a deeper stack. If the channel of the output changes, or the size of the output changes, a projection layer is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern2d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it.","title":"modules.resnet.BlockBottleneck2d"},{"location":"apis/modules/resnet/BlockBottleneck2d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int ) The kernel size of this layer. stride int or ( int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/resnet/BlockBottleneck2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/BlockBottleneck2d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the output data size.","title":" __call__"},{"location":"apis/modules/resnet/BlockBottleneck2d/#examples","text":"In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck2d ( 16 , 32 , kernel_size = 3 , stride = ( 1 , 2 ), padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 4 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 16, 4, 255] 32 PReLU-2 [-1, 16, 4, 255] 16 Conv2d-3 [-1, 16, 4, 255] 256 InstanceNorm2d-4 [-1, 16, 4, 255] 32 PReLU-5 [-1, 16, 4, 255] 16 Conv2d-6 [-1, 16, 4, 128] 2,304 InstanceNorm2d-7 [-1, 16, 4, 128] 32 PReLU-8 [-1, 16, 4, 128] 16 Conv2d-9 [-1, 32, 4, 128] 512 Conv2d-10 [-1, 32, 4, 128] 512 InstanceNorm2d-11 [-1, 32, 4, 128] 64 BlockBottleneck2d-12 [-1, 32, 4, 128] 0 ================================================================ Total params: 3,792 Trainable params: 3,792 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 1.31 Params size (MB): 0.01 Estimated Total Size (MB): 1.39 ---------------------------------------------------------------- Note that the output size would be (4, 128) in this example, because the same padding is used for both two axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck2d ( 32 , 16 , kernel_size = 3 , output_size = ( 4 , 255 ), padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 4 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 32, 4, 128] 64 PReLU-2 [-1, 32, 4, 128] 32 Conv2d-3 [-1, 32, 4, 128] 1,024 InstanceNorm2d-4 [-1, 32, 4, 128] 64 PReLU-5 [-1, 32, 4, 128] 32 Upsample-6 [-1, 32, 4, 255] 0 Conv2d-7 [-1, 32, 4, 255] 9,216 InstanceNorm2d-8 [-1, 32, 4, 255] 64 PReLU-9 [-1, 32, 4, 255] 32 Conv2d-10 [-1, 16, 4, 255] 512 Upsample-11 [-1, 32, 4, 255] 0 Conv2d-12 [-1, 16, 4, 255] 512 InstanceNorm2d-13 [-1, 16, 4, 255] 32 BlockBottleneck2d-14 [-1, 16, 4, 255] 0 ================================================================ Total params: 11,584 Trainable params: 11,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 2.37 Params size (MB): 0.04 Estimated Total Size (MB): 2.47 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/BlockBottleneck3d/","text":"modules.resnet.BlockBottleneck3d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockBottleneck3d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the bottleneck (second-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; where the projection convolutional layer is implemented by a convolution with kernel_size = 1 . Compared to the plain block, this implementation requires fewer paramters, but provides a deeper stack. If the channel of the output changes, or the size of the output changes, a projection layer is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern3d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int , int ) The kernel size of this layer. stride int or ( int , int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the output data size. Examples \u00b6 In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck3d ( 16 , 32 , kernel_size = ( 3 , 1 , 3 ), stride = ( 2 , 1 , 2 ), padding = ( 1 , 0 , 1 ), scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 32 , 4 , 63 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 16, 32, 4, 63] 32 PReLU-2 [-1, 16, 32, 4, 63] 16 Conv3d-3 [-1, 16, 32, 4, 63] 256 InstanceNorm3d-4 [-1, 16, 32, 4, 63] 32 PReLU-5 [-1, 16, 32, 4, 63] 16 Conv3d-6 [-1, 16, 16, 4, 32] 2,304 InstanceNorm3d-7 [-1, 16, 16, 4, 32] 32 PReLU-8 [-1, 16, 16, 4, 32] 16 Conv3d-9 [-1, 32, 16, 4, 32] 512 Conv3d-10 [-1, 32, 16, 4, 32] 512 InstanceNorm3d-11 [-1, 32, 16, 4, 32] 64 BlockBottleneck3d-12 [-1, 32, 16, 4, 32] 0 ================================================================ Total params: 3,792 Trainable params: 3,792 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.49 Forward/backward pass size (MB): 7.67 Params size (MB): 0.01 Estimated Total Size (MB): 8.18 ---------------------------------------------------------------- Note that the output size would be (16, 4, 32) in this example, because the same padding is used for all three axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck3d ( 32 , 16 , kernel_size = ( 3 , 1 , 3 ), output_size = ( 32 , 4 , 63 ), padding = ( 1 , 0 , 1 ), scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 16 , 4 , 32 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 32, 16, 4, 32] 64 PReLU-2 [-1, 32, 16, 4, 32] 32 Conv3d-3 [-1, 32, 16, 4, 32] 1,024 InstanceNorm3d-4 [-1, 32, 16, 4, 32] 64 PReLU-5 [-1, 32, 16, 4, 32] 32 Upsample-6 [-1, 32, 32, 4, 63] 0 Conv3d-7 [-1, 32, 32, 4, 63] 9,216 InstanceNorm3d-8 [-1, 32, 32, 4, 63] 64 PReLU-9 [-1, 32, 32, 4, 63] 32 Conv3d-10 [-1, 16, 32, 4, 63] 512 Upsample-11 [-1, 32, 32, 4, 63] 0 Conv3d-12 [-1, 16, 32, 4, 63] 512 InstanceNorm3d-13 [-1, 16, 32, 4, 63] 32 BlockBottleneck3d-14 [-1, 16, 32, 4, 63] 0 ================================================================ Total params: 11,584 Trainable params: 11,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.25 Forward/backward pass size (MB): 16.28 Params size (MB): 0.04 Estimated Total Size (MB): 16.58 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>BlockBottleneck3d</span>"},{"location":"apis/modules/resnet/BlockBottleneck3d/#modulesresnetblockbottleneck3d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockBottleneck3d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the bottleneck (second-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; where the projection convolutional layer is implemented by a convolution with kernel_size = 1 . Compared to the plain block, this implementation requires fewer paramters, but provides a deeper stack. If the channel of the output changes, or the size of the output changes, a projection layer is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Projection<br>convolution] --> conv2[Modern<br>convolution] --> conv3[Projection<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern3d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it.","title":"modules.resnet.BlockBottleneck3d"},{"location":"apis/modules/resnet/BlockBottleneck3d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int , int ) The kernel size of this layer. stride int or ( int , int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/resnet/BlockBottleneck3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/BlockBottleneck3d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the output data size.","title":" __call__"},{"location":"apis/modules/resnet/BlockBottleneck3d/#examples","text":"In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck3d ( 16 , 32 , kernel_size = ( 3 , 1 , 3 ), stride = ( 2 , 1 , 2 ), padding = ( 1 , 0 , 1 ), scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 32 , 4 , 63 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 16, 32, 4, 63] 32 PReLU-2 [-1, 16, 32, 4, 63] 16 Conv3d-3 [-1, 16, 32, 4, 63] 256 InstanceNorm3d-4 [-1, 16, 32, 4, 63] 32 PReLU-5 [-1, 16, 32, 4, 63] 16 Conv3d-6 [-1, 16, 16, 4, 32] 2,304 InstanceNorm3d-7 [-1, 16, 16, 4, 32] 32 PReLU-8 [-1, 16, 16, 4, 32] 16 Conv3d-9 [-1, 32, 16, 4, 32] 512 Conv3d-10 [-1, 32, 16, 4, 32] 512 InstanceNorm3d-11 [-1, 32, 16, 4, 32] 64 BlockBottleneck3d-12 [-1, 32, 16, 4, 32] 0 ================================================================ Total params: 3,792 Trainable params: 3,792 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.49 Forward/backward pass size (MB): 7.67 Params size (MB): 0.01 Estimated Total Size (MB): 8.18 ---------------------------------------------------------------- Note that the output size would be (16, 4, 32) in this example, because the same padding is used for all three axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockBottleneck3d ( 32 , 16 , kernel_size = ( 3 , 1 , 3 ), output_size = ( 32 , 4 , 63 ), padding = ( 1 , 0 , 1 ), scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 16 , 4 , 32 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 32, 16, 4, 32] 64 PReLU-2 [-1, 32, 16, 4, 32] 32 Conv3d-3 [-1, 32, 16, 4, 32] 1,024 InstanceNorm3d-4 [-1, 32, 16, 4, 32] 64 PReLU-5 [-1, 32, 16, 4, 32] 32 Upsample-6 [-1, 32, 32, 4, 63] 0 Conv3d-7 [-1, 32, 32, 4, 63] 9,216 InstanceNorm3d-8 [-1, 32, 32, 4, 63] 64 PReLU-9 [-1, 32, 32, 4, 63] 32 Conv3d-10 [-1, 16, 32, 4, 63] 512 Upsample-11 [-1, 32, 32, 4, 63] 0 Conv3d-12 [-1, 16, 32, 4, 63] 512 InstanceNorm3d-13 [-1, 16, 32, 4, 63] 32 BlockBottleneck3d-14 [-1, 16, 32, 4, 63] 0 ================================================================ Total params: 11,584 Trainable params: 11,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.25 Forward/backward pass size (MB): 16.28 Params size (MB): 0.04 Estimated Total Size (MB): 16.58 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/BlockPlain1d/","text":"modules.resnet.BlockPlain1d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockPlain1d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the plain (first-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; If the channel of the output changes, or the size of the output changes, a projection layer implemented by a convolution with kernel_size = 1 is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern1d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int The kernel size of this layer. stride int The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int The length of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the output data length. Examples \u00b6 In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain1d ( 16 , 32 , kernel_size = 3 , stride = 2 , padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 16, 255] 32 PReLU-2 [-1, 16, 255] 16 Conv1d-3 [-1, 16, 255] 768 InstanceNorm1d-4 [-1, 16, 255] 32 PReLU-5 [-1, 16, 255] 16 Conv1d-6 [-1, 32, 128] 1,536 Conv1d-7 [-1, 32, 128] 512 InstanceNorm1d-8 [-1, 32, 128] 64 BlockPlain1d-9 [-1, 32, 128] 0 ================================================================ Total params: 2,976 Trainable params: 2,976 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.28 Params size (MB): 0.01 Estimated Total Size (MB): 0.31 ---------------------------------------------------------------- Note that the output length would be 128 in this example, because the same padding is used for the input. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain1d ( 32 , 16 , kernel_size = 3 , output_size = 255 , padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 32, 128] 64 PReLU-2 [-1, 32, 128] 32 Conv1d-3 [-1, 32, 128] 3,072 InstanceNorm1d-4 [-1, 32, 128] 64 PReLU-5 [-1, 32, 128] 32 Upsample-6 [-1, 32, 255] 0 Conv1d-7 [-1, 16, 255] 1,536 Upsample-8 [-1, 32, 255] 0 Conv1d-9 [-1, 16, 255] 512 InstanceNorm1d-10 [-1, 16, 255] 32 BlockPlain1d-11 [-1, 16, 255] 0 ================================================================ Total params: 5,344 Trainable params: 5,344 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.41 Params size (MB): 0.02 Estimated Total Size (MB): 0.44 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>BlockPlain1d</span>"},{"location":"apis/modules/resnet/BlockPlain1d/#modulesresnetblockplain1d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockPlain1d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the plain (first-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; If the channel of the output changes, or the size of the output changes, a projection layer implemented by a convolution with kernel_size = 1 is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern1d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it.","title":"modules.resnet.BlockPlain1d"},{"location":"apis/modules/resnet/BlockPlain1d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int The kernel size of this layer. stride int The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int The length of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/resnet/BlockPlain1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/BlockPlain1d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the output data length.","title":" __call__"},{"location":"apis/modules/resnet/BlockPlain1d/#examples","text":"In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain1d ( 16 , 32 , kernel_size = 3 , stride = 2 , padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 16, 255] 32 PReLU-2 [-1, 16, 255] 16 Conv1d-3 [-1, 16, 255] 768 InstanceNorm1d-4 [-1, 16, 255] 32 PReLU-5 [-1, 16, 255] 16 Conv1d-6 [-1, 32, 128] 1,536 Conv1d-7 [-1, 32, 128] 512 InstanceNorm1d-8 [-1, 32, 128] 64 BlockPlain1d-9 [-1, 32, 128] 0 ================================================================ Total params: 2,976 Trainable params: 2,976 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.28 Params size (MB): 0.01 Estimated Total Size (MB): 0.31 ---------------------------------------------------------------- Note that the output length would be 128 in this example, because the same padding is used for the input. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain1d ( 32 , 16 , kernel_size = 3 , output_size = 255 , padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm1d-1 [-1, 32, 128] 64 PReLU-2 [-1, 32, 128] 32 Conv1d-3 [-1, 32, 128] 3,072 InstanceNorm1d-4 [-1, 32, 128] 64 PReLU-5 [-1, 32, 128] 32 Upsample-6 [-1, 32, 255] 0 Conv1d-7 [-1, 16, 255] 1,536 Upsample-8 [-1, 32, 255] 0 Conv1d-9 [-1, 16, 255] 512 InstanceNorm1d-10 [-1, 16, 255] 32 BlockPlain1d-11 [-1, 16, 255] 0 ================================================================ Total params: 5,344 Trainable params: 5,344 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 0.41 Params size (MB): 0.02 Estimated Total Size (MB): 0.44 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/BlockPlain2d/","text":"modules.resnet.BlockPlain2d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockPlain2d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the plain (first-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; If the channel of the output changes, or the size of the output changes, a projection layer implemented by a convolution with kernel_size = 1 is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern2d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int ) The kernel size of this layer. stride int or ( int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the output data size. Examples \u00b6 In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain2d ( 16 , 32 , kernel_size = 3 , stride = ( 1 , 2 ), padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 4 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 16, 4, 255] 32 PReLU-2 [-1, 16, 4, 255] 16 Conv2d-3 [-1, 16, 4, 255] 2,304 InstanceNorm2d-4 [-1, 16, 4, 255] 32 PReLU-5 [-1, 16, 4, 255] 16 Conv2d-6 [-1, 32, 4, 128] 4,608 Conv2d-7 [-1, 32, 4, 128] 512 InstanceNorm2d-8 [-1, 32, 4, 128] 64 BlockPlain2d-9 [-1, 32, 4, 128] 0 ================================================================ Total params: 7,584 Trainable params: 7,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 1.12 Params size (MB): 0.03 Estimated Total Size (MB): 1.21 ---------------------------------------------------------------- Note that the output size would be (4, 128) in this example, because the same padding is used for both two axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain2d ( 32 , 16 , kernel_size = 3 , output_size = ( 4 , 255 ), padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 4 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 32, 4, 128] 64 PReLU-2 [-1, 32, 4, 128] 32 Conv2d-3 [-1, 32, 4, 128] 9,216 InstanceNorm2d-4 [-1, 32, 4, 128] 64 PReLU-5 [-1, 32, 4, 128] 32 Upsample-6 [-1, 32, 4, 255] 0 Conv2d-7 [-1, 16, 4, 255] 4,608 Upsample-8 [-1, 32, 4, 255] 0 Conv2d-9 [-1, 16, 4, 255] 512 InstanceNorm2d-10 [-1, 16, 4, 255] 32 BlockPlain2d-11 [-1, 16, 4, 255] 0 ================================================================ Total params: 14,560 Trainable params: 14,560 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 1.62 Params size (MB): 0.06 Estimated Total Size (MB): 1.74 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>BlockPlain2d</span>"},{"location":"apis/modules/resnet/BlockPlain2d/#modulesresnetblockplain2d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockPlain2d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the plain (first-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; If the channel of the output changes, or the size of the output changes, a projection layer implemented by a convolution with kernel_size = 1 is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern2d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it.","title":"modules.resnet.BlockPlain2d"},{"location":"apis/modules/resnet/BlockPlain2d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int ) The kernel size of this layer. stride int or ( int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/resnet/BlockPlain2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/BlockPlain2d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the output data size.","title":" __call__"},{"location":"apis/modules/resnet/BlockPlain2d/#examples","text":"In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain2d ( 16 , 32 , kernel_size = 3 , stride = ( 1 , 2 ), padding = 1 , scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 4 , 255 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 16, 4, 255] 32 PReLU-2 [-1, 16, 4, 255] 16 Conv2d-3 [-1, 16, 4, 255] 2,304 InstanceNorm2d-4 [-1, 16, 4, 255] 32 PReLU-5 [-1, 16, 4, 255] 16 Conv2d-6 [-1, 32, 4, 128] 4,608 Conv2d-7 [-1, 32, 4, 128] 512 InstanceNorm2d-8 [-1, 32, 4, 128] 64 BlockPlain2d-9 [-1, 32, 4, 128] 0 ================================================================ Total params: 7,584 Trainable params: 7,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 1.12 Params size (MB): 0.03 Estimated Total Size (MB): 1.21 ---------------------------------------------------------------- Note that the output size would be (4, 128) in this example, because the same padding is used for both two axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain2d ( 32 , 16 , kernel_size = 3 , output_size = ( 4 , 255 ), padding = 1 , scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 4 , 128 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm2d-1 [-1, 32, 4, 128] 64 PReLU-2 [-1, 32, 4, 128] 32 Conv2d-3 [-1, 32, 4, 128] 9,216 InstanceNorm2d-4 [-1, 32, 4, 128] 64 PReLU-5 [-1, 32, 4, 128] 32 Upsample-6 [-1, 32, 4, 255] 0 Conv2d-7 [-1, 16, 4, 255] 4,608 Upsample-8 [-1, 32, 4, 255] 0 Conv2d-9 [-1, 16, 4, 255] 512 InstanceNorm2d-10 [-1, 16, 4, 255] 32 BlockPlain2d-11 [-1, 16, 4, 255] 0 ================================================================ Total params: 14,560 Trainable params: 14,560 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.06 Forward/backward pass size (MB): 1.62 Params size (MB): 0.06 Estimated Total Size (MB): 1.74 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/BlockPlain3d/","text":"modules.resnet.BlockPlain3d \u00b6 Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockPlain3d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the plain (first-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; If the channel of the output changes, or the size of the output changes, a projection layer implemented by a convolution with kernel_size = 1 is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern3d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it. Arguments \u00b6 Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int , int ) The kernel size of this layer. stride int or ( int , int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution). Operators \u00b6 __call__ \u00b6 y = layer ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the output data size. Examples \u00b6 In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain3d ( 16 , 32 , kernel_size = ( 3 , 1 , 3 ), stride = ( 2 , 1 , 2 ), padding = ( 1 , 0 , 1 ), scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 32 , 4 , 63 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 16, 32, 4, 63] 32 PReLU-2 [-1, 16, 32, 4, 63] 16 Conv3d-3 [-1, 16, 32, 4, 63] 2,304 InstanceNorm3d-4 [-1, 16, 32, 4, 63] 32 PReLU-5 [-1, 16, 32, 4, 63] 16 Conv3d-6 [-1, 32, 16, 4, 32] 4,608 Conv3d-7 [-1, 32, 16, 4, 32] 512 InstanceNorm3d-8 [-1, 32, 16, 4, 32] 64 BlockPlain3d-9 [-1, 32, 16, 4, 32] 0 ================================================================ Total params: 7,584 Trainable params: 7,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.49 Forward/backward pass size (MB): 6.92 Params size (MB): 0.03 Estimated Total Size (MB): 7.44 ---------------------------------------------------------------- Note that the output size would be (16, 4, 32) in this example, because the same padding is used for all three axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain3d ( 32 , 16 , kernel_size = ( 3 , 1 , 3 ), output_size = ( 32 , 4 , 63 ), padding = ( 1 , 0 , 1 ), scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 16 , 4 , 32 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 32, 16, 4, 32] 64 PReLU-2 [-1, 32, 16, 4, 32] 32 Conv3d-3 [-1, 32, 16, 4, 32] 9,216 InstanceNorm3d-4 [-1, 32, 16, 4, 32] 64 PReLU-5 [-1, 32, 16, 4, 32] 32 Upsample-6 [-1, 32, 32, 4, 63] 0 Conv3d-7 [-1, 16, 32, 4, 63] 4,608 Upsample-8 [-1, 32, 32, 4, 63] 0 Conv3d-9 [-1, 16, 32, 4, 63] 512 InstanceNorm3d-10 [-1, 16, 32, 4, 63] 32 BlockPlain3d-11 [-1, 16, 32, 4, 63] 0 ================================================================ Total params: 14,560 Trainable params: 14,560 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.25 Forward/backward pass size (MB): 10.38 Params size (MB): 0.06 Estimated Total Size (MB): 10.68 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>BlockPlain3d</span>"},{"location":"apis/modules/resnet/BlockPlain3d/#modulesresnetblockplain3d","text":"Class \u00b7 nn.Module \u00b7 Source layer = mdnc . modules . resnet . BlockPlain3d ( in_planes , out_planes , kernel_size = 3 , stride = 1 , padding = 1 , output_size = None , normalizer = 'pinst' , activator = 'prelu' , layer_order = 'new' , scaler = 'down' ) In the following paper, the authors propose two structres of the residual block. Deep Residual Learning for Image Recognition This is the implementation of the plain (first-type) residual block. The residual block could be divided into two branches (input + conv). In this plain implementation, the convolutional branch is a composed of double convolutional layers. Shown in the following chart: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; If the channel of the output changes, or the size of the output changes, a projection layer implemented by a convolution with kernel_size = 1 is required for mapping the input branch to the output space: flowchart TB in((\" \")) --> conv1[Modern<br>convolution] --> conv2[Modern<br>convolution] --> plus((\"+\")):::diagramop --> out((\" \")) in --> pconv[Projection<br>convolution] --> plus classDef diagramop fill:#FFB11B, stroke:#AF811B; In the following paper, a new op composition order is proposed for building residual block: Identity Mappings in Deep Residual Networks This implementation called \"pre-activation\" would change the order of the sub-layers in the modern convolutional layer (see mdnc.modules.conv.ConvModern3d ). We support and recommend to use this implementation, set layer_order = 'new' to enable it.","title":"modules.resnet.BlockPlain3d"},{"location":"apis/modules/resnet/BlockPlain3d/#arguments","text":"Requries Argument Type Description in_planes int The channel number of the input data. out_planes int The channel number of the output data. kernel_size int or ( int , int , int ) The kernel size of this layer. stride int or ( int , int , int ) The stride size of this layer. When scaler = 'down' , this argument serves as the down-sampling factor. When scaler = 'up' , this argument serves as the up-sampling factor. padding int or ( int , int , int ) The padding size of this layer. The zero padding would be performed on both edges of the input before the convolution. output_size int or ( int , int , int ) The size of the output data. This option is only used when scaler = 'up' . When setting this value, the size of the up-sampling would be given explicitly and the argument stride would not be used. normalizer str The normalization method, could be: 'batch' : Batch normalization. 'inst' : Instance normalization. 'pinst' : Instance normalization with tunable rescaling parameters. 'null' : Without normalization, would falls back to the \"convolution + activation\" form. In this case, the layer_order = 'new' would not take effects. activator str The activation method, could be: 'prelu' , 'relu' , 'null' . layer_order str The sub-layer composition order, could be: 'new' : normalization + activation + convolution. 'old' : convolution + normalization + activation. scaler str The scaling method, could be: 'down' : the argument stride would be used for down-sampling. 'up' : the argument stride would be used for up-sampling (equivalent to transposed convolution).","title":"Arguments"},{"location":"apis/modules/resnet/BlockPlain3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/BlockPlain3d/#__call__","text":"y = layer ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this layer. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the output data size.","title":" __call__"},{"location":"apis/modules/resnet/BlockPlain3d/#examples","text":"In the first example, we build a plain residual block with \u00bd down-sampling and same padding. Example 1 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain3d ( 16 , 32 , kernel_size = ( 3 , 1 , 3 ), stride = ( 2 , 1 , 2 ), padding = ( 1 , 0 , 1 ), scaler = 'down' ) mdnc . contribs . torchsummary . summary ( layer , ( 16 , 32 , 4 , 63 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 16, 32, 4, 63] 32 PReLU-2 [-1, 16, 32, 4, 63] 16 Conv3d-3 [-1, 16, 32, 4, 63] 2,304 InstanceNorm3d-4 [-1, 16, 32, 4, 63] 32 PReLU-5 [-1, 16, 32, 4, 63] 16 Conv3d-6 [-1, 32, 16, 4, 32] 4,608 Conv3d-7 [-1, 32, 16, 4, 32] 512 InstanceNorm3d-8 [-1, 32, 16, 4, 32] 64 BlockPlain3d-9 [-1, 32, 16, 4, 32] 0 ================================================================ Total params: 7,584 Trainable params: 7,584 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.49 Forward/backward pass size (MB): 6.92 Params size (MB): 0.03 Estimated Total Size (MB): 7.44 ---------------------------------------------------------------- Note that the output size would be (16, 4, 32) in this example, because the same padding is used for all three axes of the input size. In this case, if we want to make a reverse layer, we could specify the output_size for the up-sampling layer, for example: Example 2 Codes 1 2 3 4 import mdnc layer = mdnc . modules . resnet . BlockPlain3d ( 32 , 16 , kernel_size = ( 3 , 1 , 3 ), output_size = ( 32 , 4 , 63 ), padding = ( 1 , 0 , 1 ), scaler = 'up' ) mdnc . contribs . torchsummary . summary ( layer , ( 32 , 16 , 4 , 32 ), device = 'cpu' ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ InstanceNorm3d-1 [-1, 32, 16, 4, 32] 64 PReLU-2 [-1, 32, 16, 4, 32] 32 Conv3d-3 [-1, 32, 16, 4, 32] 9,216 InstanceNorm3d-4 [-1, 32, 16, 4, 32] 64 PReLU-5 [-1, 32, 16, 4, 32] 32 Upsample-6 [-1, 32, 32, 4, 63] 0 Conv3d-7 [-1, 16, 32, 4, 63] 4,608 Upsample-8 [-1, 32, 32, 4, 63] 0 Conv3d-9 [-1, 16, 32, 4, 63] 512 InstanceNorm3d-10 [-1, 16, 32, 4, 63] 32 BlockPlain3d-11 [-1, 16, 32, 4, 63] 0 ================================================================ Total params: 14,560 Trainable params: 14,560 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.25 Forward/backward pass size (MB): 10.38 Params size (MB): 0.06 Estimated Total Size (MB): 10.68 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/DecoderNet1d/","text":"modules.resnet.DecoderNet1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . DecoderNet1d ( channel , layers , out_size , block = 'bottleneck' , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 1D residual decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] blocks\"] u2[\"Block 2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] un[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv1d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int The length of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 1D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the input feature maps (see input_size ) respectively. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the output data size specified by the argument out_size . Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. input_size \u00b6 net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = 32 , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 33. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 132,096 Conv1d-2 [-1, 1024, 4] 3,145,728 InstanceNorm1d-3 [-1, 1024, 4] 2,048 PReLU-4 [-1, 1024, 4] 1,024 Conv1d-5 [-1, 1024, 4] 1,048,576 InstanceNorm1d-6 [-1, 1024, 4] 2,048 PReLU-7 [-1, 1024, 4] 1,024 Conv1d-8 [-1, 1024, 4] 3,145,728 InstanceNorm1d-9 [-1, 1024, 4] 2,048 PReLU-10 [-1, 1024, 4] 1,024 Conv1d-11 [-1, 512, 4] 524,288 Conv1d-12 [-1, 512, 4] 524,288 InstanceNorm1d-13 [-1, 512, 4] 1,024 _BlockBo...neckNd-14 [-1, 512, 4] 0 InstanceNorm1d-15 [-1, 512, 4] 1,024 PReLU-16 [-1, 512, 4] 512 Conv1d-17 [-1, 512, 4] 262,144 InstanceNorm1d-18 [-1, 512, 4] 1,024 PReLU-19 [-1, 512, 4] 512 Upsample-20 [-1, 512, 8] 0 Conv1d-21 [-1, 512, 8] 786,432 InstanceNorm1d-22 [-1, 512, 8] 1,024 PReLU-23 [-1, 512, 8] 512 Conv1d-24 [-1, 512, 8] 262,144 Upsample-25 [-1, 512, 8] 0 Conv1d-26 [-1, 512, 8] 262,144 InstanceNorm1d-27 [-1, 512, 8] 1,024 _BlockBo...neckNd-28 [-1, 512, 8] 0 _BlockResStkNd-29 [-1, 512, 8] 0 InstanceNorm1d-30 [-1, 512, 8] 1,024 PReLU-31 [-1, 512, 8] 512 Conv1d-32 [-1, 512, 8] 262,144 InstanceNorm1d-33 [-1, 512, 8] 1,024 PReLU-34 [-1, 512, 8] 512 Conv1d-35 [-1, 512, 8] 786,432 InstanceNorm1d-36 [-1, 512, 8] 1,024 PReLU-37 [-1, 512, 8] 512 Conv1d-38 [-1, 256, 8] 131,072 Conv1d-39 [-1, 256, 8] 131,072 InstanceNorm1d-40 [-1, 256, 8] 512 _BlockBo...neckNd-41 [-1, 256, 8] 0 InstanceNorm1d-42 [-1, 256, 8] 512 PReLU-43 [-1, 256, 8] 256 Conv1d-44 [-1, 256, 8] 65,536 InstanceNorm1d-45 [-1, 256, 8] 512 PReLU-46 [-1, 256, 8] 256 Upsample-47 [-1, 256, 16] 0 Conv1d-48 [-1, 256, 16] 196,608 InstanceNorm1d-49 [-1, 256, 16] 512 PReLU-50 [-1, 256, 16] 256 Conv1d-51 [-1, 256, 16] 65,536 Upsample-52 [-1, 256, 16] 0 Conv1d-53 [-1, 256, 16] 65,536 InstanceNorm1d-54 [-1, 256, 16] 512 _BlockBo...neckNd-55 [-1, 256, 16] 0 _BlockResStkNd-56 [-1, 256, 16] 0 InstanceNorm1d-57 [-1, 256, 16] 512 PReLU-58 [-1, 256, 16] 256 Conv1d-59 [-1, 256, 16] 65,536 InstanceNorm1d-60 [-1, 256, 16] 512 PReLU-61 [-1, 256, 16] 256 Conv1d-62 [-1, 256, 16] 196,608 InstanceNorm1d-63 [-1, 256, 16] 512 PReLU-64 [-1, 256, 16] 256 Conv1d-65 [-1, 128, 16] 32,768 Conv1d-66 [-1, 128, 16] 32,768 InstanceNorm1d-67 [-1, 128, 16] 256 _BlockBo...neckNd-68 [-1, 128, 16] 0 InstanceNorm1d-69 [-1, 128, 16] 256 PReLU-70 [-1, 128, 16] 128 Conv1d-71 [-1, 128, 16] 16,384 InstanceNorm1d-72 [-1, 128, 16] 256 PReLU-73 [-1, 128, 16] 128 Upsample-74 [-1, 128, 32] 0 Conv1d-75 [-1, 128, 32] 49,152 InstanceNorm1d-76 [-1, 128, 32] 256 PReLU-77 [-1, 128, 32] 128 Conv1d-78 [-1, 128, 32] 16,384 Upsample-79 [-1, 128, 32] 0 Conv1d-80 [-1, 128, 32] 16,384 InstanceNorm1d-81 [-1, 128, 32] 256 _BlockBo...neckNd-82 [-1, 128, 32] 0 _BlockResStkNd-83 [-1, 128, 32] 0 InstanceNorm1d-84 [-1, 128, 32] 256 PReLU-85 [-1, 128, 32] 128 Conv1d-86 [-1, 128, 32] 16,384 InstanceNorm1d-87 [-1, 128, 32] 256 PReLU-88 [-1, 128, 32] 128 Conv1d-89 [-1, 128, 32] 49,152 InstanceNorm1d-90 [-1, 128, 32] 256 PReLU-91 [-1, 128, 32] 128 Conv1d-92 [-1, 64, 32] 8,192 Conv1d-93 [-1, 64, 32] 8,192 InstanceNorm1d-94 [-1, 64, 32] 128 _BlockBo...neckNd-95 [-1, 64, 32] 0 InstanceNorm1d-96 [-1, 64, 32] 128 PReLU-97 [-1, 64, 32] 64 Conv1d-98 [-1, 64, 32] 4,096 InstanceNorm1d-99 [-1, 64, 32] 128 PReLU-100 [-1, 64, 32] 64 Upsample-101 [-1, 64, 64] 0 Conv1d-102 [-1, 64, 64] 12,288 InstanceNorm1d-103 [-1, 64, 64] 128 PReLU-104 [-1, 64, 64] 64 Conv1d-105 [-1, 64, 64] 4,096 Upsample-106 [-1, 64, 64] 0 Conv1d-107 [-1, 64, 64] 4,096 InstanceNorm1d-108 [-1, 64, 64] 128 _BlockBo...eckNd-109 [-1, 64, 64] 0 _BlockResStkNd-110 [-1, 64, 64] 0 InstanceNorm1d-111 [-1, 64, 64] 128 PReLU-112 [-1, 64, 64] 64 Conv1d-113 [-1, 64, 64] 4,096 InstanceNorm1d-114 [-1, 64, 64] 128 PReLU-115 [-1, 64, 64] 64 Conv1d-116 [-1, 64, 64] 12,288 InstanceNorm1d-117 [-1, 64, 64] 128 PReLU-118 [-1, 64, 64] 64 Conv1d-119 [-1, 64, 64] 4,096 _BlockBo...eckNd-120 [-1, 64, 64] 0 InstanceNorm1d-121 [-1, 64, 64] 128 PReLU-122 [-1, 64, 64] 64 Conv1d-123 [-1, 64, 64] 4,096 InstanceNorm1d-124 [-1, 64, 64] 128 PReLU-125 [-1, 64, 64] 64 Upsample-126 [-1, 64, 128] 0 Conv1d-127 [-1, 64, 128] 12,288 InstanceNorm1d-128 [-1, 64, 128] 128 PReLU-129 [-1, 64, 128] 64 Conv1d-130 [-1, 64, 128] 4,096 Upsample-131 [-1, 64, 128] 0 Conv1d-132 [-1, 64, 128] 4,096 InstanceNorm1d-133 [-1, 64, 128] 128 _BlockBo...eckNd-134 [-1, 64, 128] 0 _BlockResStkNd-135 [-1, 64, 128] 0 Conv1d-136 [-1, 3, 128] 963 DecoderNet1d-137 [-1, 3, 128] 0 ================================================================ Total params: 12,407,043 Trainable params: 12,407,043 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 3.97 Params size (MB): 47.33 Estimated Total Size (MB): 51.30 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = None , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 32. The input size is (1024, 4). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 3,145,728 InstanceNorm1d-2 [-1, 1024, 4] 2,048 PReLU-3 [-1, 1024, 4] 1,024 Conv1d-4 [-1, 1024, 4] 1,048,576 InstanceNorm1d-5 [-1, 1024, 4] 2,048 PReLU-6 [-1, 1024, 4] 1,024 Conv1d-7 [-1, 1024, 4] 3,145,728 InstanceNorm1d-8 [-1, 1024, 4] 2,048 PReLU-9 [-1, 1024, 4] 1,024 Conv1d-10 [-1, 512, 4] 524,288 Conv1d-11 [-1, 512, 4] 524,288 InstanceNorm1d-12 [-1, 512, 4] 1,024 _BlockBo...neckNd-13 [-1, 512, 4] 0 InstanceNorm1d-14 [-1, 512, 4] 1,024 PReLU-15 [-1, 512, 4] 512 Conv1d-16 [-1, 512, 4] 262,144 InstanceNorm1d-17 [-1, 512, 4] 1,024 PReLU-18 [-1, 512, 4] 512 Upsample-19 [-1, 512, 8] 0 Conv1d-20 [-1, 512, 8] 786,432 InstanceNorm1d-21 [-1, 512, 8] 1,024 PReLU-22 [-1, 512, 8] 512 Conv1d-23 [-1, 512, 8] 262,144 Upsample-24 [-1, 512, 8] 0 Conv1d-25 [-1, 512, 8] 262,144 InstanceNorm1d-26 [-1, 512, 8] 1,024 _BlockBo...neckNd-27 [-1, 512, 8] 0 _BlockResStkNd-28 [-1, 512, 8] 0 InstanceNorm1d-29 [-1, 512, 8] 1,024 PReLU-30 [-1, 512, 8] 512 Conv1d-31 [-1, 512, 8] 262,144 InstanceNorm1d-32 [-1, 512, 8] 1,024 PReLU-33 [-1, 512, 8] 512 Conv1d-34 [-1, 512, 8] 786,432 InstanceNorm1d-35 [-1, 512, 8] 1,024 PReLU-36 [-1, 512, 8] 512 Conv1d-37 [-1, 256, 8] 131,072 Conv1d-38 [-1, 256, 8] 131,072 InstanceNorm1d-39 [-1, 256, 8] 512 _BlockBo...neckNd-40 [-1, 256, 8] 0 InstanceNorm1d-41 [-1, 256, 8] 512 PReLU-42 [-1, 256, 8] 256 Conv1d-43 [-1, 256, 8] 65,536 InstanceNorm1d-44 [-1, 256, 8] 512 PReLU-45 [-1, 256, 8] 256 Upsample-46 [-1, 256, 16] 0 Conv1d-47 [-1, 256, 16] 196,608 InstanceNorm1d-48 [-1, 256, 16] 512 PReLU-49 [-1, 256, 16] 256 Conv1d-50 [-1, 256, 16] 65,536 Upsample-51 [-1, 256, 16] 0 Conv1d-52 [-1, 256, 16] 65,536 InstanceNorm1d-53 [-1, 256, 16] 512 _BlockBo...neckNd-54 [-1, 256, 16] 0 _BlockResStkNd-55 [-1, 256, 16] 0 InstanceNorm1d-56 [-1, 256, 16] 512 PReLU-57 [-1, 256, 16] 256 Conv1d-58 [-1, 256, 16] 65,536 InstanceNorm1d-59 [-1, 256, 16] 512 PReLU-60 [-1, 256, 16] 256 Conv1d-61 [-1, 256, 16] 196,608 InstanceNorm1d-62 [-1, 256, 16] 512 PReLU-63 [-1, 256, 16] 256 Conv1d-64 [-1, 128, 16] 32,768 Conv1d-65 [-1, 128, 16] 32,768 InstanceNorm1d-66 [-1, 128, 16] 256 _BlockBo...neckNd-67 [-1, 128, 16] 0 InstanceNorm1d-68 [-1, 128, 16] 256 PReLU-69 [-1, 128, 16] 128 Conv1d-70 [-1, 128, 16] 16,384 InstanceNorm1d-71 [-1, 128, 16] 256 PReLU-72 [-1, 128, 16] 128 Upsample-73 [-1, 128, 32] 0 Conv1d-74 [-1, 128, 32] 49,152 InstanceNorm1d-75 [-1, 128, 32] 256 PReLU-76 [-1, 128, 32] 128 Conv1d-77 [-1, 128, 32] 16,384 Upsample-78 [-1, 128, 32] 0 Conv1d-79 [-1, 128, 32] 16,384 InstanceNorm1d-80 [-1, 128, 32] 256 _BlockBo...neckNd-81 [-1, 128, 32] 0 _BlockResStkNd-82 [-1, 128, 32] 0 InstanceNorm1d-83 [-1, 128, 32] 256 PReLU-84 [-1, 128, 32] 128 Conv1d-85 [-1, 128, 32] 16,384 InstanceNorm1d-86 [-1, 128, 32] 256 PReLU-87 [-1, 128, 32] 128 Conv1d-88 [-1, 128, 32] 49,152 InstanceNorm1d-89 [-1, 128, 32] 256 PReLU-90 [-1, 128, 32] 128 Conv1d-91 [-1, 64, 32] 8,192 Conv1d-92 [-1, 64, 32] 8,192 InstanceNorm1d-93 [-1, 64, 32] 128 _BlockBo...neckNd-94 [-1, 64, 32] 0 InstanceNorm1d-95 [-1, 64, 32] 128 PReLU-96 [-1, 64, 32] 64 Conv1d-97 [-1, 64, 32] 4,096 InstanceNorm1d-98 [-1, 64, 32] 128 PReLU-99 [-1, 64, 32] 64 Upsample-100 [-1, 64, 64] 0 Conv1d-101 [-1, 64, 64] 12,288 InstanceNorm1d-102 [-1, 64, 64] 128 PReLU-103 [-1, 64, 64] 64 Conv1d-104 [-1, 64, 64] 4,096 Upsample-105 [-1, 64, 64] 0 Conv1d-106 [-1, 64, 64] 4,096 InstanceNorm1d-107 [-1, 64, 64] 128 _BlockBo...eckNd-108 [-1, 64, 64] 0 _BlockResStkNd-109 [-1, 64, 64] 0 InstanceNorm1d-110 [-1, 64, 64] 128 PReLU-111 [-1, 64, 64] 64 Conv1d-112 [-1, 64, 64] 4,096 InstanceNorm1d-113 [-1, 64, 64] 128 PReLU-114 [-1, 64, 64] 64 Conv1d-115 [-1, 64, 64] 12,288 InstanceNorm1d-116 [-1, 64, 64] 128 PReLU-117 [-1, 64, 64] 64 Conv1d-118 [-1, 64, 64] 4,096 _BlockBo...eckNd-119 [-1, 64, 64] 0 InstanceNorm1d-120 [-1, 64, 64] 128 PReLU-121 [-1, 64, 64] 64 Conv1d-122 [-1, 64, 64] 4,096 InstanceNorm1d-123 [-1, 64, 64] 128 PReLU-124 [-1, 64, 64] 64 Upsample-125 [-1, 64, 128] 0 Conv1d-126 [-1, 64, 128] 12,288 InstanceNorm1d-127 [-1, 64, 128] 128 PReLU-128 [-1, 64, 128] 64 Conv1d-129 [-1, 64, 128] 4,096 Upsample-130 [-1, 64, 128] 0 Conv1d-131 [-1, 64, 128] 4,096 InstanceNorm1d-132 [-1, 64, 128] 128 _BlockBo...eckNd-133 [-1, 64, 128] 0 _BlockResStkNd-134 [-1, 64, 128] 0 Conv1d-135 [-1, 3, 128] 963 DecoderNet1d-136 [-1, 3, 128] 0 ================================================================ Total params: 12,274,947 Trainable params: 12,274,947 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 3.94 Params size (MB): 46.83 Estimated Total Size (MB): 50.78 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>DecoderNet1d</span>"},{"location":"apis/modules/resnet/DecoderNet1d/#modulesresnetdecodernet1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . DecoderNet1d ( channel , layers , out_size , block = 'bottleneck' , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 1D residual decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] blocks\"] u2[\"Block 2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] un[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv1d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps.","title":"modules.resnet.DecoderNet1d"},{"location":"apis/modules/resnet/DecoderNet1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int The length of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/DecoderNet1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/DecoderNet1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 1D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the input feature maps (see input_size ) respectively. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the output data size specified by the argument out_size .","title":" __call__"},{"location":"apis/modules/resnet/DecoderNet1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/DecoderNet1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/resnet/DecoderNet1d/#input_size","text":"net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None .","title":" input_size"},{"location":"apis/modules/resnet/DecoderNet1d/#examples","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = 32 , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 33. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 132,096 Conv1d-2 [-1, 1024, 4] 3,145,728 InstanceNorm1d-3 [-1, 1024, 4] 2,048 PReLU-4 [-1, 1024, 4] 1,024 Conv1d-5 [-1, 1024, 4] 1,048,576 InstanceNorm1d-6 [-1, 1024, 4] 2,048 PReLU-7 [-1, 1024, 4] 1,024 Conv1d-8 [-1, 1024, 4] 3,145,728 InstanceNorm1d-9 [-1, 1024, 4] 2,048 PReLU-10 [-1, 1024, 4] 1,024 Conv1d-11 [-1, 512, 4] 524,288 Conv1d-12 [-1, 512, 4] 524,288 InstanceNorm1d-13 [-1, 512, 4] 1,024 _BlockBo...neckNd-14 [-1, 512, 4] 0 InstanceNorm1d-15 [-1, 512, 4] 1,024 PReLU-16 [-1, 512, 4] 512 Conv1d-17 [-1, 512, 4] 262,144 InstanceNorm1d-18 [-1, 512, 4] 1,024 PReLU-19 [-1, 512, 4] 512 Upsample-20 [-1, 512, 8] 0 Conv1d-21 [-1, 512, 8] 786,432 InstanceNorm1d-22 [-1, 512, 8] 1,024 PReLU-23 [-1, 512, 8] 512 Conv1d-24 [-1, 512, 8] 262,144 Upsample-25 [-1, 512, 8] 0 Conv1d-26 [-1, 512, 8] 262,144 InstanceNorm1d-27 [-1, 512, 8] 1,024 _BlockBo...neckNd-28 [-1, 512, 8] 0 _BlockResStkNd-29 [-1, 512, 8] 0 InstanceNorm1d-30 [-1, 512, 8] 1,024 PReLU-31 [-1, 512, 8] 512 Conv1d-32 [-1, 512, 8] 262,144 InstanceNorm1d-33 [-1, 512, 8] 1,024 PReLU-34 [-1, 512, 8] 512 Conv1d-35 [-1, 512, 8] 786,432 InstanceNorm1d-36 [-1, 512, 8] 1,024 PReLU-37 [-1, 512, 8] 512 Conv1d-38 [-1, 256, 8] 131,072 Conv1d-39 [-1, 256, 8] 131,072 InstanceNorm1d-40 [-1, 256, 8] 512 _BlockBo...neckNd-41 [-1, 256, 8] 0 InstanceNorm1d-42 [-1, 256, 8] 512 PReLU-43 [-1, 256, 8] 256 Conv1d-44 [-1, 256, 8] 65,536 InstanceNorm1d-45 [-1, 256, 8] 512 PReLU-46 [-1, 256, 8] 256 Upsample-47 [-1, 256, 16] 0 Conv1d-48 [-1, 256, 16] 196,608 InstanceNorm1d-49 [-1, 256, 16] 512 PReLU-50 [-1, 256, 16] 256 Conv1d-51 [-1, 256, 16] 65,536 Upsample-52 [-1, 256, 16] 0 Conv1d-53 [-1, 256, 16] 65,536 InstanceNorm1d-54 [-1, 256, 16] 512 _BlockBo...neckNd-55 [-1, 256, 16] 0 _BlockResStkNd-56 [-1, 256, 16] 0 InstanceNorm1d-57 [-1, 256, 16] 512 PReLU-58 [-1, 256, 16] 256 Conv1d-59 [-1, 256, 16] 65,536 InstanceNorm1d-60 [-1, 256, 16] 512 PReLU-61 [-1, 256, 16] 256 Conv1d-62 [-1, 256, 16] 196,608 InstanceNorm1d-63 [-1, 256, 16] 512 PReLU-64 [-1, 256, 16] 256 Conv1d-65 [-1, 128, 16] 32,768 Conv1d-66 [-1, 128, 16] 32,768 InstanceNorm1d-67 [-1, 128, 16] 256 _BlockBo...neckNd-68 [-1, 128, 16] 0 InstanceNorm1d-69 [-1, 128, 16] 256 PReLU-70 [-1, 128, 16] 128 Conv1d-71 [-1, 128, 16] 16,384 InstanceNorm1d-72 [-1, 128, 16] 256 PReLU-73 [-1, 128, 16] 128 Upsample-74 [-1, 128, 32] 0 Conv1d-75 [-1, 128, 32] 49,152 InstanceNorm1d-76 [-1, 128, 32] 256 PReLU-77 [-1, 128, 32] 128 Conv1d-78 [-1, 128, 32] 16,384 Upsample-79 [-1, 128, 32] 0 Conv1d-80 [-1, 128, 32] 16,384 InstanceNorm1d-81 [-1, 128, 32] 256 _BlockBo...neckNd-82 [-1, 128, 32] 0 _BlockResStkNd-83 [-1, 128, 32] 0 InstanceNorm1d-84 [-1, 128, 32] 256 PReLU-85 [-1, 128, 32] 128 Conv1d-86 [-1, 128, 32] 16,384 InstanceNorm1d-87 [-1, 128, 32] 256 PReLU-88 [-1, 128, 32] 128 Conv1d-89 [-1, 128, 32] 49,152 InstanceNorm1d-90 [-1, 128, 32] 256 PReLU-91 [-1, 128, 32] 128 Conv1d-92 [-1, 64, 32] 8,192 Conv1d-93 [-1, 64, 32] 8,192 InstanceNorm1d-94 [-1, 64, 32] 128 _BlockBo...neckNd-95 [-1, 64, 32] 0 InstanceNorm1d-96 [-1, 64, 32] 128 PReLU-97 [-1, 64, 32] 64 Conv1d-98 [-1, 64, 32] 4,096 InstanceNorm1d-99 [-1, 64, 32] 128 PReLU-100 [-1, 64, 32] 64 Upsample-101 [-1, 64, 64] 0 Conv1d-102 [-1, 64, 64] 12,288 InstanceNorm1d-103 [-1, 64, 64] 128 PReLU-104 [-1, 64, 64] 64 Conv1d-105 [-1, 64, 64] 4,096 Upsample-106 [-1, 64, 64] 0 Conv1d-107 [-1, 64, 64] 4,096 InstanceNorm1d-108 [-1, 64, 64] 128 _BlockBo...eckNd-109 [-1, 64, 64] 0 _BlockResStkNd-110 [-1, 64, 64] 0 InstanceNorm1d-111 [-1, 64, 64] 128 PReLU-112 [-1, 64, 64] 64 Conv1d-113 [-1, 64, 64] 4,096 InstanceNorm1d-114 [-1, 64, 64] 128 PReLU-115 [-1, 64, 64] 64 Conv1d-116 [-1, 64, 64] 12,288 InstanceNorm1d-117 [-1, 64, 64] 128 PReLU-118 [-1, 64, 64] 64 Conv1d-119 [-1, 64, 64] 4,096 _BlockBo...eckNd-120 [-1, 64, 64] 0 InstanceNorm1d-121 [-1, 64, 64] 128 PReLU-122 [-1, 64, 64] 64 Conv1d-123 [-1, 64, 64] 4,096 InstanceNorm1d-124 [-1, 64, 64] 128 PReLU-125 [-1, 64, 64] 64 Upsample-126 [-1, 64, 128] 0 Conv1d-127 [-1, 64, 128] 12,288 InstanceNorm1d-128 [-1, 64, 128] 128 PReLU-129 [-1, 64, 128] 64 Conv1d-130 [-1, 64, 128] 4,096 Upsample-131 [-1, 64, 128] 0 Conv1d-132 [-1, 64, 128] 4,096 InstanceNorm1d-133 [-1, 64, 128] 128 _BlockBo...eckNd-134 [-1, 64, 128] 0 _BlockResStkNd-135 [-1, 64, 128] 0 Conv1d-136 [-1, 3, 128] 963 DecoderNet1d-137 [-1, 3, 128] 0 ================================================================ Total params: 12,407,043 Trainable params: 12,407,043 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 3.97 Params size (MB): 47.33 Estimated Total Size (MB): 51.30 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = None , out_size = 128 , out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 32. The input size is (1024, 4). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 1024, 4] 3,145,728 InstanceNorm1d-2 [-1, 1024, 4] 2,048 PReLU-3 [-1, 1024, 4] 1,024 Conv1d-4 [-1, 1024, 4] 1,048,576 InstanceNorm1d-5 [-1, 1024, 4] 2,048 PReLU-6 [-1, 1024, 4] 1,024 Conv1d-7 [-1, 1024, 4] 3,145,728 InstanceNorm1d-8 [-1, 1024, 4] 2,048 PReLU-9 [-1, 1024, 4] 1,024 Conv1d-10 [-1, 512, 4] 524,288 Conv1d-11 [-1, 512, 4] 524,288 InstanceNorm1d-12 [-1, 512, 4] 1,024 _BlockBo...neckNd-13 [-1, 512, 4] 0 InstanceNorm1d-14 [-1, 512, 4] 1,024 PReLU-15 [-1, 512, 4] 512 Conv1d-16 [-1, 512, 4] 262,144 InstanceNorm1d-17 [-1, 512, 4] 1,024 PReLU-18 [-1, 512, 4] 512 Upsample-19 [-1, 512, 8] 0 Conv1d-20 [-1, 512, 8] 786,432 InstanceNorm1d-21 [-1, 512, 8] 1,024 PReLU-22 [-1, 512, 8] 512 Conv1d-23 [-1, 512, 8] 262,144 Upsample-24 [-1, 512, 8] 0 Conv1d-25 [-1, 512, 8] 262,144 InstanceNorm1d-26 [-1, 512, 8] 1,024 _BlockBo...neckNd-27 [-1, 512, 8] 0 _BlockResStkNd-28 [-1, 512, 8] 0 InstanceNorm1d-29 [-1, 512, 8] 1,024 PReLU-30 [-1, 512, 8] 512 Conv1d-31 [-1, 512, 8] 262,144 InstanceNorm1d-32 [-1, 512, 8] 1,024 PReLU-33 [-1, 512, 8] 512 Conv1d-34 [-1, 512, 8] 786,432 InstanceNorm1d-35 [-1, 512, 8] 1,024 PReLU-36 [-1, 512, 8] 512 Conv1d-37 [-1, 256, 8] 131,072 Conv1d-38 [-1, 256, 8] 131,072 InstanceNorm1d-39 [-1, 256, 8] 512 _BlockBo...neckNd-40 [-1, 256, 8] 0 InstanceNorm1d-41 [-1, 256, 8] 512 PReLU-42 [-1, 256, 8] 256 Conv1d-43 [-1, 256, 8] 65,536 InstanceNorm1d-44 [-1, 256, 8] 512 PReLU-45 [-1, 256, 8] 256 Upsample-46 [-1, 256, 16] 0 Conv1d-47 [-1, 256, 16] 196,608 InstanceNorm1d-48 [-1, 256, 16] 512 PReLU-49 [-1, 256, 16] 256 Conv1d-50 [-1, 256, 16] 65,536 Upsample-51 [-1, 256, 16] 0 Conv1d-52 [-1, 256, 16] 65,536 InstanceNorm1d-53 [-1, 256, 16] 512 _BlockBo...neckNd-54 [-1, 256, 16] 0 _BlockResStkNd-55 [-1, 256, 16] 0 InstanceNorm1d-56 [-1, 256, 16] 512 PReLU-57 [-1, 256, 16] 256 Conv1d-58 [-1, 256, 16] 65,536 InstanceNorm1d-59 [-1, 256, 16] 512 PReLU-60 [-1, 256, 16] 256 Conv1d-61 [-1, 256, 16] 196,608 InstanceNorm1d-62 [-1, 256, 16] 512 PReLU-63 [-1, 256, 16] 256 Conv1d-64 [-1, 128, 16] 32,768 Conv1d-65 [-1, 128, 16] 32,768 InstanceNorm1d-66 [-1, 128, 16] 256 _BlockBo...neckNd-67 [-1, 128, 16] 0 InstanceNorm1d-68 [-1, 128, 16] 256 PReLU-69 [-1, 128, 16] 128 Conv1d-70 [-1, 128, 16] 16,384 InstanceNorm1d-71 [-1, 128, 16] 256 PReLU-72 [-1, 128, 16] 128 Upsample-73 [-1, 128, 32] 0 Conv1d-74 [-1, 128, 32] 49,152 InstanceNorm1d-75 [-1, 128, 32] 256 PReLU-76 [-1, 128, 32] 128 Conv1d-77 [-1, 128, 32] 16,384 Upsample-78 [-1, 128, 32] 0 Conv1d-79 [-1, 128, 32] 16,384 InstanceNorm1d-80 [-1, 128, 32] 256 _BlockBo...neckNd-81 [-1, 128, 32] 0 _BlockResStkNd-82 [-1, 128, 32] 0 InstanceNorm1d-83 [-1, 128, 32] 256 PReLU-84 [-1, 128, 32] 128 Conv1d-85 [-1, 128, 32] 16,384 InstanceNorm1d-86 [-1, 128, 32] 256 PReLU-87 [-1, 128, 32] 128 Conv1d-88 [-1, 128, 32] 49,152 InstanceNorm1d-89 [-1, 128, 32] 256 PReLU-90 [-1, 128, 32] 128 Conv1d-91 [-1, 64, 32] 8,192 Conv1d-92 [-1, 64, 32] 8,192 InstanceNorm1d-93 [-1, 64, 32] 128 _BlockBo...neckNd-94 [-1, 64, 32] 0 InstanceNorm1d-95 [-1, 64, 32] 128 PReLU-96 [-1, 64, 32] 64 Conv1d-97 [-1, 64, 32] 4,096 InstanceNorm1d-98 [-1, 64, 32] 128 PReLU-99 [-1, 64, 32] 64 Upsample-100 [-1, 64, 64] 0 Conv1d-101 [-1, 64, 64] 12,288 InstanceNorm1d-102 [-1, 64, 64] 128 PReLU-103 [-1, 64, 64] 64 Conv1d-104 [-1, 64, 64] 4,096 Upsample-105 [-1, 64, 64] 0 Conv1d-106 [-1, 64, 64] 4,096 InstanceNorm1d-107 [-1, 64, 64] 128 _BlockBo...eckNd-108 [-1, 64, 64] 0 _BlockResStkNd-109 [-1, 64, 64] 0 InstanceNorm1d-110 [-1, 64, 64] 128 PReLU-111 [-1, 64, 64] 64 Conv1d-112 [-1, 64, 64] 4,096 InstanceNorm1d-113 [-1, 64, 64] 128 PReLU-114 [-1, 64, 64] 64 Conv1d-115 [-1, 64, 64] 12,288 InstanceNorm1d-116 [-1, 64, 64] 128 PReLU-117 [-1, 64, 64] 64 Conv1d-118 [-1, 64, 64] 4,096 _BlockBo...eckNd-119 [-1, 64, 64] 0 InstanceNorm1d-120 [-1, 64, 64] 128 PReLU-121 [-1, 64, 64] 64 Conv1d-122 [-1, 64, 64] 4,096 InstanceNorm1d-123 [-1, 64, 64] 128 PReLU-124 [-1, 64, 64] 64 Upsample-125 [-1, 64, 128] 0 Conv1d-126 [-1, 64, 128] 12,288 InstanceNorm1d-127 [-1, 64, 128] 128 PReLU-128 [-1, 64, 128] 64 Conv1d-129 [-1, 64, 128] 4,096 Upsample-130 [-1, 64, 128] 0 Conv1d-131 [-1, 64, 128] 4,096 InstanceNorm1d-132 [-1, 64, 128] 128 _BlockBo...eckNd-133 [-1, 64, 128] 0 _BlockResStkNd-134 [-1, 64, 128] 0 Conv1d-135 [-1, 3, 128] 963 DecoderNet1d-136 [-1, 3, 128] 0 ================================================================ Total params: 12,274,947 Trainable params: 12,274,947 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 3.94 Params size (MB): 46.83 Estimated Total Size (MB): 50.78 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/DecoderNet2d/","text":"modules.resnet.DecoderNet2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . DecoderNet2d ( channel , layers , out_size , block = 'bottleneck' , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 2D residual decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] blocks\"] u2[\"Block 2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] un[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv2d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 2D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the output data size specified by the argument out_size . Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. input_size \u00b6 net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = 32 , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 33. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 132,096 Conv2d-2 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-3 [-1, 1024, 2, 2] 2,048 PReLU-4 [-1, 1024, 2, 2] 1,024 Conv2d-5 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-6 [-1, 1024, 2, 2] 2,048 PReLU-7 [-1, 1024, 2, 2] 1,024 Conv2d-8 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-9 [-1, 1024, 2, 2] 2,048 PReLU-10 [-1, 1024, 2, 2] 1,024 Conv2d-11 [-1, 512, 2, 2] 524,288 Conv2d-12 [-1, 512, 2, 2] 524,288 InstanceNorm2d-13 [-1, 512, 2, 2] 1,024 _BlockBo...neckNd-14 [-1, 512, 2, 2] 0 InstanceNorm2d-15 [-1, 512, 2, 2] 1,024 PReLU-16 [-1, 512, 2, 2] 512 Conv2d-17 [-1, 512, 2, 2] 262,144 InstanceNorm2d-18 [-1, 512, 2, 2] 1,024 PReLU-19 [-1, 512, 2, 2] 512 Upsample-20 [-1, 512, 4, 4] 0 Conv2d-21 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-22 [-1, 512, 4, 4] 1,024 PReLU-23 [-1, 512, 4, 4] 512 Conv2d-24 [-1, 512, 4, 4] 262,144 Upsample-25 [-1, 512, 4, 4] 0 Conv2d-26 [-1, 512, 4, 4] 262,144 InstanceNorm2d-27 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-28 [-1, 512, 4, 4] 0 _BlockResStkNd-29 [-1, 512, 4, 4] 0 InstanceNorm2d-30 [-1, 512, 4, 4] 1,024 PReLU-31 [-1, 512, 4, 4] 512 Conv2d-32 [-1, 512, 4, 4] 262,144 InstanceNorm2d-33 [-1, 512, 4, 4] 1,024 PReLU-34 [-1, 512, 4, 4] 512 Conv2d-35 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-36 [-1, 512, 4, 4] 1,024 PReLU-37 [-1, 512, 4, 4] 512 Conv2d-38 [-1, 256, 4, 4] 131,072 Conv2d-39 [-1, 256, 4, 4] 131,072 InstanceNorm2d-40 [-1, 256, 4, 4] 512 _BlockBo...neckNd-41 [-1, 256, 4, 4] 0 InstanceNorm2d-42 [-1, 256, 4, 4] 512 PReLU-43 [-1, 256, 4, 4] 256 Conv2d-44 [-1, 256, 4, 4] 65,536 InstanceNorm2d-45 [-1, 256, 4, 4] 512 PReLU-46 [-1, 256, 4, 4] 256 Upsample-47 [-1, 256, 8, 8] 0 Conv2d-48 [-1, 256, 8, 8] 589,824 InstanceNorm2d-49 [-1, 256, 8, 8] 512 PReLU-50 [-1, 256, 8, 8] 256 Conv2d-51 [-1, 256, 8, 8] 65,536 Upsample-52 [-1, 256, 8, 8] 0 Conv2d-53 [-1, 256, 8, 8] 65,536 InstanceNorm2d-54 [-1, 256, 8, 8] 512 _BlockBo...neckNd-55 [-1, 256, 8, 8] 0 _BlockResStkNd-56 [-1, 256, 8, 8] 0 InstanceNorm2d-57 [-1, 256, 8, 8] 512 PReLU-58 [-1, 256, 8, 8] 256 Conv2d-59 [-1, 256, 8, 8] 65,536 InstanceNorm2d-60 [-1, 256, 8, 8] 512 PReLU-61 [-1, 256, 8, 8] 256 Conv2d-62 [-1, 256, 8, 8] 589,824 InstanceNorm2d-63 [-1, 256, 8, 8] 512 PReLU-64 [-1, 256, 8, 8] 256 Conv2d-65 [-1, 128, 8, 8] 32,768 Conv2d-66 [-1, 128, 8, 8] 32,768 InstanceNorm2d-67 [-1, 128, 8, 8] 256 _BlockBo...neckNd-68 [-1, 128, 8, 8] 0 InstanceNorm2d-69 [-1, 128, 8, 8] 256 PReLU-70 [-1, 128, 8, 8] 128 Conv2d-71 [-1, 128, 8, 8] 16,384 InstanceNorm2d-72 [-1, 128, 8, 8] 256 PReLU-73 [-1, 128, 8, 8] 128 Upsample-74 [-1, 128, 16, 16] 0 Conv2d-75 [-1, 128, 16, 16] 147,456 InstanceNorm2d-76 [-1, 128, 16, 16] 256 PReLU-77 [-1, 128, 16, 16] 128 Conv2d-78 [-1, 128, 16, 16] 16,384 Upsample-79 [-1, 128, 16, 16] 0 Conv2d-80 [-1, 128, 16, 16] 16,384 InstanceNorm2d-81 [-1, 128, 16, 16] 256 _BlockBo...neckNd-82 [-1, 128, 16, 16] 0 _BlockResStkNd-83 [-1, 128, 16, 16] 0 InstanceNorm2d-84 [-1, 128, 16, 16] 256 PReLU-85 [-1, 128, 16, 16] 128 Conv2d-86 [-1, 128, 16, 16] 16,384 InstanceNorm2d-87 [-1, 128, 16, 16] 256 PReLU-88 [-1, 128, 16, 16] 128 Conv2d-89 [-1, 128, 16, 16] 147,456 InstanceNorm2d-90 [-1, 128, 16, 16] 256 PReLU-91 [-1, 128, 16, 16] 128 Conv2d-92 [-1, 64, 16, 16] 8,192 Conv2d-93 [-1, 64, 16, 16] 8,192 InstanceNorm2d-94 [-1, 64, 16, 16] 128 _BlockBo...neckNd-95 [-1, 64, 16, 16] 0 InstanceNorm2d-96 [-1, 64, 16, 16] 128 PReLU-97 [-1, 64, 16, 16] 64 Conv2d-98 [-1, 64, 16, 16] 4,096 InstanceNorm2d-99 [-1, 64, 16, 16] 128 PReLU-100 [-1, 64, 16, 16] 64 Upsample-101 [-1, 64, 32, 32] 0 Conv2d-102 [-1, 64, 32, 32] 36,864 InstanceNorm2d-103 [-1, 64, 32, 32] 128 PReLU-104 [-1, 64, 32, 32] 64 Conv2d-105 [-1, 64, 32, 32] 4,096 Upsample-106 [-1, 64, 32, 32] 0 Conv2d-107 [-1, 64, 32, 32] 4,096 InstanceNorm2d-108 [-1, 64, 32, 32] 128 _BlockBo...eckNd-109 [-1, 64, 32, 32] 0 _BlockResStkNd-110 [-1, 64, 32, 32] 0 InstanceNorm2d-111 [-1, 64, 32, 32] 128 PReLU-112 [-1, 64, 32, 32] 64 Conv2d-113 [-1, 64, 32, 32] 4,096 InstanceNorm2d-114 [-1, 64, 32, 32] 128 PReLU-115 [-1, 64, 32, 32] 64 Conv2d-116 [-1, 64, 32, 32] 36,864 InstanceNorm2d-117 [-1, 64, 32, 32] 128 PReLU-118 [-1, 64, 32, 32] 64 Conv2d-119 [-1, 64, 32, 32] 4,096 _BlockBo...eckNd-120 [-1, 64, 32, 32] 0 InstanceNorm2d-121 [-1, 64, 32, 32] 128 PReLU-122 [-1, 64, 32, 32] 64 Conv2d-123 [-1, 64, 32, 32] 4,096 InstanceNorm2d-124 [-1, 64, 32, 32] 128 PReLU-125 [-1, 64, 32, 32] 64 Upsample-126 [-1, 64, 64, 64] 0 Conv2d-127 [-1, 64, 64, 64] 36,864 InstanceNorm2d-128 [-1, 64, 64, 64] 128 PReLU-129 [-1, 64, 64, 64] 64 Conv2d-130 [-1, 64, 64, 64] 4,096 Upsample-131 [-1, 64, 64, 64] 0 Conv2d-132 [-1, 64, 64, 64] 4,096 InstanceNorm2d-133 [-1, 64, 64, 64] 128 _BlockBo...eckNd-134 [-1, 64, 64, 64] 0 _BlockResStkNd-135 [-1, 64, 64, 64] 0 Conv2d-136 [-1, 3, 64, 63] 4,803 DecoderNet2d-137 [-1, 3, 64, 63] 0 ================================================================ Total params: 29,196,291 Trainable params: 29,196,291 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 42.98 Params size (MB): 111.38 Estimated Total Size (MB): 154.36 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = None , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 32. The input size is (1024, 2, 2). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-2 [-1, 1024, 2, 2] 2,048 PReLU-3 [-1, 1024, 2, 2] 1,024 Conv2d-4 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-5 [-1, 1024, 2, 2] 2,048 PReLU-6 [-1, 1024, 2, 2] 1,024 Conv2d-7 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-8 [-1, 1024, 2, 2] 2,048 PReLU-9 [-1, 1024, 2, 2] 1,024 Conv2d-10 [-1, 512, 2, 2] 524,288 Conv2d-11 [-1, 512, 2, 2] 524,288 InstanceNorm2d-12 [-1, 512, 2, 2] 1,024 _BlockBo...neckNd-13 [-1, 512, 2, 2] 0 InstanceNorm2d-14 [-1, 512, 2, 2] 1,024 PReLU-15 [-1, 512, 2, 2] 512 Conv2d-16 [-1, 512, 2, 2] 262,144 InstanceNorm2d-17 [-1, 512, 2, 2] 1,024 PReLU-18 [-1, 512, 2, 2] 512 Upsample-19 [-1, 512, 4, 4] 0 Conv2d-20 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-21 [-1, 512, 4, 4] 1,024 PReLU-22 [-1, 512, 4, 4] 512 Conv2d-23 [-1, 512, 4, 4] 262,144 Upsample-24 [-1, 512, 4, 4] 0 Conv2d-25 [-1, 512, 4, 4] 262,144 InstanceNorm2d-26 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-27 [-1, 512, 4, 4] 0 _BlockResStkNd-28 [-1, 512, 4, 4] 0 InstanceNorm2d-29 [-1, 512, 4, 4] 1,024 PReLU-30 [-1, 512, 4, 4] 512 Conv2d-31 [-1, 512, 4, 4] 262,144 InstanceNorm2d-32 [-1, 512, 4, 4] 1,024 PReLU-33 [-1, 512, 4, 4] 512 Conv2d-34 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-35 [-1, 512, 4, 4] 1,024 PReLU-36 [-1, 512, 4, 4] 512 Conv2d-37 [-1, 256, 4, 4] 131,072 Conv2d-38 [-1, 256, 4, 4] 131,072 InstanceNorm2d-39 [-1, 256, 4, 4] 512 _BlockBo...neckNd-40 [-1, 256, 4, 4] 0 InstanceNorm2d-41 [-1, 256, 4, 4] 512 PReLU-42 [-1, 256, 4, 4] 256 Conv2d-43 [-1, 256, 4, 4] 65,536 InstanceNorm2d-44 [-1, 256, 4, 4] 512 PReLU-45 [-1, 256, 4, 4] 256 Upsample-46 [-1, 256, 8, 8] 0 Conv2d-47 [-1, 256, 8, 8] 589,824 InstanceNorm2d-48 [-1, 256, 8, 8] 512 PReLU-49 [-1, 256, 8, 8] 256 Conv2d-50 [-1, 256, 8, 8] 65,536 Upsample-51 [-1, 256, 8, 8] 0 Conv2d-52 [-1, 256, 8, 8] 65,536 InstanceNorm2d-53 [-1, 256, 8, 8] 512 _BlockBo...neckNd-54 [-1, 256, 8, 8] 0 _BlockResStkNd-55 [-1, 256, 8, 8] 0 InstanceNorm2d-56 [-1, 256, 8, 8] 512 PReLU-57 [-1, 256, 8, 8] 256 Conv2d-58 [-1, 256, 8, 8] 65,536 InstanceNorm2d-59 [-1, 256, 8, 8] 512 PReLU-60 [-1, 256, 8, 8] 256 Conv2d-61 [-1, 256, 8, 8] 589,824 InstanceNorm2d-62 [-1, 256, 8, 8] 512 PReLU-63 [-1, 256, 8, 8] 256 Conv2d-64 [-1, 128, 8, 8] 32,768 Conv2d-65 [-1, 128, 8, 8] 32,768 InstanceNorm2d-66 [-1, 128, 8, 8] 256 _BlockBo...neckNd-67 [-1, 128, 8, 8] 0 InstanceNorm2d-68 [-1, 128, 8, 8] 256 PReLU-69 [-1, 128, 8, 8] 128 Conv2d-70 [-1, 128, 8, 8] 16,384 InstanceNorm2d-71 [-1, 128, 8, 8] 256 PReLU-72 [-1, 128, 8, 8] 128 Upsample-73 [-1, 128, 16, 16] 0 Conv2d-74 [-1, 128, 16, 16] 147,456 InstanceNorm2d-75 [-1, 128, 16, 16] 256 PReLU-76 [-1, 128, 16, 16] 128 Conv2d-77 [-1, 128, 16, 16] 16,384 Upsample-78 [-1, 128, 16, 16] 0 Conv2d-79 [-1, 128, 16, 16] 16,384 InstanceNorm2d-80 [-1, 128, 16, 16] 256 _BlockBo...neckNd-81 [-1, 128, 16, 16] 0 _BlockResStkNd-82 [-1, 128, 16, 16] 0 InstanceNorm2d-83 [-1, 128, 16, 16] 256 PReLU-84 [-1, 128, 16, 16] 128 Conv2d-85 [-1, 128, 16, 16] 16,384 InstanceNorm2d-86 [-1, 128, 16, 16] 256 PReLU-87 [-1, 128, 16, 16] 128 Conv2d-88 [-1, 128, 16, 16] 147,456 InstanceNorm2d-89 [-1, 128, 16, 16] 256 PReLU-90 [-1, 128, 16, 16] 128 Conv2d-91 [-1, 64, 16, 16] 8,192 Conv2d-92 [-1, 64, 16, 16] 8,192 InstanceNorm2d-93 [-1, 64, 16, 16] 128 _BlockBo...neckNd-94 [-1, 64, 16, 16] 0 InstanceNorm2d-95 [-1, 64, 16, 16] 128 PReLU-96 [-1, 64, 16, 16] 64 Conv2d-97 [-1, 64, 16, 16] 4,096 InstanceNorm2d-98 [-1, 64, 16, 16] 128 PReLU-99 [-1, 64, 16, 16] 64 Upsample-100 [-1, 64, 32, 32] 0 Conv2d-101 [-1, 64, 32, 32] 36,864 InstanceNorm2d-102 [-1, 64, 32, 32] 128 PReLU-103 [-1, 64, 32, 32] 64 Conv2d-104 [-1, 64, 32, 32] 4,096 Upsample-105 [-1, 64, 32, 32] 0 Conv2d-106 [-1, 64, 32, 32] 4,096 InstanceNorm2d-107 [-1, 64, 32, 32] 128 _BlockBo...eckNd-108 [-1, 64, 32, 32] 0 _BlockResStkNd-109 [-1, 64, 32, 32] 0 InstanceNorm2d-110 [-1, 64, 32, 32] 128 PReLU-111 [-1, 64, 32, 32] 64 Conv2d-112 [-1, 64, 32, 32] 4,096 InstanceNorm2d-113 [-1, 64, 32, 32] 128 PReLU-114 [-1, 64, 32, 32] 64 Conv2d-115 [-1, 64, 32, 32] 36,864 InstanceNorm2d-116 [-1, 64, 32, 32] 128 PReLU-117 [-1, 64, 32, 32] 64 Conv2d-118 [-1, 64, 32, 32] 4,096 _BlockBo...eckNd-119 [-1, 64, 32, 32] 0 InstanceNorm2d-120 [-1, 64, 32, 32] 128 PReLU-121 [-1, 64, 32, 32] 64 Conv2d-122 [-1, 64, 32, 32] 4,096 InstanceNorm2d-123 [-1, 64, 32, 32] 128 PReLU-124 [-1, 64, 32, 32] 64 Upsample-125 [-1, 64, 64, 64] 0 Conv2d-126 [-1, 64, 64, 64] 36,864 InstanceNorm2d-127 [-1, 64, 64, 64] 128 PReLU-128 [-1, 64, 64, 64] 64 Conv2d-129 [-1, 64, 64, 64] 4,096 Upsample-130 [-1, 64, 64, 64] 0 Conv2d-131 [-1, 64, 64, 64] 4,096 InstanceNorm2d-132 [-1, 64, 64, 64] 128 _BlockBo...eckNd-133 [-1, 64, 64, 64] 0 _BlockResStkNd-134 [-1, 64, 64, 64] 0 Conv2d-135 [-1, 3, 64, 63] 4,803 DecoderNet2d-136 [-1, 3, 64, 63] 0 ================================================================ Total params: 29,064,195 Trainable params: 29,064,195 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 42.95 Params size (MB): 110.87 Estimated Total Size (MB): 153.84 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>DecoderNet2d</span>"},{"location":"apis/modules/resnet/DecoderNet2d/#modulesresnetdecodernet2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . DecoderNet2d ( channel , layers , out_size , block = 'bottleneck' , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 2D residual decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] blocks\"] u2[\"Block 2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] un[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv2d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps.","title":"modules.resnet.DecoderNet2d"},{"location":"apis/modules/resnet/DecoderNet2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/DecoderNet2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/DecoderNet2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 2D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the output data size specified by the argument out_size .","title":" __call__"},{"location":"apis/modules/resnet/DecoderNet2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/DecoderNet2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/resnet/DecoderNet2d/#input_size","text":"net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None .","title":" input_size"},{"location":"apis/modules/resnet/DecoderNet2d/#examples","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = 32 , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 33. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 132,096 Conv2d-2 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-3 [-1, 1024, 2, 2] 2,048 PReLU-4 [-1, 1024, 2, 2] 1,024 Conv2d-5 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-6 [-1, 1024, 2, 2] 2,048 PReLU-7 [-1, 1024, 2, 2] 1,024 Conv2d-8 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-9 [-1, 1024, 2, 2] 2,048 PReLU-10 [-1, 1024, 2, 2] 1,024 Conv2d-11 [-1, 512, 2, 2] 524,288 Conv2d-12 [-1, 512, 2, 2] 524,288 InstanceNorm2d-13 [-1, 512, 2, 2] 1,024 _BlockBo...neckNd-14 [-1, 512, 2, 2] 0 InstanceNorm2d-15 [-1, 512, 2, 2] 1,024 PReLU-16 [-1, 512, 2, 2] 512 Conv2d-17 [-1, 512, 2, 2] 262,144 InstanceNorm2d-18 [-1, 512, 2, 2] 1,024 PReLU-19 [-1, 512, 2, 2] 512 Upsample-20 [-1, 512, 4, 4] 0 Conv2d-21 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-22 [-1, 512, 4, 4] 1,024 PReLU-23 [-1, 512, 4, 4] 512 Conv2d-24 [-1, 512, 4, 4] 262,144 Upsample-25 [-1, 512, 4, 4] 0 Conv2d-26 [-1, 512, 4, 4] 262,144 InstanceNorm2d-27 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-28 [-1, 512, 4, 4] 0 _BlockResStkNd-29 [-1, 512, 4, 4] 0 InstanceNorm2d-30 [-1, 512, 4, 4] 1,024 PReLU-31 [-1, 512, 4, 4] 512 Conv2d-32 [-1, 512, 4, 4] 262,144 InstanceNorm2d-33 [-1, 512, 4, 4] 1,024 PReLU-34 [-1, 512, 4, 4] 512 Conv2d-35 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-36 [-1, 512, 4, 4] 1,024 PReLU-37 [-1, 512, 4, 4] 512 Conv2d-38 [-1, 256, 4, 4] 131,072 Conv2d-39 [-1, 256, 4, 4] 131,072 InstanceNorm2d-40 [-1, 256, 4, 4] 512 _BlockBo...neckNd-41 [-1, 256, 4, 4] 0 InstanceNorm2d-42 [-1, 256, 4, 4] 512 PReLU-43 [-1, 256, 4, 4] 256 Conv2d-44 [-1, 256, 4, 4] 65,536 InstanceNorm2d-45 [-1, 256, 4, 4] 512 PReLU-46 [-1, 256, 4, 4] 256 Upsample-47 [-1, 256, 8, 8] 0 Conv2d-48 [-1, 256, 8, 8] 589,824 InstanceNorm2d-49 [-1, 256, 8, 8] 512 PReLU-50 [-1, 256, 8, 8] 256 Conv2d-51 [-1, 256, 8, 8] 65,536 Upsample-52 [-1, 256, 8, 8] 0 Conv2d-53 [-1, 256, 8, 8] 65,536 InstanceNorm2d-54 [-1, 256, 8, 8] 512 _BlockBo...neckNd-55 [-1, 256, 8, 8] 0 _BlockResStkNd-56 [-1, 256, 8, 8] 0 InstanceNorm2d-57 [-1, 256, 8, 8] 512 PReLU-58 [-1, 256, 8, 8] 256 Conv2d-59 [-1, 256, 8, 8] 65,536 InstanceNorm2d-60 [-1, 256, 8, 8] 512 PReLU-61 [-1, 256, 8, 8] 256 Conv2d-62 [-1, 256, 8, 8] 589,824 InstanceNorm2d-63 [-1, 256, 8, 8] 512 PReLU-64 [-1, 256, 8, 8] 256 Conv2d-65 [-1, 128, 8, 8] 32,768 Conv2d-66 [-1, 128, 8, 8] 32,768 InstanceNorm2d-67 [-1, 128, 8, 8] 256 _BlockBo...neckNd-68 [-1, 128, 8, 8] 0 InstanceNorm2d-69 [-1, 128, 8, 8] 256 PReLU-70 [-1, 128, 8, 8] 128 Conv2d-71 [-1, 128, 8, 8] 16,384 InstanceNorm2d-72 [-1, 128, 8, 8] 256 PReLU-73 [-1, 128, 8, 8] 128 Upsample-74 [-1, 128, 16, 16] 0 Conv2d-75 [-1, 128, 16, 16] 147,456 InstanceNorm2d-76 [-1, 128, 16, 16] 256 PReLU-77 [-1, 128, 16, 16] 128 Conv2d-78 [-1, 128, 16, 16] 16,384 Upsample-79 [-1, 128, 16, 16] 0 Conv2d-80 [-1, 128, 16, 16] 16,384 InstanceNorm2d-81 [-1, 128, 16, 16] 256 _BlockBo...neckNd-82 [-1, 128, 16, 16] 0 _BlockResStkNd-83 [-1, 128, 16, 16] 0 InstanceNorm2d-84 [-1, 128, 16, 16] 256 PReLU-85 [-1, 128, 16, 16] 128 Conv2d-86 [-1, 128, 16, 16] 16,384 InstanceNorm2d-87 [-1, 128, 16, 16] 256 PReLU-88 [-1, 128, 16, 16] 128 Conv2d-89 [-1, 128, 16, 16] 147,456 InstanceNorm2d-90 [-1, 128, 16, 16] 256 PReLU-91 [-1, 128, 16, 16] 128 Conv2d-92 [-1, 64, 16, 16] 8,192 Conv2d-93 [-1, 64, 16, 16] 8,192 InstanceNorm2d-94 [-1, 64, 16, 16] 128 _BlockBo...neckNd-95 [-1, 64, 16, 16] 0 InstanceNorm2d-96 [-1, 64, 16, 16] 128 PReLU-97 [-1, 64, 16, 16] 64 Conv2d-98 [-1, 64, 16, 16] 4,096 InstanceNorm2d-99 [-1, 64, 16, 16] 128 PReLU-100 [-1, 64, 16, 16] 64 Upsample-101 [-1, 64, 32, 32] 0 Conv2d-102 [-1, 64, 32, 32] 36,864 InstanceNorm2d-103 [-1, 64, 32, 32] 128 PReLU-104 [-1, 64, 32, 32] 64 Conv2d-105 [-1, 64, 32, 32] 4,096 Upsample-106 [-1, 64, 32, 32] 0 Conv2d-107 [-1, 64, 32, 32] 4,096 InstanceNorm2d-108 [-1, 64, 32, 32] 128 _BlockBo...eckNd-109 [-1, 64, 32, 32] 0 _BlockResStkNd-110 [-1, 64, 32, 32] 0 InstanceNorm2d-111 [-1, 64, 32, 32] 128 PReLU-112 [-1, 64, 32, 32] 64 Conv2d-113 [-1, 64, 32, 32] 4,096 InstanceNorm2d-114 [-1, 64, 32, 32] 128 PReLU-115 [-1, 64, 32, 32] 64 Conv2d-116 [-1, 64, 32, 32] 36,864 InstanceNorm2d-117 [-1, 64, 32, 32] 128 PReLU-118 [-1, 64, 32, 32] 64 Conv2d-119 [-1, 64, 32, 32] 4,096 _BlockBo...eckNd-120 [-1, 64, 32, 32] 0 InstanceNorm2d-121 [-1, 64, 32, 32] 128 PReLU-122 [-1, 64, 32, 32] 64 Conv2d-123 [-1, 64, 32, 32] 4,096 InstanceNorm2d-124 [-1, 64, 32, 32] 128 PReLU-125 [-1, 64, 32, 32] 64 Upsample-126 [-1, 64, 64, 64] 0 Conv2d-127 [-1, 64, 64, 64] 36,864 InstanceNorm2d-128 [-1, 64, 64, 64] 128 PReLU-129 [-1, 64, 64, 64] 64 Conv2d-130 [-1, 64, 64, 64] 4,096 Upsample-131 [-1, 64, 64, 64] 0 Conv2d-132 [-1, 64, 64, 64] 4,096 InstanceNorm2d-133 [-1, 64, 64, 64] 128 _BlockBo...eckNd-134 [-1, 64, 64, 64] 0 _BlockResStkNd-135 [-1, 64, 64, 64] 0 Conv2d-136 [-1, 3, 64, 63] 4,803 DecoderNet2d-137 [-1, 3, 64, 63] 0 ================================================================ Total params: 29,196,291 Trainable params: 29,196,291 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 42.98 Params size (MB): 111.38 Estimated Total Size (MB): 154.36 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = None , out_size = ( 64 , 63 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 32. The input size is (1024, 2, 2). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-2 [-1, 1024, 2, 2] 2,048 PReLU-3 [-1, 1024, 2, 2] 1,024 Conv2d-4 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-5 [-1, 1024, 2, 2] 2,048 PReLU-6 [-1, 1024, 2, 2] 1,024 Conv2d-7 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-8 [-1, 1024, 2, 2] 2,048 PReLU-9 [-1, 1024, 2, 2] 1,024 Conv2d-10 [-1, 512, 2, 2] 524,288 Conv2d-11 [-1, 512, 2, 2] 524,288 InstanceNorm2d-12 [-1, 512, 2, 2] 1,024 _BlockBo...neckNd-13 [-1, 512, 2, 2] 0 InstanceNorm2d-14 [-1, 512, 2, 2] 1,024 PReLU-15 [-1, 512, 2, 2] 512 Conv2d-16 [-1, 512, 2, 2] 262,144 InstanceNorm2d-17 [-1, 512, 2, 2] 1,024 PReLU-18 [-1, 512, 2, 2] 512 Upsample-19 [-1, 512, 4, 4] 0 Conv2d-20 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-21 [-1, 512, 4, 4] 1,024 PReLU-22 [-1, 512, 4, 4] 512 Conv2d-23 [-1, 512, 4, 4] 262,144 Upsample-24 [-1, 512, 4, 4] 0 Conv2d-25 [-1, 512, 4, 4] 262,144 InstanceNorm2d-26 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-27 [-1, 512, 4, 4] 0 _BlockResStkNd-28 [-1, 512, 4, 4] 0 InstanceNorm2d-29 [-1, 512, 4, 4] 1,024 PReLU-30 [-1, 512, 4, 4] 512 Conv2d-31 [-1, 512, 4, 4] 262,144 InstanceNorm2d-32 [-1, 512, 4, 4] 1,024 PReLU-33 [-1, 512, 4, 4] 512 Conv2d-34 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-35 [-1, 512, 4, 4] 1,024 PReLU-36 [-1, 512, 4, 4] 512 Conv2d-37 [-1, 256, 4, 4] 131,072 Conv2d-38 [-1, 256, 4, 4] 131,072 InstanceNorm2d-39 [-1, 256, 4, 4] 512 _BlockBo...neckNd-40 [-1, 256, 4, 4] 0 InstanceNorm2d-41 [-1, 256, 4, 4] 512 PReLU-42 [-1, 256, 4, 4] 256 Conv2d-43 [-1, 256, 4, 4] 65,536 InstanceNorm2d-44 [-1, 256, 4, 4] 512 PReLU-45 [-1, 256, 4, 4] 256 Upsample-46 [-1, 256, 8, 8] 0 Conv2d-47 [-1, 256, 8, 8] 589,824 InstanceNorm2d-48 [-1, 256, 8, 8] 512 PReLU-49 [-1, 256, 8, 8] 256 Conv2d-50 [-1, 256, 8, 8] 65,536 Upsample-51 [-1, 256, 8, 8] 0 Conv2d-52 [-1, 256, 8, 8] 65,536 InstanceNorm2d-53 [-1, 256, 8, 8] 512 _BlockBo...neckNd-54 [-1, 256, 8, 8] 0 _BlockResStkNd-55 [-1, 256, 8, 8] 0 InstanceNorm2d-56 [-1, 256, 8, 8] 512 PReLU-57 [-1, 256, 8, 8] 256 Conv2d-58 [-1, 256, 8, 8] 65,536 InstanceNorm2d-59 [-1, 256, 8, 8] 512 PReLU-60 [-1, 256, 8, 8] 256 Conv2d-61 [-1, 256, 8, 8] 589,824 InstanceNorm2d-62 [-1, 256, 8, 8] 512 PReLU-63 [-1, 256, 8, 8] 256 Conv2d-64 [-1, 128, 8, 8] 32,768 Conv2d-65 [-1, 128, 8, 8] 32,768 InstanceNorm2d-66 [-1, 128, 8, 8] 256 _BlockBo...neckNd-67 [-1, 128, 8, 8] 0 InstanceNorm2d-68 [-1, 128, 8, 8] 256 PReLU-69 [-1, 128, 8, 8] 128 Conv2d-70 [-1, 128, 8, 8] 16,384 InstanceNorm2d-71 [-1, 128, 8, 8] 256 PReLU-72 [-1, 128, 8, 8] 128 Upsample-73 [-1, 128, 16, 16] 0 Conv2d-74 [-1, 128, 16, 16] 147,456 InstanceNorm2d-75 [-1, 128, 16, 16] 256 PReLU-76 [-1, 128, 16, 16] 128 Conv2d-77 [-1, 128, 16, 16] 16,384 Upsample-78 [-1, 128, 16, 16] 0 Conv2d-79 [-1, 128, 16, 16] 16,384 InstanceNorm2d-80 [-1, 128, 16, 16] 256 _BlockBo...neckNd-81 [-1, 128, 16, 16] 0 _BlockResStkNd-82 [-1, 128, 16, 16] 0 InstanceNorm2d-83 [-1, 128, 16, 16] 256 PReLU-84 [-1, 128, 16, 16] 128 Conv2d-85 [-1, 128, 16, 16] 16,384 InstanceNorm2d-86 [-1, 128, 16, 16] 256 PReLU-87 [-1, 128, 16, 16] 128 Conv2d-88 [-1, 128, 16, 16] 147,456 InstanceNorm2d-89 [-1, 128, 16, 16] 256 PReLU-90 [-1, 128, 16, 16] 128 Conv2d-91 [-1, 64, 16, 16] 8,192 Conv2d-92 [-1, 64, 16, 16] 8,192 InstanceNorm2d-93 [-1, 64, 16, 16] 128 _BlockBo...neckNd-94 [-1, 64, 16, 16] 0 InstanceNorm2d-95 [-1, 64, 16, 16] 128 PReLU-96 [-1, 64, 16, 16] 64 Conv2d-97 [-1, 64, 16, 16] 4,096 InstanceNorm2d-98 [-1, 64, 16, 16] 128 PReLU-99 [-1, 64, 16, 16] 64 Upsample-100 [-1, 64, 32, 32] 0 Conv2d-101 [-1, 64, 32, 32] 36,864 InstanceNorm2d-102 [-1, 64, 32, 32] 128 PReLU-103 [-1, 64, 32, 32] 64 Conv2d-104 [-1, 64, 32, 32] 4,096 Upsample-105 [-1, 64, 32, 32] 0 Conv2d-106 [-1, 64, 32, 32] 4,096 InstanceNorm2d-107 [-1, 64, 32, 32] 128 _BlockBo...eckNd-108 [-1, 64, 32, 32] 0 _BlockResStkNd-109 [-1, 64, 32, 32] 0 InstanceNorm2d-110 [-1, 64, 32, 32] 128 PReLU-111 [-1, 64, 32, 32] 64 Conv2d-112 [-1, 64, 32, 32] 4,096 InstanceNorm2d-113 [-1, 64, 32, 32] 128 PReLU-114 [-1, 64, 32, 32] 64 Conv2d-115 [-1, 64, 32, 32] 36,864 InstanceNorm2d-116 [-1, 64, 32, 32] 128 PReLU-117 [-1, 64, 32, 32] 64 Conv2d-118 [-1, 64, 32, 32] 4,096 _BlockBo...eckNd-119 [-1, 64, 32, 32] 0 InstanceNorm2d-120 [-1, 64, 32, 32] 128 PReLU-121 [-1, 64, 32, 32] 64 Conv2d-122 [-1, 64, 32, 32] 4,096 InstanceNorm2d-123 [-1, 64, 32, 32] 128 PReLU-124 [-1, 64, 32, 32] 64 Upsample-125 [-1, 64, 64, 64] 0 Conv2d-126 [-1, 64, 64, 64] 36,864 InstanceNorm2d-127 [-1, 64, 64, 64] 128 PReLU-128 [-1, 64, 64, 64] 64 Conv2d-129 [-1, 64, 64, 64] 4,096 Upsample-130 [-1, 64, 64, 64] 0 Conv2d-131 [-1, 64, 64, 64] 4,096 InstanceNorm2d-132 [-1, 64, 64, 64] 128 _BlockBo...eckNd-133 [-1, 64, 64, 64] 0 _BlockResStkNd-134 [-1, 64, 64, 64] 0 Conv2d-135 [-1, 3, 64, 63] 4,803 DecoderNet2d-136 [-1, 3, 64, 63] 0 ================================================================ Total params: 29,064,195 Trainable params: 29,064,195 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.02 Forward/backward pass size (MB): 42.95 Params size (MB): 110.87 Estimated Total Size (MB): 153.84 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/DecoderNet3d/","text":"modules.resnet.DecoderNet3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . DecoderNet3d ( channel , layers , out_size , block = 'bottleneck' , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 3D residual decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] blocks\"] u2[\"Block 2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] un[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv3d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 3D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the output data size specified by the argument out_size . Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. input_size \u00b6 net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None . Examples \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = 32 , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 33. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 33,792 Conv3d-2 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-3 [-1, 1024, 1, 1, 1] 2,048 PReLU-4 [-1, 1024, 1, 1, 1] 1,024 Conv3d-5 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-6 [-1, 1024, 1, 1, 1] 2,048 PReLU-7 [-1, 1024, 1, 1, 1] 1,024 Conv3d-8 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-9 [-1, 1024, 1, 1, 1] 2,048 PReLU-10 [-1, 1024, 1, 1, 1] 1,024 Conv3d-11 [-1, 512, 1, 1, 1] 524,288 Conv3d-12 [-1, 512, 1, 1, 1] 524,288 InstanceNorm3d-13 [-1, 512, 1, 1, 1] 1,024 _BlockBo...neckNd-14 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-15 [-1, 512, 1, 1, 1] 1,024 PReLU-16 [-1, 512, 1, 1, 1] 512 Conv3d-17 [-1, 512, 1, 1, 1] 262,144 InstanceNorm3d-18 [-1, 512, 1, 1, 1] 1,024 PReLU-19 [-1, 512, 1, 1, 1] 512 Upsample-20 [-1, 512, 2, 2, 2] 0 Conv3d-21 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-22 [-1, 512, 2, 2, 2] 1,024 PReLU-23 [-1, 512, 2, 2, 2] 512 Conv3d-24 [-1, 512, 2, 2, 2] 262,144 Upsample-25 [-1, 512, 2, 2, 2] 0 Conv3d-26 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-27 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-28 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-29 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-30 [-1, 512, 2, 2, 2] 1,024 PReLU-31 [-1, 512, 2, 2, 2] 512 Conv3d-32 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-33 [-1, 512, 2, 2, 2] 1,024 PReLU-34 [-1, 512, 2, 2, 2] 512 Conv3d-35 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-36 [-1, 512, 2, 2, 2] 1,024 PReLU-37 [-1, 512, 2, 2, 2] 512 Conv3d-38 [-1, 256, 2, 2, 2] 131,072 Conv3d-39 [-1, 256, 2, 2, 2] 131,072 InstanceNorm3d-40 [-1, 256, 2, 2, 2] 512 _BlockBo...neckNd-41 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-42 [-1, 256, 2, 2, 2] 512 PReLU-43 [-1, 256, 2, 2, 2] 256 Conv3d-44 [-1, 256, 2, 2, 2] 65,536 InstanceNorm3d-45 [-1, 256, 2, 2, 2] 512 PReLU-46 [-1, 256, 2, 2, 2] 256 Upsample-47 [-1, 256, 4, 4, 4] 0 Conv3d-48 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-49 [-1, 256, 4, 4, 4] 512 PReLU-50 [-1, 256, 4, 4, 4] 256 Conv3d-51 [-1, 256, 4, 4, 4] 65,536 Upsample-52 [-1, 256, 4, 4, 4] 0 Conv3d-53 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-54 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-55 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-56 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-57 [-1, 256, 4, 4, 4] 512 PReLU-58 [-1, 256, 4, 4, 4] 256 Conv3d-59 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-60 [-1, 256, 4, 4, 4] 512 PReLU-61 [-1, 256, 4, 4, 4] 256 Conv3d-62 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-63 [-1, 256, 4, 4, 4] 512 PReLU-64 [-1, 256, 4, 4, 4] 256 Conv3d-65 [-1, 128, 4, 4, 4] 32,768 Conv3d-66 [-1, 128, 4, 4, 4] 32,768 InstanceNorm3d-67 [-1, 128, 4, 4, 4] 256 _BlockBo...neckNd-68 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-69 [-1, 128, 4, 4, 4] 256 PReLU-70 [-1, 128, 4, 4, 4] 128 Conv3d-71 [-1, 128, 4, 4, 4] 16,384 InstanceNorm3d-72 [-1, 128, 4, 4, 4] 256 PReLU-73 [-1, 128, 4, 4, 4] 128 Upsample-74 [-1, 128, 8, 8, 8] 0 Conv3d-75 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-76 [-1, 128, 8, 8, 8] 256 PReLU-77 [-1, 128, 8, 8, 8] 128 Conv3d-78 [-1, 128, 8, 8, 8] 16,384 Upsample-79 [-1, 128, 8, 8, 8] 0 Conv3d-80 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-81 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-82 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-83 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-84 [-1, 128, 8, 8, 8] 256 PReLU-85 [-1, 128, 8, 8, 8] 128 Conv3d-86 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-87 [-1, 128, 8, 8, 8] 256 PReLU-88 [-1, 128, 8, 8, 8] 128 Conv3d-89 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-90 [-1, 128, 8, 8, 8] 256 PReLU-91 [-1, 128, 8, 8, 8] 128 Conv3d-92 [-1, 64, 8, 8, 8] 8,192 Conv3d-93 [-1, 64, 8, 8, 8] 8,192 InstanceNorm3d-94 [-1, 64, 8, 8, 8] 128 _BlockBo...neckNd-95 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-96 [-1, 64, 8, 8, 8] 128 PReLU-97 [-1, 64, 8, 8, 8] 64 Conv3d-98 [-1, 64, 8, 8, 8] 4,096 InstanceNorm3d-99 [-1, 64, 8, 8, 8] 128 PReLU-100 [-1, 64, 8, 8, 8] 64 Upsample-101 [-1, 64, 16, 16, 16] 0 Conv3d-102 [-1, 64, 16, 16, 16] 110,592 InstanceNorm3d-103 [-1, 64, 16, 16, 16] 128 PReLU-104 [-1, 64, 16, 16, 16] 64 Conv3d-105 [-1, 64, 16, 16, 16] 4,096 Upsample-106 [-1, 64, 16, 16, 16] 0 Conv3d-107 [-1, 64, 16, 16, 16] 4,096 InstanceNorm3d-108 [-1, 64, 16, 16, 16] 128 _BlockBo...eckNd-109 [-1, 64, 16, 16, 16] 0 _BlockResStkNd-110 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-111 [-1, 64, 16, 16, 15] 128 PReLU-112 [-1, 64, 16, 16, 15] 64 Conv3d-113 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-114 [-1, 64, 16, 16, 15] 128 PReLU-115 [-1, 64, 16, 16, 15] 64 Conv3d-116 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-117 [-1, 64, 16, 16, 15] 128 PReLU-118 [-1, 64, 16, 16, 15] 64 Conv3d-119 [-1, 64, 16, 16, 15] 4,096 _BlockBo...eckNd-120 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-121 [-1, 64, 16, 16, 15] 128 PReLU-122 [-1, 64, 16, 16, 15] 64 Conv3d-123 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-124 [-1, 64, 16, 16, 15] 128 PReLU-125 [-1, 64, 16, 16, 15] 64 Upsample-126 [-1, 64, 32, 32, 30] 0 Conv3d-127 [-1, 64, 32, 32, 30] 110,592 InstanceNorm3d-128 [-1, 64, 32, 32, 30] 128 PReLU-129 [-1, 64, 32, 32, 30] 64 Conv3d-130 [-1, 64, 32, 32, 30] 4,096 Upsample-131 [-1, 64, 32, 32, 30] 0 Conv3d-132 [-1, 64, 32, 32, 30] 4,096 InstanceNorm3d-133 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-134 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-135 [-1, 64, 32, 32, 30] 0 Conv3d-136 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-137 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 79,473,411 Trainable params: 79,473,411 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 214.37 Params size (MB): 303.17 Estimated Total Size (MB): 517.53 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = None , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 32. The input size is (1024, 1, 1, 1). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-2 [-1, 1024, 1, 1, 1] 2,048 PReLU-3 [-1, 1024, 1, 1, 1] 1,024 Conv3d-4 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-5 [-1, 1024, 1, 1, 1] 2,048 PReLU-6 [-1, 1024, 1, 1, 1] 1,024 Conv3d-7 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-8 [-1, 1024, 1, 1, 1] 2,048 PReLU-9 [-1, 1024, 1, 1, 1] 1,024 Conv3d-10 [-1, 512, 1, 1, 1] 524,288 Conv3d-11 [-1, 512, 1, 1, 1] 524,288 InstanceNorm3d-12 [-1, 512, 1, 1, 1] 1,024 _BlockBo...neckNd-13 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-14 [-1, 512, 1, 1, 1] 1,024 PReLU-15 [-1, 512, 1, 1, 1] 512 Conv3d-16 [-1, 512, 1, 1, 1] 262,144 InstanceNorm3d-17 [-1, 512, 1, 1, 1] 1,024 PReLU-18 [-1, 512, 1, 1, 1] 512 Upsample-19 [-1, 512, 2, 2, 2] 0 Conv3d-20 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-21 [-1, 512, 2, 2, 2] 1,024 PReLU-22 [-1, 512, 2, 2, 2] 512 Conv3d-23 [-1, 512, 2, 2, 2] 262,144 Upsample-24 [-1, 512, 2, 2, 2] 0 Conv3d-25 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-26 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-27 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-28 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-29 [-1, 512, 2, 2, 2] 1,024 PReLU-30 [-1, 512, 2, 2, 2] 512 Conv3d-31 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-32 [-1, 512, 2, 2, 2] 1,024 PReLU-33 [-1, 512, 2, 2, 2] 512 Conv3d-34 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-35 [-1, 512, 2, 2, 2] 1,024 PReLU-36 [-1, 512, 2, 2, 2] 512 Conv3d-37 [-1, 256, 2, 2, 2] 131,072 Conv3d-38 [-1, 256, 2, 2, 2] 131,072 InstanceNorm3d-39 [-1, 256, 2, 2, 2] 512 _BlockBo...neckNd-40 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-41 [-1, 256, 2, 2, 2] 512 PReLU-42 [-1, 256, 2, 2, 2] 256 Conv3d-43 [-1, 256, 2, 2, 2] 65,536 InstanceNorm3d-44 [-1, 256, 2, 2, 2] 512 PReLU-45 [-1, 256, 2, 2, 2] 256 Upsample-46 [-1, 256, 4, 4, 4] 0 Conv3d-47 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-48 [-1, 256, 4, 4, 4] 512 PReLU-49 [-1, 256, 4, 4, 4] 256 Conv3d-50 [-1, 256, 4, 4, 4] 65,536 Upsample-51 [-1, 256, 4, 4, 4] 0 Conv3d-52 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-53 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-54 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-55 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-56 [-1, 256, 4, 4, 4] 512 PReLU-57 [-1, 256, 4, 4, 4] 256 Conv3d-58 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-59 [-1, 256, 4, 4, 4] 512 PReLU-60 [-1, 256, 4, 4, 4] 256 Conv3d-61 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-62 [-1, 256, 4, 4, 4] 512 PReLU-63 [-1, 256, 4, 4, 4] 256 Conv3d-64 [-1, 128, 4, 4, 4] 32,768 Conv3d-65 [-1, 128, 4, 4, 4] 32,768 InstanceNorm3d-66 [-1, 128, 4, 4, 4] 256 _BlockBo...neckNd-67 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-68 [-1, 128, 4, 4, 4] 256 PReLU-69 [-1, 128, 4, 4, 4] 128 Conv3d-70 [-1, 128, 4, 4, 4] 16,384 InstanceNorm3d-71 [-1, 128, 4, 4, 4] 256 PReLU-72 [-1, 128, 4, 4, 4] 128 Upsample-73 [-1, 128, 8, 8, 8] 0 Conv3d-74 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-75 [-1, 128, 8, 8, 8] 256 PReLU-76 [-1, 128, 8, 8, 8] 128 Conv3d-77 [-1, 128, 8, 8, 8] 16,384 Upsample-78 [-1, 128, 8, 8, 8] 0 Conv3d-79 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-80 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-81 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-82 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-83 [-1, 128, 8, 8, 8] 256 PReLU-84 [-1, 128, 8, 8, 8] 128 Conv3d-85 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-86 [-1, 128, 8, 8, 8] 256 PReLU-87 [-1, 128, 8, 8, 8] 128 Conv3d-88 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-89 [-1, 128, 8, 8, 8] 256 PReLU-90 [-1, 128, 8, 8, 8] 128 Conv3d-91 [-1, 64, 8, 8, 8] 8,192 Conv3d-92 [-1, 64, 8, 8, 8] 8,192 InstanceNorm3d-93 [-1, 64, 8, 8, 8] 128 _BlockBo...neckNd-94 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-95 [-1, 64, 8, 8, 8] 128 PReLU-96 [-1, 64, 8, 8, 8] 64 Conv3d-97 [-1, 64, 8, 8, 8] 4,096 InstanceNorm3d-98 [-1, 64, 8, 8, 8] 128 PReLU-99 [-1, 64, 8, 8, 8] 64 Upsample-100 [-1, 64, 16, 16, 16] 0 Conv3d-101 [-1, 64, 16, 16, 16] 110,592 InstanceNorm3d-102 [-1, 64, 16, 16, 16] 128 PReLU-103 [-1, 64, 16, 16, 16] 64 Conv3d-104 [-1, 64, 16, 16, 16] 4,096 Upsample-105 [-1, 64, 16, 16, 16] 0 Conv3d-106 [-1, 64, 16, 16, 16] 4,096 InstanceNorm3d-107 [-1, 64, 16, 16, 16] 128 _BlockBo...eckNd-108 [-1, 64, 16, 16, 16] 0 _BlockResStkNd-109 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-110 [-1, 64, 16, 16, 15] 128 PReLU-111 [-1, 64, 16, 16, 15] 64 Conv3d-112 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-113 [-1, 64, 16, 16, 15] 128 PReLU-114 [-1, 64, 16, 16, 15] 64 Conv3d-115 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-116 [-1, 64, 16, 16, 15] 128 PReLU-117 [-1, 64, 16, 16, 15] 64 Conv3d-118 [-1, 64, 16, 16, 15] 4,096 _BlockBo...eckNd-119 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-120 [-1, 64, 16, 16, 15] 128 PReLU-121 [-1, 64, 16, 16, 15] 64 Conv3d-122 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-123 [-1, 64, 16, 16, 15] 128 PReLU-124 [-1, 64, 16, 16, 15] 64 Upsample-125 [-1, 64, 32, 32, 30] 0 Conv3d-126 [-1, 64, 32, 32, 30] 110,592 InstanceNorm3d-127 [-1, 64, 32, 32, 30] 128 PReLU-128 [-1, 64, 32, 32, 30] 64 Conv3d-129 [-1, 64, 32, 32, 30] 4,096 Upsample-130 [-1, 64, 32, 32, 30] 0 Conv3d-131 [-1, 64, 32, 32, 30] 4,096 InstanceNorm3d-132 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-133 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-134 [-1, 64, 32, 32, 30] 0 Conv3d-135 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-136 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 79,439,619 Trainable params: 79,439,619 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 214.36 Params size (MB): 303.04 Estimated Total Size (MB): 517.40 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>DecoderNet3d</span>"},{"location":"apis/modules/resnet/DecoderNet3d/#modulesresnetdecodernet3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . DecoderNet3d ( channel , layers , out_size , block = 'bottleneck' , kernel_size = 3 , in_length = 2 , out_planes = 1 ) This moule is a built-in model for 3D residual decoder network. This network could be used as a part of the auto-encoder, or just a network for up-sampling (or generating) data. The network would up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB u1[\"Block 1<br>Stack of layers[0] blocks\"] u2[\"Block 2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] un[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] cin[\"Conv3d<br>with unsqueeze\"] end u1 -->|up<br>sampling| u2 -->|up<br>sampling| ui -->|up<br>sampling| un cin -.-> u1 linkStyle 0,1,2 stroke-width:4px, stroke:#080 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the up-sampling route. An optional unsqueezer and convolutional layer could be prepended to the first layer when the argument in_length != None . This optional layer is used for converting the vector features in initial feature maps.","title":"modules.resnet.DecoderNet3d"},{"location":"apis/modules/resnet/DecoderNet3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. out_size int or ( int , int , int ) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/DecoderNet3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/DecoderNet3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input data is a tensor with a size determined by configurations. The output is a 3D tensor. The channel number of the output is specified by the argument out_planes . Requries Argument Type Description x torch . Tensor A tensor, When in_length is None : the size should be ( B , L ) , where B is the batch size, and L is in_length . When in_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the input feature maps (see input_size ) respectively. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the output data size specified by the argument out_size .","title":" __call__"},{"location":"apis/modules/resnet/DecoderNet3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/DecoderNet3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/resnet/DecoderNet3d/#input_size","text":"net . input_size The size of the input data size (a tuple ). This property is useful when in_length is None . In this case, the input size is determined by the network. Warning This size contains the channel number (as the first element), because the input channel number is also determined by network when in_length is None .","title":" input_size"},{"location":"apis/modules/resnet/DecoderNet3d/#examples","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = 32 , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 33. The input size is (32,). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 33,792 Conv3d-2 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-3 [-1, 1024, 1, 1, 1] 2,048 PReLU-4 [-1, 1024, 1, 1, 1] 1,024 Conv3d-5 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-6 [-1, 1024, 1, 1, 1] 2,048 PReLU-7 [-1, 1024, 1, 1, 1] 1,024 Conv3d-8 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-9 [-1, 1024, 1, 1, 1] 2,048 PReLU-10 [-1, 1024, 1, 1, 1] 1,024 Conv3d-11 [-1, 512, 1, 1, 1] 524,288 Conv3d-12 [-1, 512, 1, 1, 1] 524,288 InstanceNorm3d-13 [-1, 512, 1, 1, 1] 1,024 _BlockBo...neckNd-14 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-15 [-1, 512, 1, 1, 1] 1,024 PReLU-16 [-1, 512, 1, 1, 1] 512 Conv3d-17 [-1, 512, 1, 1, 1] 262,144 InstanceNorm3d-18 [-1, 512, 1, 1, 1] 1,024 PReLU-19 [-1, 512, 1, 1, 1] 512 Upsample-20 [-1, 512, 2, 2, 2] 0 Conv3d-21 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-22 [-1, 512, 2, 2, 2] 1,024 PReLU-23 [-1, 512, 2, 2, 2] 512 Conv3d-24 [-1, 512, 2, 2, 2] 262,144 Upsample-25 [-1, 512, 2, 2, 2] 0 Conv3d-26 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-27 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-28 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-29 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-30 [-1, 512, 2, 2, 2] 1,024 PReLU-31 [-1, 512, 2, 2, 2] 512 Conv3d-32 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-33 [-1, 512, 2, 2, 2] 1,024 PReLU-34 [-1, 512, 2, 2, 2] 512 Conv3d-35 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-36 [-1, 512, 2, 2, 2] 1,024 PReLU-37 [-1, 512, 2, 2, 2] 512 Conv3d-38 [-1, 256, 2, 2, 2] 131,072 Conv3d-39 [-1, 256, 2, 2, 2] 131,072 InstanceNorm3d-40 [-1, 256, 2, 2, 2] 512 _BlockBo...neckNd-41 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-42 [-1, 256, 2, 2, 2] 512 PReLU-43 [-1, 256, 2, 2, 2] 256 Conv3d-44 [-1, 256, 2, 2, 2] 65,536 InstanceNorm3d-45 [-1, 256, 2, 2, 2] 512 PReLU-46 [-1, 256, 2, 2, 2] 256 Upsample-47 [-1, 256, 4, 4, 4] 0 Conv3d-48 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-49 [-1, 256, 4, 4, 4] 512 PReLU-50 [-1, 256, 4, 4, 4] 256 Conv3d-51 [-1, 256, 4, 4, 4] 65,536 Upsample-52 [-1, 256, 4, 4, 4] 0 Conv3d-53 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-54 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-55 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-56 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-57 [-1, 256, 4, 4, 4] 512 PReLU-58 [-1, 256, 4, 4, 4] 256 Conv3d-59 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-60 [-1, 256, 4, 4, 4] 512 PReLU-61 [-1, 256, 4, 4, 4] 256 Conv3d-62 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-63 [-1, 256, 4, 4, 4] 512 PReLU-64 [-1, 256, 4, 4, 4] 256 Conv3d-65 [-1, 128, 4, 4, 4] 32,768 Conv3d-66 [-1, 128, 4, 4, 4] 32,768 InstanceNorm3d-67 [-1, 128, 4, 4, 4] 256 _BlockBo...neckNd-68 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-69 [-1, 128, 4, 4, 4] 256 PReLU-70 [-1, 128, 4, 4, 4] 128 Conv3d-71 [-1, 128, 4, 4, 4] 16,384 InstanceNorm3d-72 [-1, 128, 4, 4, 4] 256 PReLU-73 [-1, 128, 4, 4, 4] 128 Upsample-74 [-1, 128, 8, 8, 8] 0 Conv3d-75 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-76 [-1, 128, 8, 8, 8] 256 PReLU-77 [-1, 128, 8, 8, 8] 128 Conv3d-78 [-1, 128, 8, 8, 8] 16,384 Upsample-79 [-1, 128, 8, 8, 8] 0 Conv3d-80 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-81 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-82 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-83 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-84 [-1, 128, 8, 8, 8] 256 PReLU-85 [-1, 128, 8, 8, 8] 128 Conv3d-86 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-87 [-1, 128, 8, 8, 8] 256 PReLU-88 [-1, 128, 8, 8, 8] 128 Conv3d-89 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-90 [-1, 128, 8, 8, 8] 256 PReLU-91 [-1, 128, 8, 8, 8] 128 Conv3d-92 [-1, 64, 8, 8, 8] 8,192 Conv3d-93 [-1, 64, 8, 8, 8] 8,192 InstanceNorm3d-94 [-1, 64, 8, 8, 8] 128 _BlockBo...neckNd-95 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-96 [-1, 64, 8, 8, 8] 128 PReLU-97 [-1, 64, 8, 8, 8] 64 Conv3d-98 [-1, 64, 8, 8, 8] 4,096 InstanceNorm3d-99 [-1, 64, 8, 8, 8] 128 PReLU-100 [-1, 64, 8, 8, 8] 64 Upsample-101 [-1, 64, 16, 16, 16] 0 Conv3d-102 [-1, 64, 16, 16, 16] 110,592 InstanceNorm3d-103 [-1, 64, 16, 16, 16] 128 PReLU-104 [-1, 64, 16, 16, 16] 64 Conv3d-105 [-1, 64, 16, 16, 16] 4,096 Upsample-106 [-1, 64, 16, 16, 16] 0 Conv3d-107 [-1, 64, 16, 16, 16] 4,096 InstanceNorm3d-108 [-1, 64, 16, 16, 16] 128 _BlockBo...eckNd-109 [-1, 64, 16, 16, 16] 0 _BlockResStkNd-110 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-111 [-1, 64, 16, 16, 15] 128 PReLU-112 [-1, 64, 16, 16, 15] 64 Conv3d-113 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-114 [-1, 64, 16, 16, 15] 128 PReLU-115 [-1, 64, 16, 16, 15] 64 Conv3d-116 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-117 [-1, 64, 16, 16, 15] 128 PReLU-118 [-1, 64, 16, 16, 15] 64 Conv3d-119 [-1, 64, 16, 16, 15] 4,096 _BlockBo...eckNd-120 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-121 [-1, 64, 16, 16, 15] 128 PReLU-122 [-1, 64, 16, 16, 15] 64 Conv3d-123 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-124 [-1, 64, 16, 16, 15] 128 PReLU-125 [-1, 64, 16, 16, 15] 64 Upsample-126 [-1, 64, 32, 32, 30] 0 Conv3d-127 [-1, 64, 32, 32, 30] 110,592 InstanceNorm3d-128 [-1, 64, 32, 32, 30] 128 PReLU-129 [-1, 64, 32, 32, 30] 64 Conv3d-130 [-1, 64, 32, 32, 30] 4,096 Upsample-131 [-1, 64, 32, 32, 30] 0 Conv3d-132 [-1, 64, 32, 32, 30] 4,096 InstanceNorm3d-133 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-134 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-135 [-1, 64, 32, 32, 30] 0 Conv3d-136 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-137 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 79,473,411 Trainable params: 79,473,411 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 214.37 Params size (MB): 303.17 Estimated Total Size (MB): 517.53 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 6 import mdnc net = mdnc . modules . resnet . DecoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_length = None , out_size = ( 31 , 32 , 30 ), out_planes = 3 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) print ( 'The input size is {0} .' . format ( net . input_size )) mdnc . contribs . torchsummary . summary ( net , net . input_size , device = 'cpu' ) Output The number of convolutional layers along the depth is 32. The input size is (1024, 1, 1, 1). ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-2 [-1, 1024, 1, 1, 1] 2,048 PReLU-3 [-1, 1024, 1, 1, 1] 1,024 Conv3d-4 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-5 [-1, 1024, 1, 1, 1] 2,048 PReLU-6 [-1, 1024, 1, 1, 1] 1,024 Conv3d-7 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-8 [-1, 1024, 1, 1, 1] 2,048 PReLU-9 [-1, 1024, 1, 1, 1] 1,024 Conv3d-10 [-1, 512, 1, 1, 1] 524,288 Conv3d-11 [-1, 512, 1, 1, 1] 524,288 InstanceNorm3d-12 [-1, 512, 1, 1, 1] 1,024 _BlockBo...neckNd-13 [-1, 512, 1, 1, 1] 0 InstanceNorm3d-14 [-1, 512, 1, 1, 1] 1,024 PReLU-15 [-1, 512, 1, 1, 1] 512 Conv3d-16 [-1, 512, 1, 1, 1] 262,144 InstanceNorm3d-17 [-1, 512, 1, 1, 1] 1,024 PReLU-18 [-1, 512, 1, 1, 1] 512 Upsample-19 [-1, 512, 2, 2, 2] 0 Conv3d-20 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-21 [-1, 512, 2, 2, 2] 1,024 PReLU-22 [-1, 512, 2, 2, 2] 512 Conv3d-23 [-1, 512, 2, 2, 2] 262,144 Upsample-24 [-1, 512, 2, 2, 2] 0 Conv3d-25 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-26 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-27 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-28 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-29 [-1, 512, 2, 2, 2] 1,024 PReLU-30 [-1, 512, 2, 2, 2] 512 Conv3d-31 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-32 [-1, 512, 2, 2, 2] 1,024 PReLU-33 [-1, 512, 2, 2, 2] 512 Conv3d-34 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-35 [-1, 512, 2, 2, 2] 1,024 PReLU-36 [-1, 512, 2, 2, 2] 512 Conv3d-37 [-1, 256, 2, 2, 2] 131,072 Conv3d-38 [-1, 256, 2, 2, 2] 131,072 InstanceNorm3d-39 [-1, 256, 2, 2, 2] 512 _BlockBo...neckNd-40 [-1, 256, 2, 2, 2] 0 InstanceNorm3d-41 [-1, 256, 2, 2, 2] 512 PReLU-42 [-1, 256, 2, 2, 2] 256 Conv3d-43 [-1, 256, 2, 2, 2] 65,536 InstanceNorm3d-44 [-1, 256, 2, 2, 2] 512 PReLU-45 [-1, 256, 2, 2, 2] 256 Upsample-46 [-1, 256, 4, 4, 4] 0 Conv3d-47 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-48 [-1, 256, 4, 4, 4] 512 PReLU-49 [-1, 256, 4, 4, 4] 256 Conv3d-50 [-1, 256, 4, 4, 4] 65,536 Upsample-51 [-1, 256, 4, 4, 4] 0 Conv3d-52 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-53 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-54 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-55 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-56 [-1, 256, 4, 4, 4] 512 PReLU-57 [-1, 256, 4, 4, 4] 256 Conv3d-58 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-59 [-1, 256, 4, 4, 4] 512 PReLU-60 [-1, 256, 4, 4, 4] 256 Conv3d-61 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-62 [-1, 256, 4, 4, 4] 512 PReLU-63 [-1, 256, 4, 4, 4] 256 Conv3d-64 [-1, 128, 4, 4, 4] 32,768 Conv3d-65 [-1, 128, 4, 4, 4] 32,768 InstanceNorm3d-66 [-1, 128, 4, 4, 4] 256 _BlockBo...neckNd-67 [-1, 128, 4, 4, 4] 0 InstanceNorm3d-68 [-1, 128, 4, 4, 4] 256 PReLU-69 [-1, 128, 4, 4, 4] 128 Conv3d-70 [-1, 128, 4, 4, 4] 16,384 InstanceNorm3d-71 [-1, 128, 4, 4, 4] 256 PReLU-72 [-1, 128, 4, 4, 4] 128 Upsample-73 [-1, 128, 8, 8, 8] 0 Conv3d-74 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-75 [-1, 128, 8, 8, 8] 256 PReLU-76 [-1, 128, 8, 8, 8] 128 Conv3d-77 [-1, 128, 8, 8, 8] 16,384 Upsample-78 [-1, 128, 8, 8, 8] 0 Conv3d-79 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-80 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-81 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-82 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-83 [-1, 128, 8, 8, 8] 256 PReLU-84 [-1, 128, 8, 8, 8] 128 Conv3d-85 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-86 [-1, 128, 8, 8, 8] 256 PReLU-87 [-1, 128, 8, 8, 8] 128 Conv3d-88 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-89 [-1, 128, 8, 8, 8] 256 PReLU-90 [-1, 128, 8, 8, 8] 128 Conv3d-91 [-1, 64, 8, 8, 8] 8,192 Conv3d-92 [-1, 64, 8, 8, 8] 8,192 InstanceNorm3d-93 [-1, 64, 8, 8, 8] 128 _BlockBo...neckNd-94 [-1, 64, 8, 8, 8] 0 InstanceNorm3d-95 [-1, 64, 8, 8, 8] 128 PReLU-96 [-1, 64, 8, 8, 8] 64 Conv3d-97 [-1, 64, 8, 8, 8] 4,096 InstanceNorm3d-98 [-1, 64, 8, 8, 8] 128 PReLU-99 [-1, 64, 8, 8, 8] 64 Upsample-100 [-1, 64, 16, 16, 16] 0 Conv3d-101 [-1, 64, 16, 16, 16] 110,592 InstanceNorm3d-102 [-1, 64, 16, 16, 16] 128 PReLU-103 [-1, 64, 16, 16, 16] 64 Conv3d-104 [-1, 64, 16, 16, 16] 4,096 Upsample-105 [-1, 64, 16, 16, 16] 0 Conv3d-106 [-1, 64, 16, 16, 16] 4,096 InstanceNorm3d-107 [-1, 64, 16, 16, 16] 128 _BlockBo...eckNd-108 [-1, 64, 16, 16, 16] 0 _BlockResStkNd-109 [-1, 64, 16, 16, 16] 0 InstanceNorm3d-110 [-1, 64, 16, 16, 15] 128 PReLU-111 [-1, 64, 16, 16, 15] 64 Conv3d-112 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-113 [-1, 64, 16, 16, 15] 128 PReLU-114 [-1, 64, 16, 16, 15] 64 Conv3d-115 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-116 [-1, 64, 16, 16, 15] 128 PReLU-117 [-1, 64, 16, 16, 15] 64 Conv3d-118 [-1, 64, 16, 16, 15] 4,096 _BlockBo...eckNd-119 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-120 [-1, 64, 16, 16, 15] 128 PReLU-121 [-1, 64, 16, 16, 15] 64 Conv3d-122 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-123 [-1, 64, 16, 16, 15] 128 PReLU-124 [-1, 64, 16, 16, 15] 64 Upsample-125 [-1, 64, 32, 32, 30] 0 Conv3d-126 [-1, 64, 32, 32, 30] 110,592 InstanceNorm3d-127 [-1, 64, 32, 32, 30] 128 PReLU-128 [-1, 64, 32, 32, 30] 64 Conv3d-129 [-1, 64, 32, 32, 30] 4,096 Upsample-130 [-1, 64, 32, 32, 30] 0 Conv3d-131 [-1, 64, 32, 32, 30] 4,096 InstanceNorm3d-132 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-133 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-134 [-1, 64, 32, 32, 30] 0 Conv3d-135 [-1, 3, 31, 32, 30] 24,003 DecoderNet3d-136 [-1, 3, 31, 32, 30] 0 ================================================================ Total params: 79,439,619 Trainable params: 79,439,619 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 214.36 Params size (MB): 303.04 Estimated Total Size (MB): 517.40 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/EncoderNet1d/","text":"modules.resnet.EncoderNet1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . EncoderNet1d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 1D residual encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None . Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the last output stage (block) respectively. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. Examples \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 4] 3,145,728 InstanceNorm1d-118 [-1, 1024, 4] 2,048 PReLU-119 [-1, 1024, 4] 1,024 Conv1d-120 [-1, 1024, 4] 1,048,576 Conv1d-121 [-1, 1024, 4] 1,048,576 InstanceNorm1d-122 [-1, 1024, 4] 2,048 _BlockBo...eckNd-123 [-1, 1024, 4] 0 _BlockResStkNd-124 [-1, 1024, 4] 0 Conv1d-125 [-1, 1024, 4] 3,146,752 Adaptive...ool1d-126 [-1, 1024, 1] 0 Linear-127 [-1, 32] 32,800 EncoderNet1d-128 [-1, 32] 0 ================================================================ Total params: 14,401,568 Trainable params: 14,401,568 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 5.54 Params size (MB): 54.94 Estimated Total Size (MB): 60.48 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 4] 3,145,728 InstanceNorm1d-118 [-1, 1024, 4] 2,048 PReLU-119 [-1, 1024, 4] 1,024 Conv1d-120 [-1, 1024, 4] 1,048,576 Conv1d-121 [-1, 1024, 4] 1,048,576 InstanceNorm1d-122 [-1, 1024, 4] 2,048 _BlockBo...eckNd-123 [-1, 1024, 4] 0 _BlockResStkNd-124 [-1, 1024, 4] 0 Conv1d-125 [-1, 1024, 4] 3,146,752 EncoderNet1d-126 [-1, 1024, 4] 0 ================================================================ Total params: 14,368,768 Trainable params: 14,368,768 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 5.56 Params size (MB): 54.81 Estimated Total Size (MB): 60.38 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>EncoderNet1d</span>"},{"location":"apis/modules/resnet/EncoderNet1d/#modulesresnetencodernet1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . EncoderNet1d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 1D residual encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None .","title":"modules.resnet.EncoderNet1d"},{"location":"apis/modules/resnet/EncoderNet1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/resnet/EncoderNet1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/EncoderNet1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L ) , where B is the batch size, C and L are the channel number and the length of the last output stage (block) respectively.","title":" __call__"},{"location":"apis/modules/resnet/EncoderNet1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/EncoderNet1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/resnet/EncoderNet1d/#examples","text":"Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 4] 3,145,728 InstanceNorm1d-118 [-1, 1024, 4] 2,048 PReLU-119 [-1, 1024, 4] 1,024 Conv1d-120 [-1, 1024, 4] 1,048,576 Conv1d-121 [-1, 1024, 4] 1,048,576 InstanceNorm1d-122 [-1, 1024, 4] 2,048 _BlockBo...eckNd-123 [-1, 1024, 4] 0 _BlockResStkNd-124 [-1, 1024, 4] 0 Conv1d-125 [-1, 1024, 4] 3,146,752 Adaptive...ool1d-126 [-1, 1024, 1] 0 Linear-127 [-1, 32] 32,800 EncoderNet1d-128 [-1, 32] 0 ================================================================ Total params: 14,401,568 Trainable params: 14,401,568 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 5.54 Params size (MB): 54.94 Estimated Total Size (MB): 60.48 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet1d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 4] 3,145,728 InstanceNorm1d-118 [-1, 1024, 4] 2,048 PReLU-119 [-1, 1024, 4] 1,024 Conv1d-120 [-1, 1024, 4] 1,048,576 Conv1d-121 [-1, 1024, 4] 1,048,576 InstanceNorm1d-122 [-1, 1024, 4] 2,048 _BlockBo...eckNd-123 [-1, 1024, 4] 0 _BlockResStkNd-124 [-1, 1024, 4] 0 Conv1d-125 [-1, 1024, 4] 3,146,752 EncoderNet1d-126 [-1, 1024, 4] 0 ================================================================ Total params: 14,368,768 Trainable params: 14,368,768 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 5.56 Params size (MB): 54.81 Estimated Total Size (MB): 60.38 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/EncoderNet2d/","text":"modules.resnet.EncoderNet2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . EncoderNet2d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 2D residual encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None . Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the last output stage (block) respectively. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. Examples \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-118 [-1, 1024, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2] 1,024 Conv2d-120 [-1, 1024, 2, 2] 1,048,576 Conv2d-121 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-122 [-1, 1024, 2, 2] 2,048 _BlockBo...eckNd-123 [-1, 1024, 2, 2] 0 _BlockResStkNd-124 [-1, 1024, 2, 2] 0 Conv2d-125 [-1, 1024, 2, 2] 9,438,208 Adaptive...ool2d-126 [-1, 1024, 1, 1] 0 Linear-127 [-1, 32] 32,800 EncoderNet2d-128 [-1, 32] 0 ================================================================ Total params: 31,190,816 Trainable params: 31,190,816 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 63.66 Params size (MB): 118.98 Estimated Total Size (MB): 182.69 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-118 [-1, 1024, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2] 1,024 Conv2d-120 [-1, 1024, 2, 2] 1,048,576 Conv2d-121 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-122 [-1, 1024, 2, 2] 2,048 _BlockBo...eckNd-123 [-1, 1024, 2, 2] 0 _BlockResStkNd-124 [-1, 1024, 2, 2] 0 Conv2d-125 [-1, 1024, 2, 2] 9,438,208 EncoderNet2d-126 [-1, 1024, 2, 2] 0 ================================================================ Total params: 31,158,016 Trainable params: 31,158,016 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 63.69 Params size (MB): 118.86 Estimated Total Size (MB): 182.59 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>EncoderNet2d</span>"},{"location":"apis/modules/resnet/EncoderNet2d/#modulesresnetencodernet2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . EncoderNet2d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 2D residual encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None .","title":"modules.resnet.EncoderNet2d"},{"location":"apis/modules/resnet/EncoderNet2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/resnet/EncoderNet2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/EncoderNet2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 ) , where B is the batch size, C and (L1, L2) are the channel number and the size of the last output stage (block) respectively.","title":" __call__"},{"location":"apis/modules/resnet/EncoderNet2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/EncoderNet2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/resnet/EncoderNet2d/#examples","text":"Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-118 [-1, 1024, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2] 1,024 Conv2d-120 [-1, 1024, 2, 2] 1,048,576 Conv2d-121 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-122 [-1, 1024, 2, 2] 2,048 _BlockBo...eckNd-123 [-1, 1024, 2, 2] 0 _BlockResStkNd-124 [-1, 1024, 2, 2] 0 Conv2d-125 [-1, 1024, 2, 2] 9,438,208 Adaptive...ool2d-126 [-1, 1024, 1, 1] 0 Linear-127 [-1, 32] 32,800 EncoderNet2d-128 [-1, 32] 0 ================================================================ Total params: 31,190,816 Trainable params: 31,190,816 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 63.66 Params size (MB): 118.98 Estimated Total Size (MB): 182.69 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet2d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 2, 2] 9,437,184 InstanceNorm2d-118 [-1, 1024, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2] 1,024 Conv2d-120 [-1, 1024, 2, 2] 1,048,576 Conv2d-121 [-1, 1024, 2, 2] 1,048,576 InstanceNorm2d-122 [-1, 1024, 2, 2] 2,048 _BlockBo...eckNd-123 [-1, 1024, 2, 2] 0 _BlockResStkNd-124 [-1, 1024, 2, 2] 0 Conv2d-125 [-1, 1024, 2, 2] 9,438,208 EncoderNet2d-126 [-1, 1024, 2, 2] 0 ================================================================ Total params: 31,158,016 Trainable params: 31,158,016 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 63.69 Params size (MB): 118.86 Estimated Total Size (MB): 182.59 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/EncoderNet3d/","text":"modules.resnet.EncoderNet3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . EncoderNet3d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 3D residual encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None . Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the last output stage (block) respectively. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration. Examples \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-118 [-1, 1024, 1, 1, 1] 2,048 PReLU-119 [-1, 1024, 1, 1, 1] 1,024 Conv3d-120 [-1, 1024, 1, 1, 1] 1,048,576 Conv3d-121 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-122 [-1, 1024, 1, 1, 1] 2,048 _BlockBo...eckNd-123 [-1, 1024, 1, 1, 1] 0 _BlockResStkNd-124 [-1, 1024, 1, 1, 1] 0 Conv3d-125 [-1, 1024, 1, 1, 1] 28,312,576 Adaptive...ool3d-126 [-1, 1024, 1, 1, 1] 0 Linear-127 [-1, 32] 32,800 EncoderNet3d-128 [-1, 32] 0 ================================================================ Total params: 81,566,240 Trainable params: 81,566,240 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 318.64 Params size (MB): 311.15 Estimated Total Size (MB): 630.13 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-118 [-1, 1024, 1, 1, 1] 2,048 PReLU-119 [-1, 1024, 1, 1, 1] 1,024 Conv3d-120 [-1, 1024, 1, 1, 1] 1,048,576 Conv3d-121 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-122 [-1, 1024, 1, 1, 1] 2,048 _BlockBo...eckNd-123 [-1, 1024, 1, 1, 1] 0 _BlockResStkNd-124 [-1, 1024, 1, 1, 1] 0 Conv3d-125 [-1, 1024, 1, 1, 1] 28,312,576 EncoderNet3d-126 [-1, 1024, 1, 1, 1] 0 ================================================================ Total params: 81,533,440 Trainable params: 81,533,440 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 318.64 Params size (MB): 311.03 Estimated Total Size (MB): 630.01 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>EncoderNet3d</span>"},{"location":"apis/modules/resnet/EncoderNet3d/#modulesresnetencodernet3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . EncoderNet3d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_length = 2 ) This moule is a built-in model for 3D residual encoder network. This network could be used as a part of the auto-encoder, or just a network for down-sampling data. The network would down-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... layers\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] optional:::blockoptional subgraph optional [Optional] fc[\"FC layer\"] end b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -.->|flatten| fc linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; classDef blockoptional fill:none, stroke-dasharray:10,10, stroke:#9370DB, width:100; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route. An optional flattener and fully-connected layer could be appended to the last layer when the argument out_length != None .","title":"modules.resnet.EncoderNet3d"},{"location":"apis/modules/resnet/EncoderNet3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/resnet/EncoderNet3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/EncoderNet3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A tensor, When out_length is None : the size should be ( B , L ) , where B is the batch size, and L is out_length . When out_length != None : the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C and (L1, L2, L3) are the channel number and the size of the last output stage (block) respectively.","title":" __call__"},{"location":"apis/modules/resnet/EncoderNet3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/EncoderNet3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network. This value would not take the fully-connected layer into consideration.","title":" nlayers"},{"location":"apis/modules/resnet/EncoderNet3d/#examples","text":"Example 1 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = 32 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-118 [-1, 1024, 1, 1, 1] 2,048 PReLU-119 [-1, 1024, 1, 1, 1] 1,024 Conv3d-120 [-1, 1024, 1, 1, 1] 1,048,576 Conv3d-121 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-122 [-1, 1024, 1, 1, 1] 2,048 _BlockBo...eckNd-123 [-1, 1024, 1, 1, 1] 0 _BlockResStkNd-124 [-1, 1024, 1, 1, 1] 0 Conv3d-125 [-1, 1024, 1, 1, 1] 28,312,576 Adaptive...ool3d-126 [-1, 1024, 1, 1, 1] 0 Linear-127 [-1, 32] 32,800 EncoderNet3d-128 [-1, 32] 0 ================================================================ Total params: 81,566,240 Trainable params: 81,566,240 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 318.64 Params size (MB): 311.15 Estimated Total Size (MB): 630.13 ---------------------------------------------------------------- Example 2 Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . EncoderNet3d ( 64 , [ 2 , 2 , 2 , 2 , 2 ], in_planes = 3 , out_length = None ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 32. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 1, 1, 1] 28,311,552 InstanceNorm3d-118 [-1, 1024, 1, 1, 1] 2,048 PReLU-119 [-1, 1024, 1, 1, 1] 1,024 Conv3d-120 [-1, 1024, 1, 1, 1] 1,048,576 Conv3d-121 [-1, 1024, 1, 1, 1] 1,048,576 InstanceNorm3d-122 [-1, 1024, 1, 1, 1] 2,048 _BlockBo...eckNd-123 [-1, 1024, 1, 1, 1] 0 _BlockResStkNd-124 [-1, 1024, 1, 1, 1] 0 Conv3d-125 [-1, 1024, 1, 1, 1] 28,312,576 EncoderNet3d-126 [-1, 1024, 1, 1, 1] 0 ================================================================ Total params: 81,533,440 Trainable params: 81,533,440 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 318.64 Params size (MB): 311.03 Estimated Total Size (MB): 630.01 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/UNet1d/","text":"modules.resnet.UNet1d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . UNet1d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D residual U-Net. The network is inspired by: nikhilroxtomar/Deep-Residual-Unet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . UNet1d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 [-1, 64, 128] InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 [-1, 128, 64] InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 [-1, 256, 32] InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 [-1, 512, 16] InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 8] 3,145,728 InstanceNorm1d-118 [-1, 1024, 8] 2,048 PReLU-119 [-1, 1024, 8] 1,024 Conv1d-120 [-1, 1024, 8] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 8] 0 InstanceNorm1d-122 [-1, 1024, 8] 2,048 PReLU-123 [-1, 1024, 8] 1,024 Conv1d-124 [-1, 1024, 8] 1,048,576 InstanceNorm1d-125 [-1, 1024, 8] 2,048 PReLU-126 [-1, 1024, 8] 1,024 Upsample-127 [-1, 1024, 16] 0 Conv1d-128 [-1, 1024, 16] 3,145,728 InstanceNorm1d-129 [-1, 1024, 16] 2,048 PReLU-130 [-1, 1024, 16] 1,024 Conv1d-131 [-1, 512, 16] 524,288 Upsample-132 [-1, 1024, 16] 0 Conv1d-133 [-1, 512, 16] 524,288 InstanceNorm1d-134 [-1, 512, 16] 1,024 _BlockBo...eckNd-135 [-1, 512, 16] 0 _BlockResStkNd-136 [-1, 512, 16] 0 InstanceNorm1d-137 [-1, 1024, 16] 2,048 PReLU-138 [-1, 1024, 16] 1,024 Conv1d-139 [-1, 1024, 16] 1,048,576 InstanceNorm1d-140 [-1, 1024, 16] 2,048 PReLU-141 [-1, 1024, 16] 1,024 Conv1d-142 [-1, 1024, 16] 3,145,728 InstanceNorm1d-143 [-1, 1024, 16] 2,048 PReLU-144 [-1, 1024, 16] 1,024 Conv1d-145 [-1, 512, 16] 524,288 Conv1d-146 [-1, 512, 16] 524,288 InstanceNorm1d-147 [-1, 512, 16] 1,024 _BlockBo...eckNd-148 [-1, 512, 16] 0 InstanceNorm1d-149 [-1, 512, 16] 1,024 PReLU-150 [-1, 512, 16] 512 Conv1d-151 [-1, 512, 16] 262,144 InstanceNorm1d-152 [-1, 512, 16] 1,024 PReLU-153 [-1, 512, 16] 512 Upsample-154 [-1, 512, 32] 0 Conv1d-155 [-1, 512, 32] 786,432 InstanceNorm1d-156 [-1, 512, 32] 1,024 PReLU-157 [-1, 512, 32] 512 Conv1d-158 [-1, 256, 32] 131,072 Upsample-159 [-1, 512, 32] 0 Conv1d-160 [-1, 256, 32] 131,072 InstanceNorm1d-161 [-1, 256, 32] 512 _BlockBo...eckNd-162 [-1, 256, 32] 0 _BlockResStkNd-163 [-1, 256, 32] 0 InstanceNorm1d-164 [-1, 512, 32] 1,024 PReLU-165 [-1, 512, 32] 512 Conv1d-166 [-1, 512, 32] 262,144 InstanceNorm1d-167 [-1, 512, 32] 1,024 PReLU-168 [-1, 512, 32] 512 Conv1d-169 [-1, 512, 32] 786,432 InstanceNorm1d-170 [-1, 512, 32] 1,024 PReLU-171 [-1, 512, 32] 512 Conv1d-172 [-1, 256, 32] 131,072 Conv1d-173 [-1, 256, 32] 131,072 InstanceNorm1d-174 [-1, 256, 32] 512 _BlockBo...eckNd-175 [-1, 256, 32] 0 InstanceNorm1d-176 [-1, 256, 32] 512 PReLU-177 [-1, 256, 32] 256 Conv1d-178 [-1, 256, 32] 65,536 InstanceNorm1d-179 [-1, 256, 32] 512 PReLU-180 [-1, 256, 32] 256 Upsample-181 [-1, 256, 64] 0 Conv1d-182 [-1, 256, 64] 196,608 InstanceNorm1d-183 [-1, 256, 64] 512 PReLU-184 [-1, 256, 64] 256 Conv1d-185 [-1, 128, 64] 32,768 Upsample-186 [-1, 256, 64] 0 Conv1d-187 [-1, 128, 64] 32,768 InstanceNorm1d-188 [-1, 128, 64] 256 _BlockBo...eckNd-189 [-1, 128, 64] 0 _BlockResStkNd-190 [-1, 128, 64] 0 InstanceNorm1d-191 [-1, 256, 64] 512 PReLU-192 [-1, 256, 64] 256 Conv1d-193 [-1, 256, 64] 65,536 InstanceNorm1d-194 [-1, 256, 64] 512 PReLU-195 [-1, 256, 64] 256 Conv1d-196 [-1, 256, 64] 196,608 InstanceNorm1d-197 [-1, 256, 64] 512 PReLU-198 [-1, 256, 64] 256 Conv1d-199 [-1, 128, 64] 32,768 Conv1d-200 [-1, 128, 64] 32,768 InstanceNorm1d-201 [-1, 128, 64] 256 _BlockBo...eckNd-202 [-1, 128, 64] 0 InstanceNorm1d-203 [-1, 128, 64] 256 PReLU-204 [-1, 128, 64] 128 Conv1d-205 [-1, 128, 64] 16,384 InstanceNorm1d-206 [-1, 128, 64] 256 PReLU-207 [-1, 128, 64] 128 Upsample-208 [-1, 128, 128] 0 Conv1d-209 [-1, 128, 128] 49,152 InstanceNorm1d-210 [-1, 128, 128] 256 PReLU-211 [-1, 128, 128] 128 Conv1d-212 [-1, 64, 128] 8,192 Upsample-213 [-1, 128, 128] 0 Conv1d-214 [-1, 64, 128] 8,192 InstanceNorm1d-215 [-1, 64, 128] 128 _BlockBo...eckNd-216 [-1, 64, 128] 0 _BlockResStkNd-217 [-1, 64, 128] 0 InstanceNorm1d-218 [-1, 128, 128] 256 PReLU-219 [-1, 128, 128] 128 Conv1d-220 [-1, 128, 128] 16,384 InstanceNorm1d-221 [-1, 128, 128] 256 PReLU-222 [-1, 128, 128] 128 Conv1d-223 [-1, 128, 128] 49,152 InstanceNorm1d-224 [-1, 128, 128] 256 PReLU-225 [-1, 128, 128] 128 Conv1d-226 [-1, 64, 128] 8,192 Conv1d-227 [-1, 64, 128] 8,192 InstanceNorm1d-228 [-1, 64, 128] 128 _BlockBo...eckNd-229 [-1, 64, 128] 0 InstanceNorm1d-230 [-1, 64, 128] 128 PReLU-231 [-1, 64, 128] 64 Conv1d-232 [-1, 64, 128] 4,096 InstanceNorm1d-233 [-1, 64, 128] 128 PReLU-234 [-1, 64, 128] 64 Conv1d-235 [-1, 64, 128] 12,288 InstanceNorm1d-236 [-1, 64, 128] 128 PReLU-237 [-1, 64, 128] 64 Conv1d-238 [-1, 64, 128] 4,096 _BlockBo...eckNd-239 [-1, 64, 128] 0 _BlockResStkNd-240 [-1, 64, 128] 0 Conv1d-241 [-1, 1, 128] 321 UNet1d-242 [-1, 1, 128] 0 ================================================================ Total params: 24,157,569 Trainable params: 24,157,569 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 16.50 Params size (MB): 92.15 Estimated Total Size (MB): 108.66 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>UNet1d</span>"},{"location":"apis/modules/resnet/UNet1d/#modulesresnetunet1d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . UNet1d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 1D residual U-Net. The network is inspired by: nikhilroxtomar/Deep-Residual-Unet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain1d and mdnc.modules.resnet.BlockBottleneck1d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation.","title":"modules.resnet.UNet1d"},{"location":"apis/modules/resnet/UNet1d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain1d . 'bottleneck' : see BlockBottleneck1d . kernel_size int The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/UNet1d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/UNet1d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 1D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the input channel number, and L is the input data length. Returns Argument Description y A 1D tensor, the size should be ( B , C , L ) , where B is the batch size, C is the output channel number, and L is the input data length.","title":" __call__"},{"location":"apis/modules/resnet/UNet1d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/UNet1d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/resnet/UNet1d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . UNet1d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 128 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv1d-1 [-1, 64, 128] 960 InstanceNorm1d-2 [-1, 64, 128] 128 PReLU-3 [-1, 64, 128] 64 Conv1d-4 [-1, 64, 128] 4,096 InstanceNorm1d-5 [-1, 64, 128] 128 PReLU-6 [-1, 64, 128] 64 Conv1d-7 [-1, 64, 128] 12,288 InstanceNorm1d-8 [-1, 64, 128] 128 PReLU-9 [-1, 64, 128] 64 Conv1d-10 [-1, 64, 128] 4,096 _BlockBo...neckNd-11 [-1, 64, 128] 0 InstanceNorm1d-12 [-1, 64, 128] 128 PReLU-13 [-1, 64, 128] 64 Conv1d-14 [-1, 64, 128] 4,096 InstanceNorm1d-15 [-1, 64, 128] 128 PReLU-16 [-1, 64, 128] 64 Conv1d-17 [-1, 64, 64] 12,288 InstanceNorm1d-18 [-1, 64, 64] 128 PReLU-19 [-1, 64, 64] 64 Conv1d-20 [-1, 64, 64] 4,096 Conv1d-21 [-1, 64, 64] 4,096 InstanceNorm1d-22 [-1, 64, 64] 128 _BlockBo...neckNd-23 [-1, 64, 64] 0 _BlockResStkNd-24 [-1, 64, 64] 0 [-1, 64, 128] InstanceNorm1d-25 [-1, 64, 64] 128 PReLU-26 [-1, 64, 64] 64 Conv1d-27 [-1, 64, 64] 4,096 InstanceNorm1d-28 [-1, 64, 64] 128 PReLU-29 [-1, 64, 64] 64 Conv1d-30 [-1, 64, 64] 12,288 InstanceNorm1d-31 [-1, 64, 64] 128 PReLU-32 [-1, 64, 64] 64 Conv1d-33 [-1, 128, 64] 8,192 Conv1d-34 [-1, 128, 64] 8,192 InstanceNorm1d-35 [-1, 128, 64] 256 _BlockBo...neckNd-36 [-1, 128, 64] 0 InstanceNorm1d-37 [-1, 128, 64] 256 PReLU-38 [-1, 128, 64] 128 Conv1d-39 [-1, 128, 64] 16,384 InstanceNorm1d-40 [-1, 128, 64] 256 PReLU-41 [-1, 128, 64] 128 Conv1d-42 [-1, 128, 32] 49,152 InstanceNorm1d-43 [-1, 128, 32] 256 PReLU-44 [-1, 128, 32] 128 Conv1d-45 [-1, 128, 32] 16,384 Conv1d-46 [-1, 128, 32] 16,384 InstanceNorm1d-47 [-1, 128, 32] 256 _BlockBo...neckNd-48 [-1, 128, 32] 0 _BlockResStkNd-49 [-1, 128, 32] 0 [-1, 128, 64] InstanceNorm1d-50 [-1, 128, 32] 256 PReLU-51 [-1, 128, 32] 128 Conv1d-52 [-1, 128, 32] 16,384 InstanceNorm1d-53 [-1, 128, 32] 256 PReLU-54 [-1, 128, 32] 128 Conv1d-55 [-1, 128, 32] 49,152 InstanceNorm1d-56 [-1, 128, 32] 256 PReLU-57 [-1, 128, 32] 128 Conv1d-58 [-1, 256, 32] 32,768 Conv1d-59 [-1, 256, 32] 32,768 InstanceNorm1d-60 [-1, 256, 32] 512 _BlockBo...neckNd-61 [-1, 256, 32] 0 InstanceNorm1d-62 [-1, 256, 32] 512 PReLU-63 [-1, 256, 32] 256 Conv1d-64 [-1, 256, 32] 65,536 InstanceNorm1d-65 [-1, 256, 32] 512 PReLU-66 [-1, 256, 32] 256 Conv1d-67 [-1, 256, 16] 196,608 InstanceNorm1d-68 [-1, 256, 16] 512 PReLU-69 [-1, 256, 16] 256 Conv1d-70 [-1, 256, 16] 65,536 Conv1d-71 [-1, 256, 16] 65,536 InstanceNorm1d-72 [-1, 256, 16] 512 _BlockBo...neckNd-73 [-1, 256, 16] 0 _BlockResStkNd-74 [-1, 256, 16] 0 [-1, 256, 32] InstanceNorm1d-75 [-1, 256, 16] 512 PReLU-76 [-1, 256, 16] 256 Conv1d-77 [-1, 256, 16] 65,536 InstanceNorm1d-78 [-1, 256, 16] 512 PReLU-79 [-1, 256, 16] 256 Conv1d-80 [-1, 256, 16] 196,608 InstanceNorm1d-81 [-1, 256, 16] 512 PReLU-82 [-1, 256, 16] 256 Conv1d-83 [-1, 512, 16] 131,072 Conv1d-84 [-1, 512, 16] 131,072 InstanceNorm1d-85 [-1, 512, 16] 1,024 _BlockBo...neckNd-86 [-1, 512, 16] 0 InstanceNorm1d-87 [-1, 512, 16] 1,024 PReLU-88 [-1, 512, 16] 512 Conv1d-89 [-1, 512, 16] 262,144 InstanceNorm1d-90 [-1, 512, 16] 1,024 PReLU-91 [-1, 512, 16] 512 Conv1d-92 [-1, 512, 8] 786,432 InstanceNorm1d-93 [-1, 512, 8] 1,024 PReLU-94 [-1, 512, 8] 512 Conv1d-95 [-1, 512, 8] 262,144 Conv1d-96 [-1, 512, 8] 262,144 InstanceNorm1d-97 [-1, 512, 8] 1,024 _BlockBo...neckNd-98 [-1, 512, 8] 0 _BlockResStkNd-99 [-1, 512, 8] 0 [-1, 512, 16] InstanceNorm1d-100 [-1, 512, 8] 1,024 PReLU-101 [-1, 512, 8] 512 Conv1d-102 [-1, 512, 8] 262,144 InstanceNorm1d-103 [-1, 512, 8] 1,024 PReLU-104 [-1, 512, 8] 512 Conv1d-105 [-1, 512, 8] 786,432 InstanceNorm1d-106 [-1, 512, 8] 1,024 PReLU-107 [-1, 512, 8] 512 Conv1d-108 [-1, 1024, 8] 524,288 Conv1d-109 [-1, 1024, 8] 524,288 InstanceNorm1d-110 [-1, 1024, 8] 2,048 _BlockBo...eckNd-111 [-1, 1024, 8] 0 InstanceNorm1d-112 [-1, 1024, 8] 2,048 PReLU-113 [-1, 1024, 8] 1,024 Conv1d-114 [-1, 1024, 8] 1,048,576 InstanceNorm1d-115 [-1, 1024, 8] 2,048 PReLU-116 [-1, 1024, 8] 1,024 Conv1d-117 [-1, 1024, 8] 3,145,728 InstanceNorm1d-118 [-1, 1024, 8] 2,048 PReLU-119 [-1, 1024, 8] 1,024 Conv1d-120 [-1, 1024, 8] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 8] 0 InstanceNorm1d-122 [-1, 1024, 8] 2,048 PReLU-123 [-1, 1024, 8] 1,024 Conv1d-124 [-1, 1024, 8] 1,048,576 InstanceNorm1d-125 [-1, 1024, 8] 2,048 PReLU-126 [-1, 1024, 8] 1,024 Upsample-127 [-1, 1024, 16] 0 Conv1d-128 [-1, 1024, 16] 3,145,728 InstanceNorm1d-129 [-1, 1024, 16] 2,048 PReLU-130 [-1, 1024, 16] 1,024 Conv1d-131 [-1, 512, 16] 524,288 Upsample-132 [-1, 1024, 16] 0 Conv1d-133 [-1, 512, 16] 524,288 InstanceNorm1d-134 [-1, 512, 16] 1,024 _BlockBo...eckNd-135 [-1, 512, 16] 0 _BlockResStkNd-136 [-1, 512, 16] 0 InstanceNorm1d-137 [-1, 1024, 16] 2,048 PReLU-138 [-1, 1024, 16] 1,024 Conv1d-139 [-1, 1024, 16] 1,048,576 InstanceNorm1d-140 [-1, 1024, 16] 2,048 PReLU-141 [-1, 1024, 16] 1,024 Conv1d-142 [-1, 1024, 16] 3,145,728 InstanceNorm1d-143 [-1, 1024, 16] 2,048 PReLU-144 [-1, 1024, 16] 1,024 Conv1d-145 [-1, 512, 16] 524,288 Conv1d-146 [-1, 512, 16] 524,288 InstanceNorm1d-147 [-1, 512, 16] 1,024 _BlockBo...eckNd-148 [-1, 512, 16] 0 InstanceNorm1d-149 [-1, 512, 16] 1,024 PReLU-150 [-1, 512, 16] 512 Conv1d-151 [-1, 512, 16] 262,144 InstanceNorm1d-152 [-1, 512, 16] 1,024 PReLU-153 [-1, 512, 16] 512 Upsample-154 [-1, 512, 32] 0 Conv1d-155 [-1, 512, 32] 786,432 InstanceNorm1d-156 [-1, 512, 32] 1,024 PReLU-157 [-1, 512, 32] 512 Conv1d-158 [-1, 256, 32] 131,072 Upsample-159 [-1, 512, 32] 0 Conv1d-160 [-1, 256, 32] 131,072 InstanceNorm1d-161 [-1, 256, 32] 512 _BlockBo...eckNd-162 [-1, 256, 32] 0 _BlockResStkNd-163 [-1, 256, 32] 0 InstanceNorm1d-164 [-1, 512, 32] 1,024 PReLU-165 [-1, 512, 32] 512 Conv1d-166 [-1, 512, 32] 262,144 InstanceNorm1d-167 [-1, 512, 32] 1,024 PReLU-168 [-1, 512, 32] 512 Conv1d-169 [-1, 512, 32] 786,432 InstanceNorm1d-170 [-1, 512, 32] 1,024 PReLU-171 [-1, 512, 32] 512 Conv1d-172 [-1, 256, 32] 131,072 Conv1d-173 [-1, 256, 32] 131,072 InstanceNorm1d-174 [-1, 256, 32] 512 _BlockBo...eckNd-175 [-1, 256, 32] 0 InstanceNorm1d-176 [-1, 256, 32] 512 PReLU-177 [-1, 256, 32] 256 Conv1d-178 [-1, 256, 32] 65,536 InstanceNorm1d-179 [-1, 256, 32] 512 PReLU-180 [-1, 256, 32] 256 Upsample-181 [-1, 256, 64] 0 Conv1d-182 [-1, 256, 64] 196,608 InstanceNorm1d-183 [-1, 256, 64] 512 PReLU-184 [-1, 256, 64] 256 Conv1d-185 [-1, 128, 64] 32,768 Upsample-186 [-1, 256, 64] 0 Conv1d-187 [-1, 128, 64] 32,768 InstanceNorm1d-188 [-1, 128, 64] 256 _BlockBo...eckNd-189 [-1, 128, 64] 0 _BlockResStkNd-190 [-1, 128, 64] 0 InstanceNorm1d-191 [-1, 256, 64] 512 PReLU-192 [-1, 256, 64] 256 Conv1d-193 [-1, 256, 64] 65,536 InstanceNorm1d-194 [-1, 256, 64] 512 PReLU-195 [-1, 256, 64] 256 Conv1d-196 [-1, 256, 64] 196,608 InstanceNorm1d-197 [-1, 256, 64] 512 PReLU-198 [-1, 256, 64] 256 Conv1d-199 [-1, 128, 64] 32,768 Conv1d-200 [-1, 128, 64] 32,768 InstanceNorm1d-201 [-1, 128, 64] 256 _BlockBo...eckNd-202 [-1, 128, 64] 0 InstanceNorm1d-203 [-1, 128, 64] 256 PReLU-204 [-1, 128, 64] 128 Conv1d-205 [-1, 128, 64] 16,384 InstanceNorm1d-206 [-1, 128, 64] 256 PReLU-207 [-1, 128, 64] 128 Upsample-208 [-1, 128, 128] 0 Conv1d-209 [-1, 128, 128] 49,152 InstanceNorm1d-210 [-1, 128, 128] 256 PReLU-211 [-1, 128, 128] 128 Conv1d-212 [-1, 64, 128] 8,192 Upsample-213 [-1, 128, 128] 0 Conv1d-214 [-1, 64, 128] 8,192 InstanceNorm1d-215 [-1, 64, 128] 128 _BlockBo...eckNd-216 [-1, 64, 128] 0 _BlockResStkNd-217 [-1, 64, 128] 0 InstanceNorm1d-218 [-1, 128, 128] 256 PReLU-219 [-1, 128, 128] 128 Conv1d-220 [-1, 128, 128] 16,384 InstanceNorm1d-221 [-1, 128, 128] 256 PReLU-222 [-1, 128, 128] 128 Conv1d-223 [-1, 128, 128] 49,152 InstanceNorm1d-224 [-1, 128, 128] 256 PReLU-225 [-1, 128, 128] 128 Conv1d-226 [-1, 64, 128] 8,192 Conv1d-227 [-1, 64, 128] 8,192 InstanceNorm1d-228 [-1, 64, 128] 128 _BlockBo...eckNd-229 [-1, 64, 128] 0 InstanceNorm1d-230 [-1, 64, 128] 128 PReLU-231 [-1, 64, 128] 64 Conv1d-232 [-1, 64, 128] 4,096 InstanceNorm1d-233 [-1, 64, 128] 128 PReLU-234 [-1, 64, 128] 64 Conv1d-235 [-1, 64, 128] 12,288 InstanceNorm1d-236 [-1, 64, 128] 128 PReLU-237 [-1, 64, 128] 64 Conv1d-238 [-1, 64, 128] 4,096 _BlockBo...eckNd-239 [-1, 64, 128] 0 _BlockResStkNd-240 [-1, 64, 128] 0 Conv1d-241 [-1, 1, 128] 321 UNet1d-242 [-1, 1, 128] 0 ================================================================ Total params: 24,157,569 Trainable params: 24,157,569 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 16.50 Params size (MB): 92.15 Estimated Total Size (MB): 108.66 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/UNet2d/","text":"modules.resnet.UNet2d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . UNet2d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D residual U-Net. The network is inspired by: nikhilroxtomar/Deep-Residual-Unet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . UNet2d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 [-1, 64, 64, 63] InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 [-1, 128, 32, 32] InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 [-1, 256, 16, 16] InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 [-1, 512, 8, 8] InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 4, 4] 9,437,184 InstanceNorm2d-118 [-1, 1024, 4, 4] 2,048 PReLU-119 [-1, 1024, 4, 4] 1,024 Conv2d-120 [-1, 1024, 4, 4] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 4, 4] 0 InstanceNorm2d-122 [-1, 1024, 4, 4] 2,048 PReLU-123 [-1, 1024, 4, 4] 1,024 Conv2d-124 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-125 [-1, 1024, 4, 4] 2,048 PReLU-126 [-1, 1024, 4, 4] 1,024 Upsample-127 [-1, 1024, 8, 8] 0 Conv2d-128 [-1, 1024, 8, 8] 9,437,184 InstanceNorm2d-129 [-1, 1024, 8, 8] 2,048 PReLU-130 [-1, 1024, 8, 8] 1,024 Conv2d-131 [-1, 512, 8, 8] 524,288 Upsample-132 [-1, 1024, 8, 8] 0 Conv2d-133 [-1, 512, 8, 8] 524,288 InstanceNorm2d-134 [-1, 512, 8, 8] 1,024 _BlockBo...eckNd-135 [-1, 512, 8, 8] 0 _BlockResStkNd-136 [-1, 512, 8, 8] 0 InstanceNorm2d-137 [-1, 1024, 8, 8] 2,048 PReLU-138 [-1, 1024, 8, 8] 1,024 Conv2d-139 [-1, 1024, 8, 8] 1,048,576 InstanceNorm2d-140 [-1, 1024, 8, 8] 2,048 PReLU-141 [-1, 1024, 8, 8] 1,024 Conv2d-142 [-1, 1024, 8, 8] 9,437,184 InstanceNorm2d-143 [-1, 1024, 8, 8] 2,048 PReLU-144 [-1, 1024, 8, 8] 1,024 Conv2d-145 [-1, 512, 8, 8] 524,288 Conv2d-146 [-1, 512, 8, 8] 524,288 InstanceNorm2d-147 [-1, 512, 8, 8] 1,024 _BlockBo...eckNd-148 [-1, 512, 8, 8] 0 InstanceNorm2d-149 [-1, 512, 8, 8] 1,024 PReLU-150 [-1, 512, 8, 8] 512 Conv2d-151 [-1, 512, 8, 8] 262,144 InstanceNorm2d-152 [-1, 512, 8, 8] 1,024 PReLU-153 [-1, 512, 8, 8] 512 Upsample-154 [-1, 512, 16, 16] 0 Conv2d-155 [-1, 512, 16, 16] 2,359,296 InstanceNorm2d-156 [-1, 512, 16, 16] 1,024 PReLU-157 [-1, 512, 16, 16] 512 Conv2d-158 [-1, 256, 16, 16] 131,072 Upsample-159 [-1, 512, 16, 16] 0 Conv2d-160 [-1, 256, 16, 16] 131,072 InstanceNorm2d-161 [-1, 256, 16, 16] 512 _BlockBo...eckNd-162 [-1, 256, 16, 16] 0 _BlockResStkNd-163 [-1, 256, 16, 16] 0 InstanceNorm2d-164 [-1, 512, 16, 16] 1,024 PReLU-165 [-1, 512, 16, 16] 512 Conv2d-166 [-1, 512, 16, 16] 262,144 InstanceNorm2d-167 [-1, 512, 16, 16] 1,024 PReLU-168 [-1, 512, 16, 16] 512 Conv2d-169 [-1, 512, 16, 16] 2,359,296 InstanceNorm2d-170 [-1, 512, 16, 16] 1,024 PReLU-171 [-1, 512, 16, 16] 512 Conv2d-172 [-1, 256, 16, 16] 131,072 Conv2d-173 [-1, 256, 16, 16] 131,072 InstanceNorm2d-174 [-1, 256, 16, 16] 512 _BlockBo...eckNd-175 [-1, 256, 16, 16] 0 InstanceNorm2d-176 [-1, 256, 16, 16] 512 PReLU-177 [-1, 256, 16, 16] 256 Conv2d-178 [-1, 256, 16, 16] 65,536 InstanceNorm2d-179 [-1, 256, 16, 16] 512 PReLU-180 [-1, 256, 16, 16] 256 Upsample-181 [-1, 256, 32, 32] 0 Conv2d-182 [-1, 256, 32, 32] 589,824 InstanceNorm2d-183 [-1, 256, 32, 32] 512 PReLU-184 [-1, 256, 32, 32] 256 Conv2d-185 [-1, 128, 32, 32] 32,768 Upsample-186 [-1, 256, 32, 32] 0 Conv2d-187 [-1, 128, 32, 32] 32,768 InstanceNorm2d-188 [-1, 128, 32, 32] 256 _BlockBo...eckNd-189 [-1, 128, 32, 32] 0 _BlockResStkNd-190 [-1, 128, 32, 32] 0 InstanceNorm2d-191 [-1, 256, 32, 32] 512 PReLU-192 [-1, 256, 32, 32] 256 Conv2d-193 [-1, 256, 32, 32] 65,536 InstanceNorm2d-194 [-1, 256, 32, 32] 512 PReLU-195 [-1, 256, 32, 32] 256 Conv2d-196 [-1, 256, 32, 32] 589,824 InstanceNorm2d-197 [-1, 256, 32, 32] 512 PReLU-198 [-1, 256, 32, 32] 256 Conv2d-199 [-1, 128, 32, 32] 32,768 Conv2d-200 [-1, 128, 32, 32] 32,768 InstanceNorm2d-201 [-1, 128, 32, 32] 256 _BlockBo...eckNd-202 [-1, 128, 32, 32] 0 InstanceNorm2d-203 [-1, 128, 32, 32] 256 PReLU-204 [-1, 128, 32, 32] 128 Conv2d-205 [-1, 128, 32, 32] 16,384 InstanceNorm2d-206 [-1, 128, 32, 32] 256 PReLU-207 [-1, 128, 32, 32] 128 Upsample-208 [-1, 128, 64, 64] 0 Conv2d-209 [-1, 128, 64, 64] 147,456 InstanceNorm2d-210 [-1, 128, 64, 64] 256 PReLU-211 [-1, 128, 64, 64] 128 Conv2d-212 [-1, 64, 64, 64] 8,192 Upsample-213 [-1, 128, 64, 64] 0 Conv2d-214 [-1, 64, 64, 64] 8,192 InstanceNorm2d-215 [-1, 64, 64, 64] 128 _BlockBo...eckNd-216 [-1, 64, 64, 64] 0 _BlockResStkNd-217 [-1, 64, 64, 64] 0 InstanceNorm2d-218 [-1, 128, 64, 63] 256 PReLU-219 [-1, 128, 64, 63] 128 Conv2d-220 [-1, 128, 64, 63] 16,384 InstanceNorm2d-221 [-1, 128, 64, 63] 256 PReLU-222 [-1, 128, 64, 63] 128 Conv2d-223 [-1, 128, 64, 63] 147,456 InstanceNorm2d-224 [-1, 128, 64, 63] 256 PReLU-225 [-1, 128, 64, 63] 128 Conv2d-226 [-1, 64, 64, 63] 8,192 Conv2d-227 [-1, 64, 64, 63] 8,192 InstanceNorm2d-228 [-1, 64, 64, 63] 128 _BlockBo...eckNd-229 [-1, 64, 64, 63] 0 InstanceNorm2d-230 [-1, 64, 64, 63] 128 PReLU-231 [-1, 64, 64, 63] 64 Conv2d-232 [-1, 64, 64, 63] 4,096 InstanceNorm2d-233 [-1, 64, 64, 63] 128 PReLU-234 [-1, 64, 64, 63] 64 Conv2d-235 [-1, 64, 64, 63] 36,864 InstanceNorm2d-236 [-1, 64, 64, 63] 128 PReLU-237 [-1, 64, 64, 63] 64 Conv2d-238 [-1, 64, 64, 63] 4,096 _BlockBo...eckNd-239 [-1, 64, 64, 63] 0 _BlockResStkNd-240 [-1, 64, 64, 63] 0 Conv2d-241 [-1, 1, 64, 63] 1,601 UNet2d-242 [-1, 1, 64, 63] 0 ================================================================ Total params: 51,392,897 Trainable params: 51,392,897 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 229.44 Params size (MB): 196.05 Estimated Total Size (MB): 425.53 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>UNet2d</span>"},{"location":"apis/modules/resnet/UNet2d/#modulesresnetunet2d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . UNet2d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 2D residual U-Net. The network is inspired by: nikhilroxtomar/Deep-Residual-Unet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain2d and mdnc.modules.resnet.BlockBottleneck2d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation.","title":"modules.resnet.UNet2d"},{"location":"apis/modules/resnet/UNet2d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain2d . 'bottleneck' : see BlockBottleneck2d . kernel_size int or ( int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/UNet2d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/UNet2d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 2D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the input channel number, and (L1, L2) is the input data size. Returns Argument Description y A 2D tensor, the size should be ( B , C , L1 , L2 ) , where B is the batch size, C is the output channel number, and (L1, L2) is the input data size.","title":" __call__"},{"location":"apis/modules/resnet/UNet2d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/UNet2d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/resnet/UNet2d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . UNet2d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 64 , 63 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv2d-1 [-1, 64, 64, 63] 4,800 InstanceNorm2d-2 [-1, 64, 64, 63] 128 PReLU-3 [-1, 64, 64, 63] 64 Conv2d-4 [-1, 64, 64, 63] 4,096 InstanceNorm2d-5 [-1, 64, 64, 63] 128 PReLU-6 [-1, 64, 64, 63] 64 Conv2d-7 [-1, 64, 64, 63] 36,864 InstanceNorm2d-8 [-1, 64, 64, 63] 128 PReLU-9 [-1, 64, 64, 63] 64 Conv2d-10 [-1, 64, 64, 63] 4,096 _BlockBo...neckNd-11 [-1, 64, 64, 63] 0 InstanceNorm2d-12 [-1, 64, 64, 63] 128 PReLU-13 [-1, 64, 64, 63] 64 Conv2d-14 [-1, 64, 64, 63] 4,096 InstanceNorm2d-15 [-1, 64, 64, 63] 128 PReLU-16 [-1, 64, 64, 63] 64 Conv2d-17 [-1, 64, 32, 32] 36,864 InstanceNorm2d-18 [-1, 64, 32, 32] 128 PReLU-19 [-1, 64, 32, 32] 64 Conv2d-20 [-1, 64, 32, 32] 4,096 Conv2d-21 [-1, 64, 32, 32] 4,096 InstanceNorm2d-22 [-1, 64, 32, 32] 128 _BlockBo...neckNd-23 [-1, 64, 32, 32] 0 _BlockResStkNd-24 [-1, 64, 32, 32] 0 [-1, 64, 64, 63] InstanceNorm2d-25 [-1, 64, 32, 32] 128 PReLU-26 [-1, 64, 32, 32] 64 Conv2d-27 [-1, 64, 32, 32] 4,096 InstanceNorm2d-28 [-1, 64, 32, 32] 128 PReLU-29 [-1, 64, 32, 32] 64 Conv2d-30 [-1, 64, 32, 32] 36,864 InstanceNorm2d-31 [-1, 64, 32, 32] 128 PReLU-32 [-1, 64, 32, 32] 64 Conv2d-33 [-1, 128, 32, 32] 8,192 Conv2d-34 [-1, 128, 32, 32] 8,192 InstanceNorm2d-35 [-1, 128, 32, 32] 256 _BlockBo...neckNd-36 [-1, 128, 32, 32] 0 InstanceNorm2d-37 [-1, 128, 32, 32] 256 PReLU-38 [-1, 128, 32, 32] 128 Conv2d-39 [-1, 128, 32, 32] 16,384 InstanceNorm2d-40 [-1, 128, 32, 32] 256 PReLU-41 [-1, 128, 32, 32] 128 Conv2d-42 [-1, 128, 16, 16] 147,456 InstanceNorm2d-43 [-1, 128, 16, 16] 256 PReLU-44 [-1, 128, 16, 16] 128 Conv2d-45 [-1, 128, 16, 16] 16,384 Conv2d-46 [-1, 128, 16, 16] 16,384 InstanceNorm2d-47 [-1, 128, 16, 16] 256 _BlockBo...neckNd-48 [-1, 128, 16, 16] 0 _BlockResStkNd-49 [-1, 128, 16, 16] 0 [-1, 128, 32, 32] InstanceNorm2d-50 [-1, 128, 16, 16] 256 PReLU-51 [-1, 128, 16, 16] 128 Conv2d-52 [-1, 128, 16, 16] 16,384 InstanceNorm2d-53 [-1, 128, 16, 16] 256 PReLU-54 [-1, 128, 16, 16] 128 Conv2d-55 [-1, 128, 16, 16] 147,456 InstanceNorm2d-56 [-1, 128, 16, 16] 256 PReLU-57 [-1, 128, 16, 16] 128 Conv2d-58 [-1, 256, 16, 16] 32,768 Conv2d-59 [-1, 256, 16, 16] 32,768 InstanceNorm2d-60 [-1, 256, 16, 16] 512 _BlockBo...neckNd-61 [-1, 256, 16, 16] 0 InstanceNorm2d-62 [-1, 256, 16, 16] 512 PReLU-63 [-1, 256, 16, 16] 256 Conv2d-64 [-1, 256, 16, 16] 65,536 InstanceNorm2d-65 [-1, 256, 16, 16] 512 PReLU-66 [-1, 256, 16, 16] 256 Conv2d-67 [-1, 256, 8, 8] 589,824 InstanceNorm2d-68 [-1, 256, 8, 8] 512 PReLU-69 [-1, 256, 8, 8] 256 Conv2d-70 [-1, 256, 8, 8] 65,536 Conv2d-71 [-1, 256, 8, 8] 65,536 InstanceNorm2d-72 [-1, 256, 8, 8] 512 _BlockBo...neckNd-73 [-1, 256, 8, 8] 0 _BlockResStkNd-74 [-1, 256, 8, 8] 0 [-1, 256, 16, 16] InstanceNorm2d-75 [-1, 256, 8, 8] 512 PReLU-76 [-1, 256, 8, 8] 256 Conv2d-77 [-1, 256, 8, 8] 65,536 InstanceNorm2d-78 [-1, 256, 8, 8] 512 PReLU-79 [-1, 256, 8, 8] 256 Conv2d-80 [-1, 256, 8, 8] 589,824 InstanceNorm2d-81 [-1, 256, 8, 8] 512 PReLU-82 [-1, 256, 8, 8] 256 Conv2d-83 [-1, 512, 8, 8] 131,072 Conv2d-84 [-1, 512, 8, 8] 131,072 InstanceNorm2d-85 [-1, 512, 8, 8] 1,024 _BlockBo...neckNd-86 [-1, 512, 8, 8] 0 InstanceNorm2d-87 [-1, 512, 8, 8] 1,024 PReLU-88 [-1, 512, 8, 8] 512 Conv2d-89 [-1, 512, 8, 8] 262,144 InstanceNorm2d-90 [-1, 512, 8, 8] 1,024 PReLU-91 [-1, 512, 8, 8] 512 Conv2d-92 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-93 [-1, 512, 4, 4] 1,024 PReLU-94 [-1, 512, 4, 4] 512 Conv2d-95 [-1, 512, 4, 4] 262,144 Conv2d-96 [-1, 512, 4, 4] 262,144 InstanceNorm2d-97 [-1, 512, 4, 4] 1,024 _BlockBo...neckNd-98 [-1, 512, 4, 4] 0 _BlockResStkNd-99 [-1, 512, 4, 4] 0 [-1, 512, 8, 8] InstanceNorm2d-100 [-1, 512, 4, 4] 1,024 PReLU-101 [-1, 512, 4, 4] 512 Conv2d-102 [-1, 512, 4, 4] 262,144 InstanceNorm2d-103 [-1, 512, 4, 4] 1,024 PReLU-104 [-1, 512, 4, 4] 512 Conv2d-105 [-1, 512, 4, 4] 2,359,296 InstanceNorm2d-106 [-1, 512, 4, 4] 1,024 PReLU-107 [-1, 512, 4, 4] 512 Conv2d-108 [-1, 1024, 4, 4] 524,288 Conv2d-109 [-1, 1024, 4, 4] 524,288 InstanceNorm2d-110 [-1, 1024, 4, 4] 2,048 _BlockBo...eckNd-111 [-1, 1024, 4, 4] 0 InstanceNorm2d-112 [-1, 1024, 4, 4] 2,048 PReLU-113 [-1, 1024, 4, 4] 1,024 Conv2d-114 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-115 [-1, 1024, 4, 4] 2,048 PReLU-116 [-1, 1024, 4, 4] 1,024 Conv2d-117 [-1, 1024, 4, 4] 9,437,184 InstanceNorm2d-118 [-1, 1024, 4, 4] 2,048 PReLU-119 [-1, 1024, 4, 4] 1,024 Conv2d-120 [-1, 1024, 4, 4] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 4, 4] 0 InstanceNorm2d-122 [-1, 1024, 4, 4] 2,048 PReLU-123 [-1, 1024, 4, 4] 1,024 Conv2d-124 [-1, 1024, 4, 4] 1,048,576 InstanceNorm2d-125 [-1, 1024, 4, 4] 2,048 PReLU-126 [-1, 1024, 4, 4] 1,024 Upsample-127 [-1, 1024, 8, 8] 0 Conv2d-128 [-1, 1024, 8, 8] 9,437,184 InstanceNorm2d-129 [-1, 1024, 8, 8] 2,048 PReLU-130 [-1, 1024, 8, 8] 1,024 Conv2d-131 [-1, 512, 8, 8] 524,288 Upsample-132 [-1, 1024, 8, 8] 0 Conv2d-133 [-1, 512, 8, 8] 524,288 InstanceNorm2d-134 [-1, 512, 8, 8] 1,024 _BlockBo...eckNd-135 [-1, 512, 8, 8] 0 _BlockResStkNd-136 [-1, 512, 8, 8] 0 InstanceNorm2d-137 [-1, 1024, 8, 8] 2,048 PReLU-138 [-1, 1024, 8, 8] 1,024 Conv2d-139 [-1, 1024, 8, 8] 1,048,576 InstanceNorm2d-140 [-1, 1024, 8, 8] 2,048 PReLU-141 [-1, 1024, 8, 8] 1,024 Conv2d-142 [-1, 1024, 8, 8] 9,437,184 InstanceNorm2d-143 [-1, 1024, 8, 8] 2,048 PReLU-144 [-1, 1024, 8, 8] 1,024 Conv2d-145 [-1, 512, 8, 8] 524,288 Conv2d-146 [-1, 512, 8, 8] 524,288 InstanceNorm2d-147 [-1, 512, 8, 8] 1,024 _BlockBo...eckNd-148 [-1, 512, 8, 8] 0 InstanceNorm2d-149 [-1, 512, 8, 8] 1,024 PReLU-150 [-1, 512, 8, 8] 512 Conv2d-151 [-1, 512, 8, 8] 262,144 InstanceNorm2d-152 [-1, 512, 8, 8] 1,024 PReLU-153 [-1, 512, 8, 8] 512 Upsample-154 [-1, 512, 16, 16] 0 Conv2d-155 [-1, 512, 16, 16] 2,359,296 InstanceNorm2d-156 [-1, 512, 16, 16] 1,024 PReLU-157 [-1, 512, 16, 16] 512 Conv2d-158 [-1, 256, 16, 16] 131,072 Upsample-159 [-1, 512, 16, 16] 0 Conv2d-160 [-1, 256, 16, 16] 131,072 InstanceNorm2d-161 [-1, 256, 16, 16] 512 _BlockBo...eckNd-162 [-1, 256, 16, 16] 0 _BlockResStkNd-163 [-1, 256, 16, 16] 0 InstanceNorm2d-164 [-1, 512, 16, 16] 1,024 PReLU-165 [-1, 512, 16, 16] 512 Conv2d-166 [-1, 512, 16, 16] 262,144 InstanceNorm2d-167 [-1, 512, 16, 16] 1,024 PReLU-168 [-1, 512, 16, 16] 512 Conv2d-169 [-1, 512, 16, 16] 2,359,296 InstanceNorm2d-170 [-1, 512, 16, 16] 1,024 PReLU-171 [-1, 512, 16, 16] 512 Conv2d-172 [-1, 256, 16, 16] 131,072 Conv2d-173 [-1, 256, 16, 16] 131,072 InstanceNorm2d-174 [-1, 256, 16, 16] 512 _BlockBo...eckNd-175 [-1, 256, 16, 16] 0 InstanceNorm2d-176 [-1, 256, 16, 16] 512 PReLU-177 [-1, 256, 16, 16] 256 Conv2d-178 [-1, 256, 16, 16] 65,536 InstanceNorm2d-179 [-1, 256, 16, 16] 512 PReLU-180 [-1, 256, 16, 16] 256 Upsample-181 [-1, 256, 32, 32] 0 Conv2d-182 [-1, 256, 32, 32] 589,824 InstanceNorm2d-183 [-1, 256, 32, 32] 512 PReLU-184 [-1, 256, 32, 32] 256 Conv2d-185 [-1, 128, 32, 32] 32,768 Upsample-186 [-1, 256, 32, 32] 0 Conv2d-187 [-1, 128, 32, 32] 32,768 InstanceNorm2d-188 [-1, 128, 32, 32] 256 _BlockBo...eckNd-189 [-1, 128, 32, 32] 0 _BlockResStkNd-190 [-1, 128, 32, 32] 0 InstanceNorm2d-191 [-1, 256, 32, 32] 512 PReLU-192 [-1, 256, 32, 32] 256 Conv2d-193 [-1, 256, 32, 32] 65,536 InstanceNorm2d-194 [-1, 256, 32, 32] 512 PReLU-195 [-1, 256, 32, 32] 256 Conv2d-196 [-1, 256, 32, 32] 589,824 InstanceNorm2d-197 [-1, 256, 32, 32] 512 PReLU-198 [-1, 256, 32, 32] 256 Conv2d-199 [-1, 128, 32, 32] 32,768 Conv2d-200 [-1, 128, 32, 32] 32,768 InstanceNorm2d-201 [-1, 128, 32, 32] 256 _BlockBo...eckNd-202 [-1, 128, 32, 32] 0 InstanceNorm2d-203 [-1, 128, 32, 32] 256 PReLU-204 [-1, 128, 32, 32] 128 Conv2d-205 [-1, 128, 32, 32] 16,384 InstanceNorm2d-206 [-1, 128, 32, 32] 256 PReLU-207 [-1, 128, 32, 32] 128 Upsample-208 [-1, 128, 64, 64] 0 Conv2d-209 [-1, 128, 64, 64] 147,456 InstanceNorm2d-210 [-1, 128, 64, 64] 256 PReLU-211 [-1, 128, 64, 64] 128 Conv2d-212 [-1, 64, 64, 64] 8,192 Upsample-213 [-1, 128, 64, 64] 0 Conv2d-214 [-1, 64, 64, 64] 8,192 InstanceNorm2d-215 [-1, 64, 64, 64] 128 _BlockBo...eckNd-216 [-1, 64, 64, 64] 0 _BlockResStkNd-217 [-1, 64, 64, 64] 0 InstanceNorm2d-218 [-1, 128, 64, 63] 256 PReLU-219 [-1, 128, 64, 63] 128 Conv2d-220 [-1, 128, 64, 63] 16,384 InstanceNorm2d-221 [-1, 128, 64, 63] 256 PReLU-222 [-1, 128, 64, 63] 128 Conv2d-223 [-1, 128, 64, 63] 147,456 InstanceNorm2d-224 [-1, 128, 64, 63] 256 PReLU-225 [-1, 128, 64, 63] 128 Conv2d-226 [-1, 64, 64, 63] 8,192 Conv2d-227 [-1, 64, 64, 63] 8,192 InstanceNorm2d-228 [-1, 64, 64, 63] 128 _BlockBo...eckNd-229 [-1, 64, 64, 63] 0 InstanceNorm2d-230 [-1, 64, 64, 63] 128 PReLU-231 [-1, 64, 64, 63] 64 Conv2d-232 [-1, 64, 64, 63] 4,096 InstanceNorm2d-233 [-1, 64, 64, 63] 128 PReLU-234 [-1, 64, 64, 63] 64 Conv2d-235 [-1, 64, 64, 63] 36,864 InstanceNorm2d-236 [-1, 64, 64, 63] 128 PReLU-237 [-1, 64, 64, 63] 64 Conv2d-238 [-1, 64, 64, 63] 4,096 _BlockBo...eckNd-239 [-1, 64, 64, 63] 0 _BlockResStkNd-240 [-1, 64, 64, 63] 0 Conv2d-241 [-1, 1, 64, 63] 1,601 UNet2d-242 [-1, 1, 64, 63] 0 ================================================================ Total params: 51,392,897 Trainable params: 51,392,897 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.05 Forward/backward pass size (MB): 229.44 Params size (MB): 196.05 Estimated Total Size (MB): 425.53 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/UNet3d/","text":"modules.resnet.UNet3d \u00b6 Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . UNet3d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D residual U-Net. The network is inspired by: nikhilroxtomar/Deep-Residual-Unet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation. Arguments \u00b6 Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. Operators \u00b6 __call__ \u00b6 y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size. Properties \u00b6 nlayers \u00b6 net . nlayers The total number of convolutional layers along the depth of the network. Examples \u00b6 Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . UNet3d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 [-1, 64, 31, 32, 30] InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 [-1, 128, 16, 16, 15] InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 [-1, 256, 8, 8, 8] InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 [-1, 512, 4, 4, 4] InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 2, 2, 2] 28,311,552 InstanceNorm3d-118 [-1, 1024, 2, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2, 2] 1,024 Conv3d-120 [-1, 1024, 2, 2, 2] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-122 [-1, 1024, 2, 2, 2] 2,048 PReLU-123 [-1, 1024, 2, 2, 2] 1,024 Conv3d-124 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-125 [-1, 1024, 2, 2, 2] 2,048 PReLU-126 [-1, 1024, 2, 2, 2] 1,024 Upsample-127 [-1, 1024, 4, 4, 4] 0 Conv3d-128 [-1, 1024, 4, 4, 4] 28,311,552 InstanceNorm3d-129 [-1, 1024, 4, 4, 4] 2,048 PReLU-130 [-1, 1024, 4, 4, 4] 1,024 Conv3d-131 [-1, 512, 4, 4, 4] 524,288 Upsample-132 [-1, 1024, 4, 4, 4] 0 Conv3d-133 [-1, 512, 4, 4, 4] 524,288 InstanceNorm3d-134 [-1, 512, 4, 4, 4] 1,024 _BlockBo...eckNd-135 [-1, 512, 4, 4, 4] 0 _BlockResStkNd-136 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-137 [-1, 1024, 4, 4, 4] 2,048 PReLU-138 [-1, 1024, 4, 4, 4] 1,024 Conv3d-139 [-1, 1024, 4, 4, 4] 1,048,576 InstanceNorm3d-140 [-1, 1024, 4, 4, 4] 2,048 PReLU-141 [-1, 1024, 4, 4, 4] 1,024 Conv3d-142 [-1, 1024, 4, 4, 4] 28,311,552 InstanceNorm3d-143 [-1, 1024, 4, 4, 4] 2,048 PReLU-144 [-1, 1024, 4, 4, 4] 1,024 Conv3d-145 [-1, 512, 4, 4, 4] 524,288 Conv3d-146 [-1, 512, 4, 4, 4] 524,288 InstanceNorm3d-147 [-1, 512, 4, 4, 4] 1,024 _BlockBo...eckNd-148 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-149 [-1, 512, 4, 4, 4] 1,024 PReLU-150 [-1, 512, 4, 4, 4] 512 Conv3d-151 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-152 [-1, 512, 4, 4, 4] 1,024 PReLU-153 [-1, 512, 4, 4, 4] 512 Upsample-154 [-1, 512, 8, 8, 8] 0 Conv3d-155 [-1, 512, 8, 8, 8] 7,077,888 InstanceNorm3d-156 [-1, 512, 8, 8, 8] 1,024 PReLU-157 [-1, 512, 8, 8, 8] 512 Conv3d-158 [-1, 256, 8, 8, 8] 131,072 Upsample-159 [-1, 512, 8, 8, 8] 0 Conv3d-160 [-1, 256, 8, 8, 8] 131,072 InstanceNorm3d-161 [-1, 256, 8, 8, 8] 512 _BlockBo...eckNd-162 [-1, 256, 8, 8, 8] 0 _BlockResStkNd-163 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-164 [-1, 512, 8, 8, 8] 1,024 PReLU-165 [-1, 512, 8, 8, 8] 512 Conv3d-166 [-1, 512, 8, 8, 8] 262,144 InstanceNorm3d-167 [-1, 512, 8, 8, 8] 1,024 PReLU-168 [-1, 512, 8, 8, 8] 512 Conv3d-169 [-1, 512, 8, 8, 8] 7,077,888 InstanceNorm3d-170 [-1, 512, 8, 8, 8] 1,024 PReLU-171 [-1, 512, 8, 8, 8] 512 Conv3d-172 [-1, 256, 8, 8, 8] 131,072 Conv3d-173 [-1, 256, 8, 8, 8] 131,072 InstanceNorm3d-174 [-1, 256, 8, 8, 8] 512 _BlockBo...eckNd-175 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-176 [-1, 256, 8, 8, 8] 512 PReLU-177 [-1, 256, 8, 8, 8] 256 Conv3d-178 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-179 [-1, 256, 8, 8, 8] 512 PReLU-180 [-1, 256, 8, 8, 8] 256 Upsample-181 [-1, 256, 16, 16, 16] 0 Conv3d-182 [-1, 256, 16, 16, 16] 1,769,472 InstanceNorm3d-183 [-1, 256, 16, 16, 16] 512 PReLU-184 [-1, 256, 16, 16, 16] 256 Conv3d-185 [-1, 128, 16, 16, 16] 32,768 Upsample-186 [-1, 256, 16, 16, 16] 0 Conv3d-187 [-1, 128, 16, 16, 16] 32,768 InstanceNorm3d-188 [-1, 128, 16, 16, 16] 256 _BlockBo...eckNd-189 [-1, 128, 16, 16, 16] 0 _BlockResStkNd-190 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-191 [-1, 256, 16, 16, 15] 512 PReLU-192 [-1, 256, 16, 16, 15] 256 Conv3d-193 [-1, 256, 16, 16, 15] 65,536 InstanceNorm3d-194 [-1, 256, 16, 16, 15] 512 PReLU-195 [-1, 256, 16, 16, 15] 256 Conv3d-196 [-1, 256, 16, 16, 15] 1,769,472 InstanceNorm3d-197 [-1, 256, 16, 16, 15] 512 PReLU-198 [-1, 256, 16, 16, 15] 256 Conv3d-199 [-1, 128, 16, 16, 15] 32,768 Conv3d-200 [-1, 128, 16, 16, 15] 32,768 InstanceNorm3d-201 [-1, 128, 16, 16, 15] 256 _BlockBo...eckNd-202 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-203 [-1, 128, 16, 16, 15] 256 PReLU-204 [-1, 128, 16, 16, 15] 128 Conv3d-205 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-206 [-1, 128, 16, 16, 15] 256 PReLU-207 [-1, 128, 16, 16, 15] 128 Upsample-208 [-1, 128, 32, 32, 30] 0 Conv3d-209 [-1, 128, 32, 32, 30] 442,368 InstanceNorm3d-210 [-1, 128, 32, 32, 30] 256 PReLU-211 [-1, 128, 32, 32, 30] 128 Conv3d-212 [-1, 64, 32, 32, 30] 8,192 Upsample-213 [-1, 128, 32, 32, 30] 0 Conv3d-214 [-1, 64, 32, 32, 30] 8,192 InstanceNorm3d-215 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-216 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-217 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-218 [-1, 128, 31, 32, 30] 256 PReLU-219 [-1, 128, 31, 32, 30] 128 Conv3d-220 [-1, 128, 31, 32, 30] 16,384 InstanceNorm3d-221 [-1, 128, 31, 32, 30] 256 PReLU-222 [-1, 128, 31, 32, 30] 128 Conv3d-223 [-1, 128, 31, 32, 30] 442,368 InstanceNorm3d-224 [-1, 128, 31, 32, 30] 256 PReLU-225 [-1, 128, 31, 32, 30] 128 Conv3d-226 [-1, 64, 31, 32, 30] 8,192 Conv3d-227 [-1, 64, 31, 32, 30] 8,192 InstanceNorm3d-228 [-1, 64, 31, 32, 30] 128 _BlockBo...eckNd-229 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-230 [-1, 64, 31, 32, 30] 128 PReLU-231 [-1, 64, 31, 32, 30] 64 Conv3d-232 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-233 [-1, 64, 31, 32, 30] 128 PReLU-234 [-1, 64, 31, 32, 30] 64 Conv3d-235 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-236 [-1, 64, 31, 32, 30] 128 PReLU-237 [-1, 64, 31, 32, 30] 64 Conv3d-238 [-1, 64, 31, 32, 30] 4,096 _BlockBo...eckNd-239 [-1, 64, 31, 32, 30] 0 _BlockResStkNd-240 [-1, 64, 31, 32, 30] 0 Conv3d-241 [-1, 1, 31, 32, 30] 8,001 UNet3d-242 [-1, 1, 31, 32, 30] 0 ================================================================ Total params: 133,109,121 Trainable params: 133,109,121 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 1218.39 Params size (MB): 507.77 Estimated Total Size (MB): 1726.50 ----------------------------------------------------------------","title":"<span class='magic-codeicon-class'>UNet3d</span>"},{"location":"apis/modules/resnet/UNet3d/#modulesresnetunet3d","text":"Class \u00b7 nn.Module \u00b7 Source net = mdnc . modules . resnet . UNet3d ( channel , layers , block = 'bottleneck' , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) This moule is a built-in model for 3D residual U-Net. The network is inspired by: nikhilroxtomar/Deep-Residual-Unet The network would down-sample and up-sample the input data according to the network depth. The depth is given by the length of the argument layers . The network structure is shown in the following chart: flowchart TB b1[\"Block 1<br>Stack of layers[0] blocks\"] b2[\"Block 2<br>Stack of layers[1] blocks\"] bi[\"Block ...<br>Stack of ... blocks\"] bn[\"Block n<br>Stack of layers[n-1] blocks\"] u1[\"Block 2n-1<br>Stack of layers[0] blocks\"] u2[\"Block 2n-2<br>Stack of layers[1] blocks\"] ui[\"Block ...<br>Stack of ... blocks\"] b1 -->|down<br>sampling| b2 -->|down<br>sampling| bi -->|down<br>sampling| bn bn -->|up<br>sampling| ui -->|up<br>sampling| u2 -->|up<br>sampling| u1 b1 -->|skip<br>connection| u1 b2 -->|skip<br>connection| u2 bi -->|skip<br>connection| ui linkStyle 0,1,2 stroke-width:4px, stroke:#800 ; linkStyle 3,4,5 stroke-width:4px, stroke:#080 ; linkStyle 6,7,8 stroke-width:4px, stroke:#888 ; The argument layers is a sequence of int . For each block \\(i\\) , it contains layers [ i - 1 ] repeated residual blocks (see mdnc.modules.resnet.BlockPlain3d and mdnc.modules.resnet.BlockBottleneck3d ). Each down-sampling or up-sampling is configured by stride = 2 . The channel number would be doubled in the down-sampling route and reduced to \u00bd in the up-sampling route. The skip connection is perfromed by concatenation.","title":"modules.resnet.UNet3d"},{"location":"apis/modules/resnet/UNet3d/#arguments","text":"Requries Argument Type Description channel int The channel number of the first hidden block (layer). After each down-sampling, the channel number would be doubled. After each up-sampling, the channel number would be reduced to \u00bd. layers ( int ,) A sequence of layer numbers for each block. Each number represents the number of residual blocks of a stage (block). The stage numer, i.e. the depth of the network is the length of this list. block str The residual block type, could be: 'plain' : see BlockPlain3d . 'bottleneck' : see BlockBottleneck3d . kernel_size int or ( int , int , int ) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/UNet3d/#operators","text":"","title":"Operators"},{"location":"apis/modules/resnet/UNet3d/#__call__","text":"y = net ( x ) The forward operator implemented by the forward() method. The input is a 3D tensor, and the output is the final output of this network. Requries Argument Type Description x torch . Tensor A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the input channel number, and (L1, L2, L3) is the input data size. Returns Argument Description y A 3D tensor, the size should be ( B , C , L1 , L2 , L3 ) , where B is the batch size, C is the output channel number, and (L1, L2, L3) is the input data size.","title":" __call__"},{"location":"apis/modules/resnet/UNet3d/#properties","text":"","title":"Properties"},{"location":"apis/modules/resnet/UNet3d/#nlayers","text":"net . nlayers The total number of convolutional layers along the depth of the network.","title":" nlayers"},{"location":"apis/modules/resnet/UNet3d/#examples","text":"Example Codes 1 2 3 4 5 import mdnc net = mdnc . modules . resnet . UNet3d ( 64 , [ 2 , 2 , 2 , 2 , 3 ], in_planes = 3 , out_planes = 1 ) print ( 'The number of convolutional layers along the depth is {0} .' . format ( net . nlayers )) mdnc . contribs . torchsummary . summary ( net , ( 3 , 31 , 32 , 30 ), device = 'cpu' ) Output The number of convolutional layers along the depth is 59. ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Conv3d-1 [-1, 64, 31, 32, 30] 24,000 InstanceNorm3d-2 [-1, 64, 31, 32, 30] 128 PReLU-3 [-1, 64, 31, 32, 30] 64 Conv3d-4 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-5 [-1, 64, 31, 32, 30] 128 PReLU-6 [-1, 64, 31, 32, 30] 64 Conv3d-7 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-8 [-1, 64, 31, 32, 30] 128 PReLU-9 [-1, 64, 31, 32, 30] 64 Conv3d-10 [-1, 64, 31, 32, 30] 4,096 _BlockBo...neckNd-11 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-12 [-1, 64, 31, 32, 30] 128 PReLU-13 [-1, 64, 31, 32, 30] 64 Conv3d-14 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-15 [-1, 64, 31, 32, 30] 128 PReLU-16 [-1, 64, 31, 32, 30] 64 Conv3d-17 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-18 [-1, 64, 16, 16, 15] 128 PReLU-19 [-1, 64, 16, 16, 15] 64 Conv3d-20 [-1, 64, 16, 16, 15] 4,096 Conv3d-21 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-22 [-1, 64, 16, 16, 15] 128 _BlockBo...neckNd-23 [-1, 64, 16, 16, 15] 0 _BlockResStkNd-24 [-1, 64, 16, 16, 15] 0 [-1, 64, 31, 32, 30] InstanceNorm3d-25 [-1, 64, 16, 16, 15] 128 PReLU-26 [-1, 64, 16, 16, 15] 64 Conv3d-27 [-1, 64, 16, 16, 15] 4,096 InstanceNorm3d-28 [-1, 64, 16, 16, 15] 128 PReLU-29 [-1, 64, 16, 16, 15] 64 Conv3d-30 [-1, 64, 16, 16, 15] 110,592 InstanceNorm3d-31 [-1, 64, 16, 16, 15] 128 PReLU-32 [-1, 64, 16, 16, 15] 64 Conv3d-33 [-1, 128, 16, 16, 15] 8,192 Conv3d-34 [-1, 128, 16, 16, 15] 8,192 InstanceNorm3d-35 [-1, 128, 16, 16, 15] 256 _BlockBo...neckNd-36 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-37 [-1, 128, 16, 16, 15] 256 PReLU-38 [-1, 128, 16, 16, 15] 128 Conv3d-39 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-40 [-1, 128, 16, 16, 15] 256 PReLU-41 [-1, 128, 16, 16, 15] 128 Conv3d-42 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-43 [-1, 128, 8, 8, 8] 256 PReLU-44 [-1, 128, 8, 8, 8] 128 Conv3d-45 [-1, 128, 8, 8, 8] 16,384 Conv3d-46 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-47 [-1, 128, 8, 8, 8] 256 _BlockBo...neckNd-48 [-1, 128, 8, 8, 8] 0 _BlockResStkNd-49 [-1, 128, 8, 8, 8] 0 [-1, 128, 16, 16, 15] InstanceNorm3d-50 [-1, 128, 8, 8, 8] 256 PReLU-51 [-1, 128, 8, 8, 8] 128 Conv3d-52 [-1, 128, 8, 8, 8] 16,384 InstanceNorm3d-53 [-1, 128, 8, 8, 8] 256 PReLU-54 [-1, 128, 8, 8, 8] 128 Conv3d-55 [-1, 128, 8, 8, 8] 442,368 InstanceNorm3d-56 [-1, 128, 8, 8, 8] 256 PReLU-57 [-1, 128, 8, 8, 8] 128 Conv3d-58 [-1, 256, 8, 8, 8] 32,768 Conv3d-59 [-1, 256, 8, 8, 8] 32,768 InstanceNorm3d-60 [-1, 256, 8, 8, 8] 512 _BlockBo...neckNd-61 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-62 [-1, 256, 8, 8, 8] 512 PReLU-63 [-1, 256, 8, 8, 8] 256 Conv3d-64 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-65 [-1, 256, 8, 8, 8] 512 PReLU-66 [-1, 256, 8, 8, 8] 256 Conv3d-67 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-68 [-1, 256, 4, 4, 4] 512 PReLU-69 [-1, 256, 4, 4, 4] 256 Conv3d-70 [-1, 256, 4, 4, 4] 65,536 Conv3d-71 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-72 [-1, 256, 4, 4, 4] 512 _BlockBo...neckNd-73 [-1, 256, 4, 4, 4] 0 _BlockResStkNd-74 [-1, 256, 4, 4, 4] 0 [-1, 256, 8, 8, 8] InstanceNorm3d-75 [-1, 256, 4, 4, 4] 512 PReLU-76 [-1, 256, 4, 4, 4] 256 Conv3d-77 [-1, 256, 4, 4, 4] 65,536 InstanceNorm3d-78 [-1, 256, 4, 4, 4] 512 PReLU-79 [-1, 256, 4, 4, 4] 256 Conv3d-80 [-1, 256, 4, 4, 4] 1,769,472 InstanceNorm3d-81 [-1, 256, 4, 4, 4] 512 PReLU-82 [-1, 256, 4, 4, 4] 256 Conv3d-83 [-1, 512, 4, 4, 4] 131,072 Conv3d-84 [-1, 512, 4, 4, 4] 131,072 InstanceNorm3d-85 [-1, 512, 4, 4, 4] 1,024 _BlockBo...neckNd-86 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-87 [-1, 512, 4, 4, 4] 1,024 PReLU-88 [-1, 512, 4, 4, 4] 512 Conv3d-89 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-90 [-1, 512, 4, 4, 4] 1,024 PReLU-91 [-1, 512, 4, 4, 4] 512 Conv3d-92 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-93 [-1, 512, 2, 2, 2] 1,024 PReLU-94 [-1, 512, 2, 2, 2] 512 Conv3d-95 [-1, 512, 2, 2, 2] 262,144 Conv3d-96 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-97 [-1, 512, 2, 2, 2] 1,024 _BlockBo...neckNd-98 [-1, 512, 2, 2, 2] 0 _BlockResStkNd-99 [-1, 512, 2, 2, 2] 0 [-1, 512, 4, 4, 4] InstanceNorm3d-100 [-1, 512, 2, 2, 2] 1,024 PReLU-101 [-1, 512, 2, 2, 2] 512 Conv3d-102 [-1, 512, 2, 2, 2] 262,144 InstanceNorm3d-103 [-1, 512, 2, 2, 2] 1,024 PReLU-104 [-1, 512, 2, 2, 2] 512 Conv3d-105 [-1, 512, 2, 2, 2] 7,077,888 InstanceNorm3d-106 [-1, 512, 2, 2, 2] 1,024 PReLU-107 [-1, 512, 2, 2, 2] 512 Conv3d-108 [-1, 1024, 2, 2, 2] 524,288 Conv3d-109 [-1, 1024, 2, 2, 2] 524,288 InstanceNorm3d-110 [-1, 1024, 2, 2, 2] 2,048 _BlockBo...eckNd-111 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-112 [-1, 1024, 2, 2, 2] 2,048 PReLU-113 [-1, 1024, 2, 2, 2] 1,024 Conv3d-114 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-115 [-1, 1024, 2, 2, 2] 2,048 PReLU-116 [-1, 1024, 2, 2, 2] 1,024 Conv3d-117 [-1, 1024, 2, 2, 2] 28,311,552 InstanceNorm3d-118 [-1, 1024, 2, 2, 2] 2,048 PReLU-119 [-1, 1024, 2, 2, 2] 1,024 Conv3d-120 [-1, 1024, 2, 2, 2] 1,048,576 _BlockBo...eckNd-121 [-1, 1024, 2, 2, 2] 0 InstanceNorm3d-122 [-1, 1024, 2, 2, 2] 2,048 PReLU-123 [-1, 1024, 2, 2, 2] 1,024 Conv3d-124 [-1, 1024, 2, 2, 2] 1,048,576 InstanceNorm3d-125 [-1, 1024, 2, 2, 2] 2,048 PReLU-126 [-1, 1024, 2, 2, 2] 1,024 Upsample-127 [-1, 1024, 4, 4, 4] 0 Conv3d-128 [-1, 1024, 4, 4, 4] 28,311,552 InstanceNorm3d-129 [-1, 1024, 4, 4, 4] 2,048 PReLU-130 [-1, 1024, 4, 4, 4] 1,024 Conv3d-131 [-1, 512, 4, 4, 4] 524,288 Upsample-132 [-1, 1024, 4, 4, 4] 0 Conv3d-133 [-1, 512, 4, 4, 4] 524,288 InstanceNorm3d-134 [-1, 512, 4, 4, 4] 1,024 _BlockBo...eckNd-135 [-1, 512, 4, 4, 4] 0 _BlockResStkNd-136 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-137 [-1, 1024, 4, 4, 4] 2,048 PReLU-138 [-1, 1024, 4, 4, 4] 1,024 Conv3d-139 [-1, 1024, 4, 4, 4] 1,048,576 InstanceNorm3d-140 [-1, 1024, 4, 4, 4] 2,048 PReLU-141 [-1, 1024, 4, 4, 4] 1,024 Conv3d-142 [-1, 1024, 4, 4, 4] 28,311,552 InstanceNorm3d-143 [-1, 1024, 4, 4, 4] 2,048 PReLU-144 [-1, 1024, 4, 4, 4] 1,024 Conv3d-145 [-1, 512, 4, 4, 4] 524,288 Conv3d-146 [-1, 512, 4, 4, 4] 524,288 InstanceNorm3d-147 [-1, 512, 4, 4, 4] 1,024 _BlockBo...eckNd-148 [-1, 512, 4, 4, 4] 0 InstanceNorm3d-149 [-1, 512, 4, 4, 4] 1,024 PReLU-150 [-1, 512, 4, 4, 4] 512 Conv3d-151 [-1, 512, 4, 4, 4] 262,144 InstanceNorm3d-152 [-1, 512, 4, 4, 4] 1,024 PReLU-153 [-1, 512, 4, 4, 4] 512 Upsample-154 [-1, 512, 8, 8, 8] 0 Conv3d-155 [-1, 512, 8, 8, 8] 7,077,888 InstanceNorm3d-156 [-1, 512, 8, 8, 8] 1,024 PReLU-157 [-1, 512, 8, 8, 8] 512 Conv3d-158 [-1, 256, 8, 8, 8] 131,072 Upsample-159 [-1, 512, 8, 8, 8] 0 Conv3d-160 [-1, 256, 8, 8, 8] 131,072 InstanceNorm3d-161 [-1, 256, 8, 8, 8] 512 _BlockBo...eckNd-162 [-1, 256, 8, 8, 8] 0 _BlockResStkNd-163 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-164 [-1, 512, 8, 8, 8] 1,024 PReLU-165 [-1, 512, 8, 8, 8] 512 Conv3d-166 [-1, 512, 8, 8, 8] 262,144 InstanceNorm3d-167 [-1, 512, 8, 8, 8] 1,024 PReLU-168 [-1, 512, 8, 8, 8] 512 Conv3d-169 [-1, 512, 8, 8, 8] 7,077,888 InstanceNorm3d-170 [-1, 512, 8, 8, 8] 1,024 PReLU-171 [-1, 512, 8, 8, 8] 512 Conv3d-172 [-1, 256, 8, 8, 8] 131,072 Conv3d-173 [-1, 256, 8, 8, 8] 131,072 InstanceNorm3d-174 [-1, 256, 8, 8, 8] 512 _BlockBo...eckNd-175 [-1, 256, 8, 8, 8] 0 InstanceNorm3d-176 [-1, 256, 8, 8, 8] 512 PReLU-177 [-1, 256, 8, 8, 8] 256 Conv3d-178 [-1, 256, 8, 8, 8] 65,536 InstanceNorm3d-179 [-1, 256, 8, 8, 8] 512 PReLU-180 [-1, 256, 8, 8, 8] 256 Upsample-181 [-1, 256, 16, 16, 16] 0 Conv3d-182 [-1, 256, 16, 16, 16] 1,769,472 InstanceNorm3d-183 [-1, 256, 16, 16, 16] 512 PReLU-184 [-1, 256, 16, 16, 16] 256 Conv3d-185 [-1, 128, 16, 16, 16] 32,768 Upsample-186 [-1, 256, 16, 16, 16] 0 Conv3d-187 [-1, 128, 16, 16, 16] 32,768 InstanceNorm3d-188 [-1, 128, 16, 16, 16] 256 _BlockBo...eckNd-189 [-1, 128, 16, 16, 16] 0 _BlockResStkNd-190 [-1, 128, 16, 16, 16] 0 InstanceNorm3d-191 [-1, 256, 16, 16, 15] 512 PReLU-192 [-1, 256, 16, 16, 15] 256 Conv3d-193 [-1, 256, 16, 16, 15] 65,536 InstanceNorm3d-194 [-1, 256, 16, 16, 15] 512 PReLU-195 [-1, 256, 16, 16, 15] 256 Conv3d-196 [-1, 256, 16, 16, 15] 1,769,472 InstanceNorm3d-197 [-1, 256, 16, 16, 15] 512 PReLU-198 [-1, 256, 16, 16, 15] 256 Conv3d-199 [-1, 128, 16, 16, 15] 32,768 Conv3d-200 [-1, 128, 16, 16, 15] 32,768 InstanceNorm3d-201 [-1, 128, 16, 16, 15] 256 _BlockBo...eckNd-202 [-1, 128, 16, 16, 15] 0 InstanceNorm3d-203 [-1, 128, 16, 16, 15] 256 PReLU-204 [-1, 128, 16, 16, 15] 128 Conv3d-205 [-1, 128, 16, 16, 15] 16,384 InstanceNorm3d-206 [-1, 128, 16, 16, 15] 256 PReLU-207 [-1, 128, 16, 16, 15] 128 Upsample-208 [-1, 128, 32, 32, 30] 0 Conv3d-209 [-1, 128, 32, 32, 30] 442,368 InstanceNorm3d-210 [-1, 128, 32, 32, 30] 256 PReLU-211 [-1, 128, 32, 32, 30] 128 Conv3d-212 [-1, 64, 32, 32, 30] 8,192 Upsample-213 [-1, 128, 32, 32, 30] 0 Conv3d-214 [-1, 64, 32, 32, 30] 8,192 InstanceNorm3d-215 [-1, 64, 32, 32, 30] 128 _BlockBo...eckNd-216 [-1, 64, 32, 32, 30] 0 _BlockResStkNd-217 [-1, 64, 32, 32, 30] 0 InstanceNorm3d-218 [-1, 128, 31, 32, 30] 256 PReLU-219 [-1, 128, 31, 32, 30] 128 Conv3d-220 [-1, 128, 31, 32, 30] 16,384 InstanceNorm3d-221 [-1, 128, 31, 32, 30] 256 PReLU-222 [-1, 128, 31, 32, 30] 128 Conv3d-223 [-1, 128, 31, 32, 30] 442,368 InstanceNorm3d-224 [-1, 128, 31, 32, 30] 256 PReLU-225 [-1, 128, 31, 32, 30] 128 Conv3d-226 [-1, 64, 31, 32, 30] 8,192 Conv3d-227 [-1, 64, 31, 32, 30] 8,192 InstanceNorm3d-228 [-1, 64, 31, 32, 30] 128 _BlockBo...eckNd-229 [-1, 64, 31, 32, 30] 0 InstanceNorm3d-230 [-1, 64, 31, 32, 30] 128 PReLU-231 [-1, 64, 31, 32, 30] 64 Conv3d-232 [-1, 64, 31, 32, 30] 4,096 InstanceNorm3d-233 [-1, 64, 31, 32, 30] 128 PReLU-234 [-1, 64, 31, 32, 30] 64 Conv3d-235 [-1, 64, 31, 32, 30] 110,592 InstanceNorm3d-236 [-1, 64, 31, 32, 30] 128 PReLU-237 [-1, 64, 31, 32, 30] 64 Conv3d-238 [-1, 64, 31, 32, 30] 4,096 _BlockBo...eckNd-239 [-1, 64, 31, 32, 30] 0 _BlockResStkNd-240 [-1, 64, 31, 32, 30] 0 Conv3d-241 [-1, 1, 31, 32, 30] 8,001 UNet3d-242 [-1, 1, 31, 32, 30] 0 ================================================================ Total params: 133,109,121 Trainable params: 133,109,121 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.34 Forward/backward pass size (MB): 1218.39 Params size (MB): 507.77 Estimated Total Size (MB): 1726.50 ----------------------------------------------------------------","title":"Examples"},{"location":"apis/modules/resnet/aestar/","text":"modules.resnet.ae* \u00b6 Function \u00b7 nn.Module net = ae * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.resnet.AE*d . Arguments \u00b6 Requries Argument Type Description order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. APIs \u00b6 API Net depth Channel Block type Layer configs Source ae16 3 64 plain [ 2 , 1 , 1 ] ae32 3 64 bottleneck [ 2 , 2 , 2 ] ae44 4 64 bottleneck [ 2 , 2 , 2 , 2 ] ae65 4 64 bottleneck [ 3 , 3 , 3 , 3 ] ae83 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ]","title":"<span class='magic-codeicon-function'>ae*</span>"},{"location":"apis/modules/resnet/aestar/#modulesresnetae","text":"Function \u00b7 nn.Module net = ae * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.resnet.AE*d .","title":"modules.resnet.ae*"},{"location":"apis/modules/resnet/aestar/#arguments","text":"Requries Argument Type Description order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/aestar/#apis","text":"API Net depth Channel Block type Layer configs Source ae16 3 64 plain [ 2 , 1 , 1 ] ae32 3 64 bottleneck [ 2 , 2 , 2 ] ae44 4 64 bottleneck [ 2 , 2 , 2 , 2 ] ae65 4 64 bottleneck [ 3 , 3 , 3 , 3 ] ae83 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ]","title":"APIs"},{"location":"apis/modules/resnet/decnetstar/","text":"modules.resnet.decnet* \u00b6 Function \u00b7 nn.Module net = decnet * ( out_size , order = 2 , kernel_size = 3 , in_length = 2 , out_planes = 1 ) Instant presents of mdnc.module.resnet.DecoderNet*d . Arguments \u00b6 Requries Argument Type Description out_size int or ( int ,) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data. APIs \u00b6 API Net depth Channel Block type Layer configs Source decnet13 5 64 plain [ 1 , 1 , 1 , 1 , 1 ] decnet33 5 64 bottleneck [ 2 , 2 , 2 , 2 , 2 ] decnet48 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ] decnet63 5 64 bottleneck [ 4 , 4 , 4 , 4 , 4 ]","title":"<span class='magic-codeicon-function'>decnet*</span>"},{"location":"apis/modules/resnet/decnetstar/#modulesresnetdecnet","text":"Function \u00b7 nn.Module net = decnet * ( out_size , order = 2 , kernel_size = 3 , in_length = 2 , out_planes = 1 ) Instant presents of mdnc.module.resnet.DecoderNet*d .","title":"modules.resnet.decnet*"},{"location":"apis/modules/resnet/decnetstar/#arguments","text":"Requries Argument Type Description out_size int or ( int ,) The size of the output data. This argument needs to be specified by users, because the network needs to configure its layers according to the output size. order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_length int The length of the input vector, if not set, the input needs to be feature maps. See the property input_size to check the input data size in this case. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/decnetstar/#apis","text":"API Net depth Channel Block type Layer configs Source decnet13 5 64 plain [ 1 , 1 , 1 , 1 , 1 ] decnet33 5 64 bottleneck [ 2 , 2 , 2 , 2 , 2 ] decnet48 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ] decnet63 5 64 bottleneck [ 4 , 4 , 4 , 4 , 4 ]","title":"APIs"},{"location":"apis/modules/resnet/encnetstar/","text":"modules.resnet.encnet* \u00b6 Function \u00b7 nn.Module net = encnet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_length = 2 ) Instant presents of mdnc.module.resnet.EncoderNet*d . Arguments \u00b6 Requries Argument Type Description order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened. APIs \u00b6 API Net depth Channel Block type Layer configs Source encnet12 5 64 plain [ 1 , 1 , 1 , 1 , 1 ] encnet32 5 64 bottleneck [ 2 , 2 , 2 , 2 , 2 ] encnet47 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ] encnet62 5 64 bottleneck [ 4 , 4 , 4 , 4 , 4 ]","title":"<span class='magic-codeicon-function'>encnet*</span>"},{"location":"apis/modules/resnet/encnetstar/#modulesresnetencnet","text":"Function \u00b7 nn.Module net = encnet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_length = 2 ) Instant presents of mdnc.module.resnet.EncoderNet*d .","title":"modules.resnet.encnet*"},{"location":"apis/modules/resnet/encnetstar/#arguments","text":"Requries Argument Type Description order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_planes int The channel number of the input data. out_length int The length of the output vector, if not set, the output would not be flattened.","title":"Arguments"},{"location":"apis/modules/resnet/encnetstar/#apis","text":"API Net depth Channel Block type Layer configs Source encnet12 5 64 plain [ 1 , 1 , 1 , 1 , 1 ] encnet32 5 64 bottleneck [ 2 , 2 , 2 , 2 , 2 ] encnet47 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ] encnet62 5 64 bottleneck [ 4 , 4 , 4 , 4 , 4 ]","title":"APIs"},{"location":"apis/modules/resnet/unetstar/","text":"modules.resnet.unet* \u00b6 Function \u00b7 nn.Module net = unet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.resnet.UNet*d . Arguments \u00b6 Requries Argument Type Description order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data. APIs \u00b6 API Net depth Channel Block type Layer configs Source unet16 3 64 plain [ 2 , 1 , 1 ] unet32 3 64 bottleneck [ 2 , 2 , 2 ] unet44 4 64 bottleneck [ 2 , 2 , 2 , 2 ] unet65 4 64 bottleneck [ 3 , 3 , 3 , 3 ] unet83 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ] where unet16 is a nearly replicated work of nikhilroxtomar/Deep-Residual-Unet . The only difference of unet16 is one more modern convolutional layer in the first stage.","title":"<span class='magic-codeicon-function'>unet*</span>"},{"location":"apis/modules/resnet/unetstar/#modulesresnetunet","text":"Function \u00b7 nn.Module net = unet * ( order = 2 , kernel_size = 3 , in_planes = 1 , out_planes = 1 ) Instant presents of mdnc.module.resnet.UNet*d .","title":"modules.resnet.unet*"},{"location":"apis/modules/resnet/unetstar/#arguments","text":"Requries Argument Type Description order int The order of the residual blocks, could be 1 , 2 , or 3 . kernel_size int or ( int ,) The kernel size of each residual block. in_planes int The channel number of the input data. out_planes int The channel number of the output data.","title":"Arguments"},{"location":"apis/modules/resnet/unetstar/#apis","text":"API Net depth Channel Block type Layer configs Source unet16 3 64 plain [ 2 , 1 , 1 ] unet32 3 64 bottleneck [ 2 , 2 , 2 ] unet44 4 64 bottleneck [ 2 , 2 , 2 , 2 ] unet65 4 64 bottleneck [ 3 , 3 , 3 , 3 ] unet83 5 64 bottleneck [ 3 , 3 , 3 , 3 , 3 ] where unet16 is a nearly replicated work of nikhilroxtomar/Deep-Residual-Unet . The only difference of unet16 is one more modern convolutional layer in the first stage.","title":"APIs"},{"location":"apis/utils/draw/AxisMultipleTicker/","text":"utils.draw.AxisMultipleTicker \u00b6 Class \u00b7 Source amticker = mdnc . utils . draw . AxisMultipleTicker ( den_major = 2 , den_minor = 5 , number = np . pi , symbol = r '\\pi' ) Use multiple locator to define the formatted axis. Inspired by the following post: https://stackoverflow.com/a/53586826 This class is a factory class, which could produce 3 properties used for formatting a special axis (like a trigonometric axis): formatter , major_locator , minor_locator . See the Examples to view how to use it. Arguments \u00b6 Requries Argument Type Description den_major int The denominator for the major ticks. den_minor int The denominator for the minor ticks. number float The value that each symbol represents. symbol int The displayed symbol of the major ticks. Properties \u00b6 formatter \u00b6 amticker . formatter The major formatter. Use the matplotlib API axis . set_major_formatter () to set it. major_locator \u00b6 amticker . major_locator The major locator. Use the matplotlib API axis . set_major_locator () to set it. minor_locator \u00b6 amticker . minor_locator The minor locator. Use the matplotlib API axis . set_set_minor_locator () to set it. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( - 3 , 3 , 100 ) amticker = mdnc . utils . draw . AxisMultipleTicker ( number = np . pi , symbol = r '\\pi' ) _ , ax = plt . subplots ( nrows = 1 , ncols = 1 , figsize = ( 8 , 3 )) ax . plot ( t , np . arctan2 ( np . sin ( t * np . pi ), np . cos ( t * np . pi ))) ax . yaxis . set_major_formatter ( amticker . formatter ) ax . yaxis . set_major_locator ( amticker . major_locator ) ax . yaxis . set_minor_locator ( amticker . minor_locator ) plt . tight_layout () plt . show () Output","title":"<span class='magic-codeicon-class'>AxisMultipleTicker</span>"},{"location":"apis/utils/draw/AxisMultipleTicker/#utilsdrawaxismultipleticker","text":"Class \u00b7 Source amticker = mdnc . utils . draw . AxisMultipleTicker ( den_major = 2 , den_minor = 5 , number = np . pi , symbol = r '\\pi' ) Use multiple locator to define the formatted axis. Inspired by the following post: https://stackoverflow.com/a/53586826 This class is a factory class, which could produce 3 properties used for formatting a special axis (like a trigonometric axis): formatter , major_locator , minor_locator . See the Examples to view how to use it.","title":"utils.draw.AxisMultipleTicker"},{"location":"apis/utils/draw/AxisMultipleTicker/#arguments","text":"Requries Argument Type Description den_major int The denominator for the major ticks. den_minor int The denominator for the minor ticks. number float The value that each symbol represents. symbol int The displayed symbol of the major ticks.","title":"Arguments"},{"location":"apis/utils/draw/AxisMultipleTicker/#properties","text":"","title":"Properties"},{"location":"apis/utils/draw/AxisMultipleTicker/#formatter","text":"amticker . formatter The major formatter. Use the matplotlib API axis . set_major_formatter () to set it.","title":" formatter"},{"location":"apis/utils/draw/AxisMultipleTicker/#major_locator","text":"amticker . major_locator The major locator. Use the matplotlib API axis . set_major_locator () to set it.","title":" major_locator"},{"location":"apis/utils/draw/AxisMultipleTicker/#minor_locator","text":"amticker . minor_locator The minor locator. Use the matplotlib API axis . set_set_minor_locator () to set it.","title":" minor_locator"},{"location":"apis/utils/draw/AxisMultipleTicker/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( - 3 , 3 , 100 ) amticker = mdnc . utils . draw . AxisMultipleTicker ( number = np . pi , symbol = r '\\pi' ) _ , ax = plt . subplots ( nrows = 1 , ncols = 1 , figsize = ( 8 , 3 )) ax . plot ( t , np . arctan2 ( np . sin ( t * np . pi ), np . cos ( t * np . pi ))) ax . yaxis . set_major_formatter ( amticker . formatter ) ax . yaxis . set_major_locator ( amticker . major_locator ) ax . yaxis . set_minor_locator ( amticker . minor_locator ) plt . tight_layout () plt . show () Output","title":"Examples"},{"location":"apis/utils/draw/fix_log_axis/","text":"utils.draw.fix_log_axis \u00b6 Function \u00b7 Source mdnc . utils . draw . fix_log_axis ( ax = None , axis = 'y' ) Control the log axis to be limited in 10 n ticks. This function is not recommended, because it would take effects globally. Please use mdnc.utils.draw.setFigure as a safer way. Arguments \u00b6 Requries Argument Type Description ax object The subplot that requires to be controlled. If set None , the plt . gca () would be used. axis str Make which axis to be normalized, could be: 'x' , 'y' or 'xy' . Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( 0 , 1 , 100 ) plt . plot ( t , 0.01 + 0.04 * t ) plt . yscale ( 'log' ) mdnc . utils . draw . fix_log_axis ( axis = 'y' ) plt . show () Output","title":"<span class='magic-codeicon-function'>fix_log_axis</span>"},{"location":"apis/utils/draw/fix_log_axis/#utilsdrawfix_log_axis","text":"Function \u00b7 Source mdnc . utils . draw . fix_log_axis ( ax = None , axis = 'y' ) Control the log axis to be limited in 10 n ticks. This function is not recommended, because it would take effects globally. Please use mdnc.utils.draw.setFigure as a safer way.","title":"utils.draw.fix_log_axis"},{"location":"apis/utils/draw/fix_log_axis/#arguments","text":"Requries Argument Type Description ax object The subplot that requires to be controlled. If set None , the plt . gca () would be used. axis str Make which axis to be normalized, could be: 'x' , 'y' or 'xy' .","title":"Arguments"},{"location":"apis/utils/draw/fix_log_axis/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 import numpy as np import matplotlib.pyplot as plt import mdnc t = np . linspace ( 0 , 1 , 100 ) plt . plot ( t , 0.01 + 0.04 * t ) plt . yscale ( 'log' ) mdnc . utils . draw . fix_log_axis ( axis = 'y' ) plt . show () Output","title":"Examples"},{"location":"apis/utils/draw/plot_bar/","text":"utils.draw.plot_bar \u00b6 Function \u00b7 Source mdnc . utils . draw . plot_bar ( gen , num , xlabel = None , ylabel = 'value' , x_tick_labels = None , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a bar graph for multiple result groups. Each group is given by a 1D data sample array. Arguments \u00b6 Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns a 1D data. num int The total number of data samples yield by the generator. xlabel str The x axis label. ylabel str The y axis label. x_tick_labels ( str , ) The x tick labels (a sequence) that is used for overriding the original value [ 0 , 1 , 2 , ... ] . y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'dark_background' , font_size = 14 ) def test_plot_bar (): def func_gen (): size = 5 x1 = np . abs ( np . random . normal ( loc = 6.0 , scale = 3.0 , size = size )) yield x1 , { 'label' : '$x_1$' } x2 = np . abs ( np . random . normal ( loc = 9.0 , scale = 6.0 , size = size )) yield x2 , { 'label' : '$x_2$' } x3 = np . abs ( np . random . normal ( loc = 12.0 , scale = 6.0 , size = size )) yield x3 , { 'label' : '$x_3$' } mdnc . utils . draw . plot_bar ( func_gen (), num = 3 , ylabel = 'y' , y_log = False , x_tick_labels = [ 'Jan.' , 'Feb.' , 'Mar.' , 'Apr.' , 'May' ]) plt . show () test_plot_bar () Output","title":"<span class='magic-codeicon-function'>plot_bar</span>"},{"location":"apis/utils/draw/plot_bar/#utilsdrawplot_bar","text":"Function \u00b7 Source mdnc . utils . draw . plot_bar ( gen , num , xlabel = None , ylabel = 'value' , x_tick_labels = None , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a bar graph for multiple result groups. Each group is given by a 1D data sample array.","title":"utils.draw.plot_bar"},{"location":"apis/utils/draw/plot_bar/#arguments","text":"Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns a 1D data. num int The total number of data samples yield by the generator. xlabel str The x axis label. ylabel str The y axis label. x_tick_labels ( str , ) The x tick labels (a sequence) that is used for overriding the original value [ 0 , 1 , 2 , ... ] . y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead.","title":"Arguments"},{"location":"apis/utils/draw/plot_bar/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'dark_background' , font_size = 14 ) def test_plot_bar (): def func_gen (): size = 5 x1 = np . abs ( np . random . normal ( loc = 6.0 , scale = 3.0 , size = size )) yield x1 , { 'label' : '$x_1$' } x2 = np . abs ( np . random . normal ( loc = 9.0 , scale = 6.0 , size = size )) yield x2 , { 'label' : '$x_2$' } x3 = np . abs ( np . random . normal ( loc = 12.0 , scale = 6.0 , size = size )) yield x3 , { 'label' : '$x_3$' } mdnc . utils . draw . plot_bar ( func_gen (), num = 3 , ylabel = 'y' , y_log = False , x_tick_labels = [ 'Jan.' , 'Feb.' , 'Mar.' , 'Apr.' , 'May' ]) plt . show () test_plot_bar () Output","title":"Examples"},{"location":"apis/utils/draw/plot_distribution_curves/","text":"utils.draw.plot_distribution_curves \u00b6 Function \u00b7 Source mdnc . utils . draw . plot_distribution_curves ( gen , method = 'mean' , level = 3 , outlier = 0.1 , xlabel = None , ylabel = 'value' , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot lines with multi-level distribution for multiple data groups. Each group is given by: one 1D array and one 2D array, representing the x axis (assuming to have a shape of ( N ,) ) and a stack of data samples (with a shape of ( N , M ) ). Each time point corresponds to M samples in the same distribution. or a 2D array. In this case, we only have the ( N , M ) data stack. Tip This function has similar meaning of plot_error_curves . It is used for compressing the time-series histograms. Its output is similar to tensorboard.distribution . Arguments \u00b6 Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 4 1D arrays, or 2 2D arrays, or 2 1D arrays, or a 4D array, or a 2D array, or a 1D array. method str The method for calculating curves, could be 'mean' or 'middle' , representing the mean value and the median value respectively. level int The histogram level. outlier float Outlier proportion (should be 0~1), the part marked as outliers would be thrown away when drawing the figures. xlabel str The x axis label. ylabel str The y axis label. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'classic' , font_size = 16 ) def test_distribution (): def func_gen (): size = 100 x = np . arange ( start = 0 , stop = size ) for i in range ( 1 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () exp_v = np . square (( x - size ) / size ) - 1.0 exp_vnoise = np . random . normal ( 0.0 , np . expand_dims (( size - x ) / ( 10 * size ), axis =- 1 ), ( size , 50 )) v = begin * np . exp (( np . expand_dims ( exp_v , axis =- 1 ) + exp_vnoise ) * end ) yield x , v , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_distribution_curves ( func_gen (), method = 'mean' , level = 5 , outlier = 0.05 , xlabel = 'Step' , ylabel = r '$\\mathcal {L} $' , y_log = True ) plt . show () test_distribution () Output","title":"<span class='magic-codeicon-function'>plot_distribution_curves</span>"},{"location":"apis/utils/draw/plot_distribution_curves/#utilsdrawplot_distribution_curves","text":"Function \u00b7 Source mdnc . utils . draw . plot_distribution_curves ( gen , method = 'mean' , level = 3 , outlier = 0.1 , xlabel = None , ylabel = 'value' , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot lines with multi-level distribution for multiple data groups. Each group is given by: one 1D array and one 2D array, representing the x axis (assuming to have a shape of ( N ,) ) and a stack of data samples (with a shape of ( N , M ) ). Each time point corresponds to M samples in the same distribution. or a 2D array. In this case, we only have the ( N , M ) data stack. Tip This function has similar meaning of plot_error_curves . It is used for compressing the time-series histograms. Its output is similar to tensorboard.distribution .","title":"utils.draw.plot_distribution_curves"},{"location":"apis/utils/draw/plot_distribution_curves/#arguments","text":"Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 4 1D arrays, or 2 2D arrays, or 2 1D arrays, or a 4D array, or a 2D array, or a 1D array. method str The method for calculating curves, could be 'mean' or 'middle' , representing the mean value and the median value respectively. level int The histogram level. outlier float Outlier proportion (should be 0~1), the part marked as outliers would be thrown away when drawing the figures. xlabel str The x axis label. ylabel str The y axis label. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead.","title":"Arguments"},{"location":"apis/utils/draw/plot_distribution_curves/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'classic' , font_size = 16 ) def test_distribution (): def func_gen (): size = 100 x = np . arange ( start = 0 , stop = size ) for i in range ( 1 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () exp_v = np . square (( x - size ) / size ) - 1.0 exp_vnoise = np . random . normal ( 0.0 , np . expand_dims (( size - x ) / ( 10 * size ), axis =- 1 ), ( size , 50 )) v = begin * np . exp (( np . expand_dims ( exp_v , axis =- 1 ) + exp_vnoise ) * end ) yield x , v , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_distribution_curves ( func_gen (), method = 'mean' , level = 5 , outlier = 0.05 , xlabel = 'Step' , ylabel = r '$\\mathcal {L} $' , y_log = True ) plt . show () test_distribution () Output","title":"Examples"},{"location":"apis/utils/draw/plot_error_curves/","text":"utils.draw.plot_error_curves \u00b6 Function \u00b7 Source mdnc . utils . draw . plot_error_curves ( gen , x_error_num = 10 , y_error_method = 'std' , plot_method = 'error' , xlabel = None , ylabel = 'value' , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot lines with error bars for multiple data groups. Each group is given by: one 1D array and one 2D array, representing the x axis (assuming to have a shape of ( N ,) ) and a stack of data samples (with a shape of ( N , M ) ). Each time point corresponds to M samples in the same distribution. or a 2D array. In this case, we only have the ( N , M ) data stack. Arguments \u00b6 Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 1D + 2D arrays, or a single 2D array. x_error_num int The number of displayed error bars. y_error_method str The method for calculating the error bar. Could be: 'std' : use standard error. 'minmax' : use the range of the data. plot_method str The method for plotting the figure. Could be: 'error' : use error bar graph. 'fill' : use fill_between graph. xlabel str The x axis label. ylabel str The y axis label. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'bmh' , font_size = 16 ) def test_error_bar (): def func_gen (): size = 100 x = np . arange ( start = 0 , stop = size ) for i in range ( 3 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () exp_v = np . square (( x - size ) / size ) - 1.0 exp_vnoise = np . random . normal ( 0.0 , np . expand_dims (( size - x ) / ( 10 * size ), axis =- 1 ), ( size , 50 )) v = begin * np . exp (( np . expand_dims ( exp_v , axis =- 1 ) + exp_vnoise ) * end ) yield x , v , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_error_curves ( func_gen (), y_log = True , y_error_method = 'minmax' , xlabel = 'Step' , ylabel = r '$\\mathcal {L} $' ) plt . show () mdnc . utils . draw . plot_error_curves ( func_gen (), y_log = True , y_error_method = 'minmax' , plot_method = 'fill' , xlabel = 'Step' , ylabel = r '$\\mathcal {L} $' ) plt . show () test_error_bar () Output","title":"<span class='magic-codeicon-function'>plot_error_curves</span>"},{"location":"apis/utils/draw/plot_error_curves/#utilsdrawplot_error_curves","text":"Function \u00b7 Source mdnc . utils . draw . plot_error_curves ( gen , x_error_num = 10 , y_error_method = 'std' , plot_method = 'error' , xlabel = None , ylabel = 'value' , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot lines with error bars for multiple data groups. Each group is given by: one 1D array and one 2D array, representing the x axis (assuming to have a shape of ( N ,) ) and a stack of data samples (with a shape of ( N , M ) ). Each time point corresponds to M samples in the same distribution. or a 2D array. In this case, we only have the ( N , M ) data stack.","title":"utils.draw.plot_error_curves"},{"location":"apis/utils/draw/plot_error_curves/#arguments","text":"Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 1D + 2D arrays, or a single 2D array. x_error_num int The number of displayed error bars. y_error_method str The method for calculating the error bar. Could be: 'std' : use standard error. 'minmax' : use the range of the data. plot_method str The method for plotting the figure. Could be: 'error' : use error bar graph. 'fill' : use fill_between graph. xlabel str The x axis label. ylabel str The y axis label. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead.","title":"Arguments"},{"location":"apis/utils/draw/plot_error_curves/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'bmh' , font_size = 16 ) def test_error_bar (): def func_gen (): size = 100 x = np . arange ( start = 0 , stop = size ) for i in range ( 3 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () exp_v = np . square (( x - size ) / size ) - 1.0 exp_vnoise = np . random . normal ( 0.0 , np . expand_dims (( size - x ) / ( 10 * size ), axis =- 1 ), ( size , 50 )) v = begin * np . exp (( np . expand_dims ( exp_v , axis =- 1 ) + exp_vnoise ) * end ) yield x , v , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_error_curves ( func_gen (), y_log = True , y_error_method = 'minmax' , xlabel = 'Step' , ylabel = r '$\\mathcal {L} $' ) plt . show () mdnc . utils . draw . plot_error_curves ( func_gen (), y_log = True , y_error_method = 'minmax' , plot_method = 'fill' , xlabel = 'Step' , ylabel = r '$\\mathcal {L} $' ) plt . show () test_error_bar () Output","title":"Examples"},{"location":"apis/utils/draw/plot_hist/","text":"utils.draw.plot_hist \u00b6 Function \u00b7 Source mdnc . utils . draw . plot_hist ( gen , normalized = False , cumulative = False , xlabel = 'Value' , ylabel = 'Number of samples' , x_log = False , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a histogram for multiple distributions. Each distribution is given by a 1D data sample array. Arguments \u00b6 Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns a 1D data. normalized bool A flag. Whether to use normalization for each group when drawing the histogram. xlabel str The x axis label. ylabel str The y axis label. x_log bool A flag. Whether to convert the x axis into the logarithmic format. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'ggplot' , font_size = 14 ) def test_plot_hist (): def func_gen (): getbins = np . linspace ( 0 , 25 , 80 ) x1 = np . random . normal ( loc = 7.0 , scale = 1.0 , size = 100 ) yield x1 , { 'bins' : getbins , 'label' : '$x_1$' } x2 = np . random . normal ( loc = 12.0 , scale = 3.0 , size = 1000 ) yield x2 , { 'bins' : getbins , 'label' : '$x_2$' } mdnc . utils . draw . plot_hist ( func_gen (), xlabel = 'x' , normalized = True , cumulative = False ) plt . show () test_plot_hist () Output","title":"<span class='magic-codeicon-function'>plot_hist</span>"},{"location":"apis/utils/draw/plot_hist/#utilsdrawplot_hist","text":"Function \u00b7 Source mdnc . utils . draw . plot_hist ( gen , normalized = False , cumulative = False , xlabel = 'Value' , ylabel = 'Number of samples' , x_log = False , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a histogram for multiple distributions. Each distribution is given by a 1D data sample array.","title":"utils.draw.plot_hist"},{"location":"apis/utils/draw/plot_hist/#arguments","text":"Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns a 1D data. normalized bool A flag. Whether to use normalization for each group when drawing the histogram. xlabel str The x axis label. ylabel str The y axis label. x_log bool A flag. Whether to convert the x axis into the logarithmic format. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead.","title":"Arguments"},{"location":"apis/utils/draw/plot_hist/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'ggplot' , font_size = 14 ) def test_plot_hist (): def func_gen (): getbins = np . linspace ( 0 , 25 , 80 ) x1 = np . random . normal ( loc = 7.0 , scale = 1.0 , size = 100 ) yield x1 , { 'bins' : getbins , 'label' : '$x_1$' } x2 = np . random . normal ( loc = 12.0 , scale = 3.0 , size = 1000 ) yield x2 , { 'bins' : getbins , 'label' : '$x_2$' } mdnc . utils . draw . plot_hist ( func_gen (), xlabel = 'x' , normalized = True , cumulative = False ) plt . show () test_plot_hist () Output","title":"Examples"},{"location":"apis/utils/draw/plot_scatter/","text":"utils.draw.plot_scatter \u00b6 Function \u00b7 Source mdnc . utils . draw . plot_scatter ( gen , xlabel = None , ylabel = 'value' , x_log = None , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a scatter graph for multiple data groups. Each group is given by: a 1D data arrays (x, y coordinates), or a 2D array ( N , 2 ) , the second axis represents the x, y corrdincates. Arguments \u00b6 Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 2 1D arrays or a 2D array. xlabel str The x axis label. ylabel str The y axis label. x_log bool A flag. Whether to convert the x axis into the logarithmic format. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'seaborn-darkgrid' , font_size = 16 ) def test_scatter (): def func_gen (): size = 100 for i in range ( 3 ): center = - 4.0 + 4.0 * np . random . rand ( 2 ) scale = 0.5 + 2.0 * np . random . rand ( 2 ) x1 = np . random . normal ( loc = center [ 0 ], scale = scale [ 0 ], size = size ) x2 = np . random . normal ( loc = center [ 1 ], scale = scale [ 1 ], size = size ) yield np . power ( 10 , x1 ), np . power ( 10 , x2 ), { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_scatter ( func_gen (), x_log = True , y_log = True , xlabel = 'Metric 1' , ylabel = 'Metric 2' ) plt . show () test_scatter () Output","title":"<span class='magic-codeicon-function'>plot_scatter</span>"},{"location":"apis/utils/draw/plot_scatter/#utilsdrawplot_scatter","text":"Function \u00b7 Source mdnc . utils . draw . plot_scatter ( gen , xlabel = None , ylabel = 'value' , x_log = None , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a scatter graph for multiple data groups. Each group is given by: a 1D data arrays (x, y coordinates), or a 2D array ( N , 2 ) , the second axis represents the x, y corrdincates.","title":"utils.draw.plot_scatter"},{"location":"apis/utils/draw/plot_scatter/#arguments","text":"Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 2 1D arrays or a 2D array. xlabel str The x axis label. ylabel str The y axis label. x_log bool A flag. Whether to convert the x axis into the logarithmic format. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead.","title":"Arguments"},{"location":"apis/utils/draw/plot_scatter/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'seaborn-darkgrid' , font_size = 16 ) def test_scatter (): def func_gen (): size = 100 for i in range ( 3 ): center = - 4.0 + 4.0 * np . random . rand ( 2 ) scale = 0.5 + 2.0 * np . random . rand ( 2 ) x1 = np . random . normal ( loc = center [ 0 ], scale = scale [ 0 ], size = size ) x2 = np . random . normal ( loc = center [ 1 ], scale = scale [ 1 ], size = size ) yield np . power ( 10 , x1 ), np . power ( 10 , x2 ), { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_scatter ( func_gen (), x_log = True , y_log = True , xlabel = 'Metric 1' , ylabel = 'Metric 2' ) plt . show () test_scatter () Output","title":"Examples"},{"location":"apis/utils/draw/plot_training_records/","text":"utils.draw.plot_training_records \u00b6 Function \u00b7 Source mdnc . utils . draw . plot_training_records ( gen , xlabel = None , ylabel = 'value' , x_mark_num = None , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a training curve graph for multiple data groups. Each group is given by: 4 1D arrays, representing the x axis of training metrics, the trainining metric values, the x axis of validation metrics, the validation metric values respectively. or 2 2D arrays. Both of them have a shape of ( N , 2 ) . The two arrays represents the x axis and training metrics, the x axis and validation metric values respectively. or 2 1D arrays. In this case, the validation data is not provided. The two arrays represents the x axis of training metrics, the trainining metric values repspectively. or a 4D array. The 4 columns represents the x axis of training metrics, the trainining metric values, the x axis of validation metrics, the validation metric values respectively. or a 2D array. In this case, the validation data is not provided. The two columns represents the x axis of training metrics, the trainining metric values repspectively. or a 1D array. In this case, the validation data is not provided. The data represnets the training metrics. The x axis would be generated automatically. Arguments \u00b6 Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 4 1D arrays, or 2 2D arrays, or 2 1D arrays, or a 4D array, or a 2D array, or a 1D array. xlabel str The x axis label. ylabel str The y axis label. x_mark_num int The number of markers for the x axis. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'Solarize_Light2' , font_size = 14 ) def test_training_records (): def func_gen_batch (): size = 100 x = np . arange ( start = 0 , stop = size ) for i in range ( 3 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () v = begin * np . exp (( np . square (( x - size ) / size ) - 1.0 ) * end ) yield x , v , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } def func_gen_epoch (): size = 10 x = np . arange ( start = 0 , stop = size ) for i in range ( 3 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () v = begin * np . exp (( np . square (( x - size ) / size ) - 1.0 ) * end ) val_v = begin * np . exp (( np . square (( x - size ) / size ) - 1.0 ) * ( end - 1 )) data = np . stack ([ x , v , x , val_v ], axis = 0 ) yield data , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_training_records ( func_gen_batch (), y_log = True , x_mark_num = 10 , xlabel = 'Step' , ylabel = r 'Batch $\\mathcal {L} $' ) plt . show () mdnc . utils . draw . plot_training_records ( func_gen_epoch (), y_log = True , x_mark_num = 10 , xlabel = 'Step' , ylabel = r 'Epoch $\\mathcal {L} $' ) plt . show () test_training_records () Output","title":"<span class='magic-codeicon-function'>plot_training_records</span>"},{"location":"apis/utils/draw/plot_training_records/#utilsdrawplot_training_records","text":"Function \u00b7 Source mdnc . utils . draw . plot_training_records ( gen , xlabel = None , ylabel = 'value' , x_mark_num = None , y_log = False , figure_size = ( 6 , 5.5 ), legend_loc = None , legend_col = None , fig = None , ax = None ) Plot a training curve graph for multiple data groups. Each group is given by: 4 1D arrays, representing the x axis of training metrics, the trainining metric values, the x axis of validation metrics, the validation metric values respectively. or 2 2D arrays. Both of them have a shape of ( N , 2 ) . The two arrays represents the x axis and training metrics, the x axis and validation metric values respectively. or 2 1D arrays. In this case, the validation data is not provided. The two arrays represents the x axis of training metrics, the trainining metric values repspectively. or a 4D array. The 4 columns represents the x axis of training metrics, the trainining metric values, the x axis of validation metrics, the validation metric values respectively. or a 2D array. In this case, the validation data is not provided. The two columns represents the x axis of training metrics, the trainining metric values repspectively. or a 1D array. In this case, the validation data is not provided. The data represnets the training metrics. The x axis would be generated automatically.","title":"utils.draw.plot_training_records"},{"location":"apis/utils/draw/plot_training_records/#arguments","text":"Requries Argument Type Description gen object A generator callable object (function), each yield returns a sample. It allows users to provide an extra kwargs dict for each iteration (see Examples ). For each iteration, it returns 4 1D arrays, or 2 2D arrays, or 2 1D arrays, or a 4D array, or a 2D array, or a 1D array. xlabel str The x axis label. ylabel str The y axis label. x_mark_num int The number of markers for the x axis. y_log bool A flag. Whether to convert the y axis into the logarithmic format. figure_size ( float , float ) A tuple with two values representing the (width, height) of the output figure. The unit is inch. legend_loc str or int or ( float , float ) The localtion of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). legend_col int The number of columns of the legend, see matplotlib.pyplot.legend to view details. (The legend only works when passing label to each iteration). fig object A matplotlib figure instance. If not given, would use plt . gcf () for instead. ax object A matplotlib subplot instance. If not given, would use plt . gca () for instead.","title":"Arguments"},{"location":"apis/utils/draw/plot_training_records/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'Solarize_Light2' , font_size = 14 ) def test_training_records (): def func_gen_batch (): size = 100 x = np . arange ( start = 0 , stop = size ) for i in range ( 3 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () v = begin * np . exp (( np . square (( x - size ) / size ) - 1.0 ) * end ) yield x , v , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } def func_gen_epoch (): size = 10 x = np . arange ( start = 0 , stop = size ) for i in range ( 3 ): begin = 1 + 99.0 * np . random . rand () end = 2 + 10 * np . random . rand () v = begin * np . exp (( np . square (( x - size ) / size ) - 1.0 ) * end ) val_v = begin * np . exp (( np . square (( x - size ) / size ) - 1.0 ) * ( end - 1 )) data = np . stack ([ x , v , x , val_v ], axis = 0 ) yield data , { 'label' : r '$x_{' + str ( i + 1 ) + r '}$' } mdnc . utils . draw . plot_training_records ( func_gen_batch (), y_log = True , x_mark_num = 10 , xlabel = 'Step' , ylabel = r 'Batch $\\mathcal {L} $' ) plt . show () mdnc . utils . draw . plot_training_records ( func_gen_epoch (), y_log = True , x_mark_num = 10 , xlabel = 'Step' , ylabel = r 'Epoch $\\mathcal {L} $' ) plt . show () test_training_records () Output","title":"Examples"},{"location":"apis/utils/draw/setFigure/","text":"utils.draw.setFigure \u00b6 Class \u00b7 Decorator \u00b7 Context \u00b7 Source mdnc . utils . draw . setFigure ( style = None , font_name = None , font_size = None , use_tex = None ) A context decorator class, which is used for changing the figure's configurations locally for a specific function. Could be used by two different ways: Example As decorator 1 2 3 4 5 @mdnc . utils . draw . setFigure ( font_size = 12 ) def plot_curve (): plot_figures1 ( ... ) plot_figures2 ( ... ) ... As context 1 2 3 4 with mdnc . utils . draw . setFigure ( font_size = 12 ): plot_figures1 ( ... ) plot_figures2 ( ... ) ... Arguments \u00b6 Requries The following arguments would take effect only when they are configured explicitly. Argument Type Description style str or dict or Path or list The local stylesheet of the figure. The details could be found in matplotlib.style.context . We could also find some examples here . font_name str The local font family name for the output figure. The specified font should be installed and available for any software. font_size int The local font size for the output figure. use_tex bool Whether to use LaTeX backend for the output figure. Recommend to enable it when drawing figures for a paper. Warning If you are using a not installed or not supported font as font_name , the context decorator would not raise an error, but only show some warnings. This behavior is the same as matplotlib . Warning In the above argument list, the latter argument would override the former argument. For example, if some style has already specified font_size , configuring the argument font_size would override the configuration in the stylesheet. Please pay attention to your desired configurations. Examples \u00b6 As decorator Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'classic' , font_size = 16 , font_name = 'arial' ) def plot_local_setting (): t = np . linspace ( - 10 , 10 , 100 ) plt . plot ( t , 1 / ( 1 + np . exp ( - t ))) plt . title ( 'In the context, font: arial.' ) plt . show () plot_local_setting () Output As context Codes 1 2 3 4 5 6 7 8 9 import numpy as np import matplotlib.pyplot as plt import mdnc with mdnc . utils . draw . setFigure ( style = 'classic' , font_size = 16 , font_name = 'arial' ): t = np . linspace ( - 10 , 10 , 100 ) plt . plot ( t , 1 / ( 1 + np . exp ( - t ))) plt . title ( 'In the context, font: arial.' ) plt . show () Output","title":"<span class='magic-codeicon-class'>setFigure</span>"},{"location":"apis/utils/draw/setFigure/#utilsdrawsetfigure","text":"Class \u00b7 Decorator \u00b7 Context \u00b7 Source mdnc . utils . draw . setFigure ( style = None , font_name = None , font_size = None , use_tex = None ) A context decorator class, which is used for changing the figure's configurations locally for a specific function. Could be used by two different ways: Example As decorator 1 2 3 4 5 @mdnc . utils . draw . setFigure ( font_size = 12 ) def plot_curve (): plot_figures1 ( ... ) plot_figures2 ( ... ) ... As context 1 2 3 4 with mdnc . utils . draw . setFigure ( font_size = 12 ): plot_figures1 ( ... ) plot_figures2 ( ... ) ...","title":"utils.draw.setFigure"},{"location":"apis/utils/draw/setFigure/#arguments","text":"Requries The following arguments would take effect only when they are configured explicitly. Argument Type Description style str or dict or Path or list The local stylesheet of the figure. The details could be found in matplotlib.style.context . We could also find some examples here . font_name str The local font family name for the output figure. The specified font should be installed and available for any software. font_size int The local font size for the output figure. use_tex bool Whether to use LaTeX backend for the output figure. Recommend to enable it when drawing figures for a paper. Warning If you are using a not installed or not supported font as font_name , the context decorator would not raise an error, but only show some warnings. This behavior is the same as matplotlib . Warning In the above argument list, the latter argument would override the former argument. For example, if some style has already specified font_size , configuring the argument font_size would override the configuration in the stylesheet. Please pay attention to your desired configurations.","title":"Arguments"},{"location":"apis/utils/draw/setFigure/#examples","text":"As decorator Codes 1 2 3 4 5 6 7 8 9 10 11 12 import numpy as np import matplotlib.pyplot as plt import mdnc @mdnc . utils . draw . setFigure ( style = 'classic' , font_size = 16 , font_name = 'arial' ) def plot_local_setting (): t = np . linspace ( - 10 , 10 , 100 ) plt . plot ( t , 1 / ( 1 + np . exp ( - t ))) plt . title ( 'In the context, font: arial.' ) plt . show () plot_local_setting () Output As context Codes 1 2 3 4 5 6 7 8 9 import numpy as np import matplotlib.pyplot as plt import mdnc with mdnc . utils . draw . setFigure ( style = 'classic' , font_size = 16 , font_name = 'arial' ): t = np . linspace ( - 10 , 10 , 100 ) plt . plot ( t , 1 / ( 1 + np . exp ( - t ))) plt . title ( 'In the context, font: arial.' ) plt . show () Output","title":"Examples"},{"location":"apis/utils/draw/use_tex/","text":"utils.draw.use_tex \u00b6 Function \u00b7 Source mdnc . utils . draw . use_tex ( flag = False ) Switch the maplotlib font backend to \\(\\LaTeX\\) . This function is not recommended, because it would take effects globally. Please use mdnc.utils.draw.setFigure as a safer way. Arguments \u00b6 Requries Argument Type Description flag bool A flag, indicating whether to use the \\(\\LaTeX\\) backend for rendering figure fonts. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 import numpy as np import matplotlib.pyplot as plt import mdnc mdnc . utils . draw . use_tex ( True ) t = np . linspace ( - 10 , 10 , 100 ) plt . plot ( t , 1 / ( 1 + np . exp ( - t ))) plt . title ( r 'Use latex: $\\frac {1} {1 + \\exp (-t)}$.' ) plt . show () Output","title":"<span class='magic-codeicon-function'>use_tex</span>"},{"location":"apis/utils/draw/use_tex/#utilsdrawuse_tex","text":"Function \u00b7 Source mdnc . utils . draw . use_tex ( flag = False ) Switch the maplotlib font backend to \\(\\LaTeX\\) . This function is not recommended, because it would take effects globally. Please use mdnc.utils.draw.setFigure as a safer way.","title":"utils.draw.use_tex"},{"location":"apis/utils/draw/use_tex/#arguments","text":"Requries Argument Type Description flag bool A flag, indicating whether to use the \\(\\LaTeX\\) backend for rendering figure fonts.","title":"Arguments"},{"location":"apis/utils/draw/use_tex/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 import numpy as np import matplotlib.pyplot as plt import mdnc mdnc . utils . draw . use_tex ( True ) t = np . linspace ( - 10 , 10 , 100 ) plt . plot ( t , 1 / ( 1 + np . exp ( - t ))) plt . title ( r 'Use latex: $\\frac {1} {1 + \\exp (-t)}$.' ) plt . show () Output","title":"Examples"},{"location":"apis/utils/tools/ContextWrapper/","text":"utils.tools.ContextWrapper \u00b6 Class \u00b7 Context \u00b7 Source inst_wctx = mdnc . utils . tools . ContextWrapper ( instance , exit_method = None ) A simple wrapper for adding context support to some special classes. Example For example, there is an instance f, it defines f.close(), but does not support the context. In this case, we could use this wrapper to add context support: 1 2 3 4 5 6 import mdnc f = create_f ( ... ) with mdnc . utils . tools . ContextWrapper ( f ) as fc : do some thing ... # When leaving the context, the f.close() method would be called # automatically. Tip Actually, the standard lib has already provided a tool with the similar usage. See contextlib.closing for viewing the details. In most cases, we recommend to use the solution from the contextlib . However, if an instance implements an exiting function not named close() , then we have to use this class. Arguments \u00b6 Requries Argument Type Description instance object An instance requring the context support. exit_method object A callable object (function), if not provided, would call the instance . close () method during the exiting stage. If provided, would call exit_method ( instance ) instead. Operators \u00b6 __enter__ , __exit__ \u00b6 with mdnc . utils . tools . ContextWrapper ( instance , exit_method = None ) as inst_wctx : ... Work with the context. When leaving the context for any reason (sucessfully running all codes or meeting any exceptions), the exit_method would be called. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 import time import tqdm import mdnc num_iters = 100 with mdnc . utils . tools . ContextWrapper ( tqdm . tqdm ( total = num_iters )) as tq : for i in range ( num_iters ): tq . update ( 1 ) time . sleep ( 0.001 ) Output 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 62.98it/s]","title":"<span class='magic-codeicon-class'>ContextWrapper</span>"},{"location":"apis/utils/tools/ContextWrapper/#utilstoolscontextwrapper","text":"Class \u00b7 Context \u00b7 Source inst_wctx = mdnc . utils . tools . ContextWrapper ( instance , exit_method = None ) A simple wrapper for adding context support to some special classes. Example For example, there is an instance f, it defines f.close(), but does not support the context. In this case, we could use this wrapper to add context support: 1 2 3 4 5 6 import mdnc f = create_f ( ... ) with mdnc . utils . tools . ContextWrapper ( f ) as fc : do some thing ... # When leaving the context, the f.close() method would be called # automatically. Tip Actually, the standard lib has already provided a tool with the similar usage. See contextlib.closing for viewing the details. In most cases, we recommend to use the solution from the contextlib . However, if an instance implements an exiting function not named close() , then we have to use this class.","title":"utils.tools.ContextWrapper"},{"location":"apis/utils/tools/ContextWrapper/#arguments","text":"Requries Argument Type Description instance object An instance requring the context support. exit_method object A callable object (function), if not provided, would call the instance . close () method during the exiting stage. If provided, would call exit_method ( instance ) instead.","title":"Arguments"},{"location":"apis/utils/tools/ContextWrapper/#operators","text":"","title":"Operators"},{"location":"apis/utils/tools/ContextWrapper/#__enter__-__exit__","text":"with mdnc . utils . tools . ContextWrapper ( instance , exit_method = None ) as inst_wctx : ... Work with the context. When leaving the context for any reason (sucessfully running all codes or meeting any exceptions), the exit_method would be called.","title":" __enter__, __exit__"},{"location":"apis/utils/tools/ContextWrapper/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 9 import time import tqdm import mdnc num_iters = 100 with mdnc . utils . tools . ContextWrapper ( tqdm . tqdm ( total = num_iters )) as tq : for i in range ( num_iters ): tq . update ( 1 ) time . sleep ( 0.001 ) Output 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 62.98it/s]","title":"Examples"},{"location":"apis/utils/tools/EpochMetrics/","text":"utils.tools.EpochMetrics \u00b6 Class \u00b7 Source emdict = mdnc . utils . tools . EpochMetrics ( reducer = np . mean ) A dictionary for storing metrics. The __setitem__ and __getitem__ operators are overloaded. This tool is used for calculating the statistics of epoch metrics easily. The following codes are equivalent: Example With EpochMetrics 1 2 3 4 5 emdict = EpochMetrics () for i in range ( 10 ): emdict [ 'loss' ] = i / 10 emdict [ 'metric2' ] = - i / 10 print ( emdict [ 'loss' ], emdict [ 'metric2' ]) Without EpochMetrics 1 2 3 4 5 emdict = { 'loss' : list ()} for i in range ( 10 ): emdict [ 'loss' ] . append ( i / 10 ) emdict [ 'metric2' ] . append ( - i / 10 ) print ( np . mean ( emdict [ 'loss' ]), np . mean ( emdict [ 'metric2' ])) Arguments \u00b6 Requries Argument Type Description reducer object A callable object (function). The input of this function should be a sequence and the output should be a scalar. Methods \u00b6 keys , values , items \u00b6 for k in emdict . keys (): ... for v in emdict . values (): ... for k , v in emdict . items (): ... Used as iterators returned by a python dictionary. pop , popitem \u00b6 v = emdict . pop ( k , default = None ) k , v = emdict . popitem ( k ) Used as pop() and popitem() of a python dictionary. get , setdefault \u00b6 v = emdict . get ( k , default = None ) emdict . setdefault ( k , default = None ) Used as get() and setdefault() of a python dictionary. Operators \u00b6 __getitem__ \u00b6 v = emdict [ keyword ] Get the reduced value of a specific keyword. Requries Argument Type Description keyword object A python object that could be used as the keyword. This is the name of the metric. Returns Argument Description v The returned metric, this value is a scalar reduced by the reducer provided in the initialization. __setitem__ \u00b6 emdict [ keyword ] = v Set a new value for a specific keyword. Requries Argument Type Description keyword object A python object that could be used as the keyword. This is the name of the metric. v int or float A scalar value. This value would be appended in the stored metric list. Examples \u00b6 Example Codes 1 2 3 4 5 6 7 8 import mdnc emdict = mdnc . utils . tools . EpochMetrics () for i in range ( 10 ): emdict [ 'loss' ] = i / 10 emdict [ 'metric2' ] = - i / 10 for k , v in emdict . items (): print ( k , v ) Output loss 0.45 metric2 -0.45","title":"<span class='magic-codeicon-class'>EpochMetrics</span>"},{"location":"apis/utils/tools/EpochMetrics/#utilstoolsepochmetrics","text":"Class \u00b7 Source emdict = mdnc . utils . tools . EpochMetrics ( reducer = np . mean ) A dictionary for storing metrics. The __setitem__ and __getitem__ operators are overloaded. This tool is used for calculating the statistics of epoch metrics easily. The following codes are equivalent: Example With EpochMetrics 1 2 3 4 5 emdict = EpochMetrics () for i in range ( 10 ): emdict [ 'loss' ] = i / 10 emdict [ 'metric2' ] = - i / 10 print ( emdict [ 'loss' ], emdict [ 'metric2' ]) Without EpochMetrics 1 2 3 4 5 emdict = { 'loss' : list ()} for i in range ( 10 ): emdict [ 'loss' ] . append ( i / 10 ) emdict [ 'metric2' ] . append ( - i / 10 ) print ( np . mean ( emdict [ 'loss' ]), np . mean ( emdict [ 'metric2' ]))","title":"utils.tools.EpochMetrics"},{"location":"apis/utils/tools/EpochMetrics/#arguments","text":"Requries Argument Type Description reducer object A callable object (function). The input of this function should be a sequence and the output should be a scalar.","title":"Arguments"},{"location":"apis/utils/tools/EpochMetrics/#methods","text":"","title":"Methods"},{"location":"apis/utils/tools/EpochMetrics/#keys-values-items","text":"for k in emdict . keys (): ... for v in emdict . values (): ... for k , v in emdict . items (): ... Used as iterators returned by a python dictionary.","title":" keys, values, items"},{"location":"apis/utils/tools/EpochMetrics/#pop-popitem","text":"v = emdict . pop ( k , default = None ) k , v = emdict . popitem ( k ) Used as pop() and popitem() of a python dictionary.","title":" pop, popitem"},{"location":"apis/utils/tools/EpochMetrics/#get-setdefault","text":"v = emdict . get ( k , default = None ) emdict . setdefault ( k , default = None ) Used as get() and setdefault() of a python dictionary.","title":" get, setdefault"},{"location":"apis/utils/tools/EpochMetrics/#operators","text":"","title":"Operators"},{"location":"apis/utils/tools/EpochMetrics/#__getitem__","text":"v = emdict [ keyword ] Get the reduced value of a specific keyword. Requries Argument Type Description keyword object A python object that could be used as the keyword. This is the name of the metric. Returns Argument Description v The returned metric, this value is a scalar reduced by the reducer provided in the initialization.","title":" __getitem__"},{"location":"apis/utils/tools/EpochMetrics/#__setitem__","text":"emdict [ keyword ] = v Set a new value for a specific keyword. Requries Argument Type Description keyword object A python object that could be used as the keyword. This is the name of the metric. v int or float A scalar value. This value would be appended in the stored metric list.","title":" __setitem__"},{"location":"apis/utils/tools/EpochMetrics/#examples","text":"Example Codes 1 2 3 4 5 6 7 8 import mdnc emdict = mdnc . utils . tools . EpochMetrics () for i in range ( 10 ): emdict [ 'loss' ] = i / 10 emdict [ 'metric2' ] = - i / 10 for k , v in emdict . items (): print ( k , v ) Output loss 0.45 metric2 -0.45","title":"Examples"}]}