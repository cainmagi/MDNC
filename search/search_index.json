{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M odern D eep N etwork Toolkits for pyTor c h (MDNC) \u00b6 This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.2 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document. Overview \u00b6 The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools. Current progress \u00b6 Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model.summary() in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0% Compatibility test \u00b6 Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Overview"},{"location":"#modern-deep-network-toolkits-for-pytorch-mdnc","text":"This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.2 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document.","title":"Modern Deep Network Toolkits for pyTorch (MDNC)"},{"location":"#overview","text":"The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools.","title":"Overview"},{"location":"#current-progress","text":"Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model.summary() in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0%","title":"Current progress"},{"location":"#compatibility-test","text":"Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Compatibility test"},{"location":"installation/","text":"Installation \u00b6 Use the nightly version on Github \u00b6 Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by import mdnc.mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive Install the package \u00b6 Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#use-the-nightly-version-on-github","text":"Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by import mdnc.mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive","title":"Use the nightly version on Github"},{"location":"installation/#install-the-package","text":"Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Install the package"},{"location":"licenses/","text":"Licenses \u00b6 License of MDNC \u00b6 MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Licenses"},{"location":"licenses/#licenses","text":"","title":"Licenses"},{"location":"licenses/#license-of-mdnc","text":"MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License of MDNC"},{"location":"apis/overview/","text":"Overview \u00b6 The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: %%{init: {'theme':'default'}}%% flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B;","title":"Overview"},{"location":"apis/overview/#overview","text":"The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: %%{init: {'theme':'default'}}%% flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B;","title":"Overview"},{"location":"apis/contribs/torchsummary/summary/","text":"contribs.torchsummary.summary \u00b6 Function params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Example \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"summary"},{"location":"apis/contribs/torchsummary/summary/#contribstorchsummarysummary","text":"Function params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary/#example","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"Example"},{"location":"apis/contribs/torchsummary/summary_string/","text":"contribs.torchsummary.summary \u00b6 Function summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str the summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Example \u00b6 See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"summary_string"},{"location":"apis/contribs/torchsummary/summary_string/#contribstorchsummarysummary","text":"Function summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary_string/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str the summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary_string/#example","text":"See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"Example"},{"location":"apis/data/h5py/H5Converter/","text":"data.h5py.H5Converter \u00b6 Class converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Arguments \u00b6 Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () ) Methods \u00b6 convert \u00b6 converter . convert () Perform the data conversion. Example \u00b6 Example Codes 1 2 3 4 5 import mdnc cvt_o = mdnc . data . h5py . H5Converter ( 'dataset.h5' , 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. cvt_i = mdnc . data . h5py . H5Converter ( 'dataset.h5' , 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset.","title":"H5Converter"},{"location":"apis/data/h5py/H5Converter/#datah5pyh5converter","text":"Class converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets .","title":"data.h5py.H5Converter"},{"location":"apis/data/h5py/H5Converter/#arguments","text":"Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () )","title":"Arguments"},{"location":"apis/data/h5py/H5Converter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5Converter/#convert","text":"converter . convert () Perform the data conversion.","title":"convert"},{"location":"apis/data/h5py/H5Converter/#example","text":"Example Codes 1 2 3 4 5 import mdnc cvt_o = mdnc . data . h5py . H5Converter ( 'dataset.h5' , 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. cvt_i = mdnc . data . h5py . H5Converter ( 'dataset.h5' , 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset.","title":"Example"},{"location":"apis/data/h5py/H5SeqConverter/","text":"data.h5py.H5SeqConverter \u00b6 Class converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group . Arguments \u00b6 Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . Methods \u00b6 config \u00b6 converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. convert \u00b6 converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. copy \u00b6 converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. open \u00b6 converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . close \u00b6 converter . close () Close the converter. Example \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc with mdnc . data . h5py . H5SeqConverter ( 'dataset.h5' ) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Example 2 Codes 1 2 3 4 5 6 7 8 9 import mdnc converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( 'dataset.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( 'dataset2.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' )","title":"H5SeqConverter"},{"location":"apis/data/h5py/H5SeqConverter/#datah5pyh5seqconverter","text":"Class converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group .","title":"data.h5py.H5SeqConverter"},{"location":"apis/data/h5py/H5SeqConverter/#arguments","text":"Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":"Arguments"},{"location":"apis/data/h5py/H5SeqConverter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SeqConverter/#config","text":"converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":"config"},{"location":"apis/data/h5py/H5SeqConverter/#convert","text":"converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":"convert"},{"location":"apis/data/h5py/H5SeqConverter/#copy","text":"converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":"copy"},{"location":"apis/data/h5py/H5SeqConverter/#open","text":"converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":"open"},{"location":"apis/data/h5py/H5SeqConverter/#close","text":"converter . close () Close the converter.","title":"close"},{"location":"apis/data/h5py/H5SeqConverter/#example","text":"Example 1 Codes 1 2 3 4 5 import mdnc with mdnc . data . h5py . H5SeqConverter ( 'dataset.h5' ) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Example 2 Codes 1 2 3 4 5 6 7 8 9 import mdnc converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( 'dataset.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( 'dataset2.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' )","title":"Example"},{"location":"apis/data/h5py/H5SupSaver/","text":"data.h5py.H5SupSaver \u00b6 Class saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of !#py h5py.Group and !# h5py.Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py.Softlink , h5py.Attributes and h5py.VirtualDataSet . Add context nesting supports for h5py.Group . This would makes the codes more elegant. Arguments \u00b6 Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. Methods \u00b6 config \u00b6 saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. get_config \u00b6 saver . get_config ( name = None ) Get the current configuration value by the given name . Warning This method does not support nesting APIs. It should only be loaded by the Requries Argument Type Description name str The name of the required config value. open \u00b6 saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value. close \u00b6 saver . close () Close the saver. dump \u00b6 saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. set_link \u00b6 saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset . set_attrs \u00b6 saver . set_attrs ( keyword , attrs = None , ** kwattrs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () . set_virtual_set \u00b6 saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset. Properties \u00b6 attrs \u00b6 attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager . Example \u00b6 Example 1 Codes 1 2 3 4 5 import mdnc with mdnc . data . h5py . H5SupSaver ( 'new_file.h5' , enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( 'new_file_2.h5' ) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open ))","title":"H5SupSaver"},{"location":"apis/data/h5py/H5SupSaver/#datah5pyh5supsaver","text":"Class saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of !#py h5py.Group and !# h5py.Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py.Softlink , h5py.Attributes and h5py.VirtualDataSet . Add context nesting supports for h5py.Group . This would makes the codes more elegant.","title":"data.h5py.H5SupSaver"},{"location":"apis/data/h5py/H5SupSaver/#arguments","text":"Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file.","title":"Arguments"},{"location":"apis/data/h5py/H5SupSaver/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SupSaver/#config","text":"saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":"config"},{"location":"apis/data/h5py/H5SupSaver/#get_config","text":"saver . get_config ( name = None ) Get the current configuration value by the given name . Warning This method does not support nesting APIs. It should only be loaded by the Requries Argument Type Description name str The name of the required config value.","title":"get_config"},{"location":"apis/data/h5py/H5SupSaver/#open","text":"saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value.","title":"open"},{"location":"apis/data/h5py/H5SupSaver/#close","text":"saver . close () Close the saver.","title":"close"},{"location":"apis/data/h5py/H5SupSaver/#dump","text":"saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":"dump"},{"location":"apis/data/h5py/H5SupSaver/#set_link","text":"saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset .","title":"set_link"},{"location":"apis/data/h5py/H5SupSaver/#set_attrs","text":"saver . set_attrs ( keyword , attrs = None , ** kwattrs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () .","title":"set_attrs"},{"location":"apis/data/h5py/H5SupSaver/#set_virtual_set","text":"saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset.","title":"set_virtual_set"},{"location":"apis/data/h5py/H5SupSaver/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5SupSaver/#attrs","text":"attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager .","title":"attrs"},{"location":"apis/data/h5py/H5SupSaver/#example","text":"Example 1 Codes 1 2 3 4 5 import mdnc with mdnc . data . h5py . H5SupSaver ( 'new_file.h5' , enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( 'new_file_2.h5' ) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open ))","title":"Example"}]}