{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"M odern D eep N etwork Toolkits for pyTor c h (MDNC) \u00b6 This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.2 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document. Overview \u00b6 The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools. Current progress \u00b6 Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model . summary () in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0% Compatibility test \u00b6 Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Overview"},{"location":"#modern-deep-network-toolkits-for-pytorch-mdnc","text":"This is a pyTorch framework used for Creating specially designed networks or layers. Parallel Data pre- and post- processing as a powerful alternative of torch.utils.data.DataLoader . Callback-function based data visualizer as an alternative of seaborn . Web tools for downloading tarball-packed datasets from Github. Some modified third-party utilities. Info Currently, this module is still under development. The current version is 0.1.2 However, you could use this nightly version anyway. All available (stable) APIs of the current version would be recorded in this document.","title":"Modern Deep Network Toolkits for pyTorch (MDNC)"},{"location":"#overview","text":"The pyTorch has its own coding style. In the shortest words, this style could be summarized as \"Let users to implement as much as possible.\". It seems to be very inconvenient, however, the pyTorch could benefit from this coding philosophy from the following aspects: Everything is under-controlled by users. In comparison, a well-enclosed deep learning framework, like Keras, is certainly easier to use. However, it would be very difficult to hack into the enclosed APIs and perform some complicated modifications. A well-enclosed tool often requires a lot of not exposed logics. Because the pyTorch is focused on the low-level APIs, the design of pyTorch is simple and neat enough. Users could contribute to the package easily. In details, such a coding style is implemented mainly in the following methods: Modules, optimizers, and the training logic are separated from each other. The module is only used for defining the network graph. The optimizers are provided as instances used for loss functions. The training and testing logics require to be implemented by users. The data loading and processing are paralleled by torch.utils.data.DataLoader and torchvision respectively. The users do not need to write codes about multi-processing management. The data type conversion requires to be implemented by the users. For example, the predicted variables of the network require to be converted to the correct data type explicitly. Because the training logic is implemented by users, arbitrary codes are allowed to be injected during the training loop. Instead of writing callbacks (we do such things when using keras-team/keras or scikit-learn/scikit-learn ), users could invoke their customized functions (like saving records, showing progress) easily. This toolkit is designed according to the style. We do not want to make this toolkit look like keras-team/keras or PyTorchLightning/pytorch-lightning . In other words, we want to make it special enough for you to use it. The motivations why we develop this toolkit include: Provide simpler interfaces for building more complicated networks, like residual network and DenseNet. The built-in APIs in this toolkit would help users avoid building such widely used models from scratch. Provide implementations of some advanced tools, including some special optimizers and loss functions. Currently, the pyTorch DataLoader does not support managing a large-file dataset in the initialization function. To manage the data more efficiently, we provide interfaces for loading large datasets like HDF5 files by parallel. The alternative for transformers is also provided. Some APIs related to file IO and online requests are not safe enough. We wrap them by context and guarantee these ops are safe when errors occur. Provide some useful tools like record visualizers, and some open-sourced third-party tools.","title":"Overview"},{"location":"#current-progress","text":"Now we have such progress on the semi-product: optimizers modules conv : Modern convolutional layers and networks. 100% resnet : Residual blocks and networks. 100% resnext : ResNeXt blocks and networks. 0% incept : Google inception blocks and networks. 0% densenet : Dense-net blocks and networks. 0% models data h5py : Wrapped HDF5 datasets saver and loader. 100% netcdf4 : Wrapped NETCDF4 datasets saver and loader. 0% bcolz : Wrapped Bcolz datasets saver and loader. 0% text : Wrapped text-based datasets saver and loader (CSV, JSON, TXT). 0% preprocs : Useful pre- and post- processing tools for all data handles in this package. 100% webtools : Web tools for downloading tarball-packed datasets from Github. 100% funcs utils tools : Light-weighted record parsing tools used during training or testing. 10% draw : Wrapped matplotlib drawing tools. Most of the utilities are designed as call-back based functions. 80% contribs torchsummary : Keras style model . summary () in pyTorch, with some bugs gotten fixed (modified) (MIT licensed). 100% tensorboard : Wrapped torch.utils.tensorboard , supporting context-style writer and tensorboard.log converted to h5py format (not modified). 0%","title":"Current progress"},{"location":"#compatibility-test","text":"Info Currently, this project has not been checked by compatibility tests. During the developing stage, we are using pyTorch 1.7.0+ and Python 3.6+. To perform the compatibility test, just run cd <root-of-this-repo> python -m mdnc The compatibility test is shown as below. The checked item means this package performs well in the specific enviroment. Enviroment Win Linux pyTorch 1.7.0, Python 3.8 pyTorch 1.8.0, Python 3.8 pyTorch 1.6.0, Python 3.7 pyTorch 1.4.0, Python 3.7 pyTorch 1.2.0, Python 3.6 pyTorch 1.0.0, Python 3.5","title":"Compatibility test"},{"location":"installation/","text":"Installation \u00b6 Use the nightly version on Github \u00b6 Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by from mdnc import mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive Install the package \u00b6 Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#use-the-nightly-version-on-github","text":"Currently, this project is still under-development. We suggest to use the following steps to add the package as a sub-module in your git-project, cd <your-project-folder> git submodule add https://github.com/cainmagi/MDNC.git mdnc git submodule update --init --recursive After that, you could use the pacakge by from mdnc import mdnc If you want to update the sub-module to the newest version, please use git submodule update --remote --recursive","title":"Use the nightly version on Github"},{"location":"installation/#install-the-package","text":"Warning We strongly do not recommend to install the package by PyPI now. Because the pacakage is still under development. This package could be also installed by the following command: Github python -m pip install git+https://github.com/cainmagi/MDNC.git PyPI to be implmented in the future... Install the package by this way would make the package available globally. Make sure that the version is exactly what you want. After the installation, the module could be imported by import mdnc","title":"Install the package"},{"location":"licenses/","text":"Licenses \u00b6 License of MDNC \u00b6 MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. License of pytorch-summary \u00b6 The third-party module, torchsummary used in MDNC ( sksq96/pytorch-summary ), grants: MIT License https://github.com/sksq96/pytorch-summary/blob/master/LICENSE License of MkDocs-Material Theme \u00b6 The theme of this website ( squidfunk/mkdocs-material ) grants: MIT License https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE License of VSCode-Codeicons \u00b6 The Codeicons pack ( microsoft/vscode-codicons ) used in this website grants: MIT License for all source codes https://github.com/microsoft/vscode-codicons/blob/main/LICENSE-CODE CC BY 4.0 License for all materials https://github.com/microsoft/vscode-codicons/blob/main/LICENSE","title":"Licenses"},{"location":"licenses/#licenses","text":"","title":"Licenses"},{"location":"licenses/#license-of-mdnc","text":"MIT License Copyright \u00a9 2021 Yuchen Jin (cainmagi) Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License of MDNC"},{"location":"licenses/#license-of-pytorch-summary","text":"The third-party module, torchsummary used in MDNC ( sksq96/pytorch-summary ), grants: MIT License https://github.com/sksq96/pytorch-summary/blob/master/LICENSE","title":"License of pytorch-summary"},{"location":"licenses/#license-of-mkdocs-material-theme","text":"The theme of this website ( squidfunk/mkdocs-material ) grants: MIT License https://github.com/squidfunk/mkdocs-material/blob/master/LICENSE","title":"License of MkDocs-Material Theme"},{"location":"licenses/#license-of-vscode-codeicons","text":"The Codeicons pack ( microsoft/vscode-codicons ) used in this website grants: MIT License for all source codes https://github.com/microsoft/vscode-codicons/blob/main/LICENSE-CODE CC BY 4.0 License for all materials https://github.com/microsoft/vscode-codicons/blob/main/LICENSE","title":"License of VSCode-Codeicons"},{"location":"apis/overview/","text":"Overview \u00b6 The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: %%{init: {'theme':'default'}}%% flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup sequence(sequence) subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B; List of packages \u00b6 optimizers \u00b6 To be built ... modules \u00b6 To be documented ... models \u00b6 To be built ... data \u00b6 sequence : The infrastructures of CPU-based parallel I/O and processing. This module is used by all data loaders. h5py : Wrapped HDF5 datasets savers, data converters and data loaders. preprocs : Useful pre- and post- processing tools for all data loaders in this package. webtools : Web tools for downloading tarball-packed datasets from Github. funcs \u00b6 To be built ... utils \u00b6 To be documented ... contribs \u00b6 torchsummary : The revised sksq96/pytorch-summary . This is a Keras style model . summary () in pyTorch, with some bugs gotten fixed. To view my modified version, see sksq96/pytorch-summary!165 .","title":"Overview"},{"location":"apis/overview/#overview","text":"The APIs of this package could be divided into the following sub-packages: Package name Description optimizers To be implemented ... modules A collection of specially designed pyTorch modules, including special network layers and network models. models To be implemented ... data A collection of dataset loaders, online dataset management tools, and data processing tools. funcs To be implemented ... utils A collection of data processing or visualization tools not related to datasets or pyTorch. contribs A collection of third-party packages, including the modified third-party packages and some enhancement APIs on the top of the third-party packages. The diagram of the MDNC is shown as follows: %%{init: {'theme':'default'}}%% flowchart LR mdnc:::module subgraph mdnc optimizers:::blank subgraph modules conv(conv) resnet(resnet) end models:::blank subgraph data dg_parse:::modgroup sequence(sequence) subgraph dg_parse [Dataloaders] h5py(h5py) end preprocs(preprocs) webtools(webtools) end funcs:::blank subgraph utils tools(tools) draw(draw) end subgraph contribs torchsummary(torchsummary) end end classDef module fill:#ffffde, stroke: #aaaa33; classDef blank fill:#eeeeee, stroke: #aaaaaa; classDef modgroup stroke-dasharray:10,10, width:100; classDef ops fill:#FFB11B, stroke:#AF811B;","title":"Overview"},{"location":"apis/overview/#list-of-packages","text":"","title":"List of packages"},{"location":"apis/overview/#optimizers","text":"To be built ...","title":" optimizers"},{"location":"apis/overview/#modules","text":"To be documented ...","title":" modules"},{"location":"apis/overview/#models","text":"To be built ...","title":" models"},{"location":"apis/overview/#data","text":"sequence : The infrastructures of CPU-based parallel I/O and processing. This module is used by all data loaders. h5py : Wrapped HDF5 datasets savers, data converters and data loaders. preprocs : Useful pre- and post- processing tools for all data loaders in this package. webtools : Web tools for downloading tarball-packed datasets from Github.","title":" data"},{"location":"apis/overview/#funcs","text":"To be built ...","title":" funcs"},{"location":"apis/overview/#utils","text":"To be documented ...","title":" utils"},{"location":"apis/overview/#contribs","text":"torchsummary : The revised sksq96/pytorch-summary . This is a Keras style model . summary () in pyTorch, with some bugs gotten fixed. To view my modified version, see sksq96/pytorch-summary!165 .","title":" contribs"},{"location":"apis/contribs/torchsummary/summary/","text":"contribs.torchsummary.summary \u00b6 Function \u00b7 Source params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Example \u00b6 Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"<span class='magic-codeicon-function'>summary</span>"},{"location":"apis/contribs/torchsummary/summary/#contribstorchsummarysummary","text":"Function \u00b7 Source params_info = mdnc . contribs . torchsummary . summary ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary/#example","text":"Example Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch import torch.nn as nn import torch.nn.functional as F import mdnc class TestTupleOutModule ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1a = nn . Linear ( 300 , 50 ) self . fc1b = nn . Linear ( 50 , 10 ) self . fc2a = nn . Linear ( 300 , 50 ) self . fc2b = nn . Linear ( 50 , 10 ) def forward ( self , x1 , x2 ): x1 = F . relu ( self . fc1a ( x1 )) x1 = self . fc1b ( x1 ) x2 = x2 . type ( torch . FloatTensor ) x2 = F . relu ( self . fc2a ( x2 )) x2 = self . fc2b ( x2 ) # set x2 to FloatTensor x = torch . cat (( x1 , x2 ), 0 ) return F . log_softmax ( x , dim = 1 ), F . log_softmax ( x1 , dim = 1 ), F . log_softmax ( x2 , dim = 1 ) input1 = ( 1 , 300 ) input2 = ( 1 , 300 ) dtypes = ( torch . FloatTensor , torch . LongTensor ) total_params , trainable_params = mdnc . contribs . torchsummary . summary ( TestTupleOutModule (), ( input1 , input2 ), device = 'cpu' , dtypes = dtypes ) Output ---------------------------------------------------------------- Layer (type) Output Shape Param # ================================================================ Linear-1 [-1, 1, 50] 15,050 Linear-2 [-1, 1, 10] 510 Linear-3 [-1, 1, 50] 15,050 Linear-4 [-1, 1, 10] 510 TestTupleOutModule-5 [-1, 1, 10] 0 [-1, 1, 10] [-1, 1, 10] ================================================================ Total params: 31,120 Trainable params: 31,120 Non-trainable params: 0 ---------------------------------------------------------------- Input size (MB): 0.00 Forward/backward pass size (MB): 0.00 Params size (MB): 0.12 Estimated Total Size (MB): 0.12 ----------------------------------------------------------------","title":"Example"},{"location":"apis/contribs/torchsummary/summary_string/","text":"contribs.torchsummary.summary \u00b6 Function \u00b7 Source summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str. Arguments \u00b6 Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str The summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers. Example \u00b6 See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"<span class='magic-codeicon-function'>summary_string</span>"},{"location":"apis/contribs/torchsummary/summary_string/#contribstorchsummarysummary","text":"Function \u00b7 Source summary_str , params_info = mdnc . contribs . torchsummary . summary_str ( model , input_size , batch_size =- 1 , device = 'cuda:0' , dtypes = None ) Iterate the whole pytorch model and summarize the infomation as a Keras-style text report. The output would be store in a str.","title":"contribs.torchsummary.summary"},{"location":"apis/contribs/torchsummary/summary_string/#arguments","text":"Requries Argument Type Description model nn . Module The pyTorch network module instance. It is to be analyzed. input_size ( seq / int , ) A sequence ( list / tuple ) or a sequence of sequnces, indicating the size of the each model input variable. batch_size int The batch size used for testing and displaying the results. device str or torch . device Should be set according to the deployed device of the argument model . dtypes ( torch . dtype , ) A sequence of torch data type for each input variable. If set None , would use float type for all variables. Returns Argument Description summary_str The summary text report. params_info A tuple of two values. The first value is the total parameter numbers. The second value is the trainable parameter numbers.","title":"Arguments"},{"location":"apis/contribs/torchsummary/summary_string/#example","text":"See the example of mdnc.contribs.torchsummary.summary Tip This function could be used for generating the text log file: 1 2 3 4 ... with open ( 'my_module.log' , 'w' ) as f : report , _ = mdnc . contribs . torchsummary . summary_string ( model , ... ) f . write ( report )","title":"Example"},{"location":"apis/data/h5py/H5CParser/","text":"data.h5py.H5CParser \u00b6 Class \u00b7 Source dset = mdnc . data . h5py . H5CParser ( file_name , keywords_sequence , keywords_single , batch_size = 32 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and parse it by mdnc.data.sequence.MPSequence . The realization could be described as: This parser is the upgraded version of mdnc.data.h5py.H5GParser , it is specially designed for parsing data to LSTM/ConvLSTM. A sequence dimension would be inserted between batches and channels . In each batch, the sequence is continuously extracted in the order of the batches. During each epoch, a sliding window would iterate the first axis (samples). The number of batches would be the same as using mdnc.data.h5py.H5GParser . For each variable specified by keywords_sequence , each sample in the mini-batch is a sequence. This parser could also read the dataset converted by mdnc.data.h5py.H5SeqConverter . The workflow is shown in the following figure: Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords_sequence ( str , ) The keyword of sequence data. The keywords in this list would be parsed as (B, S, C1, C2, ...) , where B and S are the sample number and sequence length (given by the argument sequence_size ) respectively. It should be a list of keywords (or a single keyword). keyword_single ( str , ) The keyword of single values. The keywords in this list would be parsed as (B, C1, C2, ...) , where B is the sample number. It should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. sequence_size int The size of each sequence. It represents S of (B, S, C1, C2, ...) . sequence_position int The aligned position between the single values and the sequence values. It should be in the range of >= 0 and < sequence_size . sequence_padding int The padding method for each epoch, it will influence the first or the final samples in the dataset. Could be 'same' , 'zero' or 'none' . If set None , the number of batches of each epoch would be a little bit smaller than the actual number. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip At least one keyword requires to be given in keywords_sequence or keyword_single . In some cases, we need to use both kinds of keywords. For example, the input could be a sequence, and the label may be a scalar. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5CParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. sequence_size \u00b6 dset . sequence_size The length of each sequence. This value is given by the argument sequence_size during the initialization. sequence_position \u00b6 dset . sequence_position The alignment between keywords_sequence and keyword_single . This value is given by the argument sequence_position during the initialization. sequence_padding \u00b6 dset . sequence_position The padding method of each sequence. This value is given by the argument sequence_padding during the initialization. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Example \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position = 0 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 1 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Example 2 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser_seq' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 10 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:,:], d2 . shape , d3 )","title":"<span class='magic-codeicon-class'>H5CParser</span>"},{"location":"apis/data/h5py/H5CParser/#datah5pyh5cparser","text":"Class \u00b7 Source dset = mdnc . data . h5py . H5CParser ( file_name , keywords_sequence , keywords_single , batch_size = 32 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and parse it by mdnc.data.sequence.MPSequence . The realization could be described as: This parser is the upgraded version of mdnc.data.h5py.H5GParser , it is specially designed for parsing data to LSTM/ConvLSTM. A sequence dimension would be inserted between batches and channels . In each batch, the sequence is continuously extracted in the order of the batches. During each epoch, a sliding window would iterate the first axis (samples). The number of batches would be the same as using mdnc.data.h5py.H5GParser . For each variable specified by keywords_sequence , each sample in the mini-batch is a sequence. This parser could also read the dataset converted by mdnc.data.h5py.H5SeqConverter . The workflow is shown in the following figure:","title":"data.h5py.H5CParser"},{"location":"apis/data/h5py/H5CParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords_sequence ( str , ) The keyword of sequence data. The keywords in this list would be parsed as (B, S, C1, C2, ...) , where B and S are the sample number and sequence length (given by the argument sequence_size ) respectively. It should be a list of keywords (or a single keyword). keyword_single ( str , ) The keyword of single values. The keywords in this list would be parsed as (B, C1, C2, ...) , where B is the sample number. It should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. sequence_size int The size of each sequence. It represents S of (B, S, C1, C2, ...) . sequence_position int The aligned position between the single values and the sequence values. It should be in the range of >= 0 and < sequence_size . sequence_padding int The padding method for each epoch, it will influence the first or the final samples in the dataset. Could be 'same' , 'zero' or 'none' . If set None , the number of batches of each epoch would be a little bit smaller than the actual number. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip At least one keyword requires to be given in keywords_sequence or keyword_single . In some cases, we need to use both kinds of keywords. For example, the input could be a sequence, and the label may be a scalar. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5CParser ( ... , keywords_sequence = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5CParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5CParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5CParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5CParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5CParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5CParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5CParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5CParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5CParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5CParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5CParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5CParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5CParser/#batch_size","text":"dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/h5py/H5CParser/#sequence_size","text":"dset . sequence_size The length of each sequence. This value is given by the argument sequence_size during the initialization.","title":" sequence_size"},{"location":"apis/data/h5py/H5CParser/#sequence_position","text":"dset . sequence_position The alignment between keywords_sequence and keyword_single . This value is given by the argument sequence_position during the initialization.","title":" sequence_position"},{"location":"apis/data/h5py/H5CParser/#sequence_padding","text":"dset . sequence_position The padding method of each sequence. This value is given by the argument sequence_padding during the initialization.","title":" sequence_padding"},{"location":"apis/data/h5py/H5CParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5CParser/#example","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position = 0 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 1 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:, :], d2 . shape , d3 ) Example 2 Codes 1 2 3 4 5 6 7 8 9 import mdnc dset = mdnc . data . h5py . H5CParser ( 'test_cparser_seq' , keywords_sequence = [ 'key1' , 'key3' ], keywords_single = [ 'key2' ], batch_size = 1 , sequence_size = 5 , sequence_position =- 1 , sequence_padding = 'same' , shuffle = False , preprocfunc = None , num_workers = 10 , num_buffer = 1 ) with dset . start () as p : for i , data in enumerate ( p ): d1 , d2 , d3 = data print ( 'data.h5py:' , i , d1 [:,:], d2 . shape , d3 )","title":"Example"},{"location":"apis/data/h5py/H5Converter/","text":"data.h5py.H5Converter \u00b6 Class \u00b7 Source converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Arguments \u00b6 Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () ) Methods \u00b6 convert \u00b6 converter . convert () Perform the data conversion. Example \u00b6 Example Codes 1 2 3 4 5 6 import mdnc cvt_o = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. cvt_i = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset.","title":"<span class='magic-codeicon-class'>H5Converter</span>"},{"location":"apis/data/h5py/H5Converter/#datah5pyh5converter","text":"Class \u00b7 Source converter = mdnc . data . h5py . H5Converter ( file_name , oformat , to_other = True ) Conversion between HDF5 data and other formats. The \"other formats\" would be arranged in to form of several nested folders and files. Each data group would be mapped into a folder, and each dataset would be mapped into a file. Warning When the argument to_other is True , the data would be converted to other formats. During this process, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets .","title":"data.h5py.H5Converter"},{"location":"apis/data/h5py/H5Converter/#arguments","text":"Requries Argument Type Description file_name str A path where we find the dataset. If the conversion is from h5 to other, the path should refer a folder containing several subfiles, otherwise, it should refer an HDF5 file. oformat object The format function for a single dataset, it could be provided by users, or use the default configurations ( str ). (avaliable: 'txt' , 'bin' .) to_other bool The flag for conversion mode. If set True, the mode would be h52other, i.e. an HDF5 set would be converted into other formats. If set False, the conversion would be reversed. Tip The argument oformat could be a user defined custome object. It should provide two methods: read() and write() . An example of txt IO is shown as below: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os import io import numpy as np class H52TXT : '''An example of converter between HDF5 and TXT''' def read ( self , file_name ): '''read function, for converting TXT to HDF5. file_name is the name of the single input file return an numpy array.''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'r' ) as f : sizeText = io . StringIO ( f . readline ()) sze = np . loadtxt ( sizeText , dtype = np . int ) data = np . loadtxt ( f , dtype = np . float32 ) return np . reshape ( data , sze ) def write ( self , h5data , file_name ): '''write function, for converting HDF5 to TXT. h5data is the h5py.Dataset file_name is the name of the single output file. ''' with open ( os . path . splitext ( file_name )[ 0 ] + '.txt' , 'w' ) as f : np . savetxt ( f , np . reshape ( h5data . shape , ( 1 , h5data . ndim )), fmt = ' %d ' ) if h5data . ndim > 1 : for i in range ( h5data . shape [ 0 ]): np . savetxt ( f , h5data [ i , ... ] . ravel (), delimiter = ' \\n ' ) else : np . savetxt ( f , h5data [:] . ravel (), delimiter = ' \\n ' ) converter = mdnc . data . h5py . H5Converter ( ... , oformat = H52TXT () )","title":"Arguments"},{"location":"apis/data/h5py/H5Converter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5Converter/#convert","text":"converter . convert () Perform the data conversion.","title":" convert"},{"location":"apis/data/h5py/H5Converter/#example","text":"Example Codes 1 2 3 4 5 6 import mdnc cvt_o = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = True ) cvt_o . convert () # From HDF5 dataset to txt files. cvt_i = mdnc . data . h5py . H5Converter ( 'test_converter.h5' , 'txt' , to_other = False ) cvt_i . convert () # From txt files to HDF5 dataset.","title":"Example"},{"location":"apis/data/h5py/H5GParser/","text":"data.h5py.H5GParser \u00b6 Class \u00b7 Source dset = mdnc . data . h5py . H5GParser ( file_name , keywords , batch_size = 32 , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) Grouply parsing dataset. This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file indexer, this indexer would be initialized by sequence.MPSequence . It would use the user defined keywords to get a group of h5py . Dataset s. Estimate the h5py . Dataset sizes, each dataset should share the same size (but could have different shapes). Use the dataset size to create a sequence.MPSequence , and allows it to randomly shuffle the indices in each epoch. Invoke the sequence.MPSequence APIs to serve the parallel dataset parsing. Certainly, you could use this parser to load a single h5py . Dataset . To find details about the parallel parsing workflow, please check mdnc.data.sequence.MPSequence . Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Example \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = None ) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 2, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = mdnc . preprocs . ProcScaler ()) with dset . start () as p : for i , ( d_one , d_two ) in enumerate ( p ): d_one , d_two = d_one . cpu () . numpy (), d_two . cpu () . numpy () std_one , std_two = np . std ( d_one ), np . std ( d_two ) d_one , d_two = p . preproc . postprocess ( d_one , d_two ) std_one_ , std_two_ = np . std ( d_one ), np . std ( d_two ) print ( 'Before: {0} , {1} ; After: {0} , {1} .' . format ( std_one , std_two , std_one_ , std_two_ ))","title":"<span class='magic-codeicon-class'>H5GParser</span>"},{"location":"apis/data/h5py/H5GParser/#datah5pyh5gparser","text":"Class \u00b7 Source dset = mdnc . data . h5py . H5GParser ( file_name , keywords , batch_size = 32 , shuffle = True , shuffle_seed = 1000 , preprocfunc = None , num_workers = 4 , num_buffer = 10 ) Grouply parsing dataset. This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file indexer, this indexer would be initialized by sequence.MPSequence . It would use the user defined keywords to get a group of h5py . Dataset s. Estimate the h5py . Dataset sizes, each dataset should share the same size (but could have different shapes). Use the dataset size to create a sequence.MPSequence , and allows it to randomly shuffle the indices in each epoch. Invoke the sequence.MPSequence APIs to serve the parallel dataset parsing. Certainly, you could use this parser to load a single h5py . Dataset . To find details about the parallel parsing workflow, please check mdnc.data.sequence.MPSequence .","title":"data.h5py.H5GParser"},{"location":"apis/data/h5py/H5GParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). batch_size int Number of samples in each mini-batch. shuffle bool If enabled, shuffle the data set at the beginning of each epoch. shuffle_seed int The seed for random shuffling. preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. Note that this tool would process the batches produced by the parser. The details about this argument would be shown in the following tips. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The minimal requirement for the argument preprocfunc is to be a function, or implemented with the __call__ () method. This function accepts all input mini-batch variables formatted as np . ndarray , and returns the pre-processed results. The returned varaible number could be different from the input variable number. In some cases, you could use the provided pre-processors in the mdnc.data.preprocs module. The processors in these module support our Broadcasting Pre- and Post- Processor Protocol. For example: Example No args 1 2 3 4 5 6 7 import mdnc def preprocfunc ( x1 , x2 ): return x1 + x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = preprocfunc ) With args 1 2 3 4 5 6 7 8 9 10 11 import mdnc class PreprocWithArgs : def __init__ ( self , a ): self . a = a def __call__ ( self , x1 , x2 ): return x1 , self . a * x2 mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = PreprocWithArgs ( a = 0.1 )) Use data.preprocs 1 2 3 4 import mdnc mdnc . data . h5py . H5GParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ()) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5GParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5GParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5GParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5GParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5GParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5GParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5GParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5GParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5GParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5GParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5GParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5GParser/#batch_size","text":"dset . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/h5py/H5GParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5GParser/#example","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = None ) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 2, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np import mdnc dset = mdnc . data . h5py . H5GParser ( 'test_gparser' , [ 'one' , 'zero' ], batch_size = 3 , num_workers = 4 , shuffle = True , preprocfunc = mdnc . preprocs . ProcScaler ()) with dset . start () as p : for i , ( d_one , d_two ) in enumerate ( p ): d_one , d_two = d_one . cpu () . numpy (), d_two . cpu () . numpy () std_one , std_two = np . std ( d_one ), np . std ( d_two ) d_one , d_two = p . preproc . postprocess ( d_one , d_two ) std_one_ , std_two_ = np . std ( d_one ), np . std ( d_two ) print ( 'Before: {0} , {1} ; After: {0} , {1} .' . format ( std_one , std_two , std_one_ , std_two_ ))","title":"Example"},{"location":"apis/data/h5py/H5RParser/","text":"data.h5py.H5RParser \u00b6 Class \u00b7 Source dset = mdnc . data . h5py . H5RParser ( file_name , keywords , preprocfunc , batch_num = 100 , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file handle. Using the user defined keywords to get a group of datasets. Check the dataset size, and register the dataset list. For each batch, the data is randomly picked from the whole set. The h5py.Dataset variable would be transparent in the preprocfunc , i.e. the method how to pick up the random samples need to be implemented by users. Certainly, you could use this parser to load a single dataset. Arguments \u00b6 Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. This function is required because the random sampling needs to be implemented here. batch_num int Number of mini-batches in each epoch. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The preprocfunc is required in this case. The provided pre-processors in data.preprocs should not be used directly, because users need to implment their own random sampling pre-processor first. For example, Example Without data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] mdnc . data . h5py . H5RParser ( ... , preprocfunc = ProcCustom (), keywords = [ 'x_1' , 'x_2' ]) Use data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x mdnc . data . h5py . H5RParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case. Methods \u00b6 check_dsets \u00b6 sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets. get_attrs \u00b6 attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values. get_file \u00b6 f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file. start \u00b6 dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , batch_num \u00b6 len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization. size \u00b6 dset . size The size of the dataset. It contains the total number of samples for each epoch. preproc \u00b6 dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually. Example \u00b6 Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape )","title":"<span class='magic-codeicon-class'>H5RParser</span>"},{"location":"apis/data/h5py/H5RParser/#datah5pyh5rparser","text":"Class \u00b7 Source dset = mdnc . data . h5py . H5RParser ( file_name , keywords , preprocfunc , batch_num = 100 , num_workers = 4 , num_buffer = 10 ) This class allows users to feed one .h5 file, and convert it to mdnc.data.sequence.MPSequence . The realization could be described as: Create .h5 file handle. Using the user defined keywords to get a group of datasets. Check the dataset size, and register the dataset list. For each batch, the data is randomly picked from the whole set. The h5py.Dataset variable would be transparent in the preprocfunc , i.e. the method how to pick up the random samples need to be implemented by users. Certainly, you could use this parser to load a single dataset.","title":"data.h5py.H5RParser"},{"location":"apis/data/h5py/H5RParser/#arguments","text":"Requries Argument Type Description file_name str The path of the .h5 file (could be without postfix). keywords ( str , ) Should be a list of keywords (or a single keyword). preprocfunc object This function would be added to the produced data so that it could serve as a pre-processing tool. This function is required because the random sampling needs to be implemented here. batch_num int Number of mini-batches in each epoch. num_workers int The number of parallel workers. num_buffer int The buffer size of the data pool, it means the maximal number of mini-batches stored in the memory. Tip The preprocfunc is required in this case. The provided pre-processors in data.preprocs should not be used directly, because users need to implment their own random sampling pre-processor first. For example, Example Without data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] mdnc . data . h5py . H5RParser ( ... , preprocfunc = ProcCustom (), keywords = [ 'x_1' , 'x_2' ]) Use data.preprocs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x mdnc . data . h5py . H5RParser ( ... , keywords = [ 'x_1' , 'x_2' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) Warning The argument preprocfunc requires to be a picklable object . Therefore, a lambda function or a function implemented inside if __name__ == '__main__' is not allowed in this case.","title":"Arguments"},{"location":"apis/data/h5py/H5RParser/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5RParser/#check_dsets","text":"sze = dset . check_dsets ( file_path , keywords ) Check the size of h5py . Dataset and validate all datasets. A valid group of datasets requires each h5py . Dataset shares the same length (sample number). If success, would return the size of the datasets. This method is invoked during the initialization, and do not requires users to call explicitly. Requries Argument Type Description file_path str The path of the HDF5 dataset to be validated. keywords ( str , ) The keywords to be validated. Each keyword should point to or redict to an h5py . Dataset . Returns Argument Description sze A int , the size of all datasets.","title":" check_dsets"},{"location":"apis/data/h5py/H5RParser/#get_attrs","text":"attrs = dset . get_attrs ( keyword , * args , attr_names = None ) Get the attributes by the keyword. Requries Argument Type Description keyword str The keyword of the to a h5py.Dataset in the to-be-loaded file. attr_names ( str , ) A sequence of required attribute names. *args other attribute names, would be attached to the argument attr_names by list . extend () . Returns Argument Description attrs A list of the required attribute values.","title":" get_attrs"},{"location":"apis/data/h5py/H5RParser/#get_file","text":"f = dset . get_file ( enable_write = False ) Get a file object of the to-be-loaded file. Requries Argument Type Description enable_write bool If enabled, would use the a mode to open the file. Otherwise, use the r mode. Returns Argument Description f The h5py . File object of the to-be-loaded file.","title":" get_file"},{"location":"apis/data/h5py/H5RParser/#start","text":"dset . start ( compat = None ) Start the process pool. This method is implemented by mdnc.data.sequence.MPSequence . It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 dset . start () for ... in dset : ... dset . finish () With context 1 2 3 with dset . start () as ds : for ... in ds : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/h5py/H5RParser/#start_test","text":"dset . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel dset . start () mode. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/h5py/H5RParser/#finish","text":"dset . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/h5py/H5RParser/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5RParser/#len-batch_num","text":"len ( dset ) dset . batch_num The length of the dataset. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), batch_num"},{"location":"apis/data/h5py/H5RParser/#iter","text":"for x1 , x2 , ... in dset : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are ordered according to the given argument keywords during the initialization.","title":" iter()"},{"location":"apis/data/h5py/H5RParser/#size","text":"dset . size The size of the dataset. It contains the total number of samples for each epoch.","title":" size"},{"location":"apis/data/h5py/H5RParser/#preproc","text":"dset . preproc The argument preprocfunc during the initialziation. This property helps users to invoke the preprocessor manually.","title":" preproc"},{"location":"apis/data/h5py/H5RParser/#example","text":"Example 1 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import numpy as np import mdnc class ProcCustom : def __init__ ( self , seed = 1000 , batch_size = 16 ): self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def __call__ ( self , ds_x1 , ds_x2 ): ind_x1 = np . sort ( self . random_rng . integers ( len ( ds_x1 ), size = batch_size )) ind_x2 = np . sort ( self . random_rng . integers ( len ( ds_x2 ), size = batch_size )) return ds_x1 [ ind_x1 , ... ], ds_x2 [ ind_x2 , ... ] dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = ProcCustom ()) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import numpy as np import mdnc class ProcCustom ( mdnc . data . preprocs . ProcAbstract ): def __init__ ( self , seed = 1000 , batch_size = 16 , inds = None , parent = None ): super () . __init__ ( inds = inds , parent = parent ) self . batch_size = batch_size self . random_rng = np . random . default_rng ( seed ) def preprocess ( self , ds ): ind = np . sort ( self . random_rng . integers ( len ( ds ), size = batch_size )) return ds [ ind , ... ] def postprocess ( self , x ): return x dset = mdnc . data . h5py . H5RParser ( 'test_rparser' , keywords = [ 'one' , 'zero' ], preprocfunc = mdnc . data . preprocs . ProcScaler ( parent = ProcCustom ())) with dset . start () as p : for i , data in enumerate ( p ): print ( 'data.h5py: Epoch 1, Batch {0} ' . format ( i ), data [ 0 ] . shape , data [ 1 ] . shape )","title":"Example"},{"location":"apis/data/h5py/H5SeqConverter/","text":"data.h5py.H5SeqConverter \u00b6 Class \u00b7 Source converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group . Arguments \u00b6 Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . Methods \u00b6 config \u00b6 converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. convert \u00b6 converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. copy \u00b6 converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. open \u00b6 converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' . close \u00b6 converter . close () Close the converter. Example \u00b6 Example 1 Codes 1 2 3 4 5 6 import mdnc with mdnc . data . h5py . H5SeqConverter ( 'test_seqconverter.h5' ) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( 'test_seqconverter.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( 'test_seqconverter2.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' )","title":"<span class='magic-codeicon-class'>H5SeqConverter</span>"},{"location":"apis/data/h5py/H5SeqConverter/#datah5pyh5seqconverter","text":"Class \u00b7 Source converter = mdnc . data . h5py . H5SeqConverter ( file_in_name = None , file_out_name = None ) Convert any supervised .h5 data file into sequence version. This class allows users to choose some keywords and convert them into sequence version. Those keywords would be saved as in the format of continuous sequence. It could serve as a random splitter for preparing the training of LSTM. The following figure shows how the data get converted. The converted dataset would be cut into several segments with random lengths. The converted files should only get loaded by mdnc.data.h5py.H5CParser . Warning During the conversion, attributes would be lost, and the links and virtual datasets would be treated as h5py . Datasets . Although this class supports context, it does not support dictionary-style APIs like h5py . Group .","title":"data.h5py.H5SeqConverter"},{"location":"apis/data/h5py/H5SeqConverter/#arguments","text":"Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. If not set, would not open the dataset. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":"Arguments"},{"location":"apis/data/h5py/H5SeqConverter/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SeqConverter/#config","text":"converter . config ( logver = 0 , set_shuffle = False , seq_len = 10 , seq_len_max = 20 , random_seed = 2048 , ** kwargs ) Make configuration for the converter. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. set_shuffle bool Whether to shuffle the order of segments during the conversion. seq_len int The lower bound of the random segment length. seq_len_max int The super bound of the random segment length. random_seed int The random seed used in this instance. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":" config"},{"location":"apis/data/h5py/H5SeqConverter/#convert","text":"converter . convert ( keyword , ** kwargs ) Convert the h5py . Dataset given by keyword into the segmented dataset, and save it. The data would be converted into sequence. Note that before the conversion, the data should be arranged continuously of the batch axis. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be converted into segmented dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" convert"},{"location":"apis/data/h5py/H5SeqConverter/#copy","text":"converter . copy ( keyword , ** kwargs ) Copy the h5py . Dataset given by keyword into the output file. If you have already converted or copied the keyword, please do not do it again. Requries Argument Type Description keyword str The keyword that would be copied into the output file. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" copy"},{"location":"apis/data/h5py/H5SeqConverter/#open","text":"converter . open ( file_in_name , file_out_name = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_in_name ) support context management. Requries Argument Type Description file_in_name str A path where we read the non-sequence formatted file. file_out_name str The path of the output data file. If not set, it would be configured as file_in_name + '_seq' .","title":" open"},{"location":"apis/data/h5py/H5SeqConverter/#close","text":"converter . close () Close the converter.","title":" close"},{"location":"apis/data/h5py/H5SeqConverter/#example","text":"Example 1 Codes 1 2 3 4 5 6 import mdnc with mdnc . data . h5py . H5SeqConverter ( 'test_seqconverter.h5' ) as cvt : cvt . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 import mdnc converter = mdnc . data . h5py . H5SeqConverter () converter . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with converter . open ( 'test_seqconverter.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' ) with converter . open ( 'test_seqconverter2.h5' ) as cvt : cvt . convert ( 'data_to_sequence' ) cvt . copy ( 'data_only_copied' )","title":"Example"},{"location":"apis/data/h5py/H5SupSaver/","text":"data.h5py.H5SupSaver \u00b6 Class \u00b7 Source saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of h5py . Group and h5py . Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py . Softlink , h5py . Attributes and h5py . VirtualDataSet . Add context nesting supports for h5py . Group . This would makes the codes more elegant. Arguments \u00b6 Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. Methods \u00b6 config \u00b6 saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation. get_config \u00b6 cfg = saver . get_config ( name = None ) Get the current configuration value by the given name . Requries Argument Type Description name str The name of the required config value. Returns Argument Description cfg The required config value. open \u00b6 saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value. close \u00b6 saver . close () Close the saver. dump \u00b6 saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation. set_link \u00b6 saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset . set_attrs \u00b6 saver . set_attrs ( keyword , attrs = None , ** kwargs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () . set_virtual_set \u00b6 saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset. Properties \u00b6 attrs \u00b6 attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager . Example \u00b6 Example 1 Codes 1 2 3 4 5 6 7 import numpy as np import mdnc with mdnc . data . h5py . H5SupSaver ( 'test_h5supsaver.h5' , enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( 'test_h5supsaver.h5' ) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open ))","title":"<span class='magic-codeicon-class'>H5SupSaver</span>"},{"location":"apis/data/h5py/H5SupSaver/#datah5pyh5supsaver","text":"Class \u00b7 Source saver = mdnc . data . h5py . H5SupSaver ( file_name = None , enable_read = False ) Save supervised data set as .h5 file. This class allows users to dump multiple datasets into one file handle, then it would save it as a .h5 file. The keywords of the sets should be assigned by users. It supports both the context management and the dictionary-style nesting. It is built on top of h5py . Group and h5py . Dataset . The motivation of using this saver includes: Provide an easier way for saving resizable datasets. All datasets created by this saver are resizable. Provide convenient APIs for creating h5py . Softlink , h5py . Attributes and h5py . VirtualDataSet . Add context nesting supports for h5py . Group . This would makes the codes more elegant.","title":"data.h5py.H5SupSaver"},{"location":"apis/data/h5py/H5SupSaver/#arguments","text":"Requries Argument Type Description file_name str A path where we save the file. If not set, the saver would not open a file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file.","title":"Arguments"},{"location":"apis/data/h5py/H5SupSaver/#methods","text":"","title":"Methods"},{"location":"apis/data/h5py/H5SupSaver/#config","text":"saver . config ( logver = 0 , ** kwargs ) Make configuration for the saver. Only the explicitly given argument would be used for changing the configuration of this instance. Requries Argument Type Description logver int The verbose level of the outputs. When setting 0, would run silently. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value during the dataset creation.","title":" config"},{"location":"apis/data/h5py/H5SupSaver/#get_config","text":"cfg = saver . get_config ( name = None ) Get the current configuration value by the given name . Requries Argument Type Description name str The name of the required config value. Returns Argument Description cfg The required config value.","title":" get_config"},{"location":"apis/data/h5py/H5SupSaver/#open","text":"saver . open ( file_name , enable_read = None ) Open a new file. If a file has been opened before, this file would be closed. This method and the __init__ method (need to specify file_name ) support context management. Requries Argument Type Description file_name str A path where we save the file. enable_read bool When setting True , enable the a mode. Otherwise, use w mode. This option is used when adding data to an existed file. If not set, the enable_read would be inherited from the class definition. Otherwise, the class definition enable_read would be updated by this new value.","title":" open"},{"location":"apis/data/h5py/H5SupSaver/#close","text":"saver . close () Close the saver.","title":" close"},{"location":"apis/data/h5py/H5SupSaver/#dump","text":"saver . dump ( keyword , data , ** kwargs ) Dump the dataset with a keyword into the file. The dataset is resizable, so this method could be used repeatly. The data would be always attached at the end of the current dataset. Requries Argument Type Description file_name str The keyword of the dumped dataset. data np . ndarray A new batch of data items, should be a numpy array. The axes data [ 1 :] should match the shape of existing dataset. **kwargs Any argument that would be used for creating h5py . Dataset . The given argument would override the default value and configs set by config () during the dataset creation.","title":" dump"},{"location":"apis/data/h5py/H5SupSaver/#set_link","text":"saver . set_link ( keyword , target , overwrite = True ) Create a h5py.Softlink. Requries Argument Type Description keyword str The keyword of the to-be created soft link. target str The reference (pointting position) of the soft link. overwrite bool if not True , would skip this step when the the keyword exists. Otherwise, the keyword would be overwritten, even if it contains an h5py . Dataset .","title":" set_link"},{"location":"apis/data/h5py/H5SupSaver/#set_attrs","text":"saver . set_attrs ( keyword , attrs = None , ** kwargs ) Set attrs for an existed data group or dataset. Requries Argument Type Description keyword str The keyword where we set the attributes. attrs dict The attributes those would be set. **kwargs More attributes those would be combined with attrs by dict . update () .","title":" set_attrs"},{"location":"apis/data/h5py/H5SupSaver/#set_virtual_set","text":"saver . set_virtual_set ( keyword , sub_set_keys , fill_value = 0.0 ) Create a virtual dataset based on a list of subsets. All subsets require to be h5py.Dataset and need to share the same shape (excepting the first dimension, i.e. the sample number). The subsets would be concatenated at the axis = 1 . For example, when d1 . shape = [ 100 , 20 ] , d2 . shape = [ 80 , 20 ] , the output virtual set would be d . shape = [ 100 , 2 , 20 ] . In this case, d [ 80 :, 1 , :] are filled by fill_value . Requries Argument Type Description keyword str The keyword of the dumped dataset. sub_set_keys ( str , ) A sequence of sub-set keywords. Each sub-set should share the same shape (except for the first dimension). fill_value float The value used for filling the blank area in the virtual dataset.","title":" set_virtual_set"},{"location":"apis/data/h5py/H5SupSaver/#properties","text":"","title":"Properties"},{"location":"apis/data/h5py/H5SupSaver/#attrs","text":"attrs = saver . attrs # Return the h5py.AttributeManager saver . attrs = dict ( ... ) # Use a dictionary to update attrs. Supports using a dictionary to update the attributes of the current h5py object. The returned attrs is used as h5py . AttributeManager .","title":" attrs"},{"location":"apis/data/h5py/H5SupSaver/#example","text":"Example 1 Codes 1 2 3 4 5 6 7 import numpy as np import mdnc with mdnc . data . h5py . H5SupSaver ( 'test_h5supsaver.h5' , enable_read = False ) as s : s . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) s . dump ( 'one' , np . ones ([ 25 , 20 ]), chunks = ( 1 , 20 )) s . dump ( 'zero' , np . zeros ([ 25 , 10 ]), chunks = ( 1 , 10 )) Example 2 Codes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np import mdnc saver = mdnc . data . h5py . H5SupSaver ( enable_read = False ) saver . config ( logver = 1 , shuffle = True , fletcher32 = True , compression = 'gzip' ) with saver . open ( 'test_h5supsaver.h5' ) as s : s . dump ( 'test1' , np . zeros ([ 100 , 20 ])) gb = s [ 'group1' ] with gb [ 'group2' ] as g : g . dump ( 'test2' , np . zeros ([ 100 , 20 ])) g . dump ( 'test2' , np . ones ([ 100 , 20 ])) g . attrs = { 'new' : 1 } g . set_link ( 'test3' , '/test1' ) print ( 'data.h5py: Check open: s[\"group1\"]= {0} , s[\"group1/group2\"]= {1} ' . format ( gb . is_open , g . is_open ))","title":"Example"},{"location":"apis/data/sequence/MPSequence/","text":"data.sequence.MPSequence \u00b6 Class \u00b7 Source manager = mdnc . data . sequence . MPSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-processing. It is designed as an alternative keras.utils.Sequence . The multi-processing codes are built on top of the multiprocessing module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by torch.multiprocessing . The workflow of this class is described in the following figure: %%{init: {'theme':'default'}}%% flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MPSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MPSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MPSequence would store the indexer during the initialization. When the start() method is invoked, two process pools would be created. The first pool maintains several processes, each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MPSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes or threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Example \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"<span class='magic-codeicon-class'>MPSequence</span>"},{"location":"apis/data/sequence/MPSequence/#datasequencempsequence","text":"Class \u00b7 Source manager = mdnc . data . sequence . MPSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-processing. It is designed as an alternative keras.utils.Sequence . The multi-processing codes are built on top of the multiprocessing module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by torch.multiprocessing . The workflow of this class is described in the following figure: %%{init: {'theme':'default'}}%% flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MPSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MPSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MPSequence would store the indexer during the initialization. When the start() method is invoked, two process pools would be created. The first pool maintains several processes, each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MPSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed).","title":"data.sequence.MPSequence"},{"location":"apis/data/sequence/MPSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes or threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used.","title":"Arguments"},{"location":"apis/data/sequence/MPSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MPSequence/#start","text":"manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MPSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MPSequence/#finish","text":"manager . finish () Finish the process pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MPSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MPSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MPSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MPSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MPSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MPSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MPSequence/#example","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MPSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"Example"},{"location":"apis/data/sequence/MSequence/","text":"data.sequence.MSequence \u00b6 Class \u00b7 Source manager = mdnc . data . sequence . MSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , thread_type = 'proc' , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading or multi-processing. It is designed as an alternative keras.utils.Sequence . The multi-threading and multi-processing codes are built on top of the threading and multiprocessing modules respectively. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by torch.multiprocessing . The workflow of this class is described in the following figure: %%{init: {'theme':'default'}}%% flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MSequence would store the indexer during the initialization. When the start() method is invoked, two process (or threading) pools would be created. The first pool maintains several processes (or threads), each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Warning We do not recommend to use mdnc.data.sequence.MSequence , because it is a base class. Instead, please use mdnc.data.sequence.MTSequence or mdnc.data.sequence.MPSequence according to your preference. The only case where you use this class is, you want to make the multi-threading or multi-processing options exposed to users. Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes or threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. thread_type str The backend of the MSequence , could be 'proc' or 'thread' . out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process (or threading) pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the process (or threading) pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Example \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"<span class='magic-codeicon-class'>MSequence</span>"},{"location":"apis/data/sequence/MSequence/#datasequencemsequence","text":"Class \u00b7 Source manager = mdnc . data . sequence . MSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , thread_type = 'proc' , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading or multi-processing. It is designed as an alternative keras.utils.Sequence . The multi-threading and multi-processing codes are built on top of the threading and multiprocessing modules respectively. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. If the pyTorch is detected, the multiprocessing backend would be provided by torch.multiprocessing . The workflow of this class is described in the following figure: %%{init: {'theme':'default'}}%% flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MSequence] subgraph procs [Process Pool] proc1[[Process 1]] proc2[[Process 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Process Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MSequence would store the indexer during the initialization. When the start() method is invoked, two process (or threading) pools would be created. The first pool maintains several processes (or threads), each process would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel processes (in pool 1) by the input queue . Each process would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the process would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Warning We do not recommend to use mdnc.data.sequence.MSequence , because it is a base class. Instead, please use mdnc.data.sequence.MTSequence or mdnc.data.sequence.MPSequence according to your preference. The only case where you use this class is, you want to make the multi-threading or multi-processing options exposed to users.","title":"data.sequence.MSequence"},{"location":"apis/data/sequence/MSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different processes. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the processes or threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. thread_type str The backend of the MSequence , could be 'proc' or 'thread' . out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Warning The argument worker requires to be a picklable object . It means: The worker itself should be defined in a global domain, not inside a function or a method. All attributes of the worker should be picklable, i.e. a local function like lambda expression should not be used.","title":"Arguments"},{"location":"apis/data/sequence/MSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MSequence/#start","text":"manager . start ( compat = None ) Start the process pool. When this method is invoked, the process (or theread) pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Whether to fall back to multi-threading for the sequence out-type converter. If set None, the decision would be made by checking os . name . The compatible mode requires to be enabled on Windows. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Danger The cuda . Tensor could not be put into the queue on Windows (but on Linux we could), see https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations To solve this problem, we need to fall back to multi-threading for the sequence out-type converter on Windows. Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the process (or threading) pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MSequence/#finish","text":"manager . finish () Finish the process (or threading) pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MSequence/#example","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , thread_type = 'proc' , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"Example"},{"location":"apis/data/sequence/MTSequence/","text":"data.sequence.MTSequence \u00b6 Class \u00b7 Source manager = mdnc . data . sequence . MTSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading. It is designed as an alternative keras.utils.Sequence . The multi-threading codes are built on top of the threading module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. The workflow of this class is described in the following figure: %%{init: {'theme':'default'}}%% flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MTSequence] subgraph procs [Threading Pool] proc1[[Thread 1]] proc2[[Thread 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Threading Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MTSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MTSequence would store the indexer during the initialization. When the start() method is invoked, two threading pools would be created. The first pool maintains several threads, each thread would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MTSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel threads (in pool 1) by the input queue . Each thread would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the thread would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed). Arguments \u00b6 Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Info The argument worker does not require to be picklable in this case, because all threads are mainted in the same process. Methods \u00b6 start \u00b6 manager . start ( compat = None ) Start the threading pool. When this method is invoked, the thereading pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Only reserved for compatibility for switching from MPSequence to MTSequence . This flag would not influence anything. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead. start_test \u00b6 manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the threading pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it. finish \u00b6 manager . finish () Finish the threading pool. The compatible mode would be auto detected by the previous start() . Properties \u00b6 len() , length \u00b6 len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch. iter() \u00b6 for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker . dset_size \u00b6 manager . dset_size The size of the dataset. It contains the total number of samples for each epoch. batch_size \u00b6 manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value. use_cuda \u00b6 manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available. Example \u00b6 Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"<span class='magic-codeicon-class'>MTSequence</span>"},{"location":"apis/data/sequence/MTSequence/#datasequencemtsequence","text":"Class \u00b7 Source manager = mdnc . data . sequence . MTSequence ( worker , dset_size , num_workers = 4 , num_converters = None , batch_size = 32 , buffer = 10 , shuffle = True , out_type = 'cuda' , seed = None ) This class is a scheduler based on multi-threading. It is designed as an alternative keras.utils.Sequence . The multi-threading codes are built on top of the threading module. It supports different workers and allows users to read datasets asynchronously and shuffle dataset randomly. This class could be loaded without pyTorch. The workflow of this class is described in the following figure: %%{init: {'theme':'default'}}%% flowchart LR subgraph indexer [Indexer] data[(Data)] getitem[\"__getitem__()\"] --x data end mseq:::msequenceroot subgraph mseq [MTSequence] subgraph procs [Threading Pool] proc1[[Thread 1]] proc2[[Thread 2]] procn[[...]] subgraph indexer1 [Indexer1] getitem1[\"__getitem__()\"] end subgraph indexer2 [Indexer2] getitem2[\"__getitem__()\"] end subgraph indexern [...] getitemn[\"__getitem__()\"] end proc1 -->|invoke| getitem1 --> data1[(Data 1)] proc2 -->|invoke| getitem2 --> data2[(Data 2)] procn -->|invoke| getitemn --> datan[(...)] end subgraph procs2 [Threading Pool 2] cvt1[[Type converter 1]] --> datam1[(Data 1)] cvtn[[...]] --> datamn[(...)] end data1 & data2 & datan -->|send| queue_m cvt1 & cvtn -->|fetch| queue_m datam1 & datamn -->|send| queue_o queue_i{{Input queue}} queue_m{{Middle queue}} queue_o{{Output queue}} mainthread[\"Main<br>thread\"] -->|generate| indices[(Indices)] indices -->|send| queue_i mainthread -->|fetch| queue_o end proc1 & proc2 & procn -->|fetch| queue_i indexer -->|copy| indexer1 & indexer2 & indexern classDef msequenceroot fill:#FEEEF0, stroke: #b54051; The workflow could be divided into steps: An indexer is initialized outside of the MTSequence . The indexer would maintain the dataset during the initialization, and provide a __getitem__(bidx) method, where the argument bidx is a sequence of indicies. This method would read the dataset according to the indices and return a mini-batch of data in the np.ndarray format. The MTSequence would store the indexer during the initialization. When the start() method is invoked, two threading pools would be created. The first pool maintains several threads, each thread would get a copy of the indexer provided in step 1. The second pool maintains several output data type converters. These converters are designed in MDNC and do not require users to implement. There are 3 queues maintained by MTSequence . During the asynchronous data parsing, the main thread would generate a sequence of indicies in the beginning of each epoch. The indicies would be depatched to these parallel threads (in pool 1) by the input queue . Each thread would listen to the event of the input queue and try to get the depatched indicies. Once getting a sequence of indicies, the thread would invoke the __getitem__() method of its indexer, the output data would be sent to the second queue, i.e. the middle queue . The converters in pool 2 would listen to the middle queue, get the mini-batches, and convert them to torch.Tensor or torch.cuda.Tensor . The converted data would be sent to the last queue, i.e. the output queue . The main thread is an iterator. It keeps listening the output queue during the workflow. Once the __next__ () method is invoked, it would get one output mini-batch from the output queue . This behavior would repeat until the finish() method is invoked (or the context is closed).","title":"data.sequence.MTSequence"},{"location":"apis/data/sequence/MTSequence/#arguments","text":"Requries Argument Type Description worker type A class used for generating worker instances, with __getitem__ () method implemented. This instance would be copied and used as indexer for different threads. dset_size int The number of samples in the dataset. If given an np . ndarray , the array would be used as indices, the size of the dataset would be inferred as the length of the array. num_workers int The number of parallel workers, each worker is created by the argument worker () inside the threads. num_converters int The number of converters, only used when cuda is enabled. If set None , would be determined by num_workers . batch_size int The number of samples in each batch, used for depatching the indicies. shuffle bool If enabled, shuffle the dataset at the end of each epoch. out_type str The output type. Could be 'cuda' , 'cpu' or 'null' . If set 'null' , the results would not be converted to torch.Tensor . num_workers int The number of parallel workers. seed int : the seed used for shuffling the data. If not set, would use random shuffle without seed. Info The argument worker does not require to be picklable in this case, because all threads are mainted in the same process.","title":"Arguments"},{"location":"apis/data/sequence/MTSequence/#methods","text":"","title":"Methods"},{"location":"apis/data/sequence/MTSequence/#start","text":"manager . start ( compat = None ) Start the threading pool. When this method is invoked, the thereading pools would be initialized. It supports context management. Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description compat bool Only reserved for compatibility for switching from MPSequence to MTSequence . This flag would not influence anything. Tip This method supports context management. Using the context is recommended. Here we show two examples: Without context 1 2 3 4 manager . start () for ... in manager : ... manager . finish () With context 1 2 3 with manager . start () as mng : for ... in mng : ... Warning Even if you set shuffle = False , due to the mechanism of the parallelization, the sample order during the iteration may still get a little bit shuffled. To ensure your sample order not changed, please use shuffle = False during the initialization and use start_test() instead.","title":" start"},{"location":"apis/data/sequence/MTSequence/#start_test","text":"manager . start_test ( test_mode = 'default' ) Start the test mode. In the test mode, the threading pool would not be open. All operations would be finished in the main thread. However, the random indices are still generated with the same seed of the parallel manager . start () mode (if the indicies are not provided). Running start() or start_test() would interrupt the started sequence. Requries Argument Type Description test_mode str Could be 'default' , 'cpu' , or 'numpy' . 'default' : the output would be converted as start() mode. 'cpu' : even set 'cuda' as output type, the testing output would be still not converted to GPU. 'numpy' : would ignore all out_type configurations and return the original output. This output is still pre-processed. Tip This method also supports context management. See start() to check how to use it.","title":" start_test"},{"location":"apis/data/sequence/MTSequence/#finish","text":"manager . finish () Finish the threading pool. The compatible mode would be auto detected by the previous start() .","title":" finish"},{"location":"apis/data/sequence/MTSequence/#properties","text":"","title":"Properties"},{"location":"apis/data/sequence/MTSequence/#len-length","text":"len ( dset ) manager . length The length of the epoch. It is the number of mini-batches, also the number of iterations for each epoch.","title":" len(), length"},{"location":"apis/data/sequence/MTSequence/#iter","text":"for x1 , x2 , ... in manager : ... The iterator. Recommend to use it inside the context. The unpacked variables x1 , x2 ... are returned by the provided argument worker .","title":" iter()"},{"location":"apis/data/sequence/MTSequence/#dset_size","text":"manager . dset_size The size of the dataset. It contains the total number of samples for each epoch.","title":" dset_size"},{"location":"apis/data/sequence/MTSequence/#batch_size","text":"manager . batch_size The size of each batch. This value is given by the argument batch_size during the initialization. The last size of the batch may be smaller than this value.","title":" batch_size"},{"location":"apis/data/sequence/MTSequence/#use_cuda","text":"manager . use_cuda A bool , whether to return torch.cuda.Tensor . This value would be only true when: The argument out_type is 'cuda' , or 'cuda:x' during the initialization. The pyTorch is available.","title":" use_cuda"},{"location":"apis/data/sequence/MTSequence/#example","text":"Example 1: default mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start () as mng : for i in mng : print ( i ) Example 2: test mode Codes 1 2 3 4 5 6 7 8 9 10 11 12 import mdnc class TestSequenceWorker : def __getitem__ ( self , indx ): # print('data.sequence: thd =', indx) return indx manager = mdnc . data . sequence . MTSequence ( TestSequenceWorker , dset_size = 512 , batch_size = 1 , out_type = 'cuda' , shuffle = False , num_workers = 1 ) with manager . start_test ( 'numpy' ) as mng : for i in mng : print ( i )","title":"Example"}]}